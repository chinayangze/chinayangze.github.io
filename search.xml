<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>presto 连接clickhouse</title>
    <url>/2021/12/23/presto%20%E8%BF%9E%E6%8E%A5clickhouse/</url>
    <content><![CDATA[<h1>presto 连接clickhouse</h1>
<p>presto350版本，不支持clickhouse ，需要升级，</p>
<p>presto350版本之后 presto改名为trino，导致不兼容，升级后 对应的jdbc驱动 ，客户端都需要换成新的。</p>
<p>客户端工具用DBeaver比较好，DbVisualizer 不支持trino</p>
]]></content>
  </entry>
  <entry>
    <title>clickhouse实时同步mysql数据</title>
    <url>/2021/12/23/clickhouse%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5mysql%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h1>clickhouse实时同步mysql数据</h1>
<p>clickhouse-client -m --password ‘admin’</p>
<h1>使用 MaterializedMySQL 引擎实时同步</h1>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> allow_experimental_database_materialized_mysql = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> dbtest <span class="keyword">ENGINE</span> = MaterializedMySQL(<span class="string">'10.6.123.23:3306'</span>, <span class="string">'dbtest'</span>, <span class="string">'cs_yangz'</span>, <span class="string">'***'</span>) <span class="keyword">SETTINGS</span> allows_query_when_mysql_lost=<span class="literal">true</span>,max_wait_time_when_mysql_unavailable=<span class="number">10000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看磁盘容量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    <span class="keyword">path</span>,</span><br><span class="line">    formatReadableSize(free_space) <span class="keyword">AS</span> free,</span><br><span class="line">    formatReadableSize(total_space) <span class="keyword">AS</span> total,</span><br><span class="line">    formatReadableSize(keep_free_space) <span class="keyword">AS</span> reserved</span><br><span class="line"><span class="keyword">FROM</span> system.disks</span><br></pre></td></tr></table></figure>
<h2 id="安装配置-MySQL-同步用户">安装配置 MySQL 同步用户</h2>
<ul>
<li>创建用户</li>
<li>全局赋予 replication client,replication slave, reload 权限</li>
<li>对同步库 db 赋予 select 权限</li>
</ul>
<p>如果赋予权限不正确，会报错，</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">'clickhouse'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'alitrack'</span>;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">select</span> <span class="keyword">ON</span> db.* <span class="keyword">TO</span> <span class="string">'clickhouse'</span>@<span class="string">'%'</span>;</span><br><span class="line"><span class="keyword">GRANT</span>  <span class="keyword">replication</span> <span class="keyword">client</span>,<span class="keyword">replication</span> <span class="keyword">slave</span>, reload <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'clickhouse'</span>@<span class="string">'%'</span>;</span><br><span class="line"><span class="keyword">FLUSH</span> <span class="keyword">PRIVILEGES</span>;</span><br></pre></td></tr></table></figure>
<p>遇到没有主键的表，</p>
<p>如果没有初始化完成，删掉没有主键的表，直接重启clickhouse就可以了</p>
<p>如果已经初始化，修改GTID，再重启。</p>
<p>vim /data/clickhouse/metadata/dbtest/.metadata   修改GTID 跳过  重启clickhouse服务</p>
<p>– 不支持的数据类型</p>
<h1>MaterializeMySQL don’t support the json,bit,time data type</h1>
]]></content>
  </entry>
  <entry>
    <title>clickhouse运维</title>
    <url>/2021/12/22/clickhouse%E8%BF%90%E7%BB%B4/</url>
    <content><![CDATA[<p>查看后台进程并杀死</p>
<p>– 这个命令和mysql是一样的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">show processlist</span><br></pre></td></tr></table></figure>
<p>– 如果进程太多，也可用通过查询系统表 processes，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from system.processes</span><br></pre></td></tr></table></figure>
<p>– 指定主要关心字段</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select </span><br><span class="line">  user,query_id,query,elapsed,memory_usage</span><br><span class="line">from system.processes</span><br></pre></td></tr></table></figure>
<p>杀死后台进程<br>
–  通过上面指令获取到进程相关信息后，可以用query_id条件kill进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KILL QUERY WHERE query_id&#x3D;&#39;2-857d-4a57-9ee0-327da5d60a90&#39;</span><br></pre></td></tr></table></figure>
<p>– 杀死default用户下的所有进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KILL QUERY WHERE user&#x3D;&#39;default&#39;</span><br></pre></td></tr></table></figure>
<p>clickhouse启动停止服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service clickhouse-server start</span><br><span class="line">service clickhouse-server stop</span><br><span class="line">service clickhouse-server restart</span><br></pre></td></tr></table></figure>
<h1>Clickhouse删除表某一天分区</h1>
<p>方法一：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE kuming.tableName DELETE WHERE toDate(insert_at_timestamp)&#x3D;&#39;2020-07-21&#39;;</span><br></pre></td></tr></table></figure>
<p>方法二：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE kuming.tableName DELETE WHERE insert_at_timestamp&lt;&#x3D;1596470399;</span><br></pre></td></tr></table></figure>
<p>方法三：（当前两种方法分区数据没有删除掉的时候可以用方法三）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE kuming.tableName DROP PARTITION &#39;2020-08-03&#39;;</span><br></pre></td></tr></table></figure>
<p>alter table 表名 drop partition 分区名</p>
<p>分区名可以用下语句查询</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from system.parts p where table &#x3D; &#39;表名&#39;</span><br></pre></td></tr></table></figure>
<h1>查看日志</h1>
<p>可以在clickhouse结点上查看/var/log/clickhouse-server/clickhouse-server.log，注意ERROR级别日志</p>
<p>– 特别主要：</p>
<p>可以在clickhouse结点上查看/var/log/clickhouse-server/clickhouse-server.log，注意ERROR级别日志</p>
<p>/var/log/clickhouse-server/clickhouse-server.err.log</p>
<h3 id="创建角色和普通用户">创建角色和普通用户</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">ROLE</span> accountant;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> dbtest.* <span class="keyword">TO</span> accountant;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> mira HOST IP <span class="string">'127.0.0.1'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">WITH</span> sha256_password <span class="keyword">BY</span> <span class="string">'qwerty'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">GRANT</span> accountant <span class="keyword">TO</span> mira;</span><br></pre></td></tr></table></figure>
<h1>合并分区</h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">optimize table xxoxx final;</span><br><span class="line"></span><br><span class="line">optimize table xxoxx  partition ‘20211206’ final;</span><br></pre></td></tr></table></figure>
<h1>在客户端打印日志  --send_logs_level=trace</h1>
<p>[atguigu@hadoop102 lib]$ clickhouse-client --send_logs_level=trace &lt;&lt;&lt; 'select*from t_order_mt2 where total_amount &gt; toDecimal32(900.,2)&quot;;</p>
<h1>ReplacingMergeTree</h1>
<p>通过测试得到结论<br>
实际上是使用order by字段作为唯一键<br>
去重不能跨分区<br>
只有同—批插入(新版本)或合并分区时才会进行去重<br>
认定重复的数据保留，版本字段值最大的<br>
如果版本字段相同则按插入J顺序保留最后一笔<br>
I</p>
<h1>ClickHouse 分片双副本集群部署</h1>
<p>参考   <a href="https://www.jianshu.com/p/5bcaad0a02b1" target="_blank" rel="noopener">https://www.jianshu.com/p/5bcaad0a02b1</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建分布式数据库</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> PAY_TRADECENTER <span class="keyword">on</span> cluster ck_cluster;</span><br><span class="line"><span class="comment">--drop database PAY_TRADECENTER on cluster ck_cluster ;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在集群的每个机器上面创建mysql镜像库</span></span><br><span class="line"><span class="comment">-- drop database mysql_pay_tradecenter on cluster ck_cluster ;</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> mysql_pay_tradecenter <span class="keyword">on</span> cluster ck_cluster <span class="keyword">ENGINE</span> = MySQL(<span class="string">'****:3306'</span>, <span class="string">'PAY_TRADECENTER'</span>, <span class="string">'cs_hadoop'</span>, <span class="string">'***'</span>) ;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在集群的每个机器上面建立本地表 (设置分片)</span></span><br><span class="line"><span class="comment">-- drop table PAY_TRADECENTER.T_TC_BASE_LOCAL on cluster ck_cluster</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> PAY_TRADECENTER.T_TC_BASE_LOCAL <span class="keyword">on</span> cluster ck_cluster <span class="keyword">as</span> mysql_pay_tradecenter.T_TC_BASE  </span><br><span class="line"><span class="keyword">ENGINE</span> = ReplicatedMergeTree(<span class="string">'/clickhouse/tables/&#123;shard&#125;/T_TC_BASE_LOCAL'</span>, <span class="string">'&#123;replica&#125;'</span>) </span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYYYYMM(CREATED_AT) </span><br><span class="line">primary <span class="keyword">key</span> (<span class="keyword">ID</span>)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> (<span class="keyword">ID</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在集群的每个机器上面建立分布式表</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> PAY_TRADECENTER.T_TC_BASE <span class="keyword">on</span> cluster ck_cluster </span><br><span class="line"><span class="keyword">as</span> PAY_TRADECENTER.T_TC_BASE_LOCAL</span><br><span class="line"><span class="keyword">ENGINE</span> = <span class="keyword">Distributed</span>(ck_cluster, PAY_TRADECENTER, T_TC_BASE_LOCAL, <span class="keyword">rand</span>());</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> PAY_TRADECENTER.T_TC_BASE <span class="keyword">SELECT</span> * <span class="keyword">from</span> mysql_pay_tradecenter.T_TC_BASE</span><br></pre></td></tr></table></figure>
<h2 id="clickhouse用presto查询">clickhouse用presto查询</h2>
<p>测试下来clickhouse用presto查询  效率低</p>
<h2 id="卸载及删除安装文件">卸载及删除安装文件</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum list installed | grep clickhouse</span><br><span class="line">yum remove -y clickhouse-common-static</span><br><span class="line">yum remove -y clickhouse-server-common</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;clickhouse</span><br><span class="line">rm -rf &#x2F;etc&#x2F;clickhouse-*</span><br><span class="line">rm -rf &#x2F;var&#x2F;log&#x2F;clickhouse-server</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>解决freeipa证书过期</title>
    <url>/2021/12/10/%E8%A7%A3%E5%86%B3freeipa%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F/</url>
    <content><![CDATA[<h1>FreeIPA 服务器在重启后不会启动</h1>
<p>参考   <a href="https://redhatlinux.guru/2020/10/09/freeipa-server-will-not-start-after-reboot/" target="_blank" rel="noopener">https://redhatlinux.guru/2020/10/09/freeipa-server-will-not-start-after-reboot/</a></p>
<p><strong>原因 证书过期：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@freeipa ~]# ipa-getcert list </span><br><span class="line">Number of certificates and requests being tracked: 9.</span><br><span class="line">Request ID &#39;20190830074301&#39;:</span><br><span class="line">	status: CA_UNREACHABLE</span><br><span class="line">	ca-error: Server at https:&#x2F;&#x2F;freeipa.baofoo.cn&#x2F;ipa&#x2F;xml failed request, will retry: -504 (libcurl failed to execute the HTTP POST transaction, explaining:  Failed connect to freeipa.baofoo.cn:443; Connection refused).</span><br><span class="line">	stuck: no</span><br><span class="line">	key pair storage: type&#x3D;NSSDB,location&#x3D;&#39;&#x2F;etc&#x2F;dirsrv&#x2F;slapd-BAOFOO-CN&#39;,nickname&#x3D;&#39;Server-Cert&#39;,token&#x3D;&#39;NSS Certificate DB&#39;,pinfile&#x3D;&#39;&#x2F;etc&#x2F;dirsrv&#x2F;slapd-BAOFOO-CN&#x2F;pwdfile.txt&#39;</span><br><span class="line">	certificate: type&#x3D;NSSDB,location&#x3D;&#39;&#x2F;etc&#x2F;dirsrv&#x2F;slapd-BAOFOO-CN&#39;,nickname&#x3D;&#39;Server-Cert&#39;,token&#x3D;&#39;NSS Certificate DB&#39;</span><br><span class="line">	CA: IPA</span><br><span class="line">	issuer: CN&#x3D;Certificate Authority,O&#x3D;BAOFOO.CN</span><br><span class="line">	subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">	expires: 2021-08-30 07:43:01 UTC</span><br><span class="line">	dns: freeipa.baofoo.cn</span><br><span class="line">	principal name: ldap&#x2F;freeipa.baofoo.cn@BAOFOO.CN</span><br><span class="line">	key usage: digitalSignature,nonRepudiation,keyEncipherment,dataEncipherment</span><br><span class="line">	eku: id-kp-serverAuth,id-kp-clientAuth</span><br><span class="line">	pre-save command: </span><br><span class="line">	post-save command: &#x2F;usr&#x2F;libexec&#x2F;ipa&#x2F;certmonger&#x2F;restart_dirsrv BAOFOO-CN</span><br><span class="line">	track: yes</span><br><span class="line">	auto-renew: yes</span><br></pre></td></tr></table></figure>
<p>问题<br>
这更多是利基问题。但可能对其他人有用。我在家里有一个用于 DNS 的 FreeIPA 服务器设置。在简单的重新启动以向 VM 添加一些 RAM 后，服务器将无法启动。我收到如下错误。</p>
<p>IPA 服务器错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl status ipa</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">● ipa.service - Identity, Policy, Audit</span><br><span class="line">Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ipa.service; enabled; vendor preset: disabled)</span><br><span class="line">Active: failed (Result: exit-code) since Fri 2020-10-09 14:57:15 EDT; 1s ago</span><br><span class="line">Process: 1110 ExecStart&#x3D;&#x2F;usr&#x2F;sbin&#x2F;ipactl start (code&#x3D;exited, status&#x3D;1&#x2F;FAILURE)</span><br><span class="line">Main PID: 1110 (code&#x3D;exited, status&#x3D;1&#x2F;FAILURE)</span><br><span class="line"></span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Aborting ipactl</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Starting Directory Service</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Starting krb5kdc Service</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Starting kadmin Service</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Starting named Service</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Starting httpd Service</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local systemd[1]: ipa.service: main process exited, code&#x3D;exited, status&#x3D;1&#x2F;FAILURE</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local systemd[1]: Failed to start Identity, Policy, Audit.</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local systemd[1]: Unit ipa.service entered failed state.</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local systemd[1]: ipa.service failed.</span><br></pre></td></tr></table></figure>
<h5 id="Apache-服务错误">Apache 服务错误</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl status httpd -l</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">● httpd.service - The Apache HTTP Server</span><br><span class="line">Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;httpd.service; disabled; vendor preset: disabled)</span><br><span class="line">Drop-In: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;httpd.service.d</span><br><span class="line">└─ipa.conf</span><br><span class="line">Active: failed (Result: exit-code) since Fri 2020-10-09 14:57:44 EDT; 9s ago</span><br><span class="line">Docs: man:httpd(8)</span><br><span class="line">man:apachectl(8)</span><br><span class="line">Process: 1532 ExecStart&#x3D;&#x2F;usr&#x2F;sbin&#x2F;httpd $OPTIONS -DFOREGROUND (code&#x3D;exited, status&#x3D;1&#x2F;FAILURE)</span><br><span class="line">Process: 1529 ExecStartPre&#x3D;&#x2F;usr&#x2F;libexec&#x2F;ipa&#x2F;ipa-httpd-kdcproxy (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line">Main PID: 1532 (code&#x3D;exited, status&#x3D;1&#x2F;FAILURE)</span><br><span class="line"></span><br><span class="line">Oct 09 14:57:42 ipasrv.home.local systemd[1]: Starting The Apache HTTP Server...</span><br><span class="line">Oct 09 14:57:43 ipasrv.home.local ipa-httpd-kdcproxy[1529]: ipa: WARNING: Unable to connect to dirsrv: cannot connect to &amp;#039;ldapi:&#x2F;&#x2F;%2fvar%2frun%2fslapd-HOME-LOCAL.socket&amp;#039;:</span><br><span class="line">Oct 09 14:57:43 ipasrv.home.local ipa-httpd-kdcproxy[1529]: ipa-httpd-kdcproxy: WARNING  Unable to connect to dirsrv: cannot connect to &amp;#039;ldapi:&#x2F;&#x2F;%2fvar%2frun%2fslapd-HOME-LOCAL.socket&amp;#039;:</span><br><span class="line">Oct 09 14:57:43 ipasrv.home.local ipa-httpd-kdcproxy[1529]: ipa: WARNING: Disabling KDC proxy</span><br><span class="line">Oct 09 14:57:43 ipasrv.home.local ipa-httpd-kdcproxy[1529]: ipa-httpd-kdcproxy: WARNING  Disabling KDC proxy</span><br><span class="line">Oct 09 14:57:44 ipasrv.home.local systemd[1]: httpd.service: main process exited, code&#x3D;exited, status&#x3D;1&#x2F;FAILURE</span><br><span class="line">Oct 09 14:57:44 ipasrv.home.local systemd[1]: Failed to start The Apache HTTP Server.</span><br><span class="line">Oct 09 14:57:44 ipasrv.home.local systemd[1]: Unit httpd.service entered failed state.</span><br><span class="line">Oct 09 14:57:44 ipasrv.home.local systemd[1]: httpd.service failed.</span><br></pre></td></tr></table></figure>
<p>解析度<br>
不会用这个绕过灌木丛。底线是我的证书已过期。下面是解决它的步骤。</p>
<h3 id="1-–-使用忽略失败服务的选项启动-IPA-服务器。">1 – 使用忽略失败服务的选项启动 IPA 服务器。</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipactl start --ignore-service-failure</span><br></pre></td></tr></table></figure>
<p>示例输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Existing service file detected!</span><br><span class="line">Assuming stale, cleaning and proceeding</span><br><span class="line">Starting Directory Service</span><br><span class="line">Starting krb5kdc Service</span><br><span class="line">Starting kadmin Service</span><br><span class="line">Starting named Service</span><br><span class="line">Starting httpd Service</span><br><span class="line">Failed to start httpd Service</span><br><span class="line">Forced start, ignoring httpd Service, continuing normal operation</span><br><span class="line">Starting ipa-custodia Service</span><br><span class="line">Starting ntpd Service</span><br><span class="line">Starting pki-tomcatd Service</span><br></pre></td></tr></table></figure>
<h2 id="2-–-接下来运行ipa-cert-fix命令来更新过期的证书。">2 – 接下来运行ipa-cert-fix命令来更新过期的证书。</h2>
<p>ipa-cert-fix</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">                          WARNING</span><br><span class="line"></span><br><span class="line">ipa-cert-fix is intended for recovery when expired certificates</span><br><span class="line">prevent the normal operation of IPA.  It should ONLY be used</span><br><span class="line">in such scenarios, and backup of the system, especially certificates</span><br><span class="line">and keys, is STRONGLY RECOMMENDED.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The following certificates will be renewed: </span><br><span class="line"></span><br><span class="line">Dogtag sslserver certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  3</span><br><span class="line">  Expires: 2021-08-19 07:42:09</span><br><span class="line"></span><br><span class="line">Dogtag subsystem certificate:</span><br><span class="line">  Subject: CN&#x3D;CA Subsystem,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  4</span><br><span class="line">  Expires: 2021-08-19 07:42:09</span><br><span class="line"></span><br><span class="line">Dogtag ca_ocsp_signing certificate:</span><br><span class="line">  Subject: CN&#x3D;OCSP Subsystem,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  2</span><br><span class="line">  Expires: 2021-08-19 07:42:09</span><br><span class="line"></span><br><span class="line">Dogtag ca_audit_signing certificate:</span><br><span class="line">  Subject: CN&#x3D;CA Audit,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  5</span><br><span class="line">  Expires: 2021-08-19 07:42:09</span><br><span class="line"></span><br><span class="line">IPA IPA RA certificate:</span><br><span class="line">  Subject: CN&#x3D;IPA RA,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  7</span><br><span class="line">  Expires: 2021-08-19 07:42:30</span><br><span class="line"></span><br><span class="line">IPA Apache HTTPS certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  9</span><br><span class="line">  Expires: 2021-08-30 07:43:31</span><br><span class="line"></span><br><span class="line">IPA LDAP certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  8</span><br><span class="line">  Expires: 2021-08-30 07:43:01</span><br><span class="line"></span><br><span class="line">IPA KDC certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  10</span><br><span class="line">  Expires: 2021-08-30 07:43:38</span><br><span class="line"></span><br><span class="line">Enter &quot;yes&quot; to proceed: yes</span><br><span class="line">Proceeding.</span><br><span class="line">Renewed Dogtag sslserver certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  11</span><br><span class="line">  Expires: 2023-11-30 01:22:29</span><br><span class="line"></span><br><span class="line">Renewed Dogtag subsystem certificate:</span><br><span class="line">  Subject: CN&#x3D;CA Subsystem,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  12</span><br><span class="line">  Expires: 2023-11-30 01:22:30</span><br><span class="line"></span><br><span class="line">Renewed Dogtag ca_ocsp_signing certificate:</span><br><span class="line">  Subject: CN&#x3D;OCSP Subsystem,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  13</span><br><span class="line">  Expires: 2023-11-30 01:22:31</span><br><span class="line"></span><br><span class="line">Renewed Dogtag ca_audit_signing certificate:</span><br><span class="line">  Subject: CN&#x3D;CA Audit,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  14</span><br><span class="line">  Expires: 2023-11-30 01:22:31</span><br><span class="line"></span><br><span class="line">Renewed IPA IPA RA certificate:</span><br><span class="line">  Subject: CN&#x3D;IPA RA,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  15</span><br><span class="line">  Expires: 2023-11-30 01:22:32</span><br><span class="line"></span><br><span class="line">Renewed IPA Apache HTTPS certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  16</span><br><span class="line">  Expires: 2023-12-11 01:22:32</span><br><span class="line"></span><br><span class="line">Renewed IPA LDAP certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  17</span><br><span class="line">  Expires: 2023-12-11 01:22:32</span><br><span class="line"></span><br><span class="line">Renewed IPA KDC certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  18</span><br><span class="line">  Expires: 2023-12-11 01:22:33</span><br><span class="line"></span><br><span class="line">Becoming renewal master.</span><br><span class="line">The ipa-cert-fix command was successful</span><br><span class="line">[root@freeipa ~]# ipactl restart</span><br></pre></td></tr></table></figure>
<h3 id="3-–-更新证书后，重新启动-IPA-服务器，">3 – 更新证书后，重新启动 IPA 服务器，</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipactl restart</span><br></pre></td></tr></table></figure>
<p>示例输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Restarting Directory Service</span><br><span class="line">Restarting krb5kdc Service</span><br><span class="line">Restarting kadmin Service</span><br><span class="line">Restarting named Service</span><br><span class="line">Restarting httpd Service</span><br><span class="line">Restarting ipa-custodia Service</span><br><span class="line">Restarting ntpd Service</span><br><span class="line">Restarting pki-tomcatd Service</span><br><span class="line">Restarting ipa-otpd Service</span><br><span class="line">Restarting ipa-dnskeysyncd Service</span><br><span class="line">ipa: INFO: The ipactl command was successful</span><br></pre></td></tr></table></figure>
<h3 id="4-–-最后使用ipactl-status命令验证IPA服务器启动。">4 – 最后使用ipactl status命令验证IPA服务器启动。</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipactl status</span><br></pre></td></tr></table></figure>
<p>示例输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Directory Service: RUNNING</span><br><span class="line">krb5kdc Service: RUNNING</span><br><span class="line">kadmin Service: RUNNING</span><br><span class="line">named Service: RUNNING</span><br><span class="line">httpd Service: RUNNING</span><br><span class="line">ipa-custodia Service: RUNNING</span><br><span class="line">ntpd Service: RUNNING</span><br><span class="line">pki-tomcatd Service: RUNNING</span><br><span class="line">ipa-otpd Service: RUNNING</span><br><span class="line">ipa-dnskeysyncd Service: RUNNING</span><br><span class="line">ipa: INFO: The ipactl command was successful</span><br></pre></td></tr></table></figure>
<h1>Upgrade   版本升级</h1>
<p>[root@freeipa ipa]# rpm -aq ipa-server<br>
ipa-server-4.6.5-11.el7.centos.x86_64</p>
<p>ipa-server-upgrade</p>
]]></content>
  </entry>
  <entry>
    <title>ckman部署</title>
    <url>/2021/12/08/ckman%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>ckman部署</p>
<h3 id="安装">安装</h3>
<p><code>rpm</code>安装直接使用命令安装即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -ivh ckman-1.3.1.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>安装完成后，在<code>/etc/ckman</code>目录下，会生成工作目录（日志和配置文件等都在该目录下）。</p>
<h3 id="启动">启动</h3>
<p><code>rpm</code>方式安装的<code>ckman</code>有两种启动方式：</p>
<h4 id="方式一：">方式一：</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;bin&#x2F;ckman -c&#x3D;&#x2F;etc&#x2F;ckman&#x2F;conf&#x2F;ckman.yaml -p&#x3D;&#x2F;run&#x2F;ckman&#x2F;ckman.pid -l&#x3D;&#x2F;var&#x2F;log&#x2F;ckman&#x2F;ckman.log -d</span><br></pre></td></tr></table></figure>
<h4 id="方式二：">方式二：</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start ckman</span><br></pre></td></tr></table></figure>
<h2 id="tar-gz包安装">tar.gz包安装</h2>
<h3 id="安装-2">安装</h3>
<p>可以在任意目录进行安装。安装方式为直接解压安装包即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xzvf ckman-1.3.1-210428.Linux.x86_64.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="启动-2">启动</h3>
<p>进入<code>ckman</code>的工作目录，执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ckman</span><br><span class="line">bin&#x2F;start</span><br></pre></td></tr></table></figure>
<p>启动之后，在浏览器输入 <a href="http://localhost:8808/" target="_blank" rel="noopener">http://localhost:8808</a> 跳出如下界面，说明启动成功：</p>
<p>– 特别主要：</p>
<p>clickhouse占用9000的端口 需要修改cloudera manager agent 默认端口9000</p>
<p>可以在clickhouse结点上查看/var/log/clickhouse-server/clickhouse-server.log，注意ERROR级别日志</p>
<p>/var/log/clickhouse-server/clickhouse-server.err.log</p>
<p>clickhouse  9000 9004 9005 9009</p>
<p>部署集群:认证方式:<br>
1．密码认证（保存密码）﹔使用密码登录，密码加密保存在本地，后续运维动作无需输入密码<br>
2．密码认证（不保存密码)﹔使用密码登录，但是密码不保存，后续运维动作（增删节点、启停等)，需要输入密码3．公钥认证:使用公钥登录，无需保存密码，后续运维操作可直接操作，是默认的认证方式<br>
公钥认证需要注意:<br>
1）需要提前配置ckman所在服务器到clickhouse各节点之间的互信（使用哪个用户去部署就配置哪个用户的)<br>
ssh-copy-id<br>
2）需要将公钥文件~/.ssh/id_rsa拷贝到ckman/conf目录下，并保证c kman用户有权限访间该支件3)如果是使用普通用户，公钥方式认证，那么该普通用户需要具有sudo权限，且在/etc/sudoers<br>
文件中配置了NOPASSwD</p>
<p>–客户端访问</p>
<p>clickhouse-client --port 9002 --password admin -m</p>
<p>CREATE USER ‘clickhouse’@’%’ IDENTIFIED BY ‘alitrack’;<br>
GRANT select ON db.* TO ‘clickhouse’@’%’;<br>
GRANT replication client,replication slave, reload on <em>.</em> to ‘clickhouse’@’%’;<br>
FLUSH PRIVILEGES;</p>
]]></content>
  </entry>
  <entry>
    <title>升级OpenSSH之后登录不上去</title>
    <url>/2021/12/06/%E5%8D%87%E7%BA%A7OpenSSH%E4%B9%8B%E5%90%8E%E7%99%BB%E5%BD%95%E4%B8%8D%E4%B8%8A%E5%8E%BB/</url>
    <content><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure>
<p>需要在配置文件中添加：</p>
<p>PermitRootLogin yes</p>
]]></content>
  </entry>
  <entry>
    <title>DBeaver Enterprise 21.2.0 破解版 安装教程</title>
    <url>/2021/11/23/DBeaver%20Enterprise%2021.2.0%20%E7%A0%B4%E8%A7%A3%E7%89%88%20%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h1>DBeaver Enterprise 21.2.0 破解版 安装教程</h1>
<h2 id="制作绿色包-开箱即用">制作绿色包  开箱即用</h2>
<p><a href="https://blog.csdn.net/u012234419/article/details/120091206" target="_blank" rel="noopener">https://blog.csdn.net/u012234419/article/details/120091206</a></p>
<p>亲测可用，感谢楼主~</p>
]]></content>
  </entry>
  <entry>
    <title>clickhouse之HDFS云存储</title>
    <url>/2021/11/19/clickhouse%E4%B9%8BHDFS%E4%BA%91%E5%AD%98%E5%82%A8/</url>
    <content><![CDATA[<h1>clickhouse之HDFS云存储</h1>
<p>参考： <a href="https://blog.csdn.net/weixin_40104766/article/details/120029525" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40104766/article/details/120029525</a></p>
<p>按照官方文档说明，在config.xml文件中添加如下配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">storage_configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">disks</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">diskname</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">type</span>&gt;</span>hdfs<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">endpoint</span>&gt;</span>hdfs://10.0.19.241:8020/ckdata/<span class="tag">&lt;/<span class="name">endpoint</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">diskname</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">disks</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">policies</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">hdfs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">volumes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">volumename</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">disk</span>&gt;</span>diskname<span class="tag">&lt;/<span class="name">disk</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">volumename</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">volumes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">hdfs</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">policies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">storage_configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>配置完成后，重启clickhouse-server服务，验证是否生效：</p>
<p>1.通过system.storage_policies表查看存储策略是否OK</p>
<p>select * from system.storage_policies;</p>
<p>2.通过settings参数指定storage_policy为我们配置好的hdfs，建表语句如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_data <span class="keyword">engine</span>=MergeTree <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">id</span> <span class="keyword">settings</span> storage_policy=<span class="string">'hdfs'</span> <span class="keyword">as</span> <span class="keyword">with</span> (<span class="keyword">select</span> [<span class="string">'A'</span>,<span class="string">'a'</span>,<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'A'</span>,<span class="string">'59'</span>,<span class="string">'90'</span>,<span class="string">'80'</span>,<span class="string">'85'</span>,<span class="string">'90'</span>,<span class="string">'929'</span>,<span class="string">'80'</span>,<span class="string">'72'</span>,<span class="string">'90'</span>,<span class="string">'123'</span>]) <span class="keyword">AS</span> dict <span class="keyword">select</span> dict[<span class="built_in">number</span>%<span class="number">10</span> + <span class="number">1</span>] <span class="keyword">as</span> <span class="keyword">id</span>,dict[<span class="built_in">number</span> + <span class="number">11</span>] <span class="keyword">as</span> val <span class="keyword">from</span> system.numbers <span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>磁盘读写测试</title>
    <url>/2021/10/28/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h2 id="磁盘读写测试">磁盘读写测试</h2>
<p>FIO安装<br>
wget <a href="http://brick.kernel.dk/snaps/fio-2.2.5.tar.gz" target="_blank" rel="noopener">http://brick.kernel.dk/snaps/fio-2.2.5.tar.gz</a></p>
<p>yum install libaio-devel<br>
tar -zxvf fio-2.2.5.tar.gz<br>
cd fio-2.2.5<br>
make<br>
make install</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">测试：</span><br><span class="line">顺序读：</span><br><span class="line">fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=mytest</span><br><span class="line"></span><br><span class="line">随机写：</span><br><span class="line">fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=mytest</span><br><span class="line"></span><br><span class="line">顺序写：</span><br><span class="line">fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=mytest</span><br><span class="line"></span><br><span class="line">混合随机读写：</span><br><span class="line">fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=mytest -ioscheduler=noop</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>智能大数据平台USDP操作部署</title>
    <url>/2021/07/20/%E6%99%BA%E8%83%BD%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0USDP%E6%93%8D%E4%BD%9C%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h2 id="智能大数据平台USDP操作部署">智能大数据平台USDP操作部署</h2>
<p>USDP部署操作指南：<a href="https://mp.weixin.qq.com/s/COnkLXPTWL5OK1PFYooThw?scene=25#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/COnkLXPTWL5OK1PFYooThw?scene=25#wechat_redirect</a></p>
<p>bash <a href="http://repair.sh" target="_blank" rel="noopener">repair.sh</a> initAll   这个步骤大约要30多分钟，重复执行也慢。</p>
<p>注意：</p>
<p>1.在repair-host-info-add.properties文件中，仅需配置每次新增的节点信息即可，若存在已修复过的节点信息时，在下次运行“<a href="http://repair.sh" target="_blank" rel="noopener">repair.sh</a> initSingle”指令前，请清除。</p>
<p>2.jdk 安装在 /opt/module 下面，不允许随意删除，否则 java 环境失效。</p>
<p>3.数据库密码中不得包含 “@” 。</p>
<p>4.主机名设置不得包含 “_”，&quot;-&quot; 。</p>
<p>启动服务  bin/stop-udp-server.sh</p>
<p>查看日志目录  /var/log/udp</p>
<p>一般如果数据库已经安装完毕了的话，初始化是不会修改数据库的，所以重复执行bash <a href="http://repair.sh" target="_blank" rel="noopener">repair.sh</a> initAll 也是没有用的。</p>
<p>不管之前设置什么密码，修改数据库密码的统一操作：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "skip-grant-tables" &gt;&gt; /etc/my.cnf</span><br><span class="line">systemctl restart mysqld</span><br><span class="line">systemctl enable mysqld</span><br><span class="line">sh /opt/usdp-srv/usdp/repair/bin/repair-modifyMysqlPassword.sh  '密码'</span><br><span class="line">sed -i 's/skip-grant-tables/#&amp;/' /etc/my.cnf</span><br><span class="line">systemctl restart mysqld</span><br><span class="line">sh /opt/usdp-srv/usdp/repair/bin/repair-init-mysql-udp.sh '密码'</span><br></pre></td></tr></table></figure>
<h3 id="1-0-升级到2-0">1.0 升级到2.0</h3>
<p>下载  usdp-01-master-privatization-free-2.0.0.0.tar.gz</p>
<p>解压，把2.0中/opt/usdp-srv/usdp/repair/remove  目录  复制到1.0 /opt/usdp-srv/usdp/repair</p>
<p>sh <a href="http://remove-all.sh" target="_blank" rel="noopener">remove-all.sh</a> 1.0.0.0</p>
<table>
<thead>
<tr>
<th><strong>集群清除模块文件</strong></th>
<th><strong>文件说明</strong></th>
<th><strong>文件位置</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://remove-all.sh" target="_blank" rel="noopener">remove-all.sh</a></td>
<td>执行删除集群入口</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
<tr>
<td>remove-host-info.properties</td>
<td>配置需要删除集群的相关节点信息</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
<tr>
<td>remove-init-mysql- <a href="http://udp.sh" target="_blank" rel="noopener">udp.sh</a></td>
<td>清除数据库脚本文件</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
<tr>
<td>remove_db_udp.sql</td>
<td>删除数据库 sql 文件</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
<tr>
<td>stop-all-residual- <a href="http://process.sh" target="_blank" rel="noopener">process.sh</a></td>
<td>清除残留进程，删除残留目录</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
<tr>
<td>stop-management- <a href="http://program.sh" target="_blank" rel="noopener">program.sh</a></td>
<td>停止 USDP 管理端 server 和agent 进程</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
</tbody>
</table>
<h1><strong>1.1</strong> <em><strong>*.2*</strong></em> <em><strong>*清除集群步骤*</strong></em></h1>
<h2 id="1-1-1-修改-remove-host-info-properties-配置文件"><strong>1.1.1</strong> <em><strong>*修改*</strong></em> <em><strong>*remove-host-info.properties*</strong></em> <em><strong>*配置文件*</strong></em></h2>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><img src="../images/wps1.png" alt="img"></td>
</tr>
</tbody>
</table>
<p><img src="../images/wps2.png" alt="img"></p>
<p>上述配置项解释如下：</p>
<table>
<thead>
<tr>
<th><strong>remove-host-info.properties</strong> <strong>文件配置项</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>usdp.ip.x</td>
<td>配置删除集群的节点 ip</td>
</tr>
<tr>
<td>usdp.ssh.port.x</td>
<td>配置删除集群的节点端口号</td>
</tr>
<tr>
<td>remove.host.num</td>
<td>配置清除集群包含的节点数</td>
</tr>
<tr>
<td>mysql.ip</td>
<td>mysql 所在节点 ip</td>
</tr>
<tr>
<td>mysql.host.ssh.port</td>
<td>mysql 所在节点 ssh 端口号</td>
</tr>
<tr>
<td>mysql.host.ssh.password</td>
<td>mysql 所在节点密码</td>
</tr>
<tr>
<td>mysql.password</td>
<td>mysql 数据库登录密码</td>
</tr>
</tbody>
</table>
<h2 id="1-1-2-执行清除脚本"><strong>1.1.2</strong> <em><strong>*执行清除脚本*</strong></em></h2>
<p>修改完上述配置文件，即可进入 remove 目录，执行一键清除脚本</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><img src="../images/wps3.png" alt="img"></td>
</tr>
</tbody>
</table>
]]></content>
  </entry>
  <entry>
    <title>黑鲨游戏手机解BL锁，刷REC，ROOT 和问题总结</title>
    <url>/2021/07/14/%E9%BB%91%E9%B2%A8%E6%B8%B8%E6%88%8F%E6%89%8B%E6%9C%BA%E8%A7%A3BL%E9%94%81%EF%BC%8C%E5%88%B7REC%EF%BC%8CROOT%20%E5%92%8C%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="黑鲨游戏手机解BL锁，刷REC，ROOT-和问题总结">黑鲨游戏手机解BL锁，刷REC，ROOT 和问题总结</h2>
<p>参考帮助  <a href="https://www.bilibili.com/video/BV1Cf4y1U7gn?from=search&amp;seid=14731760489436726457" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Cf4y1U7gn?from=search&amp;seid=14731760489436726457</a></p>
<h2 id="总的步骤：">总的步骤：</h2>
<p><img src="/images/image-20210714100219695.png" alt="image-20210714100219695"></p>
<p>需要用到的文件</p>
<p><img src="/images/image-20210714110634447.png" alt="image-20210714110634447"></p>
<h3 id="第一步-系统降级">第一步.系统降级</h3>
<p>黑鲨1代<br>
系统版本:G66X1811300CN00MPX安卓版本:8.1下载地址:<a href="https://ota-1256119282.file.myqcloud.com/SKR-A0/2018-11-30_shark_mp_ota11_20180929shark-ota-eng.buildfarm.zip" target="_blank" rel="noopener">https://ota-1256119282.file.myqcloud.com/SKR-A0/2018-11-30_shark_mp_ota11_20180929shark-ota-eng.buildfarm.zip</a><br>
系统版本:G66X1905170CN00MPP安卓版本:P(安卓9)下载地址:<a href="https://ota-1256119282.file.myqcloud.com/SKR-A0/shark-ota-eng.buildfarm_2019-05-17_sdm845_p_mp_ota1_20190430.zip" target="_blank" rel="noopener">https://ota-1256119282.file.myqcloud.com/SKR-A0/shark-ota-eng.buildfarm_2019-05-17_sdm845_p_mp_ota1_20190430.zip</a></p>
<p><strong>刷机的准备工作：</strong></p>
<p>1、根据自己机型下载对应版本型号的黑鲨手机全量包到电脑，将文件重命名为<strong>update.zip</strong></p>
<p>2、将<strong>update.zip</strong>文件放到手机根目录的<strong>ota文件夹</strong>下（如果手机本身有一个OTA文件夹（大写的），就删掉重建一个ota文件夹），</p>
<p><strong>1、打开手机的拨号界面，输入*#*#1027#*#*后，就自动进入本地升级界面</strong></p>
<img src="/images/image-20210714100123909.png" alt="image-20210714100123909" style="zoom:50%;" />
<p>2、当更新的时，图示变成了为</p>
<p><strong>＋＋＋＋＋＋OTA success,please Reboot＋＋＋＋＋＋</strong></p>
<p>字样，说明你已经成功升级，只需重启系统就可以正常的进入新系统了。</p>
<img src="/images/image-20210714100503207.png" alt="image-20210714100503207" style="zoom:50%;" />
<h4 id="遇到的问题：">遇到的问题：</h4>
<p>手机降级后，重启手机需要密码</p>
<img src="/images/image-20210714100618904.png" alt="image-20210714100618904" style="zoom:50%;" />
<p>需要双清手机</p>
<p><strong>按住音量上键不放按住电源键重启，手机一重启就松开电源键，音量键不松</strong></p>
<h2 id="双清：">双清：</h2>
<p>wipe data/ factory reset, 清除用户数据并恢复出厂设置（刷机前必须执行的选项）</p>
<p>wipe cache partition，清除系统缓存（刷机前执行）（系统出问题也可尝试此选项，一般都能够解决）</p>
<p>这样就完成了系统降级，本人是刷到了安卓8.1</p>
<img src="/images/image-20210714101353437.png" alt="image-20210714101353437" style="zoom:50%;" />
<h3 id="第二步-解BL锁">第二步 解BL锁</h3>
<p>用的 黑鲨全机型全版本一键解锁BL，QQ群177600200 的文件里可以下载</p>
<p><img src="/images/image-20210714102642570.png" alt="image-20210714102642570"></p>
<h4 id="遇到的问题">遇到的问题</h4>
<p>出现多个端口</p>
<p><img src="/images/image-20210714101613505.png" alt="image-20210714101613505"></p>
<p>禁用掉多的端口就可以了，刷好之后 再解禁端口。</p>
<p><img src="/images/image-20210714102255099.png" alt="image-20210714102255099"></p>
<h3 id="第三步-刷入第三方中文版TWRP-Recovery，简称-“刷rec”">第三步  刷入第三方中文版TWRP-Recovery，简称 “刷rec”</h3>
<p>recovery下载：<a href="https://pan.baidu.com/s/1QzY2qS7TsTGr-Y-H2-OC5Q" target="_blank" rel="noopener">https://pan.baidu.com/s/1QzY2qS7TsTGr-Y-H2-OC5Q</a></p>
<p>我下载的黑鲨1安卓8的   TWRP-3.3.1-0526-BLACKSHARK_SHARK-CN-wzsx150-fastboot.7z</p>
<p>刷入方法：</p>
<p>下载后解压（存放路径中不要有中文和空格等），</p>
<p>手机连接电脑，</p>
<p>双击运行一键刷入recovery工具.bat文件，按照提示操作</p>
<p>需要提供手机内系统相同版本的boot镜像文件（网盘地址中有提供，也可以自己提取 ），我直接下载网盘里的 boot-G66X1811300CN00MPX.img 文件</p>
<p>进入第三方TWRP_Recovery方法：</p>
<p>手机彻底关机，音量上和开机按键双手一起按住，不要松开<br>
等待手机震动后，松开开机按键，音量上不放手，就进入了recovery</p>
<h4 id="遇到的问题：-2">遇到的问题：</h4>
<p>存放路径中有中文，放到没中文的目录就可以了。</p>
<h3 id="第四步-通过Magisk-刷root">第四步 通过Magisk 刷root</h3>
<p>由于直接用recovery里的清楚Root功能 ，重启后并没有生效。我通过刷面具Magisk-v19.3(19300).zip 实现</p>
<p>参考：<a href="https://www.guanjiaxiaoe.com/130.html" target="_blank" rel="noopener">https://www.guanjiaxiaoe.com/130.html</a></p>
<p>4.1 把Magisk-v19.3(19300).zip导入到手机根目录</p>
<p>4.2 进入rec ，点击安装，选择Magisk-v19.3(19300).zip</p>
<p>安装好面具后，需要root权限的应用，当第一次运行的时候它会要求获取Root权限，此时Magisk会自动弹出，点击允许即可获取Root权限。</p>
<h4 id="遇到的问题-2">遇到的问题</h4>
<p>直接用recovery里的清除Root功能 ，重启后并没有生效。可能是版本低了。</p>
<img src="/images/E7051906A010BA6576A73B7DCD94B818.jpg" alt="img" style="zoom:50%;" />
]]></content>
  </entry>
  <entry>
    <title>服务器管理规范(V1)</title>
    <url>/2021/07/13/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AE%A1%E7%90%86%E8%A7%84%E8%8C%83(V1)/</url>
    <content><![CDATA[<h4 id="服务器管理规范-V1">服务器管理规范(V1)</h4>
<p><strong>一 配置管理规范</strong></p>
<p>所有设备信息必须录入配置管理系统，在配置系统中能随时查询到现网业务的部署分布情况 具体信息待配置管理系统建立后再补充</p>
<p>主机命名规范</p>
<p>网卡vlan规范</p>
<p>安全策略命名规范</p>
<p>监控/部署/插件/模块命名规范</p>
<p>版本命名规范</p>
<p><strong>二 文件系统管理规范</strong></p>
<p>2.1 文件布局</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 系统服务及配置文件优先采用LSF标准，其余文件可采用BSD标准[注1]，例如:</span><br><span class="line">&#x2F;etc&#x2F;vimrc         ---- vim默认配置</span><br><span class="line">&#x2F;etc&#x2F;bash.bashrc   ---- shell环境配置</span><br><span class="line">. 用户脚本</span><br><span class="line">&#x2F;opt&#x2F;admin&#x2F;shell&#x2F;  ---- 自定义脚本，函数</span><br><span class="line">&#x2F;opt&#x2F;admin&#x2F;cron&#x2F;   ---- cron脚本</span><br><span class="line">&#x2F;opt&#x2F;admin&#x2F;firwall&#x2F;   ---- 防火墙脚本</span><br><span class="line">. data分区(&#x2F;data或&#x2F;home, 这里以&#x2F;data为基准)</span><br><span class="line">&#x2F;data&#x2F;db_dir&#x2F;mysql        ---- mysql data目录</span><br><span class="line">&#x2F;data&#x2F;db_dir&#x2F;redis        ---- redis data目录</span><br><span class="line">&#x2F;data&#x2F;www          ---- web目录</span><br><span class="line">&#x2F;data&#x2F;backup       ---- 备份目录</span><br><span class="line">注1: LSF(Linux Stand Filesytem)</span><br><span class="line">     BSD: 软件包通常位于:   &#x2F;usr&#x2F;local&#x2F;$package</span><br><span class="line">     SYS V: 软件包通常位于: &#x2F;opt&#x2F;$package</span><br><span class="line"> 程序原始文件必须放置于&#x2F;data目录下，可以自行加入软连接 如：ln -s &#x2F;data&#x2F;apps &#x2F;opt&#x2F;apps</span><br><span class="line"> 数据放置于&#x2F;data下</span><br></pre></td></tr></table></figure>
<p>2.2 文件修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 默认配置文件    ---- xxxx.orig</span><br><span class="line">  在没有任何改动的情况下以orig扩展名做一次备份并保留注释，以供参考</span><br><span class="line">. 修改配置文件    ---- xxxx.$date</span><br><span class="line">  对当前配置文件做出修改时, 建议首先以xxxx.$date的命名方式对其做一次备份.</span><br><span class="line">. 当前配置文件</span><br><span class="line">  建议移除相关注释及空行, 在有缩进的情况下以四个空格作为缩进，以保证阅读的清爽性.</span><br></pre></td></tr></table></figure>
<p><strong>三 软件包管理规范</strong></p>
<p>3.1 包管理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 采用自动部署工具(salt, puppet等)管理相应软件包，应避免手动直接安装。</span><br><span class="line">. 只保留安全补丁升级，应避免系统库及相应服务升级。</span><br><span class="line">. 建立官方仓库本地镜像及私有仓库。</span><br></pre></td></tr></table></figure>
<p>3.2 包安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 尽量采用官方源，及稳定的三方源安装相应软件包。</span><br><span class="line">. 如有必须源码编译[注1]，务必遵照Debian官方打包方式进行打包[注2]，以保持LSF规范及自动化管理。</span><br><span class="line">. 自打包程序通过测试及审核后放入私有仓库。</span><br><span class="line">注1: GCC保持默认的o2就好，不要修改CFLAG，以稳定为优先原则。</span><br><span class="line">注2: 勿用checkinstall直接打包。</span><br></pre></td></tr></table></figure>
<p><strong>四 日志管理规范</strong></p>
<p>4.1 系统日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 系统日志服务统一采用syslog-ng, 不应与rsyslogd混用</span><br><span class="line">. 针对不同日志类型, 存于不同的文件. 例如</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;auth.log      ---- 安全日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;kern.log      ---- 内核日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;user.log      ---- 用户日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;daemon.log    ---- 守护进程日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;misc.log      ---- </span><br><span class="line">&#x2F;var&#x2F;log&#x2F;cron.log      ---- 计划任务日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;syslog        ---- 系统日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;boot.log      ---- 引导日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;messages      ---- 所有日志</span><br><span class="line">. 系统级日志保留7天回滚, 服务级日志保留15天回滚, 并做定期检查</span><br></pre></td></tr></table></figure>
<p>4.2 操作日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 将所有ssh操作日志记录于文件, 方便系统管理员定位具体时间点的操作, 例如:</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;audit&#x2F;sysop   </span><br><span class="line">&#x2F;var&#x2F;log&#x2F;audit&#x2F;dba</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;audit&#x2F;root</span><br></pre></td></tr></table></figure>
<p><strong>五 安全管理规范</strong></p>
<p>5.1 .网络访问规则 业务模块类型不同策略组尽量不通，比如db类，前端类，下载类等等。（这样做的目的是尽量把权限控制死，减少黑客入口） ACL 外网策略默认入默认全关，针对访问需求开放 外网策略默认出全开 内网默认出和入均开放 不同vlan之间做绝对隔离 iptables 外网策略默认入全关，针对访问需求开放 外网策略默认出全开 内网默认出和入均开放 5.2 .程序监听端口 非特殊需求1024以下端口禁止使用，且定义为高危端口，若发现高危端口暴露公网则进行罚款警告 数据库端口和ssh进程端口严谨暴露外网 只使用内网访问的程序禁止使用0.0.0.0监听</p>
<p><strong>六 DB操作规范</strong></p>
<p>6.1 用户权限分级 业务账户 备份账户 管理账户 其他需求账户（主要指查询） 主从复制账户 6.2 修改db和数据前先备份,大型db变更可以先停掉slave,待变更完成后再开启。 6.3 禁止擅自修改数据，若要修改需要提交需求 6.4 db 备份必须要有异地备份,db 需要打开binlog,备份需要有slave. 6.5 不确定情况请找DBA 确认。</p>
<p><strong>七 版本更新规范</strong></p>
<p>7.4 版本更新checklist模板制定 每次版本更新需要针对实际操作情况根据checklist模板进行细化制定。 7.5 禁止开发登陆服务器进行更新和修改操作（特殊情况请说明） 7.6 未经测试通过或者有严重bug的版本禁止对外发布，如需要发布，需要项目PM和QA确认。 7.7 临时修改发布内容视实际情况自行评估，原则上确定的内容临时调整不接受。</p>
<p><strong>八 故障处理规范 .</strong></p>
<p>大型故障处理 .服务器故障 .业务故障处 主要是checklist,需要包含故障现象，分析问题过程和故障处理恢复过程</p>
<p>**九 监控规范 **</p>
<p>业务上线后必须马上加入监控,此作为上线的其他步骤同等重要 监控中必需监控指标项必须加入（之前lorin有提供文档）</p>
<p><strong>十 其他变更规范</strong></p>
<p>搬迁，开服，新功能上线等都属于变更范畴。</p>
<p>10.1 新服开放 需要提前主动搜集运营需求，进行资源的准备和规划 需要准备变更所需checklist 10.2 搬迁及升级 升级扩容预案 搬迁方案的准备 回滚方案准备 数据一致性校验 以上方案在操作前需要提交运维团队进行评审确认。</p>
]]></content>
  </entry>
  <entry>
    <title>Docker常用命令</title>
    <url>/2021/06/29/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th>COMMAND</th>
<th>DESC</th>
</tr>
</thead>
<tbody>
<tr>
<td>查看</td>
<td></td>
</tr>
<tr>
<td>docker images</td>
<td>列出所有镜像(images)</td>
</tr>
<tr>
<td>docker ps</td>
<td>列出正在运行的容器(containers)</td>
</tr>
<tr>
<td>docker ps -a</td>
<td>列出所有的容器</td>
</tr>
<tr>
<td>docker pull centos</td>
<td>下载centos镜像</td>
</tr>
<tr>
<td>docker top ‘container’</td>
<td>查看容器内部运行程序</td>
</tr>
<tr>
<td>容器</td>
<td></td>
</tr>
<tr>
<td>docker exec -it 容器ID sh</td>
<td>进入容器</td>
</tr>
<tr>
<td>docker stop ‘container’</td>
<td>停止一个正在运行的容器，‘container’可以是容器ID或名称</td>
</tr>
<tr>
<td>docker start ‘container’</td>
<td>启动一个已经停止的容器</td>
</tr>
<tr>
<td>docker restart ‘container’</td>
<td>重启容器</td>
</tr>
<tr>
<td>docker rm ‘container’</td>
<td>删除容器</td>
</tr>
<tr>
<td>docker run -i -t -p :80 LAMP /bin/bash</td>
<td>运行容器并做http端口转发</td>
</tr>
<tr>
<td>docker exec -it ‘container’ /bin/bash</td>
<td>进入ubuntu类容器的bash</td>
</tr>
<tr>
<td>docker exec -it /bin/sh</td>
<td>进入alpine类容器的sh</td>
</tr>
<tr>
<td>docker rm <code>docker ps -a -q</code></td>
<td>删除所有已经停止的容器</td>
</tr>
<tr>
<td>docker kill $(docker ps -a -q)</td>
<td>杀死所有正在运行的容器，$()功能同``</td>
</tr>
<tr>
<td>镜像</td>
<td></td>
</tr>
<tr>
<td>docker build -t wp-api .</td>
<td>构建1个镜像,-t(镜像的名字及标签) wp-api(镜像名) .(构建的目录)</td>
</tr>
<tr>
<td>docker run -i -t wp-api</td>
<td>-t -i以交互伪终端模式运行,可以查看输出信息</td>
</tr>
<tr>
<td>docker run -d -p 80:80 wp-api</td>
<td>镜像端口 -d后台模式运行镜像</td>
</tr>
<tr>
<td>docker rmi [image-id]</td>
<td>删除镜像</td>
</tr>
<tr>
<td>docker rmi $(docker images -q)</td>
<td>删除所有镜像</td>
</tr>
<tr>
<td>docker rmi $(sudo docker images --filter “dangling=true” -q --no-trunc)</td>
<td>删除无用镜像</td>
</tr>
<tr>
<td>docker run --help</td>
<td>帮助</td>
</tr>
</tbody>
</table>
<blockquote>
<p>更多命令查看Docker 命令大全 | 菜鸟教程</p>
</blockquote>
<h3 id="场景二：下载镜像并直接运行">场景二：下载镜像并直接运行</h3>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">docker run  --name ubuntu -it ubuntu bash </span><br><span class="line">docker cp dd ubuntu:tmp/ #复制文件dd 到容器的/tmp 目录</span><br><span class="line">Ctrl-p Ctrl-q  #退出</span><br></pre></td></tr></table></figure>
<h3 id="场景三：修改镜像，并保存到私有仓库">场景三：修改镜像，并保存到私有仓库</h3>
<blockquote>
<p>期望结果:在ubuntu 镜像中添加 apache，将新的镜像保存到私有仓库中</p>
</blockquote>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">docker exec -it ubuntu bash </span><br><span class="line">apt-<span class="keyword">get</span> update</span><br><span class="line">apt-<span class="keyword">get</span> install apache2</span><br><span class="line">Ctrl-p Ctrl-q  #退出</span><br><span class="line">docker commit -a "mir355" -m "ubuntu add apache2" &#123;ID&#125;  private/ubuntu_apache:v1   #保存镜像</span><br><span class="line">docker stop ubuntu </span><br><span class="line">docker rm ubuntu</span><br><span class="line">docker run -i -t --name apache2 -p <span class="number">8080</span>:<span class="number">80</span> private/ubuntu_apache:v1 /bin/bash</span><br><span class="line">/etc/init.d/apache2 start</span><br><span class="line">Ctrl-p Ctrl-q  #退出</span><br><span class="line">#通过 docker tag重命名镜像，使之与registry匹配</span><br><span class="line">docker tag private/ubuntu_apache:v1 <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">5000</span>/private/ubuntu_apache:v1</span><br><span class="line">#保存到私有仓库</span><br><span class="line">docker push <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">5000</span>/private/ubuntu_apache:v1</span><br><span class="line">curl http:<span class="comment">//127.0.0.1:5000/v2/_catalog</span></span><br><span class="line"></span><br><span class="line">#下载镜像</span><br><span class="line">docker pull <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">5000</span>/private/ubuntu_apache:v1</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>mapreduce、spark的样例</title>
    <url>/2021/06/25/mapreduce%E3%80%81spark%E7%9A%84%E6%A0%B7%E4%BE%8B/</url>
    <content><![CDATA[<h1>mapreduce、spark的样例</h1>
<h3 id="指定队列和优先级">指定队列和优先级</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi \</span><br><span class="line">-D mapreduce.job.queuename=bf_yarn_pool.development \</span><br><span class="line">-D mapreduce.job.priority=VERY_HIGH 5 5</span><br></pre></td></tr></table></figure>
<h3 id="TestDFSIO">TestDFSIO</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u hdfs hadoop jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.0.1-tests.jar  TestDFSIO \</span><br><span class="line">-D mapreduce.job.queuename&#x3D;bf_yarn_pool.production \</span><br><span class="line">-D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark \</span><br><span class="line">-D mapreduce.output.fileoutputformat.compress&#x3D;false \</span><br><span class="line">-write -nrFiles 10 -fileSize 1000</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Windows Server 2019和Win10 安装 Linux 子系统</title>
    <url>/2021/05/18/Windows%20Server%202019%E5%92%8CWin10%20%E5%AE%89%E8%A3%85%20Linux%20%E5%AD%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p><strong>Windows Server</strong>版本 安装参考  <a href="https://blog.51cto.com/professor/2411436" target="_blank" rel="noopener">https://blog.51cto.com/professor/2411436</a></p>
<p>注意：如果先安装的ConEmu ，可能没有bash  ，刷新一系default tasks，如下图：</p>
<p><img src="/images/image-20210518154415530.png" alt="image-20210518154415530"></p>
<p>win10版本 参考之前写的文档：</p>
<p><strong>Win10 Subsystem Linux : Ubuntu 的root密码</strong></p>
<p>每次开机都有一个新的root密码。我们可以在终端输入命令 sudo passwd，</p>
<p>然后输入当前用户的密码，enter，终端会提示我们输入新的密码并确认，</p>
<p>此时的密码就是root新密码。修改成功后，输入命令 su root，再输入新的密码就ok了。</p>
<p><strong>win10 linux子系统设置默认用户</strong></p>
<p><a href="https://blog.csdn.net/ijiabao520/article/details/79285041" target="_blank" rel="noopener">https://blog.csdn.net/ijiabao520/article/details/79285041</a></p>
<p>lxrun是旧版的，不可使用了。</p>
<p>在cmd终端输入：</p>
<p>$ ubuntu1804 config --default-user root</p>
<p>window终端推荐 ConEmu  超级好用</p>
<p><a href="https://conemu.github.io/en/" target="_blank" rel="noopener">https://conemu.github.io/en/</a></p>
<p>添加到右键菜单</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;REUSE &#x2F;REUSE -run &#123;Bash::bash&#125; -cur_console:n</span><br></pre></td></tr></table></figure>
<p><img src="/images/clipboard.png" alt="img"></p>
<p><strong>win10下Linux子系统开启ssh服务</strong></p>
<p>Windows10开启Ubuntu子系统简易步骤  (现在不需要开启开发者模式了)</p>
<p><a href="https://zhuanlan.zhihu.com/p/34133795" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34133795</a></p>
<p><strong>开启win10下Ubuntu子系统的SSH服务</strong></p>
<p><a href="https://blog.csdn.net/zhouzme/article/details/81087837" target="_blank" rel="noopener">https://blog.csdn.net/zhouzme/article/details/81087837</a></p>
<p>打开/etc/rc.local文件加入：</p>
<p>/etc/init.d/ssh start</p>
]]></content>
  </entry>
  <entry>
    <title>运维和发展线路</title>
    <url>/2021/05/18/%E8%BF%90%E7%BB%B4%E5%92%8C%E5%8F%91%E5%B1%95%E7%BA%BF%E8%B7%AF/</url>
    <content><![CDATA[<h2 id="运维和发展的一个线路">运维和发展的一个线路</h2>
<ul>
<li>1.搭建服务（部署并运行起来）</li>
<li>2.用好服务（监控、管理、优化）</li>
<li>3.自动化（服务直接的关联和协同工作）</li>
<li>4.产品设计（如何设计一个监控系统）</li>
</ul>
<p><strong>云计算的核心竞争力是运维！</strong></p>
<p>系统架构师（偏管理）：网络  系统  数据库  开发  云计算  自动化  运维管理  服务管理 项目管理  测试  业务<br>
专注于某一领域<br>
解决方案架构师</p>
<h2 id="运维工作内容分类">运维工作内容分类</h2>
<ul>
<li>监控运维（7x24运维值班、故障处理）</li>
<li>应用运维（业务熟悉、服务部署、业务部署、版本管理、灰度发布、应用监控）</li>
<li>安全运维（整体的安全方案、规范、漏洞监测、安全防护等）</li>
<li>系统运维（架构层面的分布式缓存、分布式文件系统、日志收集、环境规划（测试、开发、生产）、架构设计、性能优化）</li>
<li>基础服务运维（包含运维开发）（内部DNS、负载均衡、系统监控、资产管理、运维平台）</li>
<li>基础设施运维（系统初始化、网络维护）</li>
<li>机房运维（负责设备上下架、巡检、报修、硬件监控）</li>
</ul>
<p>阿里云:<br>
SLB  LVS + Tengine（Nginx）<br>
ECS  KVM</p>
<h2 id="运维标准化">运维标准化</h2>
<p><strong>物理设备层面：</strong><br>
1.服务器标签化、设备负责人、设备采购详情、设备摆放标准<br>
2.网络划分、远程控制卡、网卡端口<br>
3.服务器机型、硬盘、内存统一，根据业务分类<br>
4.资产命名规范、编号规范、类型规范<br>
5.监控标准</p>
<p><strong>操作系统层面</strong><br>
1.操作系统版本<br>
2.系统初始化（配置DNS、NTP、内核参数调优）<br>
3.基础Agent配备（Zabbix agent、logstash agent、salt minion）<br>
4.系统监控标准（CPU、内存、硬盘、网络、进程）</p>
<p><strong>应用服务层面：</strong><br>
1.Web服务器选型（nginx、Apache）<br>
2.进程启动用户、端口监听规范、日志收集规范（访问日志、错误日志、运行日志）<br>
3.配置管理（配置文件规范、脚本规范）<br>
4.架构规范（Nginx+keepalived、LVS+keepalived等等）<br>
5.部署规范（位置、包命名等）</p>
<p><strong>运维操作层面：</strong><br>
1.机房巡检流程（周期、内容、保修流程）<br>
2.业务部署流程（先测试、后生产。回滚）<br>
3.故障处理流程（紧急处理、故障升级、重大故障处理）<br>
4.工作日志流程（如何编写工作日志）<br>
5.业务上线流程（1.项目发起人 2.系统安装 3.部署nginx 4.解析域名 5.测试 6.加监控）<br>
6.业务下线流程（谁发起，数据如何处理）<br>
7.运维安全规范（密码复杂度、更改周期、VPN使用规范、服务登陆规范、rm命令的参数写在最后面）</p>
<p>标准化：规范化  流程化  文档化</p>
<p>目标：文档化</p>
<h2 id="运维自动化发展-工具化">运维自动化发展-工具化</h2>
<p><strong>工具化：</strong></p>
<ul>
<li>1.shell脚本（功能性（流程）脚本、检查性、报表性）</li>
<li>2.开源工具：zabbix、elkstack、saltstack、cobbler</li>
</ul>
<p><strong>目标：</strong></p>
<ul>
<li>1.促进标准化的实施</li>
<li>2.将重复的操作简单化</li>
<li>3.将多次操作流程化</li>
<li>4.减少人为操作的低效和降低故障率</li>
</ul>
<p>工具化和标准化是好搭档</p>
<p><strong>痛点：</strong></p>
<ul>
<li>1.你至少要ssh到服务器执行，可能出错</li>
<li>2.多个脚本有执行顺序的时候，可能出错</li>
<li>3.权限不好管理，日志没法统计</li>
<li>4.无法避免手工操作</li>
</ul>
<p><strong>例子：</strong></p>
<p>比如某天我们要对一个数据库从库进行版本停机升级。那么要求评估：<br>
停机影响：<br>
3：00  晚上有定时任务连接该数据库，做数据报表统计</p>
<ul>
<li>1.凌晨3：00 我们所有系统的定时任务有哪些crontab</li>
<li>2.这些crontab哪些要连接我们要停止的从库</li>
<li>3.哪些可以停，哪些不能停（修改到主库），哪些可以后补</li>
<li>4.这些需要后补的脚本哪个业务的，谁加的，什么时候加的</li>
</ul>
<h2 id="运维自动化发展-web化">运维自动化发展-web化</h2>
<p>运维平台<br>
例子：Job管理平台</p>
<ul>
<li>1.做成web界面</li>
<li>2.权限控制</li>
<li>3.日志记录</li>
<li>4.弱化流程</li>
<li>5.不用ssh到服务器，减少人为操作造成故障 Web ssh</li>
</ul>
<p>DNSWeb管理 bind-DLZ<br>
负载均衡Web管理<br>
Job管理平台<br>
监控平台  zabbix<br>
操作系统安装平台</p>
<h2 id="运维自动化发展-服务化（API）">运维自动化发展-服务化（API）</h2>
<ul>
<li>DNSWeb管理 bind-DLZ  dns-api</li>
<li>负载均衡Web管理     slb-api</li>
<li>Job管理平台    job-api</li>
<li>监控平台  zabbix  zabbix-api</li>
<li>操作系统安装平台   cobbler-api</li>
<li>部署平台    deploy-api</li>
<li>配置管理    saltstack-api</li>
</ul>
<p><strong>智能化实现</strong></p>
<ul>
<li>1.调用cobbler-api安装操作系统</li>
<li>2.调用saltstack-api进行系统初始化</li>
<li>3.调用dns-api解析主机名</li>
<li>4.调用zabbix-api将该新上线机器加上监控</li>
<li>5.再次调用saltstack-api部署软件（安装nginx+php）</li>
<li>6.调用deploy-api将当前版本的代码部署到服务器上</li>
<li>7.调用test-api 测试当前服务器运行是否正常</li>
<li>8.调用slb-api 将该节点加入集群</li>
</ul>
<h2 id="运维自动化发展-智能化">运维自动化发展-智能化</h2>
<p>运维自动化发展层级：</p>
<ul>
<li>标准化、工具化</li>
<li>Web化、平台化</li>
<li>服务化、API化</li>
<li>智能化</li>
</ul>
<p>智能化的自动化扩容、缩容、服务降级、故障自愈</p>
<p><strong>自动化扩容</strong><br>
1.zabbix触发Action<br>
触发条件和决策：</p>
<ul>
<li>1.当某个集群的访问量超过最大支撑量，比如10000</li>
<li>2.并持续5分钟</li>
<li>3.不是攻击</li>
<li>4.资源池有可用资源
<ul>
<li>当前网络带宽使用率</li>
<li>如果是公有云–钱够不够</li>
</ul>
</li>
<li>5.当前后端服务支撑量是否超过阈值  如果超过应该后端先扩容</li>
<li>6.数据库是否可以支撑当前并发</li>
<li>7.当前自动化扩展队列，是否有正在扩容的节点</li>
<li>其他业务相关的</li>
</ul>
<p>创建虚拟机之前，先判断Buffer是否有最近X小时已经存在之前已经移除的虚拟机，并查询软件版本是否和当前一致，如果一致，跳过234步，如果不一致，跳过23步</p>
<p>2.Openstack 创建虚拟机<br>
3.Saltstack 配置环境<br>
4.部署系统 部署当前代码<br>
5.测试服务是否可用(注意间隔和次数)<br>
6.加入集群<br>
7.通知(短信、邮件)</p>
<p><strong>自动化缩容</strong></p>
<ul>
<li>1.触发条件和决策</li>
<li>2.从集群中移除节点</li>
<li>3.通知</li>
<li>4.移除的节点存放于Buffer里面</li>
<li>5.Buffer里面超过1天的虚拟机，自动关闭，存放于xx区</li>
<li>6.xx区的虚拟机，超过7天的清理删除</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>CDH Disk Balancer 磁盘数据均衡</title>
    <url>/2021/04/30/CDH%20Disk%20Balancer%20%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E5%9D%87%E8%A1%A1/</url>
    <content><![CDATA[<h3 id="CDH-Disk-Balancer-磁盘数据均衡">CDH Disk Balancer 磁盘数据均衡</h3>
<p>1.设置dfs.disk.balancer.enabled  为true , 可以单个datanode设置，重启单个datanode生效。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.disk.balancer.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<img src="/images/image-20210429175922996.png" alt="image-20210429175922996" >
<p>1.创建均衡任务并生成计划任务配置文件</p>
<p>sudo -u hdfs hdfs diskbalancer -plan cdh85-73</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@cdh85-73 tmp]# sudo -u hdfs hdfs diskbalancer -plan cdh85-73</span><br><span class="line">21/04/29 17:02:45 INFO planner.GreedyPlanner: Starting plan for Node : cdh85-73:9867</span><br><span class="line">21/04/29 17:02:45 INFO planner.GreedyPlanner: Disk Volume set f4da1504-1dfc-42d0-ac61-020dd7b02620 Type : DISK plan completed.</span><br><span class="line">21/04/29 17:02:45 INFO planner.GreedyPlanner: Compute Plan for Node : cdh85-73:9867 took 7 ms </span><br><span class="line">21/04/29 17:02:46 INFO command.Command: Writing plan to:</span><br><span class="line">21/04/29 17:02:46 INFO command.Command: /system/diskbalancer/2021-Apr-29-17-02-45/cdh85-73.plan.json</span><br><span class="line">Writing plan to:</span><br><span class="line">/system/diskbalancer/2021-Apr-29-17-02-45/cdh85-73.plan.json</span><br></pre></td></tr></table></figure>
<p>注意： 这个路径是HDFS的路径，不是本地路径</p>
<p>2.查看配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@cdh85-73 tmp]# hdfs dfs -ls  /system/diskbalancer/2021-Apr-29-16-45-51</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup     192586 2021-04-29 16:45 /system/diskbalancer/2021-Apr-29-16-45-51/cdh85-73.before.json</span><br><span class="line">-rw-r--r--   3 hdfs supergroup       4546 2021-04-29 16:45 /system/diskbalancer/2021-Apr-29-16-45-51/cdh85-73.plan.json</span><br></pre></td></tr></table></figure>
<p>3.启动均衡任务</p>
<p>sudo -u hdfs hdfs diskbalancer -execute /system/diskbalancer/2021-Apr-29-16-51-42/cdh85-73.plan.json</p>
<p>注意：这里执行后  立马结束了  看不到效果，实际上是生效的</p>
<p>4.查看状态</p>
<p>sudo -u hdfs hdfs diskbalancer -query cdh85-73</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@cdh85-73 tmp]# sudo -u hdfs hdfs diskbalancer -query cdh85-73</span><br><span class="line">21/04/29 17:42:24 INFO command.Command: Executing "query plan" command.</span><br><span class="line">Plan File: /system/diskbalancer/2021-四月-29-16-40-04/cdh85-73.plan.json</span><br><span class="line">Plan ID: bb2087b0d6dfbe65835e2831836fd814cf70e605</span><br><span class="line">Result: PLAN_UNDER_PROGRESS</span><br></pre></td></tr></table></figure>
<p>完成后的状态，耗时大约20几个Hours</p>
]]></content>
  </entry>
  <entry>
    <title>chia挖矿</title>
    <url>/2021/04/27/chia%E6%8C%96%E7%9F%BF/</url>
    <content><![CDATA[<h4 id="用家里的老电脑做一个测试-，p了一块k32的盘">用家里的老电脑做一个测试 ，p了一块k32的盘</h4>
<p>矿池的具体接入教程描述得非常浅显易懂了，这里我就不做重复搬运了，直接参考 <a href="https://www.hpool.com/help/tutorial/Chia%E6%8C%96%E7%9F%BF%E6%95%99%E7%A8%8B" target="_blank" rel="noopener">Chia挖矿教程</a></p>
<p>linux P盘</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup &#x2F;opt&#x2F;chia-plotter&#x2F;chia-plotter-linux-amd64 -action plotting -plotting-fpk 0x85f80829a93d960313a99ca5482703fea2caae1d07db589344e76eba135db14c8f70d08dadc991805ae917d61626fd8d -plotting-ppk 0x970214947045bd1c6fbb0b3b3499dafab837eb26b58356504aea4d8ee19e9c5c064a5dfdad0cb5f7f047e0030f088a65 -plotting-n 1 -b 8000 -t &#x2F;data1&#x2F;chia -d &#x2F;opt&#x2F;chia  &gt;&gt; plots2.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<p>晒一下收益效果：</p>
<p><img src="/images/image-20210427101445602.png" alt="image-20210427101445602"></p>
<h4 id="相关资料：">相关资料：</h4>
<p><strong>谋杀SSD磁盘</strong></p>
<p><a href="https://www.expreview.com/78802.html" target="_blank" rel="noopener">https://www.expreview.com/78802.html</a></p>
<blockquote>
<p>生成一个K=32文件大概需要6.5小时，生成三个K33两个K32文件总共占了829GB的硬盘空间，而HDD的写入量是840.1GB，但SSD的读写非常厉害，整个P盘过程，SSD读取11.8TB，写入12.06TB，因为SSD的写入次数是有限的，把这个6TB的红盘P满虽然不至于把SSD写死，但磨损也很厉害。</p>
</blockquote>
<p><strong>阿里云Chia挖矿，很不划算</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/365806867" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/365806867</a></p>
<blockquote>
<p>0.1个币提现。因为真的是亏到姥姥家哈哈。4月17到4月24，一共300块钱。估计挖到0.1个币还要再等2天。</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>hadoop decommission 卡住</title>
    <url>/2021/04/27/hadoop%20decommission%20%E5%8D%A1%E4%BD%8F/</url>
    <content><![CDATA[<p>hadoop decommission 卡住</p>
<p>hadoop decommission一个节点Datanode，几万个block都同步过去了，但是唯独剩下2个block一直停留在哪，导致该节点几个小时也无法 下线。hadoop UI中显示在Under Replicated Blocks里面有2个块始终无法消除.</p>
<p>是一个hadoop的bug，<a href="https://issues.apache.org/jira/browse/HDFS-5579" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HDFS-5579</a></p>
<p>根据blockid 查找文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs fsck -blockId blk_2050561344</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>hdfs 如何实现退役节点快速下线</title>
    <url>/2021/04/21/hdfs%20%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E9%80%80%E5%BD%B9%E8%8A%82%E7%82%B9%E5%BF%AB%E9%80%9F%E4%B8%8B%E7%BA%BF/</url>
    <content><![CDATA[<p><strong>hdfs 如何实现退役节点快速下线（也就是退役节点上的数据块快速迁移）</strong></p>
<p>参考  <a href="https://www.cnblogs.com/jiangxiaoxian/p/9665588.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangxiaoxian/p/9665588.html</a></p>
<p>进度可在HDFS的50070可视化界面的Decommissioning处查看**</p>
<p>Under replicated blocks	： 有备份的blocks</p>
<p>Blocks with no live replicas	： 没有存活备份的blocks(存备份的datanode下线了)</p>
<p>Under Replicated Blocks In files under construction   ： 备份数不够的blocks</p>
<p>可调整集群参数，对退服进行调优，注意，更改参数需要重启服务。需要修改的参数如下：</p>
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>实例</strong></th>
<th><strong>参数类别</strong></th>
<th style="text-align:left"><strong>参数名称</strong></th>
<th><strong>默认值</strong></th>
<th><strong>修改值</strong></th>
<th><strong>参数含义</strong></th>
<th><strong>调整场景</strong></th>
<th><strong>是否可以默认值调整</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>balance/退服性能参数</td>
<td style="text-align:left">dfs.datanode.balance.bandwidthPerSec</td>
<td>20971520</td>
<td>209715200</td>
<td>【说明】每个DataNode可用于负载均衡的最大带宽量（每秒的字节数）。</td>
<td>balance-性能调优</td>
<td>不建议调整默认值</td>
</tr>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>balance/退服性能参数</td>
<td style="text-align:left">dfs.datanode.balance.max.concurrent.moves</td>
<td>5</td>
<td>30</td>
<td>允许在DataNode上进行负载均衡的最大线程数。</td>
<td></td>
<td>有必要调整</td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>balance/退服性能参数</td>
<td style="text-align:left">dfs.namenode.replication.max-streams</td>
<td>10</td>
<td>64</td>
<td>DataNode上复制线程的最大数。</td>
<td></td>
<td>C70默认值已调整为64，有必要继续调整</td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>balance/退服性能参数</td>
<td style="text-align:left">dfs.namenode.replication.max-streams-hard-limit</td>
<td>20</td>
<td>500</td>
<td>对DataNode上复制线程数的硬限制。</td>
<td></td>
<td>C70默认值已调整为128，有必要继续调整</td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>balance/退服性能参数</td>
<td style="text-align:left">dfs.namenode.replication.work.multiplier.per.iteration</td>
<td>10</td>
<td>500</td>
<td>高级属性。修改时需谨慎。该参数表示NameNode通过DataNode心跳发送这样一个命令列表时DataNode上并行开始的用于复制的块传输的总量。</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>HDFS</th>
<th>NameNode</th>
<th>运行-性能调优</th>
<th>dfs.namenode.handler.count</th>
<th>64</th>
<th>192</th>
<th>NameNode处理线程数</th>
<th>大集群，性能调优</th>
<th>可以调整/更耗内存</th>
</tr>
</thead>
<tbody>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>运行-性能调优</td>
<td>dfs.datanode.handler.count</td>
<td>8</td>
<td>24</td>
<td>DataNode处理线程数</td>
<td>大集群，性能调优</td>
<td>可以调整/更耗内存</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>HDFS</th>
<th>NameNode</th>
<th>运行-性能调优</th>
<th>ipc.server.read.threadpool.size</th>
<th>1</th>
<th>10</th>
<th>NameNode处理请求线程池大小</th>
<th>大集群，性能调优</th>
<th>可以调整/更耗内存</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>HDFS</th>
<th>DataNode</th>
<th>运行-性能调优</th>
<th>dfs.datanode.max.transfer.threads</th>
<th>4096</th>
<th>8192</th>
<th>与DataNode间传输数据的线程的最大数。</th>
<th>负载高集群，性能调优</th>
<th>C70默认值已调整</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="处理“没有实时副本的块”">处理“没有实时副本的块”</h3>
<h3 id="Dealing-with-‘Blocks-with-no-live-replicas’-in-the-HDFS">Dealing with ‘Blocks with no live replicas’ in the HDFS</h3>
<p>参考： <a href="https://piyushroutray.com/2019/06/04/dealing-with-blocks-with-no-live-replicas-in-the-hdfs/" target="_blank" rel="noopener">https://piyushroutray.com/2019/06/04/dealing-with-blocks-with-no-live-replicas-in-the-hdfs/</a></p>
]]></content>
  </entry>
  <entry>
    <title>xfs_repair 磁盘修复</title>
    <url>/2021/04/19/xfs_repair%20%E7%A3%81%E7%9B%98%E4%BF%AE%E5%A4%8D/</url>
    <content><![CDATA[<h3 id="1-现状">1.现状</h3>
<p>目前网上出现大量的主机输入输出错误，原因是由于主机文件系统损坏。一线人员大部分采用的是umont 和 mount的方式恢复，这种恢复方式不能真正修复已经损坏的文件系统，在后续使用过程中，仍然会再次出现主机端输入输出错误。</p>
<h3 id="2-需要修复的场景">2.需要修复的场景</h3>
<p>&lt;1&gt;.主机侧发现存在文件系统不可读写的情况，也可以通过查看主机端日志来确认是否有文件系统异常发生： xfs_force_shutdown 、I/O error <br>
&lt;2&gt;.出现异常停电，供电恢复正常，主机和阵列系统重起之后<br>
&lt;3&gt;.存储介质故障：出现LUN失效、RAID失效、以及IO超时或者出现慢盘，对慢盘进行更换，系统恢复正常之后<br>
&lt;4&gt;.传输介质故障：如光纤、网线等损坏等，数据传输链路断开后又恢复正常之后</p>
<h3 id="3-检查文件系统">3.检查文件系统</h3>
<p>注：检查文件系统必须保证将文件系统umount成功。<br>
在根目录下输入“xfs_check /dev/sdd（盘符）；echo $?”（注意：在执行 此命令之前，必须将文件系统umount，否则会出现警告信 “xfs_check: /dev/sdd contains a mounted and writable filesystem ”）敲回车键，查看命令执行返回值：0表示正常，其他为不正常，说明文件系统 损坏，需要修复。</p>
<h3 id="4-修复过程">4.修复过程</h3>
<p>注：修复时需要暂停主机侧的业务，umount 和 mount 无法修复文件系统 。</p>
<ol>
<li>先umount要修复的文件系统的分区</li>
<li>然后输入 “xfs_repair /dev/sdd（盘符）”执行修复命令。<br>
xfs_check /dev/sdd; echo $?<br>
A）如果为0===》成功修复。<br>
B) 如果不为0===》没有成功：请执行 <strong>xfs_repair –L /dev/sdd</strong> 命令，再执 行xfs_repair（反复多修复几次）</li>
</ol>
<h3 id="5-xfs常用命令">5.xfs常用命令</h3>
<p>xfs_admin: 调整 xfs 文件系统的各种参数<br>
xfs_copy: 拷贝 xfs 文件系统的内容到一个或多个目标系统（并行方式）<br>
xfs_db: 调试或检测 xfs 文件系统（查看文件系统碎片等）<br>
xfs_check: 检测 xfs 文件系统的完整性<br>
xfs_bmap: 查看一个文件的块映射<br>
xfs_repair: 尝试修复受损的 xfs 文件系统<br>
xfs_fsr: 碎片整理<br>
xfs_quota: 管理 xfs 文件系统的磁盘配额<br>
xfs_metadump: 将 xfs 文件系统的元数据 (metadata) 拷贝到一个文件中<br>
xfs_mdrestore: 从一个文件中将元数据 (metadata) 恢复到 xfs 文件系统<br>
xfs_growfs: 调整一个 xfs 文件系统大小（只能扩展）<br>
xfs_logprint: print the log of an XFS filesystem<br>
xfs_mkfile: create an XFS file<br>
xfs_info: expand an XFS filesystem<br>
xfs_ncheck: generate pathnames from i-numbers for XFS<br>
xfs_rtcp: XFS realtime copy command<br>
xfs_freeze: suspend access to an XFS filesystem<br>
xfs_io: debug the I/O path of an XFS filesystem</p>
<h3 id="6-具体应用：">6.具体应用：</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看文件块状况: xfs_bmap -v sarubackup.tar.bz2 </span><br><span class="line">查看磁盘碎片状况: xfs_db -c frag -r &#x2F;dev&#x2F;sda1 </span><br><span class="line">文件碎片整理: xfs_fsr sarubackup.tar.bz2 </span><br><span class="line">磁盘碎片整理: xfs_fsr &#x2F;dev&#x2F;sda1</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>DolphinScheduler集群部署</title>
    <url>/2021/04/08/DolphinScheduler%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>官网部署文档</p>
<p><a href="https://dolphinscheduler.apache.org/zh-cn/docs/latest/user_doc/cluster-deployment.html" target="_blank" rel="noopener">https://dolphinscheduler.apache.org/zh-cn/docs/latest/user_doc/cluster-deployment.html</a></p>
<h1>1、基础软件安装(必装项请自行安装)</h1>
<ul>
<li>PostgreSQL (8.2.15+) or MySQL (5.7系列) : 两者任选其一即可, 如MySQL则需要JDBC Driver 5.1.47+</li>
<li><a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">JDK</a> (1.8+) : 必装，请安装好后在/etc/profile下配置 JAVA_HOME 及 PATH 变量</li>
<li>ZooKeeper (3.4.6+) ：必装</li>
<li>Hadoop (2.6+) or MinIO ：选装，如果需要用到资源上传功能，可以选择上传到Hadoop or MinIO上</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">注意：DolphinScheduler本身不依赖Hadoop、Hive、Spark,仅是会调用他们的Client，用于对应任务的提交。</span><br></pre></td></tr></table></figure>
<h1>2、下载二进制tar.gz包</h1>
<ul>
<li>请下载最新版本的后端安装包至服务器部署目录,比如创建 /tmp/dolphinscheduler 做为安装部署目录，下载地址： <a href="https://dolphinscheduler.apache.org/zh-cn/download/download.html" target="_blank" rel="noopener">下载</a>，下载后上传tar包到该目录中，并进行解压</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建部署目录,部署目录请不要创建在/root、/home等高权限目录 </span></span><br><span class="line">mkdir -p /tmp/app/dolphinscheduler;  #安装文件别放到/tmp/dolphinscheduler , 这是特殊路径  创建文件时会用到的临时路径</span><br><span class="line">cd /tmp/app/dolphinscheduler;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压缩</span></span><br><span class="line">tar -zxvf apache-dolphinscheduler-incubating-1.3.5-dolphinscheduler-bin.tar.gz -C /opt/dolphinscheduler;</span><br><span class="line"></span><br><span class="line">mv apache-dolphinscheduler-incubating-1.3.5-dolphinscheduler-bin  dolphinscheduler-bin</span><br></pre></td></tr></table></figure>
<h1>3、创建部署用户和hosts映射</h1>
<ul>
<li>在<strong>所有</strong>部署调度的机器上创建部署用户，并且一定要配置sudo免密。假如我们计划在ds1,ds2,ds3,ds4这4台机器上部署调度，首先需要在每台机器上都创建部署用户</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建用户需使用root登录，设置部署用户名，请自行修改，后面以dolphinscheduler为例</span></span><br><span class="line">useradd dolphinscheduler;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置用户密码，请自行修改，后面以dolphinscheduler123为例</span></span><br><span class="line">echo "dolphinscheduler123" | passwd --stdin dolphinscheduler</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置sudo免密</span></span><br><span class="line">echo 'dolphinscheduler  ALL=(ALL)  NOPASSWD: NOPASSWD: ALL' &gt;&gt; /etc/sudoers</span><br><span class="line">sed -i 's/Defaults    requirett/#Defaults    requirett/g' /etc/sudoers</span><br><span class="line"> 注意：</span><br><span class="line"> - 因为是以 sudo -u &#123;linux-user&#125; 切换不同linux用户的方式来实现多租户运行作业，所以部署用户需要有 sudo 权限，而且是免密的。</span><br><span class="line"> - 如果发现/etc/sudoers文件中有"Default requiretty"这行，也请注释掉</span><br><span class="line"> - 如果用到资源上传的话，还需要在`HDFS或者MinIO`上给该部署用户分配读写的权限</span><br></pre></td></tr></table></figure>
<h1>4、配置hosts映射和ssh打通及修改目录权限</h1>
<ul>
<li>
<p><em>备注：当然 通过<code>sshpass -p xxx sudo scp -r /etc/hosts $ip:/etc/</code>就可以省去输入密码了</em></p>
<blockquote>
<p>centos下sshpass的安装：</p>
<ol>
<li>
<p>先安装epel</p>
<p>yum install -y epel-release</p>
<p>yum repolist</p>
</li>
<li>
<p>安装完成epel之后，就可以按照sshpass了</p>
<p>yum install -y sshpass</p>
</li>
</ol>
</blockquote>
</li>
<li>
<p>在ds1上，切换到部署用户并配置ssh本机免密登录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> su dolphinscheduler;</span><br><span class="line"></span><br><span class="line">ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa</span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>注意：<em>正常设置后，dolphinscheduler用户在执行命令<code>ssh localhost</code> 是不需要再输入密码的</em></p>
<ul>
<li>
<p>在ds1上，配置部署用户dolphinscheduler ssh打通到其他待部署的机器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">su dolphinscheduler;</span><br><span class="line">for ip in bigdata-9.baofoo.cn bigdata-8.baofoo.cn bigdata-7.baofoo.cn ;   </span><br><span class="line">do</span><br><span class="line">    sshpass -p dolphinscheduler123 ssh-copy-id $ip  </span><br><span class="line">done</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>在ds1上，修改目录权限，使得部署用户对dolphinscheduler-bin目录有操作权限</p>
</li>
</ul>
<h1>5、数据库初始化</h1>
<ul>
<li>进入数据库，默认数据库是PostgreSQL，如选择MySQL的话，后续需要添加mysql-connector-java驱动包到DolphinScheduler的lib目录下，这里以MySQL为例</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -h192.168.xx.xx -P3306 -uroot -p</span><br></pre></td></tr></table></figure>
<ul>
<li>进入数据库命令行窗口后，执行数据库初始化命令，设置访问账号和密码。<strong>注: {user} 和 {password} 需要替换为具体的数据库用户名和密码</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE dolphinscheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON dolphinscheduler.* TO &#39;&#123;user&#125;&#39;@&#39;%&#39; IDENTIFIED BY &#39;&#123;password&#125;&#39;;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON dolphinscheduler.* TO &#39;&#123;user&#125;&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;&#123;password&#125;&#39;;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>创建表和导入基础数据</p>
<ul>
<li>修改 conf 目录下 datasource.properties 中的下列配置</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi conf/datasource.properties</span><br></pre></td></tr></table></figure>
<ul>
<li>如果选择 MySQL，请注释掉 PostgreSQL 相关配置(反之同理), 还需要手动添加 [<a href="https://downloads.mysql.com/archives/c-j/" target="_blank" rel="noopener"> mysql-connector-java 驱动 jar </a>] 包到 lib 目录下，这里下载的是mysql-connector-java-5.1.47.jar，然后正确配置数据库连接相关信息</li>
</ul>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#postgre</span></span><br><span class="line"><span class="comment">#spring.datasource.driver-class-name=org.postgresql.Driver</span></span><br><span class="line"><span class="comment">#spring.datasource.url=jdbc:postgresql://localhost:5432/dolphinscheduler</span></span><br><span class="line"><span class="comment"># mysql</span></span><br><span class="line"><span class="meta">spring.datasource.driver-class-name</span>=<span class="string">com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="meta">spring.datasource.url</span>=<span class="string">jdbc:mysql://10.0.20.107:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true    </span></span><br><span class="line"><span class="meta">spring.datasource.username</span>=<span class="string">cs_yangz						需要修改为上面的&#123;user&#125;值</span></span><br><span class="line"><span class="meta">spring.datasource.password</span>=<span class="string">xxx						需要修改为上面的&#123;password&#125;值</span></span><br></pre></td></tr></table></figure>
<ul>
<li>修改并保存完后，执行 script 目录下的创建表及导入基础数据脚本</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh script/create-dolphinscheduler.sh</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1>6、修改运行参数</h1>
<ul>
<li>
<p>修改 conf/env 目录下的 <code>dolphinscheduler_env.sh</code> 环境变量(以相关用到的软件都安装在/opt/soft下为例)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">   export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</span><br><span class="line">   export HADOOP_CONF_DIR=/etc/hadoop/conf</span><br><span class="line"><span class="meta">   #</span><span class="bash"><span class="built_in">export</span> SPARK_HOME1=/opt/soft/spark1</span></span><br><span class="line">   export SPARK_HOME2=/opt/cloudera/parcels/CDH/lib/spark</span><br><span class="line">   export PYTHON_HOME=/usr/bin/python</span><br><span class="line">   export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera</span><br><span class="line">   export HIVE_HOME=/opt/cloudera/parcels/CDH/lib/hive</span><br><span class="line"><span class="meta">   #</span><span class="bash"><span class="built_in">export</span> FLINK_HOME=/opt/soft/flink</span></span><br><span class="line"><span class="meta">   #</span><span class="bash"><span class="built_in">export</span> DATAX_HOME=/opt/soft/datax/bin/datax.py</span></span><br><span class="line">   export PATH=$HADOOP_HOME/bin:$SPARK_HOME2/bin:$PYTHON_HOME:$JAVA_HOME/bin:$HIVE_HOME/bin:$PATH</span><br><span class="line">  </span><br><span class="line">`注: 这一步非常重要,例如 JAVA_HOME 和 PATH 是必须要配置的，没有用到的可以忽略或者注释掉`</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>修改一键部署配置文件 <code>conf/config/install_config.conf</code>中的各参数，特别注意以下参数的配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> postgresql or mysql</span></span><br><span class="line">dbtype="mysql"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> db config</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> db address and port</span></span><br><span class="line">dbhost="10.0.20.107:3306"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> db username</span></span><br><span class="line">username="cs_yangz"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> database name</span></span><br><span class="line">dbname="dolphinscheduler"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> db passwprd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> NOTICE: <span class="keyword">if</span> there are special characters, please use the \ to escape, <span class="keyword">for</span> example, `[` escape to `\[`</span></span><br><span class="line">password="***"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> zk cluster</span></span><br><span class="line">zkQuorum="10.0.19.130:2181,10.0.19.131:2181,10.0.19.132:2181"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: the target installation path <span class="keyword">for</span> dolphinscheduler, please not config as the same as the current path (<span class="built_in">pwd</span>)</span></span><br><span class="line">installPath="/opt/dolphinscheduler"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> deployment user</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: the deployment user needs to have sudo privileges and permissions to operate hdfs. If hdfs is enabled, the root directory needs to be created by itself</span></span><br><span class="line">deployUser="dolphinscheduler"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> alert config</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mail server host</span></span><br><span class="line">mailServerHost="smtp.qiye.163.com"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> mail server port</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> note: Different protocols and encryption methods correspond to different ports, when SSL/TLS is enabled, make sure the port is correct.</span></span><br><span class="line">mailServerPort="25"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> sender</span></span><br><span class="line">mailSender="hadooper@baofoo.com"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> user</span></span><br><span class="line">mailUser="hadooper@baofoo.com"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> sender password</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> note: The mail.passwd is email service authorization code, not the email login password.</span></span><br><span class="line">mailPassword="***"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> TLS mail protocol support</span></span><br><span class="line">starttlsEnable="true"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> SSL mail protocol support</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> only one of TLS and SSL can be <span class="keyword">in</span> the <span class="literal">true</span> state.</span></span><br><span class="line">sslEnable="false"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">note: sslTrust is the same as mailServerHost</span></span><br><span class="line">sslTrust="smtp.qiye.163.com"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> resource storage <span class="built_in">type</span>：HDFS,S3,NONE</span></span><br><span class="line">resourceStorageType="HDFS"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> resourceStorageType is HDFS，defaultFS write namenode address，HA you need to put core-site.xml and hdfs-site.xml <span class="keyword">in</span> the conf directory.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> S3，write S3 address，HA，<span class="keyword">for</span> example ：s3a://dolphinscheduler，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note，s3 be sure to create the root directory /dolphinscheduler</span></span><br><span class="line">defaultFS="hdfs://bigdata-7.baofoo.cn:8020"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> resourceStorageType is S3, the following three configuration is required, otherwise please ignore</span></span><br><span class="line">s3Endpoint="http://192.168.xx.xx:9010"</span><br><span class="line">s3AccessKey="xxxxxxxxxx"</span><br><span class="line">s3SecretKey="xxxxxxxxxx"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> resourcemanager HA <span class="built_in">enable</span>, please <span class="built_in">type</span> the HA ips ; <span class="keyword">if</span> resourcemanager is single, make this value empty</span></span><br><span class="line">yarnHaIps="bigdata-7.baofoo.cn"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> resourcemanager HA <span class="built_in">enable</span> or not use resourcemanager, please skip this value setting; If resourcemanager is single, you only need to replace yarnIp1 to actual resourcemanager hostname.</span></span><br><span class="line">singleYarnIp="bigdata-7.baofoo.cn"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> resource store on HDFS/S3 path, resource file will store to this hadoop hdfs path, self configuration, please make sure the directory exists on hdfs and have <span class="built_in">read</span> write permissions。/dolphinscheduler is recommended</span></span><br><span class="line">resourceUploadPath="/dolphinscheduler"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> who have permissions to create directory under HDFS/S3 root path</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: <span class="keyword">if</span> kerberos is enabled, please config hdfsRootUser=</span></span><br><span class="line">hdfsRootUser="hdfs"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kerberos config</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> whether kerberos starts, <span class="keyword">if</span> kerberos starts, following four items need to config, otherwise please ignore</span></span><br><span class="line">kerberosStartUp="false"</span><br><span class="line"><span class="meta">#</span><span class="bash"> kdc krb5 config file path</span></span><br><span class="line">krb5ConfPath="$installPath/conf/krb5.conf"</span><br><span class="line"><span class="meta">#</span><span class="bash"> keytab username</span></span><br><span class="line">keytabUserName="hdfs-mycluster@ESZ.COM"</span><br><span class="line"><span class="meta">#</span><span class="bash"> username keytab path</span></span><br><span class="line">keytabPath="$installPath/conf/hdfs.headless.keytab"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> api server port</span></span><br><span class="line">apiServerPort="12345"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> install hosts</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: install the scheduled hostname list. If it is pseudo-distributed, just write a pseudo-distributed hostname</span></span><br><span class="line">ips="bigdata-7.baofoo.cn,bigdata-8.baofoo.cn,bigdata-9.baofoo.cn"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ssh port, default 22</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: <span class="keyword">if</span> ssh port is not default, modify here</span></span><br><span class="line">sshPort="22"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> run master machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: list of hosts hostname <span class="keyword">for</span> deploying master</span></span><br><span class="line">masters="bigdata-9.baofoo.cn,bigdata-8.baofoo.cn"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> run worker machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> note: need to write the worker group name of each worker, the default value is <span class="string">"default"</span></span></span><br><span class="line">workers="bigdata-7.baofoo.cn:default,bigdata-8.baofoo.cn:default,bigdata-9.baofoo.cn:default"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> run alert machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> note: list of machine hostnames <span class="keyword">for</span> deploying alert server</span></span><br><span class="line">alertServer="bigdata-8.baofoo.cn"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> run api machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> note: list of machine hostnames <span class="keyword">for</span> deploying api server</span></span><br><span class="line">apiServers="bigdata-9.baofoo.cn"</span><br></pre></td></tr></table></figure>
<p><em>特别注意：</em></p>
<ul>
<li>如果需要用资源上传到Hadoop集群功能， 并且Hadoop集群的NameNode 配置了 HA的话 ，需要开启 HDFS类型的资源上传，同时需要将Hadoop集群下的core-site.xml和hdfs-site.xml复制到/opt/dolphinscheduler/conf，非NameNode HA跳过次步骤</li>
</ul>
<h1>7、一键部署</h1>
<ul>
<li>
<p><strong>切换到部署用户dolphinscheduler</strong>，然后执行一键部署脚本</p>
<p><code>sh install.sh</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">注意：</span><br><span class="line">第一次部署的话，在运行中第3步&#96;3,stop server&#96;出现5次以下信息，此信息可以忽略</span><br><span class="line">sh: bin&#x2F;dolphinscheduler-daemon.sh: No such file or directory</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>脚本完成后，会启动以下5个服务，使用<code>jps</code>命令查看服务是否启动(<code>jps</code>为<code>java JDK</code>自带)</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MasterServer         ----- master服务</span><br><span class="line">WorkerServer         ----- worker服务</span><br><span class="line">LoggerServer         ----- logger服务</span><br><span class="line">ApiApplicationServer ----- api服务</span><br><span class="line">AlertServer          ----- alert服务</span><br></pre></td></tr></table></figure>
<p>如果以上服务都正常启动，说明自动部署成功</p>
<p>部署成功后，可以进行日志查看，日志统一存放于logs文件夹内</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">logs&#x2F;</span><br><span class="line">   ├── dolphinscheduler-alert-server.log</span><br><span class="line">   ├── dolphinscheduler-master-server.log</span><br><span class="line">   |—— dolphinscheduler-worker-server.log</span><br><span class="line">   |—— dolphinscheduler-api-server.log</span><br><span class="line">   |—— dolphinscheduler-logger-server.log</span><br></pre></td></tr></table></figure>
<h1>8、登录系统</h1>
<ul>
<li>访问前端页面地址,接口ip(自行修改) <a href="http://bigdata-9.baofoo.cn:12345/dolphinscheduler" target="_blank" rel="noopener">http://bigdata-9.baofoo.cn:12345/dolphinscheduler</a></li>
</ul>
<p>默认的用户是<code>admin</code>，默认的密码是<code>dolphinscheduler123</code></p>
<h4 id="修改-JVM-参数">修改 JVM 参数</h4>
<ul>
<li>两个文件</li>
<li>/bin/dolphinscheduler-daemon.sh</li>
<li>/scripts/dolphinscheduler-daemon.sh</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export DOLPHINSCHEDULER_OPTS&#x3D;&quot;-server -Xmx16g -Xms1g -Xss512k -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:LargePageSizeInBytes&#x3D;128m -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction&#x3D;70&quot;</span><br></pre></td></tr></table></figure>
<p>复制代码</p>
<ul>
<li>一键部署</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su  dolphinscheduler</span><br><span class="line"></span><br><span class="line">sh install.sh</span><br></pre></td></tr></table></figure>
<p>复制代码</p>
<ul>
<li>进程检查</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su dolphinscheduler;jps;</span><br></pre></td></tr></table></figure>
<p>复制代码</p>
<p><img src="../images/56bd851533e862beaedd00b76e45409e.png" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 一键停止</span><br><span class="line">sh .&#x2F;bin&#x2F;stop-all.sh</span><br><span class="line"># 一键开启</span><br><span class="line">sh .&#x2F;bin&#x2F;start-all.sh</span><br><span class="line"># 启停master</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh start master-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh stop master-server</span><br><span class="line"># 启停worker</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line"># 启停api-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh start api-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh stop api-server</span><br><span class="line"># 启停logger</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line"># 启停alert</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh stop alert-server</span><br></pre></td></tr></table></figure>
<p><strong>遇到的坑：</strong></p>
<p>1.安装文件别放到/tmp/dolphinscheduler    ， 这是特殊路径</p>
<p>2.用root用户启动服务导致一系列问题，要用新建的用户操作所有的一切。</p>
]]></content>
  </entry>
  <entry>
    <title>ClickHouse 部署</title>
    <url>/2021/04/06/ClickHouse%20%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h1>ClickHouse 部署</h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install yum-utils</span><br><span class="line">sudo rpm --import https:&#x2F;&#x2F;repo.clickhouse.tech&#x2F;CLICKHOUSE-KEY.GPG</span><br><span class="line">sudo yum-config-manager --add-repo https:&#x2F;&#x2F;repo.clickhouse.tech&#x2F;rpm&#x2F;stable&#x2F;x86_64</span><br></pre></td></tr></table></figure>
<p>如果您想使用最新的版本，请用<code>testing</code>替代<code>stable</code>(我们只推荐您用于测试环境)。<code>prestable</code>有时也可用。</p>
<p>然后运行命令安装：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install clickhouse-server clickhouse-client</span><br></pre></td></tr></table></figure>
<p>启动服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo systemctl start clickhouse-server</span><br><span class="line">sudo systemctl stop clickhouse-server</span><br><span class="line">sudo systemctl status clickhouse-server</span><br></pre></td></tr></table></figure>
<p>快速开始</p>
<p>clickhouse-client -m</p>
]]></content>
  </entry>
  <entry>
    <title>快速试用 DolphinScheduler</title>
    <url>/2021/03/31/%E5%BF%AB%E9%80%9F%E8%AF%95%E7%94%A8%20DolphinScheduler/</url>
    <content><![CDATA[<p>快速试用 DolphinScheduler</p>
<h5 id="1、下载源码-zip-包">1、下载源码 zip 包</h5>
<ul>
<li>请下载最新版本的源码包并进行解压</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建源码存放目录</span></span><br><span class="line">mkdir -p /opt/soft/dolphinscheduler;</span><br><span class="line">cd /opt/soft/dolphinscheduler;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过wget下载源码包</span></span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/incubator/dolphinscheduler/1.3.5/apache-dolphinscheduler-incubating-1.3.5-src.zip</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过curl下载源码包</span></span><br><span class="line">curl -O https://mirrors.tuna.tsinghua.edu.cn/apache/incubator/dolphinscheduler/1.3.5/apache-dolphinscheduler-incubating-1.3.5-src.zip</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压缩</span></span><br><span class="line">unzip apache-dolphinscheduler-incubating-1.3.5-src.zip</span><br><span class="line"></span><br><span class="line">mv apache-dolphinscheduler-incubating-1.3.5-src-release dolphinscheduler-src</span><br></pre></td></tr></table></figure>
<h5 id="2、安装并启动服务">2、安装并启动服务</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd dolphinscheduler-src&#x2F;docker&#x2F;docker-swarm</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure>
<h5 id="3、登录系统">3、登录系统</h5>
<p>访问前端页面： <a href="http://bigdata-3.baofoo.cn:12345/dolphinscheduler" target="_blank" rel="noopener">http://bigdata-3.baofoo.cn:12345/dolphinscheduler</a></p>
<p>默认的用户是<code>admin</code>，默认的密码是<code>dolphinscheduler123</code></p>
<p>停止所有容器:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker-compose stop</span><br></pre></td></tr></table></figure>
<p>停止所有容器并移除所有容器，网络和存储卷:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker-compose down -v</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>conda笔记</title>
    <url>/2021/03/30/conda%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><strong>Conda常用命令整理</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda --version #查看conda版本，验证是否安装</span><br><span class="line"></span><br><span class="line">conda update conda #更新至最新版本，也会更新其它相关包</span><br><span class="line"></span><br><span class="line">conda update --all #更新所有包</span><br><span class="line"></span><br><span class="line">conda update package_name #更新指定的包</span><br><span class="line"></span><br><span class="line">conda create -n env_name package_name #创建名为env_name的新环境，并在该环境下安装名为package_name 的包，可以指定新环境的版本号，例如：conda create -n python2 python=python2.7 numpy pandas，创建了python2环境，python版本为2.7，同时还安装了numpy pandas包</span><br><span class="line"></span><br><span class="line">conda activate env_name #切换至env_name环境</span><br><span class="line"></span><br><span class="line">conda deactivate #退出环境</span><br><span class="line"></span><br><span class="line">conda info -e #显示所有已经创建的环境</span><br><span class="line"></span><br><span class="line">conda create --name new_env_name --clone old_env_name #复制old_env_name为new_env_name</span><br><span class="line"></span><br><span class="line">conda remove --name env_name –all #删除环境</span><br><span class="line"></span><br><span class="line">conda list #查看所有已经安装的包</span><br><span class="line"></span><br><span class="line">conda install package_name #在当前环境中安装包</span><br><span class="line"></span><br><span class="line">conda install --name env_name package_name #在指定环境中安装包</span><br><span class="line"></span><br><span class="line">conda remove -- name env_name package #删除指定环境中的包</span><br><span class="line"></span><br><span class="line">conda remove package #删除当前环境中的包</span><br><span class="line"></span><br><span class="line">conda create -n tensorflow_env tensorflow</span><br><span class="line"></span><br><span class="line">conda activate tensorflow_env #conda 安装tensorflow的CPU版本</span><br><span class="line"></span><br><span class="line">conda create -n tensorflow_gpuenv tensorflow-gpu</span><br><span class="line"></span><br><span class="line">conda activate tensorflow_gpuenv #conda安装tensorflow的GPU版本</span><br><span class="line"></span><br><span class="line">conda env remove -n env_name #采用第10条的方法删除环境失败时，可采用这种方法</span><br></pre></td></tr></table></figure>
<p><strong>Conda常用命令整理</strong></p>
<p><a href="https://blog.csdn.net/menc15/article/details/71477949/" target="_blank" rel="noopener">https://blog.csdn.net/menc15/article/details/71477949/</a></p>
]]></content>
  </entry>
  <entry>
    <title>spark打包提交python程序</title>
    <url>/2021/03/10/spark%E6%89%93%E5%8C%85%E6%8F%90%E4%BA%A4python%E7%A8%8B%E5%BA%8F/</url>
    <content><![CDATA[<p><strong>spark-python版本依赖与三方模块方案</strong></p>
<p>（1）使用conda创建python虚拟环境、安装第三方库</p>
<p>假设虚拟环境是pyspark_py36，安装位置是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;root&#x2F;miniconda3&#x2F;envs&#x2F;pyspark_py36</span><br><span class="line">此处省略1w个字。</span><br></pre></td></tr></table></figure>
<p>安装的第三方库是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source activate pyspark_py36</span><br><span class="line"></span><br><span class="line">pip install pandas </span><br><span class="line">pip install sklearn</span><br><span class="line">pip install lightgbm</span><br></pre></td></tr></table></figure>
<p>其他省略1w字。</p>
<p>（2）打包整个虚拟环境</p>
<p>进入虚拟环境目录，压缩整个文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;root&#x2F;miniconda3&#x2F;envs&#x2F;</span><br><span class="line">zip -r -9 -q pyspark_py36.zip pyspark_py36&#x2F;</span><br></pre></td></tr></table></figure>
<p>压缩后得到压缩包pyspark_py36.zip。</p>
<p>（3）将压缩是虚拟环境上传到hdfs</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 上传</span><br><span class="line">hdfs dfs –put pyspark_py36.zip &#x2F;tmp&#x2F;</span><br></pre></td></tr></table></figure>
<p>（4）新建pyspark程序</p>
<p>没什么好说的，就是普通的pyspark程序，简单的例子如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试刚才安装的第三方库是否正常导入</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"></span><br><span class="line">dates = pd.date_range(<span class="string">'20130101'</span>,periods=<span class="number">6</span>)</span><br><span class="line">print(dates)</span><br><span class="line">print(platform.python_version())</span><br></pre></td></tr></table></figure>
<p>(5) 提交pyspark程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo -u yarn spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--queue bf_yarn_pool.development \</span><br><span class="line">--num-executors 1 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--archives hdfs://ns1/tmp/pyspark_py36.zip#pyenv \</span><br><span class="line">--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=pyenv/pyspark_py36/bin/python \</span><br><span class="line">hdfs://ns1/tmp/test_spark_env.py</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>vi模式下查找和替换</title>
    <url>/2021/03/10/vi%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%9F%A5%E6%89%BE%E5%92%8C%E6%9B%BF%E6%8D%A2/</url>
    <content><![CDATA[<p>linux基础命令之：vi模式下查找和替换</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">一、查找</span><br><span class="line">查找命令</span><br><span class="line">/pattern&lt;Enter&gt; ：向下查找pattern匹配字符串</span><br><span class="line">?pattern&lt;Enter&gt;：向上查找pattern匹配字符串</span><br><span class="line">使用了查找命令之后，使用如下两个键快速查找：</span><br><span class="line">n：按照同一方向继续查找</span><br><span class="line">N：按照反方向查找</span><br><span class="line">pattern是需要匹配的字符串，例如：</span><br><span class="line">/hello&lt;Enter&gt;      #查找hello</span><br><span class="line">/hello&lt;Enter&gt;    #查找hello单词（注意前后的空格）</span><br><span class="line">除此之外，pattern还可以使用一些特殊字符，包括（/、^、$、*、.），其中前三个这两个是vi与vim通用的，“/”为转义字符。</span><br><span class="line">/^hello&lt;Enter&gt;    #查找以hello开始的行</span><br><span class="line"><span class="meta">/hello$</span><span class="bash">&lt;Enter&gt;    <span class="comment">#查找以hello结束的行</span></span></span><br><span class="line">//^hello&lt;Enter&gt;    #查找^hello字符串</span><br><span class="line"> </span><br><span class="line">二、替换</span><br><span class="line">1.:[range]s/pattern/string/[c,e,g,i]</span><br><span class="line">range     指的是范围，1,7 指从第一行至第七行，1,$ 指从第一行至最后一行，也就是整篇文章，也可以 % 代表。 % 是目前编辑的文章，# 是前一次编辑的文章。</span><br><span class="line">pattern     就是要被替换掉的字串，可以用 regexp 来表示。</span><br><span class="line">string     将 pattern 由 string 所取代。</span><br><span class="line">c     confirm，每次替换前会询问。</span><br><span class="line">e     不显示 error。</span><br><span class="line">g     globe，不询问，整行替换。</span><br><span class="line">i     ignore 不分大小写。</span><br><span class="line">I     ignore 大小写敏感。</span><br><span class="line"> </span><br><span class="line">2.基本替换</span><br><span class="line">:s/lantian/sky/         #替换当前行第一个 lantian 为 sky</span><br><span class="line">:s/lantian/sky/g     #替换当前行所有 lantian 为 sky</span><br><span class="line">:n,$s/lantian/sky/     #替换第 n 行开始到最后一行中每一行的第一个 lantian 为 sky</span><br><span class="line">:n,$s/lantian/sky/g     #替换第 n 行开始到最后一行中每一行所有 lantian 为 sky</span><br><span class="line"><span class="meta">#</span><span class="bash">（n 为数字，若 n 为 .，表示从当前行开始到最后一行）</span></span><br><span class="line">:%s/lantian/sky/        #（等同于 :g/lantian/s//sky/） 替换每一行的第一个 lantian 为 sky</span><br><span class="line">:%s/lantian/sky/g    #（等同于 :g/lantian/s//sky/g） 替换每一行中所有 lantian 为 sky</span><br><span class="line">可以使用 #或+ 作为分隔符，此时中间出现的 / 不会作为分隔符</span><br><span class="line">:s#lantian/#sky/#         替换当前行第一个 lantian/ 为 sky/</span><br><span class="line">:%s+/oradata/apras/+/user01/apras1+ （</span><br><span class="line">使用+ 来 替换 / ）： /oradata/apras/替换成/user01/apras1/</span><br><span class="line"> </span><br><span class="line">3.删除文本中的^M</span><br><span class="line">问题描述：对于换行，window下用回车换行（0A0D）来表示，linux下是回车（0A）来表示。这样，将window上的文件拷到unix上用时，总会有个^M，请写个用在unix下的过滤windows文件的换行符（0D）的shell或c程序。</span><br><span class="line">使用命令：cat filename1 | tr -d “^V^M” &gt; newfile;</span><br><span class="line">使用命令：sed -e “s/^V^M//” filename &gt; outputfilename</span><br><span class="line">需要注意的是在1、2两种方法中，^V和^M指的是Ctrl+V和Ctrl+M。你必须要手工进行输入，而不是粘贴。</span><br><span class="line">在vi中处理：首先使用vi打开文件，然后按ESC键，接着输入命令：</span><br><span class="line">:%s/^V^M//</span><br><span class="line">:%s/^M$//g</span><br><span class="line">如果上述方法无用，则正确的解决办法是：</span><br><span class="line">tr -d “/r” &lt; src &gt;dest</span><br><span class="line">tr -d “/015″ dest</span><br><span class="line">strings A&gt;B</span><br><span class="line"> </span><br><span class="line">4.其它用法</span><br><span class="line">:s/str1/str2/          #用字符串 str2 替换行中首次出现的字符串 str1</span><br><span class="line">:s/str1/str2/g         #用字符串 str2 替换行中所有出现的字符串 str1</span><br><span class="line">:.,$ s/str1/str2/g     #用字符串 str2 替换正文当前行到末尾所有出现的字符串 str1</span><br><span class="line">:1,$ s/str1/str2/g     #用字符串 str2 替换正文中所有出现的字符串 str1</span><br><span class="line">:g/str1/s//str2/g      #功能同上</span><br><span class="line"> </span><br><span class="line">5.g的总结说明</span><br><span class="line">从上述替换命令可以看到：</span><br><span class="line">g 放在命令末尾，表示对指定行的搜索字符串的每次出现进行替换；不加 g，表示只对指定行的搜索字符串的首次出现进行替换；</span><br><span class="line">g 放在命令开头，表示对正文中所有包含搜索字符串的行进行替换操作。</span><br><span class="line">也就是说命令的开始可以添加影响的行，如果为g表示对所有行；命令的结尾可以使用g来表示是否对每一行的所有字符串都有影响。</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>用python进行量化投资</title>
    <url>/2021/03/09/%E7%94%A8python%E8%BF%9B%E8%A1%8C%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/</url>
    <content><![CDATA[<p><strong>用python进行量化投资：</strong></p>
<p>自己编写:NumPy+pandas+Matplotlib+…</p>
<p>在线平台:聚宽、优矿、米筐、Quantopian、…</p>
<p>开源框架:RQAlpha、QUANTAXIS</p>
<p>聚宽 <a href="https://www.joinquant.com/" target="_blank" rel="noopener">https://www.joinquant.com/</a></p>
<p>优矿 <a href="https://uqer.datayes.com/" target="_blank" rel="noopener">https://uqer.datayes.com/</a></p>
<p>rqalpha <a href="https://rqalpha.readthedocs.io/zh_CN/latest/index.html" target="_blank" rel="noopener">https://rqalpha.readthedocs.io/zh_CN/latest/index.html</a></p>
<p>quantaxis <a href="https://doc.yutiansut.com/" target="_blank" rel="noopener">https://doc.yutiansut.com/</a></p>
]]></content>
  </entry>
  <entry>
    <title>yarn webUI 看不到日志</title>
    <url>/2021/03/03/yarn%20webUI%20%E7%9C%8B%E4%B8%8D%E5%88%B0%E6%97%A5%E5%BF%97/</url>
    <content><![CDATA[<p>yarn webUI 看不到日志</p>
<p>解决办法：</p>
<p>Resource Manager webUI No logs available for container</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. Please verify once if you could collect logs using "yarn logs" command as below?</span><br><span class="line">&#123;&#123;</span><br><span class="line">yarn logs -applicationId &lt;appID&gt; -appOwner &lt;user&gt;</span><br><span class="line">&#125;&#125;</span><br><span class="line"></span><br><span class="line">2.  please verify the below directories if they have right permissions</span><br><span class="line">&#123;&#123;</span><br><span class="line">hdfs dfs -ls /user/history/done</span><br><span class="line">hdfs dfs -ls  /user/history/done_intermediate</span><br><span class="line">&#125;&#125;</span><br><span class="line"></span><br><span class="line">3. Please verify if "Enable Log Aggregation" has been enabled or not</span><br><span class="line">&#123;&#123;</span><br><span class="line"><span class="meta">ClouderaManager--&gt;</span><span class="bash">Yarn--&gt;Configurations--&gt;<span class="string">"Enable Log Aggregation"</span></span></span><br><span class="line">&#125;&#125;</span><br><span class="line"></span><br><span class="line">4. Verify the permissions for /tmp/logs as below,</span><br><span class="line">&#123;&#123;</span><br><span class="line">hadoop fs -chown mapred:hadoop /tmp/logs</span><br><span class="line">hadoop fs -chown -R :hadoop /tmp/logs/*</span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>用Hue中的Oozie执行Impala Sheel脚本</title>
    <url>/2021/03/02/%E7%94%A8Hue%E4%B8%AD%E7%9A%84Oozie%E6%89%A7%E8%A1%8CImpala%20Sheel%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<h3 id="用Hue中的Oozie执行Impala-Sheel脚本">用Hue中的Oozie执行Impala Sheel脚本</h3>
<p>在Oozie中不能像执行hive SQL那样直接执行impala SQL脚本。目前没有Impala操作，因此你必须使用调用impala-shell的shell操作。调用impala-shell的shell脚本中还必须包含设置PYTHON EGGS位置的环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export PYTHON_EGG_CACHE=.python-eggs   </span><br><span class="line"></span><br><span class="line">impala-shell -i 172.20.15.10:21000 -u hpt -l --auth_creds_ok_in_clear --ldap_password_cmd='echo -n ***' -q 'SET request_pool=development;'</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>hudi</title>
    <url>/2021/03/02/hudi/</url>
    <content><![CDATA[<p><strong>注意：<strong>目前Hudi使用的是</strong>hadoop2.7.3</strong>版本，CDH6.3.0 环境使用的是<strong>hadoop3.0.0</strong>， 所以在打包的时候需要加上**-Dhadoop.version=3.0.0** 参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean install -DskipTests -DskipITs -Dcheckstyle.skip&#x3D;true -Drat.skip&#x3D;true -Dhadoop.version&#x3D;3.0.0</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo -u yarn spark-shell  \</span><br><span class="line">  --queue bf_yarn_pool.development \</span><br><span class="line">  --packages org.apache.spark:spark-avro_2.11:2.4.0 \</span><br><span class="line">  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \</span><br><span class="line">  --jars `ls /opt/hudi/packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-*.*.*-SNAPSHOT.jar`</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>用FsImage查找hadoop集群小文件</title>
    <url>/2021/02/20/%E7%94%A8FsImage%E6%9F%A5%E6%89%BEhadoop%E9%9B%86%E7%BE%A4%E5%B0%8F%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /tmp</span><br><span class="line">hdfs dfsadmin -fetchImage ./tmp_meta</span><br><span class="line">hdfs oiv -i ./tmp_meta -o ./fsimage.csv -p Delimited</span><br><span class="line">hdfs dfs -mkdir -p /tmp/hdfs_metadata/fsimage</span><br><span class="line">hdfs dfs -copyFromLocal ./fsimage.csv /tmp/hdfs_metadata/fsimage</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> hdfs_meta_temp;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> hdfs_meta_temp (<span class="keyword">path</span> <span class="keyword">string</span> ,</span><br><span class="line">repl <span class="built_in">int</span> ,</span><br><span class="line">modification_time <span class="keyword">string</span> ,accesstime <span class="keyword">string</span> ,</span><br><span class="line">preferredblocksize <span class="built_in">int</span> ,blockcount <span class="keyword">double</span>,</span><br><span class="line">filesize <span class="keyword">double</span> ,nsquota <span class="built_in">int</span> ,</span><br><span class="line">dsquota <span class="built_in">int</span> ,</span><br><span class="line">permission <span class="keyword">string</span> ,username <span class="keyword">string</span> ,groupname <span class="keyword">string</span>)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> location <span class="string">'/tmp/hdfs_metadata/fsimage/'</span> ;</span><br><span class="line"></span><br><span class="line"><span class="comment">--将临时表转换为Impala的 Parquet表</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> hdfs_meta</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hdfs_meta <span class="keyword">stored</span> <span class="keyword">as</span> parquet <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">path</span>,</span><br><span class="line">repl,</span><br><span class="line"><span class="keyword">cast</span>(<span class="keyword">concat</span>(modification_time, <span class="string">' :00'</span>) <span class="keyword">as</span> <span class="built_in">timestamp</span>) modification_time,</span><br><span class="line"><span class="keyword">cast</span>(<span class="keyword">concat</span>(accesstime,<span class="string">':00'</span>) <span class="keyword">as</span> <span class="built_in">timestamp</span>) accesstime,</span><br><span class="line">preferredblocksize,</span><br><span class="line">blockcount,</span><br><span class="line">filesize,nsquota,dsquota,permission,username,groupname</span><br><span class="line"><span class="keyword">from</span> hdfs_meta_temp;</span><br></pre></td></tr></table></figure>
<p>instr(path,’/’,1,2)这两个参数主要表示指定统计的HDFS目录以及目录钻取深度，instr()函数中的最后一个参数即为目录钻取深度</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line"><span class="comment">--concat('/',split_part(path,'/',2))  basepath,</span></span><br><span class="line"><span class="comment">--concat('/',split_part(path,'/',2),'/',split_part(path,'/',3))  basepath,</span></span><br><span class="line"><span class="comment">--concat('/',split_part(path,'/',2),'/',split_part(path,'/',3),'/',split_part(path,'/',4))  basepath,</span></span><br><span class="line"><span class="keyword">concat</span>(<span class="string">'/'</span>,split_part(<span class="keyword">path</span>,<span class="string">'/'</span>,<span class="number">2</span>),<span class="string">'/'</span>,split_part(<span class="keyword">path</span>,<span class="string">'/'</span>,<span class="number">3</span>),<span class="string">'/'</span>,split_part(<span class="keyword">path</span>,<span class="string">'/'</span>,<span class="number">4</span>),<span class="string">'/'</span>,split_part(<span class="keyword">path</span>,<span class="string">'/'</span>,<span class="number">5</span>))  basepath,</span><br><span class="line"><span class="keyword">sum</span>(blockcount) blockcounts,</span><br><span class="line"><span class="keyword">cast</span>(<span class="keyword">sum</span>(filesize)/<span class="number">1024</span>/<span class="number">1024</span>/<span class="number">1024</span> <span class="keyword">as</span> <span class="built_in">decimal</span>(<span class="number">18</span>,<span class="number">2</span>) ) filesizes,</span><br><span class="line"><span class="keyword">count</span>(*) file_nums,</span><br><span class="line"><span class="keyword">cast</span>(<span class="keyword">count</span>(*)/<span class="keyword">sum</span>(blockcount) <span class="keyword">as</span> <span class="built_in">decimal</span>(<span class="number">18</span>,<span class="number">2</span>) ) <span class="keyword">as</span> avg_block ,</span><br><span class="line"><span class="keyword">cast</span>(<span class="keyword">sum</span>(filesize)/<span class="keyword">count</span>(*)/<span class="number">1024</span> <span class="keyword">as</span> <span class="built_in">decimal</span>(<span class="number">18</span>,<span class="number">2</span>) ) <span class="keyword">AS</span> avg_filesize</span><br><span class="line"><span class="keyword">FROM</span> tmp.hdfs_meta </span><br><span class="line"><span class="keyword">where</span> <span class="keyword">instr</span>(<span class="keyword">path</span>,<span class="string">'/'</span>,<span class="number">1</span>,<span class="number">4</span>)&gt;<span class="number">0</span></span><br><span class="line"><span class="comment">--and strleft(path, instr(path,'/',1,4)-1)='/user/hive/warehouse'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> basepath  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> file_nums <span class="keyword">desc</span>, avg_filesize </span><br><span class="line"><span class="keyword">limit</span> <span class="number">200</span></span><br></pre></td></tr></table></figure>
<p><strong>总结</strong></p>
<p>如上SQL的统计分析可以看到有三个比较重要的统计指标file_nums、blockcounts和avg_filesize。通过这三个指标进行小文件分析，进行如下分析：</p>
<p>如果file_nums/blockcounts的值越大且avg_filesize越小则说明该HDFS或Hive表的小文件越多。</p>
<p><strong>方法二、</strong></p>
<p>使用Sqoop脚本将Hive元数据中关于Hive库和表的信息抽取的Hive中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">-D mapred.job.queue.name=bf_yarn_pool.development \</span><br><span class="line">--connect "jdbc:mysql://10.0.20.107:3306/baofoo_hive_2" \</span><br><span class="line">--username cs_yangz \</span><br><span class="line">--password *** \</span><br><span class="line">--query 'select c.NAME,c.DB_LOCATION_URI,a.TBL_NAME,a.OWNER,a.TBL_TYPE,b.LOCATION from TBLS a,SDS b,DBS c where a.SD_ID=b.SD_ID and a.DB_ID=c.DB_ID and $CONDITIONS' \</span><br><span class="line">--fields-terminated-by ',' \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--hive-database default \</span><br><span class="line">--target-dir /tmp/hive_tables_temp \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table hive_tables_temp \</span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>hdfs性能测试</title>
    <url>/2021/01/19/hdfs%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h2 id="hdfs性能测试">hdfs性能测试</h2>
<p>hadoop自带TestDFSIO测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;tmp</span><br><span class="line">sudo -u hdfs hadoop jar \</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.0.1-tests.jar TestDFSIO \</span><br><span class="line">-D mapreduce.job.queuename&#x3D;bf_yarn_pool.production \</span><br><span class="line">-D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark \</span><br><span class="line">-D mapreduce.output.fileoutputformat.compress&#x3D;false \</span><br><span class="line">-write -nrFiles 10 -fileSize 1000</span><br></pre></td></tr></table></figure>
<p>–结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:             Date &amp; time: Tue Jan 19 15:29:04 CST 2021</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:         Number of files: 10</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:  Total MBytes processed: 10000</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:       Throughput mb&#x2F;sec: 23.96</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:  Average IO rate mb&#x2F;sec: 32.37</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:   IO rate std deviation: 29.51</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:      Test exec time sec: 68</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:</span><br></pre></td></tr></table></figure>
<p>结果说明：</p>
<blockquote>
<p>Total MBytes processed ： 总共需要写入的数据量 ＝＝》 256*1000</p>
<p>Throughput mb/sec ：总共需要写入的数据量/（每个map任务实际写入数据的执行时间之和（这个时间会远小于Test exec time sec））＝＝》256000/(map1写时间+map2写时间+…)</p>
<p>Average IO rate mb/sec ：（每个map需要写入的数据量/每个map任务实际写入数据的执行时间）之和/任务数＝＝》(1000/map1写时间＋1000/map2写时间+…)/256，所以这个值跟上面一个值总是存在差异。</p>
<p>IO rate std deviation ：上一个值的标准差</p>
<p>Test exec time sec ：整个job的执行时间</p>
</blockquote>
<p>testDFSIO的参数如下：</p>
<table>
<thead>
<tr>
<th>read</th>
<th>读测试。执行该测试之前，需要先做write测试</th>
</tr>
</thead>
<tbody>
<tr>
<td>write</td>
<td>写测试</td>
</tr>
<tr>
<td>nfFiles</td>
<td>文件个数，默认为1</td>
</tr>
<tr>
<td>fileSize</td>
<td>文件大小，默认为1MB</td>
</tr>
<tr>
<td>resFile</td>
<td>结果文件名，默认为” TestDFSIO_results.log”</td>
</tr>
<tr>
<td>bufferSize</td>
<td>设置缓存大小，默认为1000000</td>
</tr>
<tr>
<td>clean</td>
<td>清理数据</td>
</tr>
<tr>
<td>seq</td>
<td>数据是否有序，默认无序</td>
</tr>
</tbody>
</table>
<p><strong>备注</strong>：</p>
<p>如果不到/tmp目录执行 会报TestDFSIO_results.log没有写入权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ava.io.FileNotFoundException: TestDFSIO_results.log (Permission denied)</span><br><span class="line">	at java.io.FileOutputStream.open0(Native Method)</span><br><span class="line">	at java.io.FileOutputStream.open(FileOutputStream.java:270)</span><br><span class="line">	at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213)</span><br><span class="line">	at org.apache.hadoop.fs.TestDFSIO.analyzeResult(TestDFSIO.java:1068)</span><br><span class="line">	at org.apache.hadoop.fs.TestDFSIO.run(TestDFSIO.java:891)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)</span><br><span class="line">	at org.apache.hadoop.fs.TestDFSIO.main(TestDFSIO.java:742)</span><br></pre></td></tr></table></figure>
<p>如果不关闭压缩，会报part-00000不存在，因为默认启用了snappy压缩 ，文件是part-00000.snappy</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: &#x2F;tmp&#x2F;benchmark&#x2F;io_write&#x2F;part-00000</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:85)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:728)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>HDFS ACL权限设置</title>
    <url>/2021/01/13/HDFS%20ACL%E6%9D%83%E9%99%90%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<h1>HDFS ACL权限设置</h1>
<p>今天主要给大家说一下HDFS文件权限的问题。当一个<code>普通用户</code>去访问<code>HDFS文件</code>时，可能会报<code>Permission denied</code>的错误。那么你会怎么做呢？</p>
<p>像修改linux文件似的，可能的做法有：</p>
<ul>
<li>修改文件所有者</li>
<li>直接将文件赋予全部的权限，即rwx权限。</li>
</ul>
<p>上面的做法虽然可以达到目的，但是相对来说对权限的把握不是很精准，不适用于生产环境。</p>
<p><strong>本文主要讲解HDFS的ACL(Access Control List)权限，通过hdfs超级用户，来为普通用户分配权限。</strong></p>
<h3 id="一、背景"><strong>一、背景</strong></h3>
<p>如下图所示，</p>
<p><img src="/images/image-20210113155315046.png" alt="image-20210113155315046"></p>
<p>目录没有权限，所以创建失败了。</p>
<p>这里就用到了HDFS的ACL权限设置。</p>
<h3 id="二、前提条件"><strong>二、前提条件</strong></h3>
<p>需要确定<code>hdfs-site.xml</code>文件的两个配置项为<code>true</code>：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.permissions.enabled&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">    &lt;value&gt;true&lt;/</span>value&gt;</span><br><span class="line">&lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp">&lt;property&gt;</span></span><br><span class="line"><span class="regexp">    &lt;name&gt;dfs.namenode.acls.enabled&lt;/</span>name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">true</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">&lt;/</span>property&gt;</span><br></pre></td></tr></table></figure>
<h3 id="三、语法"><strong>三、语法</strong></h3>
<h4 id="1-setfacl"><strong>1. setfacl</strong></h4>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">Usage: hdfs dfs -setfacl -R|[--<span class="keyword">set</span> &lt;acl_spec&gt; &lt;path&gt;]</span><br></pre></td></tr></table></figure>
<p>设置文件和目录的访问控制列表（ACL）。</p>
<p>选项：</p>
<ul>
<li>-b: 删除基本ACL条目以外的所有条目。保留用户，组和其他条目以与权限位兼容。</li>
<li>-k: 删除默认ACL。default</li>
<li>-R: 以递归方式将操作应用于所有文件和目录。<strong>常用。</strong></li>
<li>-m: 修改ACL。新条目将添加到ACL，并保留现有条目。<strong>常用。</strong></li>
<li>-x: 删除指定的ACL条目。保留其他ACL条目。<strong>常用。</strong></li>
<li>–set: 完全替换ACL，丢弃所有现有条目。 acl_spec必须包含用户，组和其他条目，以便与权限位兼容。</li>
<li>acl_spec: 逗号分隔的ACL条目列表。</li>
<li>path: 要修改的文件或目录。</li>
</ul>
<p>示例：</p>
<ul>
<li>hdfs dfs -setfacl -m user:xy_hpt:rw- /user/hive/warehouse/file_record</li>
<li>hdfs dfs -setfacl -x user:hadoop /file</li>
<li>hdfs dfs -setfacl -b /file</li>
<li>hdfs dfs -setfacl -k /dir</li>
<li>hdfs dfs -setfacl --set user::rw-,user:hadoop:rw-,group::r–,other::r-- /file</li>
<li>hdfs dfs -setfacl -R -m user:hadoop:r-x /dir</li>
<li>hdfs dfs -setfacl -m default:user:hadoop:r-x /dir</li>
</ul>
<h4 id="2-getfacl"><strong>2. getfacl</strong></h4>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">Usage: hdfs dfs -getfacl [-R] &lt;path&gt;</span><br></pre></td></tr></table></figure>
<p>显示文件和目录的访问控制列表（ACL）。如果目录具有默认ACL，则getfacl还会显示默认ACL。</p>
<p>选项：</p>
<ul>
<li>-R: 以递归方式列出所有文件和目录的ACL。</li>
<li>path: 要列出的文件或目录。</li>
</ul>
<p>示例：</p>
<ul>
<li>hdfs dfs -getfacl /file</li>
<li>hdfs dfs -getfacl -R /dir</li>
</ul>
<h3 id="四、为hue用户赋予权限"><strong>四、为hue用户赋予权限</strong></h3>
<p>使用hdfs超级用户来设置acl：使用-m参数</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">sudo -u hdfs hdfs dfs -setfacl -m user:hue:rwx /user/hive/warehouse</span><br></pre></td></tr></table></figure>
<p>查看文件目录的acl权限：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">hdfs dfs -getfacl /user/hive/warehouse</span><br></pre></td></tr></table></figure>
<p>文件acl权限如下图所示：</p>
<p><img src="/images/image-20210113155726266.png" alt="image-20210113155726266"></p>
<p>现在<code>hue用户</code>就对<code>/user/hive/warehouse这个目录有了</code>rwx全部权限`了。</p>
<p><strong>备注：</strong></p>
<p>不过是仅限于hive这个目录，对于里面的子文件不是hue用户创建的，hue用户还是无权访问。 如果需要访问递归的子文件，可以使用<code>-R</code>参数，再次授权。</p>
<h3 id="五、总结"><strong>五、总结</strong></h3>
<p>其实这次分享的知识点很简单，但是却很实用。就安全的角度来看，比起<code>chmod 777</code>来说，也比较严谨。</p>
<p>还是希望大家多多练习本文讲述的两个命令：</p>
<ul>
<li>setfacl</li>
<li>getfacl</li>
</ul>
<p>看看这两个命令的其它参数具体什么意思。</p>
<p>关于HDFS shell其它命令，可以查看官网链接：<a href="http://hadoop.apache.org/docs/r2.6.5/hadoop-project-dist/hadoop-common/FileSystemShell.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.6.5/hadoop-project-dist/hadoop-common/FileSystemShell.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>cdh6_hbase新集群配置项整理</title>
    <url>/2020/12/29/cdh6_hbase%E6%96%B0%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E9%A1%B9%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<p>hbase-conf:</p>
<ul>
<li>RS堆栈大小: 32G</li>
<li></li>
<li>hbase.bucketcache.size=64 =64 * 1024M: 堆外缓存大小，单位为M</li>
<li></li>
<li>dfs.replication=3on=3: hdfs副本数</li>
<li></li>
<li>hbase.hregion.max.filesize=20G=20G: Region大小</li>
<li></li>
<li>hbase.hregion.memstore.flush.size=256=256M: Memstore刷新大小</li>
<li></li>
<li>hbase.regionserver.global.memstore.upperLimit=0.t=0.55: 整个RS中Memstore最大比例</li>
<li></li>
</ul>
<h2 id="hbase-regionserver-global-memstore-lowerLimit-0-t-0-5-整个RS中Memstore最小比例-默认0-95">#- hbase.regionserver.global.memstore.lowerLimit=0.t=0.5: 整个RS中Memstore最小比例       默认0.95</h2>
<ul>
<li>hbase.bucketcache.ioengine=off=offheap: 使用堆外缓存</li>
<li></li>
</ul>
<h2 id="hbase-bucketcache-percentage-in-combinebinedcache-0-9-堆外读缓存所占比例，剩余为堆内元数据缓存大小">#- hbase.bucketcache.percentage.in.combinebinedcache=0.9: 堆外读缓存所占比例，剩余为堆内元数据缓存大小</h2>
<ul>
<li>hfile.block.cache.size=0.2=0.2: 校验项,+upperLimit需要小于0.8</li>
<li></li>
<li>hbase.master.handler.count=256=256: Master处理客户端请求最大线程数</li>
<li></li>
<li>hbase.regionserver.handler.count=256=256: RS处理客户端请求最大线程数</li>
<li></li>
<li>hbase.hstore.blockingStoreFiles=100: storefile个数达到该值则block写入</li>
<li></li>
<li>hbase.hregion.memstore.block.multiplier=3:r=3: 强制刷新Memstore大小的倍数</li>
<li></li>
<li>hbase.client.retries.number: 3 : 3</li>
<li></li>
<li>hbase.rpc.timeout: 50: 5000</li>
</ul>
<p>hbase-jvm:</p>
<p>HBASE_OFFHEAPSIZE=??G<br>
HBASE_OPTS=&quot;-XX:MaxDirectMemorySize=??G -Xmx??G -Xms??G -Xmn1g -Xss256k -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m -XX:+UseParNewGC -XX:MaxTenuringThreshold=15  -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSFullGCsBeforeCompaction=0 -XX:CMSInitiatingOccupancyFraction=70 -XX:+PrintTenuringDistribution -XX:SurvivorRatio=2 -XX:+UseConcMarkSweepGC -XX:-DisableExplicitGC $HBASE_OPTS&quot;</p>
<p>-XX:+UseG1GC<br>
-XX:InitiatingHeapOccupancyPercent=65<br>
-XX:-ResizePLAB<br>
-XX:MaxGCPauseMillis=90<br>
-XX:+UnlockDiagnosticVMOptions<br>
-XX:+G1SummarizeConcMark<br>
-XX:+ParallelRefProcEnabled<br>
-XX:G1HeapRegionSize=32m<br>
-XX:G1HeapWastePercent=20<br>
-XX:ConcGCThreads=4<br>
-XX:ParallelGCThreads=16<br>
-XX:MaxTenuringThreshold=1<br>
-XX:G1MixedGCCountTarget=64<br>
-XX:+UnlockExperimentalVMOptions<br>
-XX:G1NewSizePercent=2<br>
-XX:G1OldCSetRegionThresholdPercent=5</p>
<p>HDFS:</p>
<ul>
<li></li>
<li>dfs.datanode.handler.count 64<br>
64</li>
<li></li>
<li>dfs.datanode.max.xcievers,</li>
<li>,</li>
<li></li>
<li>dfs.datanode.max.transfer.threads  12  12288</li>
<li></li>
<li>dfs.namenode.handler.count 256 256</li>
<li></li>
<li>dfs.namenode.service.handler.count 256 256</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>hbase新集群配置项整理</title>
    <url>/2020/12/29/hbase%E6%96%B0%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E9%A1%B9%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h3 id="机器概况">机器概况</h3>
<ul>
<li>总内存:256G</li>
<li>可分配内存:256*0.75=192G</li>
<li>总硬盘:1.8T*12=21.6T</li>
<li>可用硬盘空间:21.6T*0.85=18.36T</li>
</ul>
<h3 id="内存规划">内存规划</h3>
<h4 id="Disk-Java-Heap-Ratio">Disk / Java Heap Ratio</h4>
<p><strong>Disk / Java Heap Ratio=Disk Size / Java Heap = RegionSize / MemstoreSize * ReplicationFactor * HeapFractionForMemstore * 2</strong><br>
一台RegionServer上1bytes的Java内存大小需要搭配多大的硬盘大小最合理。</p>
<p>公式解释：</p>
<ul>
<li>硬盘容量维度下Region个数： Disk Size / (RegionSize ＊ReplicationFactor)</li>
<li>Java Heap维度下Region个： Java Heap * HeapFractionForMemstore / (MemstoreSize / 2 )</li>
<li>硬盘维度和Java Headp维度理论相等：Disk Size / (RegionSize ＊ReplicationFactor)  ＝ Java Heap * HeapFractionForMemstore / (MemstoreSize / 2 ) ＝&gt; Disk Size / Java Heap = RegionSize / MemstoreSize * ReplicationFactor * HeapFractionForMemstore * 2</li>
</ul>
<p>默认配置：</p>
<ul>
<li>RegionSize: hbase.hregion.max.filesize=10G</li>
<li>MemstoreSize: hbase.hregion.memstore.flush.size=128M</li>
<li>ReplicationFactor: dfs.replication=3</li>
<li>HeapFractionForMemstore: hbase.regionserver.global.memstore.lowerLimit = 0.4</li>
</ul>
<p>计算为：10G / 128M * 3 * 0.4 * 2 = 192，即RegionServer上1bytes的Java内存大小需要搭配192bytes的硬盘大小最合理。</p>
<p>默认配置为例，新集群可用内存为192G，即对应的硬盘空间需要为192G * 192 = 36T</p>
<p><strong>默认配置下1:192，硬盘空间不足，可以将内存减少，通过修改HBase配置将多余的内存资源分配给HBase读缓存的BucketCache，这样就可以保证Java Heap并没有实际浪费。</strong></p>
<h4 id="读缓存BucketCache">读缓存BucketCache</h4>
<p>BucketCache模式下HBase的内存布局如图所示：</p>
<p><img src="/Users/xiaohei/Downloads/hbase.png" alt="image"></p>
<p>该模式主要应用于线上读多写少型应用，整个RegionServer内存（Java进程内存）分为两部分：JVM内存和堆外内存。</p>
<ul>
<li>读缓存CombinedBlockCache = LRUBlockCache + 堆外内存BucketCache，用于缓存读到的Block数据</li>
<li>LRUBlockCache，用于缓存元数据Block</li>
<li>BucketCache用于缓存实际用户数据Block</li>
<li>写缓存MemStore，缓存用户写入KeyValue数据</li>
<li>其他部分用于RegionServer正常运行所必须的内存</li>
</ul>
<h4 id="配置说明">配置说明</h4>
<p><img src="/Users/xiaohei/Downloads/hbase1.png" alt="image"></p>
<p>RegionServer 堆栈大小为192G<br>
Java_Heap大小为72G</p>
<ul>
<li>dfs.replication=3: hdfs副本数</li>
<li>hbase.hregion.max.filesize=18G: Region大小</li>
<li>hbase.hregion.memstore.flush.size=256M: Memstore刷新大小</li>
<li>hbase.regionserver.global.memstore.upperLimit=0.58: 整个RS中Memstore最大比例</li>
<li>hbase.regionserver.global.memstore.lowerLimit=0.53: 整个RS中Memstore最小比例</li>
<li>hbase.bucketcache.ioengine=offheap: 使用堆外缓存</li>
<li>hbase.bucketcache.size=(118+16) * 1024M: 堆外缓存大小，单位为M</li>
<li>hbase.bucketcache.percentage.in.combinedcache=0.88: 堆外读缓存所占比例，剩余为堆内元数据缓存大小</li>
<li>hfile.block.cache.size=0.2: 校验项</li>
<li>hbase.regionserver.handler.count=100: RS处理客户端请求最大线程数</li>
<li>hbase.hstore.blockingStoreFiles=100: storefile个数达到该值则block写入</li>
<li>hbase.hregion.memstore.block.multiplier=3: 强制刷新Memstore大小的倍数</li>
</ul>
<p><strong>校验项</strong></p>
<ul>
<li>LRUBlockCache + MemStore &lt; 80% * JVM_HEAP -&gt; (16+40)/72=0.77 &lt;= 0.8</li>
<li>RegionSize / MemstoreSize * ReplicationFactor * HeapFractionForMemstore * 2 -&gt; 18 * 1024 / 256 * 3 * 0.58 * 2 = 250 -&gt; 72G * 250 = 18T &lt;= 18T</li>
<li>Memstore可能的最大大小 -&gt; 341 * 256 / 1024 = 85.25G &gt; 41.76G</li>
<li>hfile.block.cache.size + hbase.regionserver.global.memstore.upperLimit = 0.78 &lt;= 0.8</li>
</ul>
<p><strong>可能存在的风险：Memstore数量同时存在160个以上且写满，将会出现RegionServer级别的强制刷写，造成节点阻塞。</strong></p>
<h3 id="其他">其他</h3>
<h4 id="hbase-env-sh-的-HBase-客户端环境高级配置代码段"><a href="http://hbase-env.sh" target="_blank" rel="noopener">hbase-env.sh</a> 的 HBase 客户端环境高级配置代码段</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HBASE_OFFHEAPSIZE=??G</span><br><span class="line">HBASE_OPTS="-XX:MaxDirectMemorySize=??G -Xmx??G -Xms??G -Xmn1g -Xss256k -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m -XX:+UseParNewGC -XX:MaxTenuringThreshold=15  -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSFullGCsBeforeCompaction=0 -XX:CMSInitiatingOccupancyFraction=70 -XX:+PrintTenuringDistribution -XX:SurvivorRatio=2 -XX:+UseConcMarkSweepGC -XX:-DisableExplicitGC $HBASE_OPTS"</span><br></pre></td></tr></table></figure>
<h4 id="hbase-site-xml-的-RegionServer-高级配置代码段（安全阀）">hbase-site.xml 的 RegionServer 高级配置代码段（安全阀）</h4>
<p>手动split region</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.wal.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.region.server.rpc.scheduler.factory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rpc.controllerfactory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.thread.compaction.large<span class="tag">&lt;/<span class="name">name</span>&gt;</span><span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.region.split.policy<span class="tag">&lt;/<span class="name">name</span>&gt;</span><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>修改Kudu表名,和映射在impala的表名</title>
    <url>/2020/12/08/%E4%BF%AE%E6%94%B9Kudu%E8%A1%A8%E5%90%8D,%E5%92%8C%E6%98%A0%E5%B0%84%E5%9C%A8impala%E7%9A%84%E8%A1%A8%E5%90%8D/</url>
    <content><![CDATA[<h1>修改Kudu表名,和映射在impala的表名</h1>
<p>kudu的表名和impala的表名是两码事</p>
<h2 id="修改kudu表名">修改kudu表名</h2>
<h4 id="方法一、在linux中kudu节点执行">方法一、在linux中kudu节点执行</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kudu table rename_table 172.20.85.113:7051 impala::kd_baofoo_cm.cm_entry_tmp impala::kd_baofoo_cm.cm_entry</span><br></pre></td></tr></table></figure>
<h4 id="方法二、在presto中执行">方法二、在presto中执行</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE kudu.default.&quot;impala::kd_baofoo_cm.cm_entry&quot; RENAME TO kudu.default.&quot;impala::kd_baofoo_cm.cm_entry_tmp&quot;</span><br></pre></td></tr></table></figure>
<p>修改了kudu的表名后 在impala查询kudu会报错，需要修改</p>
<h2 id="修改映射的impala表名">修改映射的impala表名</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter table kd_baofoo_cm.cm_entry set tblproperties(&#39;kudu.table_name&#39;&#x3D;&#39;impala::kd_baofoo_cm.cm_entry_tmp&#39;);</span><br><span class="line">alter table kd_baofoo_cm.cm_entry rename to kd_baofoo_cm.cm_entry_tmp;</span><br></pre></td></tr></table></figure>
<h2 id="其他参考：改成外部表，删除重建。">其他参考：改成外部表，删除重建。</h2>
<p>1.将表从内部切换到外部，并删除。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter table kd_baofoo_cm.cm_entry  set tblproperties(&#39;EXTERNAL&#39;&#x3D;&#39;true&#39;);</span><br><span class="line">drop table kd_baofoo_cm.cm_entry</span><br></pre></td></tr></table></figure>
<p>2.将kudu中的表映射到impala中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE kd_baofoo_cm.cm_entry_tmp</span><br><span class="line">STORED AS KUDU</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  &#39;kudu.master_addresses&#39; &#x3D; &#39;cdh85-111:7051,cdh85-112:7051,cdh85-113:7051&#39;, </span><br><span class="line">  &#39;kudu.table_name&#39; &#x3D; &#39;impala::kd_baofoo_cm.cm_entry_tmp&#39;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>解决hdfs文件大小为0</title>
    <url>/2020/11/23/%E8%A7%A3%E5%86%B3hdfs%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%BA0/</url>
    <content><![CDATA[<p><strong>问题： 存在文件大小为0，处于打开状态的文件，程序读取这些文件会报错</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh85-29 ~]# hadoop fs -du -h  hdfs:&#x2F;&#x2F;ns1&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak</span><br><span class="line">0  1.1 G  hdfs:&#x2F;&#x2F;ns1&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594769101120.log.gz</span><br><span class="line">0  1.1 G  hdfs:&#x2F;&#x2F;ns1&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594856701472.log.gz</span><br><span class="line">0  1.1 G  hdfs:&#x2F;&#x2F;ns1&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594941000485.log.gz</span><br></pre></td></tr></table></figure>
<p>cloudera论坛也有类型的错误 ：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cannot obtain block length for LocatedBlock</span><br></pre></td></tr></table></figure>
<p><a href="https://community.cloudera.com/t5/Support-Questions/Cannot-obtain-block-length-for-LocatedBlock/td-p/117517" target="_blank" rel="noopener">https://community.cloudera.com/t5/Support-Questions/Cannot-obtain-block-length-for-LocatedBlock/td-p/117517</a></p>
<p>但是这个方法并没有解决我的问题。 hdfs debug recoverLease -path  这样也关闭不了文件  ，纠删码策略下 不知道什么bug   这些文件关闭不了。</p>
<h3 id="我的解决方法：">我的解决方法：</h3>
<p><strong>获取hdfs没有正常关闭的文件并删除</strong>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fsck &#x2F;flume&#x2F; -files -openforwrite | grep &quot;OPENFORWRITE&quot;  &gt;tmp.txt</span><br></pre></td></tr></table></figure>
<p>tmp.txt 内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Connecting to namenode via http:&#x2F;&#x2F;cdh85-39:9870&#x2F;fsck?ugi&#x3D;root&amp;files&#x3D;1&amp;openforwrite&#x3D;1&amp;path&#x3D;%2Fflume</span><br><span class="line">&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE&#x2F;pk_day&#x3D;2020-11-23&#x2F;pk_hour&#x3D;16&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1606118400268.snappy.tmp 89401 bytes, replicated: replication&#x3D;3, 1 block(s), OPENFORWRITE:  OK</span><br><span class="line">&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594769101120.log.gz 0 bytes, erasure-coded: policy&#x3D;RS-6-3-1024k, 1 block(s), OPENFORWRITE:  OK</span><br><span class="line">&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594856701472.log.gz 0 bytes, erasure-coded: policy&#x3D;RS-6-3-1024k, 1 block(s), OPENFORWRITE:  OK</span><br><span class="line">&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594941000485.log.gz 0 bytes, erasure-coded: policy&#x3D;RS-6-3-1024k, 1 block(s), OPENFORWRITE:  OK</span><br><span class="line">&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594956902326.log.gz 0 bytes, erasure-coded: policy&#x3D;RS-6-3-1024k, 1 block(s), OPENFORWRITE:  OK</span><br></pre></td></tr></table></figure>
<p>cat tmp.txt | awk -F ’ ’ ‘{print $1}’</p>
<h3 id="移动损坏的文件：">移动损坏的文件：</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat tmp.txt | awk -F &#39; &#39; &#39;&#123;print $1&#125;&#39; | xargs -t -I &#39;&#123;&#125;&#39; sudo -u hdfs hdfs dfs -mv &#123;&#125; &#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>xargs命令</title>
    <url>/2020/11/23/xargs%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>xargs可以将输入内容（通常通过命令行管道传递），转成后续命令的参数，通常用途有：</p>
<ol>
<li>命令组合：尤其是一些命令不支持管道输入，比如<code>ls</code>。</li>
<li>避免参数过长：xargs可以通过<code>-nx</code>来将参数分组，避免参数过长。</li>
</ol>
<p>使用语法如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Usage: xargs [OPTION]... COMMAND INITIAL-ARGS...</span><br><span class="line">Run COMMAND with arguments INITIAL-ARGS and more arguments read from input.</span><br></pre></td></tr></table></figure>
<h2 id="入门例子">入门例子</h2>
<p>首先，创建测试文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">touch a.js b.js c.js</span><br></pre></td></tr></table></figure>
<p>接着，运行如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls *.js | xargs ls -al</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 a.js</span><br><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 b.js</span><br><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 c.js</span><br></pre></td></tr></table></figure>
<p>命令解释：</p>
<ol>
<li>首先，<code>ls *.js</code>的输出为<code>a.js b.js c.js</code>。</li>
<li>通过管道，将<code>a.js b.js c.js</code>作为<code>xargs</code>的输入参数。</li>
<li><code>xargs</code>命令收到输入参数后，对参数进行解析，以空格/换行作为分隔符，拆分成多个参数，这里变成<code>a.js</code>、<code>b.js</code>、<code>c.js</code>。</li>
<li><code>xargs</code>将拆分后的参数，传递给后续的命令，作为后续命令的参数，也就是说，组成这样的命令<code>ls -al a.js b.js c.js</code>。</li>
</ol>
<p>可以加上<code>-t</code>参数，在执行后面的命令前，先将命令打印出来。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls *.js | xargs -t ls -al</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看到多了一行内容<code>ls -al a.js b.js c.js</code>，这就是实际运行的命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls -al a.js b.js c.js</span><br><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 a.js</span><br><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 b.js</span><br><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 c.js</span><br></pre></td></tr></table></figure>
<h2 id="例子：参数替换">例子：参数替换</h2>
<p>有的时候，我们需要用到原始的参数，可以通过参数<code>-i</code>或<code>-I</code>实现。参数说明如下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-I R                         same as --replace=R (R must be specified)</span><br><span class="line">-i,--replace=[R]             Replace R <span class="keyword">in</span> initial arguments with names</span><br><span class="line">                             <span class="built_in">read</span> from standard input. If R is</span><br><span class="line">                             unspecified, assume &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>例子如下，将所有的<code>.js</code>结尾的文件，都加上<code>.backup</code>后缀。<code>-I '{}'</code>表示将后面命令行的<code>{}</code>替换成前面解析出来的参数。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls *.js | xargs -t -I <span class="string">'&#123;&#125;'</span> mv &#123;&#125; &#123;&#125;.backup</span><br></pre></td></tr></table></figure>
<p>展开后的命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv a.js a.js.backup</span><br><span class="line">mv b.js b.js.backup</span><br><span class="line">mv c.js c.js.backup</span><br></pre></td></tr></table></figure>
<h2 id="例子：参数分组">例子：参数分组</h2>
<p>命令行对参数最大长度有限制，xargs通过<code>-nx</code>对参数进行分组来解决这个问题。</p>
<p>首先，创建4个文件用来做实验。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">touch a.js b.js c.js d.js</span><br></pre></td></tr></table></figure>
<p>然后运行如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls *.js | xargs -t -n2 ls -al</span><br></pre></td></tr></table></figure>
<p>输出如下，<code>-n2</code>表示，将参数以2个为一组，传给后面的命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls -al a.js b.js </span><br><span class="line">-rw-r--r-- 1 root root 0 Dec 18 16:52 a.js</span><br><span class="line">-rw-r--r-- 1 root root 0 Dec 18 16:52 b.js</span><br><span class="line">ls -al c.js d.js </span><br><span class="line">-rw-r--r-- 1 root root 0 Dec 18 16:52 c.js</span><br><span class="line">-rw-r--r-- 1 root root 0 Dec 18 16:52 d.js</span><br></pre></td></tr></table></figure>
<h2 id="例子：特殊文件名">例子：特殊文件名</h2>
<p>有的时候，文件名可能存在特殊字符，比如下面的文件名中存在空格。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">touch &#39;hello 01.css&#39; &#39;hello 02.css&#39;</span><br></pre></td></tr></table></figure>
<p>运行之前的命令会报错，因为<code>xargs</code>是以空格/换行作为分隔符，于是就会出现预期之外的行为。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 命令</span></span><br><span class="line">find . -name <span class="string">'*.css'</span> | xargs -t ls -al</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">ls -al ./hello 01.css ./hello 02.css <span class="comment"># 展开后的命令</span></span><br><span class="line">ls: cannot access ./hello: No such file or directory</span><br><span class="line">ls: cannot access 01.css: No such file or directory</span><br><span class="line">ls: cannot access ./hello: No such file or directory</span><br><span class="line">ls: cannot access 02.css: No such file or directory</span><br></pre></td></tr></table></figure>
<p><code>xargs</code>是这样解决这个问题的。</p>
<ol>
<li><code>-print0</code>：告诉<code>find</code>命令，在输出文件名之后，跟上<code>NULL</code>字符，而不是换行符；</li>
<li><code>-0</code>：告诉<code>xargs</code>，以<code>NULL</code>作为参数分隔符；</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find . -name <span class="string">'*.css'</span> -print0 | xargs -0 -t ls -al</span><br></pre></td></tr></table></figure>
<h2 id="例子：日志备份">例子：日志备份</h2>
<p>将7天前的日志备份到特定目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find . -mtime +7 | xargs -I <span class="string">'&#123;&#125;'</span> mv &#123;&#125; /tmp/otc-svr-logs/</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>could only be written to 0 of the 1 minReplication nodes</title>
    <url>/2020/11/20/could%20only%20be%20written%20to%200%20of%20the%201%20minReplication%20nodes/</url>
    <content><![CDATA[<p>重启dn导致集群不能写入数据，几百个任务都失败了， 经过一个通宵的折腾，记录一下这次重大事故。</p>
<h3 id="报错日志">报错日志</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DataStreamer Exception</span><br><span class="line">20-11-2020 09:48:59 CST mysql2sqoop-1-RD202006060009 INFO - org.apache.hadoop.ipc.RemoteException(java.io.IOException): File &#x2F;user&#x2F;yarn&#x2F;.staging&#x2F;job_1605809645188_1956&#x2F;job.jar could only be written to 0 of the 1 minReplication nodes. There are 66 datanode(s) running and no node(s) are excluded in this operation.</span><br></pre></td></tr></table></figure>
<h3 id="报错的日志各种误导，走了很多弯路，只说一下，最终解决了这个问题的方法：">报错的日志各种误导，走了很多弯路，只说一下，最终解决了这个问题的方法：</h3>
<p>网上有人出现这个问题是格式化解决，它们日志是There are 0 datanode(s) running and no node(s) are excluded in this operation.我的hdfs集群是正常的所有节点都在，只是不能写入数据。</p>
<p>我用的分层策略是One_SSD，查看了DFS Storage Types，发现disk的空间不够。</p>
<p><img src="/images/image-20201120151830207.png" alt="image-20201120151830207"></p>
<h3 id="解决步骤">解决步骤</h3>
<p>ssd磁盘的机器和普通磁盘的机器分2个角色组</p>
<p>普通组不加[SSD]</p>
<p><img src="/images/image-20201120151354085.png" alt="image-20201120151354085"></p>
<p>SSD组</p>
<p><img src="/images/image-20201120151421358.png" alt="image-20201120151421358"></p>
<p>重启datanode</p>
<h3 id="坑、-HDFS分层存储">坑、 HDFS分层存储</h3>
<blockquote>
<p>通过在目录路径开头的括号中添加存储类型，为每个不是标准磁盘的DataNode数据目录指定存储类型。例如：</p>
<p>[SSD]/dfs/dn1</p>
<p>[DISK]/dfs/dn2</p>
<p>[ARCHIVE]/dfs/dn3</p>
</blockquote>
<p><strong>分层存储，官网并没有要求重启datanode，而且也没有说明要分组设置。只是刷新集群配置。其实并没有生效，给以后重启datanode留下了隐患。</strong></p>
]]></content>
  </entry>
  <entry>
    <title>python时间strftime格式化去除前导0</title>
    <url>/2020/11/10/python%E6%97%B6%E9%97%B4strftime%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%8E%BB%E9%99%A4%E5%89%8D%E5%AF%BC0/</url>
    <content><![CDATA[<h1>python时间strftime格式化去除前导0</h1>
<p><strong>解决方案：</strong></p>
<h3 id="linux-加一个“-”符号">linux  (加一个“-”符号)</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">datetime.date(2020, 11, 9).strftime(&quot;%-m月%-d日&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="windows-加一个“-”符号">windows  (加一个“#”符号)</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">datetime.date(2020, 11, 9).strftime(&quot;%#m月%#d日&quot;)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>破解PyCharm 2018.3.2</title>
    <url>/2020/10/29/%E7%A0%B4%E8%A7%A3PyCharm%202018.3.2/</url>
    <content><![CDATA[<h1>破解PyCharm 2018.3.2</h1>
<p><strong>亲测可以用</strong></p>
<p>破解教程：<a href="https://www.jianshu.com/p/76a30aeb8f9d" target="_blank" rel="noopener">https://www.jianshu.com/p/76a30aeb8f9d</a></p>
<h4 id="1、key">1、key</h4>
<figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="type">K71U8DBPNE</span>-eyJsaWNlbnNlSWQiOiJLNzFVOERCUE5FIiwibGljZW5zZWVOYW1lIjoibGFuIHl1IiwiYXNzaWduZWVOYW1lIjoiIiwiYXNzaWduZWVFbWFpbCI6IiIsImxpY2Vuc2VSZXN0cmljdGlvbiI6IkZvciBlZHVjYXRpb25hbCB1c2Ugb25seSIsImNoZWNrQ29uY3VycmVudFVzZSI6ZmFsc2UsInByb2R1Y3RzIjpbeyJjb2RlIjoiSUkiLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJSUzAiLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJXUyIsInBhaWRVcFRvIjoiMjAxOS0wNS0wNCJ9LHsiY29kZSI6IlJEIiwicGFpZFVwVG8iOiIyMDE5LTA1LTA0In0seyJjb2RlIjoiUkMiLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJEQyIsInBhaWRVcFRvIjoiMjAxOS0wNS0wNCJ9LHsiY29kZSI6IkRCIiwicGFpZFVwVG8iOiIyMDE5LTA1LTA0In0seyJjb2RlIjoiUk0iLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJETSIsInBhaWRVcFRvIjoiMjAxOS0wNS0wNCJ9LHsiY29kZSI6IkFDIiwicGFpZFVwVG8iOiIyMDE5LTA1LTA0In0seyJjb2RlIjoiRFBOIiwicGFpZFVwVG8iOiIyMDE5LTA1LTA0In0seyJjb2RlIjoiR08iLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJQUyIsInBhaWRVcFRvIjoiMjAxOS0wNS0wNCJ9LHsiY29kZSI6IkNMIiwicGFpZFVwVG8iOiIyMDE5LTA1LTA0In0seyJjb2RlIjoiUEMiLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJSU1UiLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifV0sImhhc2giOiI4OTA4Mjg5LzAiLCJncmFjZVBlcmlvZERheXMiOjAsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-<span class="type">Owt3</span>/+<span class="type">LdCpedvF0eQ8635yYt0</span>+<span class="type">ZLtCfIHOKzSrx5hBtbKGYRPFDrdgQAK6lJjexl2emLBcUq729K1</span>+ukY9Js0nx1NH09l9Rw4c7k9wUksLl6RWx7Hcdcma1AHolfSp79NynSMZzQQLFohNyjD+dXfXM5GYd2OTHya0zYjTNMmAJuuRsapJMP9F1z7UTpMpLMxS/<span class="type">JaCWdyX6qIs</span>+funJdPF7bjzYAQBvtbz+6SANBgN36gG1B2xHhccTn6WE8vagwwSNuM70egpahcTktoHxI7uS1JGN9gKAr6nbp+8DbFz3a2wd+<span class="type">XoF3nSJb</span>/d2f/6zJR8yJF8AOyb30kwg3zf5cWw==-<span class="type">MIIEPjCCAiagAwIBAgIBBTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE1MTEwMjA4MjE0OFoXDTE4MTEwMTA4MjE0OFowETEPMA0GA1UEAwwGcHJvZDN5MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq</span>+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+<span class="type">IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU</span>+<span class="type">Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1</span>+<span class="type">I4ZI</span>+<span class="type">FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB</span>/xVy/<span class="type">VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE</span>/<span class="type">EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD</span>+<span class="type">AFKOetkhnQhI2Qb1t4Lm0oFKLl</span>/<span class="type">GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQC9WZuYgQedSuOc5TOUSrRigMw4</span>/+wuC5EtZBfvdl4HT/8vzMW/oUlIP4YCvA0XKyBaCJ2iX+<span class="type">ZCDKoPfiYXiaSiH</span>+<span class="type">HxAPV6J79vvouxKrWg2XV6ShFtPLP</span>+0gPdGq3x9R3+kJbmAm8w+<span class="type">FOdlWqAfJrLvpzMGNeDU14YGXiZ9bVzmIQbwrBA</span>+<span class="built_in">c</span>/<span class="type">F4tlK</span>/<span class="type">DV07dsNExihqFoibnqDiVNTGombaU2dDup2gwKdL81ua8EIcGNExHe82kjF4zwfadHk3bQVvbfdAwxcDy4xBjs3L4raPLU3yenSzr</span>/<span class="type">OEur1</span>+jfOxnQSmEcMXKXgrAQ9U55gwjcOFKrgOxEdek/<span class="type">Sk1VfOjvS</span>+nuM4eyEruFMfaZHzoQiuw4IqgGc45ohFH0UUyjYcuFxxDSU9lMCv8qdHKm+wnPRb0l9l5vXsCBDuhAGYD6ss+<span class="type">Ga</span>+aDY6f/qXZuUCEUOH3QUNbbCUlviSz6+<span class="type">GiRnt1kA9N2Qachl</span>+2yBfaqUqr8h7Z2gsx5LcIf5kYNsqJ0GavXTVyWh7PYiKX4bs354ZQLUwwa/cG++<span class="number">2</span>+wNWP+<span class="type">HtBhVxMRNTdVhSm38AknZlD</span>+<span class="type">PTAsWGu9GyLmhti2EnVwGybSD2Dxmhxk3IPCkhKAK</span>+pl0eWYGZWG3tJ9mZ7SowcXLWDFAk0lRJnKGFMTggrWjV8GYpw5bq23VmIqqDLgkNzuoog==</span><br></pre></td></tr></table></figure>
<h4 id="2、破解补丁下载">2、破解补丁下载</h4>
<p>链接：<a href="https://pan.baidu.com/s/1EvUvweSG58kFToJXB8ugPw" target="_blank" rel="noopener">https://pan.baidu.com/s/1EvUvweSG58kFToJXB8ugPw</a><br>
提取码：vnkq</p>
]]></content>
  </entry>
  <entry>
    <title>hive数据导出，并指定分隔符，元素包含引号等</title>
    <url>/2020/10/23/hive%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA%EF%BC%8C%E5%B9%B6%E6%8C%87%E5%AE%9A%E5%88%86%E9%9A%94%E7%AC%A6%EF%BC%8C%E5%85%83%E7%B4%A0%E5%8C%85%E5%90%AB%E5%BC%95%E5%8F%B7%E7%AD%89/</url>
    <content><![CDATA[<h3 id="hive数据导出，并指定分隔符，元素包含引号等">hive数据导出，并指定分隔符，元素包含引号等</h3>
<p>语法格式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">beeline -n username -p password -u jdbc:hive2://host:10000 --verbose=<span class="literal">true</span>  --showHeader=<span class="literal">false</span> --outputformat=tsv2  --color=<span class="literal">true</span>  -e <span class="string">"select * from <span class="variable">$&#123;database&#125;</span>.<span class="variable">$&#123;tablename&#125;</span>"</span> &gt; <span class="variable">$&#123;tableName&#125;</span>.csv</span><br></pre></td></tr></table></figure>
<p>通过 outputformat 指定输出格式</p>
<blockquote>
<p>–outputformat=[table/vertical/csv/tsv/dsv/csv2/tsv2] == 指定输出格式</p>
<p>–delimiterForDSV=&quot;*&quot; ‘&amp;’ 前提（–outputformat=dsv） 指定分隔符</p>
</blockquote>
<p>不同格式对应的分隔符如下表：</p>
<table>
<thead>
<tr>
<th>格式</th>
<th>分隔符</th>
</tr>
</thead>
<tbody>
<tr>
<td>table</td>
<td>表格式</td>
</tr>
<tr>
<td>vertical</td>
<td>如下所示</td>
</tr>
<tr>
<td>csv</td>
<td>‘,’ 逗号(元素包含引号)</td>
</tr>
<tr>
<td>tsv</td>
<td>‘\t’ 制表符(元素包含逗号)</td>
</tr>
<tr>
<td>dsv</td>
<td>默认‘|’ 竖线分割，可通过delimiterForDSV指定分隔符</td>
</tr>
<tr>
<td>csv2</td>
<td>‘,’ 逗号(不含引号)</td>
</tr>
<tr>
<td>tsv2</td>
<td>‘\t’ 制表符(不含引号)</td>
</tr>
</tbody>
</table>
<p>说明：</p>
<blockquote>
<p>csv格式 == 查询元素有’'单引号</p>
<p>csv2格式没有单引号</p>
<p>tsv，tsv2同上</p>
</blockquote>
<h3 id="实例">实例</h3>
<p><strong>impala  ,   ‘|’ 竖线分割 (元素不包含引号)</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">impala-shell -i cdh85-43:21000 -u yarn -l <span class="comment">--auth_creds_ok_in_clear   -B -o /opt/Z2007931000018_S3_N_20200311_00000001.TXT  --output_delimiter='|' -q " select c1,c3,c4,c5,c6,c7,COALESCE(c8,0),COALESCE(c9,0), COALESCE(c10,0),COALESCE(c11,0),'156' from baofoo_rm_regulator.hadoop_pbc_trans_order_aggregate"</span></span><br></pre></td></tr></table></figure>
<p><strong>hive      ‘|’ 竖线分割 (元素包含双引号)</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">beeline -u "jdbc:hive2://172.20.15.12:10000/" -n yarn -p <span class="comment">--outputformat=dsv --showHeader=false -e ' set mapreduce.job.queuename=bf_yarn_pool.production; SELECT  concat("\"", self_acc_name,"\"") ,  concat("\"",self_acc_no ,"\"") ,  concat("\"",bank_acc_name ,"\"") ,  concat("\"",join_code ,"\"") ,  concat("\"",`date` ,"\"") ,  concat("\"", `time`,"\"") ,  concat("\"", cur,"\"") ,  concat("\"",cast(amt as string) ,"\"") ,  concat("\"",cast(usd_amt as string) ,"\"") ,  concat("\"",lend_flag ,"\"") ,  concat("\"",prof_type ,"\"") ,  concat("\"", part_acc_name,"\"") ,  concat("\"",part_acc_no ,"\"") ,  concat("\"", acc_flag,"\"") ,  concat("\"",tran_flag ,"\"") ,  concat("\"",open_bank_name ,"\"") ,  concat("\"",ip_code ,"\"") ,  concat("\"",purpose ,"\"") ,  concat("\"",bord_flag ,"\"") ,  concat("\"",trade_order ,"\"") ,  concat("\"",trans_no ,"\"") FROM BAOFOO_STAT.tb_con_txn_2019_2020 ' &gt; tb_con_txn_2019_2020.csv</span></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>typora + hexo博客中插入图片</title>
    <url>/2020/10/23/typora%20+%20hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<p>在使用了hexo搭建了博客后，最大的问题便是如何使用一款markdown工具来编辑博客了，我采取的就是Typora，这工具免费简单易用没广告，而且把图片保存到本地还是很方便的，因此大家只要稍微了解点markdown语法就可以上手使用了。</p>
<p>关于图片和图片路径的设置，有以下教程。</p>
<p>事先声明，所有博客文件均保存在 hexo/_posts/文件夹下</p>
<p>首先在 hexo &gt; source目录下建一个文件夹叫images，用来保存博客中的图片。</p>
<p>然后打开Typora的 文件 &gt; 偏好设置，进行如下设置。</p>
<p><img src="/images/image-20200116142728587.png" alt="image-20200116142728587"></p>
<p>这样的话所有的博客中的图片都将会保存到 /source/images/该博客md文件名/图片名称</p>
<p>但是仅仅这样设置还不够，这样设置在typora中倒是能看图片了，但是使用的却是相对于当前md文件的相对路径，可是如果启动hexo，是要用服务器访问的，而服务器显然无法根据这个相对路径正确访问到图片，因此还需要在typora中进行进一步设置。</p>
<p>在typora菜单栏点击 格式-&gt;图像-&gt;设置图片根目录，将hexo/source作为其根目录即可。</p>
<p><strong>一定要先设置了图片根目录后再插入图片，否则图片路径会不正确喔！</strong></p>
<p><strong>根目录设置：</strong></p>
<p>在文章头部插入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">typora-root-url: ..</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>然后图片地址换成绝对路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![image-20200116142728587](&#x2F;images&#x2F;image-20200116142728587.png)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>插入图片模板</title>
    <url>/2020/10/14/%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[<hr>
<h2 id="typora-root-url-…">typora-root-url:…</h2>
<p><img src="/images/image-20200116143338485.png" alt="image-20200116143338485"></p>
]]></content>
  </entry>
  <entry>
    <title>hive建表create table xxx as select的问题</title>
    <url>/2020/10/14/hive%E5%BB%BA%E8%A1%A8create%20table%20xxx%20as%20select%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1>hive建表create table xxx as select的问题</h1>
<p>create table xxx as select的方式创建的表默认存储格式是text，所以要注意了假如as select的是其他格式的比如RCFile，<strong>则可能会导致一行变多行的情况</strong>（因为RCFile格式的可能字段包含换行符等），所以必须要加上<br>
create table xxx stored as RCFile as select…<br>
所以使用这种方式建表注意加上指定的存储格式。</p>
<p>测试示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> decision_model.member_close_reason;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> decision_model.member_close_reason <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">'mongodb_member'</span> <span class="keyword">as</span> intype, *</span><br><span class="line"><span class="keyword">from</span> mongo_baofoo_log.log_update_member_state</span><br><span class="line">;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> decision_model.member_close_reason <span class="keyword">where</span> intype &lt;&gt; <span class="string">'mongodb_member'</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/images/image-20201014132306707.png" alt="img"></p>
<p>正确的应该加上指定的存储格式。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> decision_model.member_close_reason;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> decision_model.member_close_reason </span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line">    <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'|'</span> </span><br><span class="line">    <span class="keyword">STORED</span> <span class="keyword">AS</span> RCFile </span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">'mongodb_member'</span> <span class="keyword">as</span> intype, *</span><br><span class="line"><span class="keyword">from</span> mongo_baofoo_log.log_update_member_state </span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> decision_model.member_close_reason <span class="keyword">where</span> intype &lt;&gt; <span class="string">'mongodb_member'</span>;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>解决spark streaming长时间运行日志不断增长问题</title>
    <url>/2020/10/12/%E8%A7%A3%E5%86%B3spark%20streaming%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97%E4%B8%8D%E6%96%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1>解决spark streaming长时间运行日志不断增长问题</h1>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">log4j.rootLogger=WARN,stdout,A1</span><br><span class="line"></span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.Threshold=WARN</span><br><span class="line">log4j.appender.stdout.encoding=UTF-8</span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=[%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;] %m %n[%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;] %p | %F:%L | %M%n%n</span><br><span class="line"></span><br><span class="line">log4j.appender.A1=org.apache.log4j.RollingFileAppender</span><br><span class="line">log4j.appender.A1.BufferedIO=true</span><br><span class="line">log4j.appender.A1.BufferSize=8192</span><br><span class="line">log4j.appender.A1.File=$&#123;spark.yarn.app.container.log.dir&#125;/stderr</span><br><span class="line">log4j.appender.A1.MaxFileSize=10MB</span><br><span class="line">log4j.appender.A1.MaxBackupIndex=9</span><br><span class="line">log4j.appender.A1.encoding=UTF-8</span><br><span class="line">log4j.appender.A1.Append=true</span><br><span class="line">log4j.appender.A1.Threshold=ERROR</span><br><span class="line">log4j.appender.A1.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.A1.layout.ConversionPattern=[%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;] %m %n[%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;] %p | %F:%L | %M%n%n</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo  "================= spark job:CbDimStreamDriver  start!!!========================"</span><br><span class="line"></span><br><span class="line">spark-submit \</span><br><span class="line">--master yarn  \</span><br><span class="line">--deploy-mode  cluster  \</span><br><span class="line">--name  stream-rm-cb-dim  \</span><br><span class="line">--queue bf_yarn_pool.production  \</span><br><span class="line">--class com.baofu.rm.streaming.CbDimStreamDriver  \</span><br><span class="line">--num-executors  32  \</span><br><span class="line">--driver-memory  3G  \</span><br><span class="line">--executor-memory  4G  \</span><br><span class="line">--executor-cores  1  \</span><br><span class="line">--conf spark.dynamicAllocation.enabled=false \</span><br><span class="line">--conf spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC \</span><br><span class="line">--conf spark.streaming.backpressure.enabled=true \</span><br><span class="line">--conf spark.streaming.kafka.maxRatePerPartition=1000 \</span><br><span class="line">--conf spark.eventLog.enabled=false \</span><br><span class="line">--conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \</span><br><span class="line">--conf spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \</span><br><span class="line">--files ./log4j.properties \</span><br><span class="line">/home/bf_app_spark/spark-jobs/streams/fxJob/cbdim/rm-streaming-analysis-pro.jar</span><br><span class="line"></span><br><span class="line">rc=$?</span><br><span class="line">if [[ $rc != 0 ]]; then</span><br><span class="line">    echo "spark task: $0  failed,please check......"  </span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo "end run spark: `date "+%Y-%m-%d %H:%M:%S"`"  </span><br><span class="line">echo "================== spark job:CbDimStreamDriver  end!!!===================="</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup sh .&#x2F;stream_CbDimStreamDriver.sh &gt; &#x2F;dev&#x2F;null 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
<p>参考： <a href="http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/" target="_blank" rel="noopener">http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/</a></p>
]]></content>
  </entry>
  <entry>
    <title>有道笔记文件备份</title>
    <url>/2020/09/30/%E6%9C%89%E9%81%93%E7%AC%94%E8%AE%B0%E6%96%87%E4%BB%B6%E5%A4%87%E4%BB%BD/</url>
    <content><![CDATA[<h1>有道笔记文件备份</h1>
<ul>
<li><a href="http://www.ask3.cn/files/(%E8%B6%85%E8%AF%A6%E7%BB%86,%E5%B8%A6%E4%BD%A0%E8%B8%A9%E5%9D%91)linux_centos7_%E5%9F%BA%E4%BA%8ECDH6.0.1%E9%85%8D%E7%BD%AEhive_on_tez_%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E(%E4%BA%B2%E6%B5%8B%E6%9C%89%E6%95%88).pdf">(超详细,带你踩坑)linux_centos7_基于CDH6.0.1配置hive_on_tez_执行引擎(亲测有效).pdf</a></li>
<li><a href="http://www.ask3.cn/files/01%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90.pdf">01需求分析.pdf</a></li>
<li><a href="http://www.ask3.cn/files/1.0_OpenLDAP%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3%E5%BC%80%E5%90%AFtls%EF%BC%8C%E4%B8%BB%E4%B8%BB%E5%90%8C%E6%AD%A5.pdf">1.0_OpenLDAP安装文档开启tls，主主同步.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Alluxio_%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95.pdf">Alluxio_效果测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Apache%E8%AE%BE%E7%BD%AE%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E8%BD%AC%E5%8F%91%E7%AB%AF%E5%8F%A3.pdf">Apache设置反向代理转发端口.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6_%E4%BB%8B%E7%BB%8D%EF%BC%8C%E5%AE%89%E8%A3%85%E5%92%8C%E6%B5%8B%E8%AF%95.pdf">CDH6_介绍，安装和测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6%E4%B8%8D%E8%83%BD%E5%85%B3%E6%8E%89_Auto-TLS_%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95.pdf">CDH6不能关掉_Auto-TLS_的解决办法.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6%E5%9C%A8%E5%AE%89%E8%A3%85agent%E6%97%B6%EF%BC%8C%E6%8F%90%E7%A4%BA%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6_Agent%E5%8F%91%E5%87%BA%E7%9A%84%E6%A3%80%E6%B5%8B%E4%BF%A1%E5%8F%B7.pdf">CDH6在安装agent时，提示安装失败无法接收_Agent发出的检测信号.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6%E7%8E%AF%E5%A2%83%E4%B8%ADPhoenix%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8C%E4%BD%BF%E7%94%A8.pdf">CDH6环境中Phoenix的搭建和使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3(1).pdf">CDH6部署文档(1).pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3.pdf">CDH6部署文档.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH_5.10.2_%E5%AE%89%E8%A3%85kudu_%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E6%95%B4%E7%90%86(1).pdf">CDH_5.10.2_安装kudu_和常见错误整理(1).pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH_5.10.2_%E5%AE%89%E8%A3%85kudu_%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E6%95%B4%E7%90%86.pdf">CDH_5.10.2_安装kudu_和常见错误整理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH_%E4%BA%A4%E6%8D%A2%E5%86%85%E5%AD%98%E8%AD%A6%E5%91%8A%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3.pdf">CDH_交换内存警告问题解决.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH_%E7%9A%84Cloudera_Manager%E5%85%8D%E8%B4%B9%E4%B8%8E%E6%94%B6%E8%B4%B9%E7%89%88%E7%9A%84%E5%AF%B9%E6%AF%94%E8%A1%A8_-_Hi%EF%BC%8C%E7%8E%8B%E6%9D%BE%E6%9F%8F_-_%E5%8D%9A%E5%AE%A2%E5%9B%AD.pdf">CDH_的Cloudera_Manager免费与收费版的对比表_-<em>Hi，王松柏</em>-_博客园.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E5%85%83%E6%95%B0%E6%8D%AE%E5%B0%8F%E7%BB%93.pdf">CDH元数据小结.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E5%90%AF%E7%94%A8Kerberos%E5%AF%BC%E8%87%B4hdfs,yarn%E7%AD%89%E9%A1%B5%E9%9D%A2%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E8%A7%A3%E5%86%B3.pdf">CDH启用Kerberos导致hdfs,yarn等页面无法访问解决.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E9%9B%86%E6%88%90%E7%9A%84KDC%E8%BF%81%E7%A7%BB%E8%87%B3FreeIPA%E7%9A%84Kerberos%E8%AE%A4%E8%AF%81.pdf">CDH集成的KDC迁移至FreeIPA的Kerberos认证.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E7%AE%80%E4%BB%8B.pdf">CDH集群搭建简介.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E9%9B%86%E7%BE%A4%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98.pdf">CDH集群时区问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2Livy.pdf">CDH集群部署Livy.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Cannot_obtain_block_length_for_LocatedBlock%E6%95%85%E9%9A%9C%E5%88%86%E8%A7%A3%E5%86%B3.pdf">Cannot_obtain_block_length_for_LocatedBlock故障分解决.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CentOS6.X_%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8.pdf">CentOS6.X_升级内核.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CentOS7%E4%B8%8A%E6%89%8B%E5%8A%A8%E9%87%8A%E6%94%BE%E5%86%85%E5%AD%98cache.pdf">CentOS7上手动释放内存cache.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CentOS7%E4%B8%8B%E5%AE%89%E8%A3%85Anaconda3%E5%92%8CTensorflow.pdf">CentOS7下安装Anaconda3和Tensorflow.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Centos7%E8%A3%85NVIDIA%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8(GPU).pdf">Centos7装NVIDIA显卡驱动(GPU).pdf</a></li>
<li><a href="http://www.ask3.cn/files/ClickHouse.pdf">ClickHouse.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ClickHouse_%E9%83%A8%E7%BD%B2%E4%B8%8E%E4%BD%BF%E7%94%A8.pdf">ClickHouse_部署与使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ClouderaManager_(cm)_%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98.pdf">ClouderaManager_(cm)_时区问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Cloudera_Manager%E4%B8%AD%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2Zeppelin%E6%9C%8D%E5%8A%A1.pdf">Cloudera_Manager中安装部署Zeppelin服务.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Cloudera_Manager%E5%9B%9E%E9%80%80.pdf">Cloudera_Manager回退.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Cloudera%E5%B9%B3%E5%8F%B0%E8%BD%AF%E4%BB%B6%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84.pdf">Cloudera平台软件体系结构.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Could_not_open_client_transport_with_JDBC_Uri_jdbchive2.pdf">Could_not_open_client_transport_with_JDBC_Uri_jdbchive2.pdf</a></li>
<li><a href="http://www.ask3.cn/files/DataX_Hdfs_HA(%E9%AB%98%E5%8F%AF%E7%94%A8)%E9%85%8D%E7%BD%AE%E6%94%AF%E6%8C%81.pdf">DataX_Hdfs_HA(高可用)配置支持.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ELK+filebeat_%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.pdf">ELK+filebeat_安装问题总结.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Elasticsearch5.0%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2_%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.pdf">Elasticsearch5.0集群部署_问题总结.pdf</a></li>
<li><a href="http://www.ask3.cn/files/FTP%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AE%BF%E9%97%AECDH%E4%B8%ADHDFS%E6%96%87%E4%BB%B6.pdf">FTP的方式访问CDH中HDFS文件.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Flink%E9%9B%86%E7%BE%A4.pdf">Flink集群.pdf</a></li>
<li><a href="http://www.ask3.cn/files/FreeIPA_%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86.pdf">FreeIPA_常用命令整理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/FreeIPA%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.pdf">FreeIPA部署及基本使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Full_GC__%E5%AF%BC%E8%87%B4RegionServer%E6%8C%82%E4%BA%86.pdf">Full_GC__导致RegionServer挂了.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase2.0_%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4%E8%A1%A8.pdf">HBase2.0_强制删除表.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase_2.0.0_META_%E6%95%B0%E6%8D%AE%E4%BF%AE%E5%A4%8D%E5%B7%A5%E5%85%B7.pdf">HBase_2.0.0_META_数据修复工具.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase_%E7%94%A8phoenix%E5%88%9B%E5%BB%BA%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95.pdf">HBase_用phoenix创建二级索引.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase_%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87%E8%AF%8A%E6%B2%BB.pdf">HBase_疑难杂症诊治.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase%E6%95%B0%E6%8D%AE%E5%9D%97NotServingRegionException%E6%8E%92%E6%9F%A5.pdf">HBase数据块NotServingRegionException排查.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase%E8%BF%90%E7%BB%B4%E5%AE%9E%E8%B7%B5.pdf">HBase运维实践.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HDFS%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8.pdf">HDFS分层存储.pdf</a></li>
<li>[HIVE删除分区表FAILED_Execution_Error,_return_code_1…_MetaException(messageInvalid_p.pdf](<a href="http://www.ask3.cn/files/HIVE%E5%88%A0%E9%99%A4%E5%88%86%E5%8C%BA%E8%A1%A8FAILED_Execution_Error,_return_code_1">http://www.ask3.cn/files/HIVE删除分区表FAILED_Execution_Error,_return_code_1</a>…_MetaException(messageInvalid_p.pdf)</li>
<li><a href="http://www.ask3.cn/files/Hadoop%E8%B0%83%E4%BC%98.pdf">Hadoop调优.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hbase_%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E4%B8%8E%E8%BF%98%E5%8E%9F.pdf">Hbase_数据迁移与还原.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hbase%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6.pdf">Hbase权限控制.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hbase%E8%87%AA%E5%B8%A6%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E5%91%BD%E4%BB%A4.pdf">Hbase自带压力测试命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hive_SQL_Syntax_for_Use_with_Sentry.pdf">Hive_SQL_Syntax_for_Use_with_Sentry.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hive%E4%B9%8B%E2%80%94%E2%80%94%E5%86%85%E9%83%A8%E8%A1%A8%E4%B8%8E%E5%A4%96%E9%83%A8%E8%A1%A8%E7%9B%B8%E4%BA%92%E7%9B%B8%E4%BA%92%E8%BD%AC%E5%8C%96.pdf">Hive之——内部表与外部表相互相互转化.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hive%E5%88%86%E5%8C%BA%E8%A1%A8%E6%96%B0%E5%A2%9E%E5%AD%97%E6%AE%B5.pdf">Hive分区表新增字段.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hive%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%A2%9E%E5%88%97.pdf">Hive实现自增列.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hive%E8%A1%A8%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81.pdf">Hive表中文乱码.pdf</a></li>
<li><a href="http://www.ask3.cn/files/IIS%E5%86%85%E9%83%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%94%99%E8%AF%AF_dedecms%E7%94%9F%E6%88%90%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E8%B6%85%E6%97%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.pdf">IIS内部服务器错误_dedecms生成静态页面超时解决方案.pdf</a></li>
<li><a href="http://www.ask3.cn/files/IPFS%E9%9F%B3%E4%B9%90%E6%92%AD%E6%94%BE%E5%99%A8.pdf">IPFS音乐播放器.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Impala%E9%85%8D%E7%BD%AELDAP%E8%BA%AB%E4%BB%BD%E8%AE%A4%E8%AF%81.pdf">Impala配置LDAP身份认证.pdf</a></li>
<li><a href="http://www.ask3.cn/files/JanusGraph%E5%8D%95%E6%9C%BA%E6%B5%8B%E8%AF%95.pdf">JanusGraph单机测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Kerberos%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.pdf">Kerberos常用命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Linux_%E5%91%BD%E4%BB%A4%E7%A7%AF%E7%B4%AF.pdf">Linux_命令积累.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Linux_%E7%BB%99%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E6%B7%BB%E5%8A%A0_%E6%96%87%E4%BB%B6%E5%A4%B4.pdf">Linux_给文件内容添加_文件头.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Linux%E4%B8%8B%E4%BD%BF%E7%94%A8Webmin%E6%90%AD%E5%BB%BADNS%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%80%8C%E4%B8%8D%E6%98%AFhosts%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90%E4%B8%BB%E6%9C%BA%E5%90%8D.pdf">Linux下使用Webmin搭建DNS服务器而不是hosts文件解析主机名.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Linux%E4%B8%8B%E6%9B%B4%E6%94%B9%E8%BD%AC%E7%A7%BBmysql%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%AE%E5%BD%95.pdf">Linux下更改转移mysql数据库目录.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Livy_%E5%A4%9A%E7%94%A8%E6%88%B7%E4%BD%BF%E7%94%A8.pdf">Livy_多用户使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/NFS%E5%BC%82%E5%B8%B8%E5%AF%BC%E8%87%B4Host_Monitor%E5%8F%8AAgent%E6%9C%8D%E5%8A%A1%E9%94%99%E8%AF%AF.pdf">NFS异常导致Host_Monitor及Agent服务错误.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Navicat_for_MySQL_%E5%BF%AB%E6%8D%B7%E9%94%AE.pdf">Navicat_for_MySQL_快捷键.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Neo4j%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.pdf">Neo4j安装与配置.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Phoenix%E5%AE%89%E8%A3%85%E5%8F%8A%E5%85%B6%E4%BD%BF%E7%94%A8.pdf">Phoenix安装及其使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Presto_%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%B7%A5%E5%85%B7%E5%92%8C%E7%95%8C%E9%9D%A2.pdf">Presto_客户端工具和界面.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Presto%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E5%8F%8A%E4%BD%BF%E7%94%A8.pdf">Presto安装部署及使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Ranger_%E9%85%8D%E7%BD%AE_LDAP_%E8%B4%A6%E5%8F%B7%EF%BC%88FreeIPA%EF%BC%89%E5%90%8C%E6%AD%A5%E8%B4%A6%E5%8F%B7.pdf">Ranger_配置_LDAP_账号（FreeIPA）同步账号.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Redis5.0%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85.pdf">Redis5.0集群安装.pdf</a></li>
<li><a href="http://www.ask3.cn/files/R%E8%AF%AD%E8%A8%80%E6%93%8D%E4%BD%9Chive,%E5%B9%B6%E8%B0%83%E5%BA%A6.pdf">R语言操作hive,并调度.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Shell_%E6%8C%89%E6%97%A5%E6%9C%9F%E5%BE%AA%E7%8E%AF%E6%89%A7%E8%A1%8C.pdf">Shell_按日期循环执行.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Solr%E5%BC%82%E5%B8%B8%E5%85%B3%E9%97%AD%E5%AF%BC%E8%87%B4index_locked.pdf">Solr异常关闭导致index_locked.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Spark,Impala,Hive_%E8%AE%BE%E7%BD%AE%E9%98%9F%E5%88%97.pdf">Spark,Impala,Hive_设置队列.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Spark_on_Yarn_%E4%B9%8BPython%E7%8E%AF%E5%A2%83%E5%AE%9A%E5%88%B6.pdf">Spark_on_Yarn_之Python环境定制.pdf</a></li>
<li><a href="http://www.ask3.cn/files/TiDB%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E5%92%8C%E6%B5%8B%E8%AF%95.pdf">TiDB快速部署和测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/WORDPRESS_%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E6%98%BE%E7%A4%BA%E5%9B%9E%E8%BD%A6%E6%8D%A2%E8%A1%8C%E7%A9%BA%E8%A1%8C%E5%9B%9E%E8%A1%8C%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.pdf">WORDPRESS_无法正常显示回车换行空行回行的解决方法.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Windows_%E4%B8%8B_Confluence_%E9%AA%8C%E8%AF%81%E7%A0%81%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA_%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95.pdf">Windows_下_Confluence_验证码无法显示_解决办法.pdf</a></li>
<li><a href="http://www.ask3.cn/files/YAML%E6%A0%BC%E5%BC%8F%E8%A7%A3%E6%9E%90.pdf">YAML格式解析.pdf</a></li>
<li><a href="http://www.ask3.cn/files/alluxio%E5%AE%89%E8%A3%85%E5%92%8C_%E7%BB%93%E5%90%88cdh%E4%BD%BF%E7%94%A8%EF%BC%8Calluxio%E5%92%8C_spark%E6%95%B4%E5%90%88.pdf">alluxio安装和_结合cdh使用，alluxio和_spark整合.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ambari_hortonworks_%EF%BC%88hdp%EF%BC%89%E5%AE%89%E8%A3%85%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9.pdf">ambari_hortonworks_（hdp）安装注意事项.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ansible_ansible-demo3_-m_copy_-a_src=testdircopytest_dest=testdir.pdf">ansible_ansible-demo3_-m_copy_-a_src=testdircopytest_dest=testdir.pdf</a></li>
<li><a href="http://www.ask3.cn/files/azkaban%E9%80%9A%E8%BF%87%E8%84%9A%E6%9C%AC%E6%89%93%E5%8C%85%E5%8F%91%E5%B8%83%E5%B7%A5%E7%A8%8B.pdf">azkaban通过脚本打包发布工程.pdf</a></li>
<li><a href="http://www.ask3.cn/files/bat_%E8%BE%93%E5%85%A5%E6%83%B3%E6%89%A7%E8%A1%8C%E7%9A%84%E6%AC%A1%E6%95%B0_%E6%AF%8F%E9%9A%943%E7%A7%92%E9%92%9F%E5%BE%AA%E7%8E%AF%E6%89%A7%E8%A1%8C%E4%B8%80%E6%AC%A1%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F.pdf">bat_输入想执行的次数_每隔3秒钟循环执行一次应用程序.pdf</a></li>
<li><a href="http://www.ask3.cn/files/can_only_run_host_inspector_when_host_is_healthy_cloudera.pdf">can_only_run_host_inspector_when_host_is_healthy_cloudera.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh6.0.1_spark%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%AE%BE%E7%BD%AE.pdf">cdh6.0.1_spark客户端设置.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh6.3.2-%E6%96%B0%E5%8A%9F%E8%83%BD%E6%B5%8B%E8%AF%95.pdf">cdh6.3.2-新功能测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh6.3.2%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2.pdf">cdh6.3.2安装部署.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh6_hbase%E6%96%B0%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E9%A1%B9%E6%95%B4%E7%90%86.pdf">cdh6_hbase新集群配置项整理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh6_oozie%E8%B0%83%E5%BA%A6shell%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81.pdf">cdh6_oozie调度shell中文乱码.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh%E6%9C%8D%E5%8A%A1_%E6%89%8B%E5%8A%A8%E6%93%8D%E4%BD%9C.pdf">cdh服务_手动操作.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh%E6%A0%B9%E6%8D%AE%E6%9C%BA%E5%99%A8%E6%95%B0%E9%87%8F%E5%88%92%E5%88%86%E8%A7%92%E8%89%B2.pdf">cdh根据机器数量划分角色.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdp_+freeipa___kerberos%E8%AE%A4%E8%AF%81.pdf">cdp_+freeipa___kerberos认证.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdp%EF%BC%88cdh7%EF%BC%89.pdf">cdp（cdh7）.pdf</a></li>
<li><a href="http://www.ask3.cn/files/centos7.%E4%BD%BF%E7%94%A8Tor_%E5%88%9B%E5%BB%BA%E5%8C%BF%E5%90%8D%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%8C%BF%E5%90%8D%E7%BD%91%E7%AB%99%EF%BC%88.onion%EF%BC%89.pdf">centos7.使用Tor_创建匿名服务和匿名网站（.onion）.pdf</a></li>
<li><a href="http://www.ask3.cn/files/centos7_openldap%E5%8F%8C%E4%B8%BB%E9%83%A8%E7%BD%B2.pdf">centos7_openldap双主部署.pdf</a></li>
<li><a href="http://www.ask3.cn/files/centos_7%E8%AE%BE%E7%BD%AE%E6%9C%80%E5%A4%A7%E6%96%87%E4%BB%B6%E6%89%93%E5%BC%80%E6%95%B0%EF%BC%8C%E4%B8%8D%E7%94%9F%E6%95%88%E9%97%AE%E9%A2%98.pdf">centos_7设置最大文件打开数，不生效问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/clickhouse%E4%BD%BF%E7%94%A8.pdf">clickhouse使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/clouder_manager_(cm)%E9%99%8D%E7%BA%A7.pdf">clouder_manager_(cm)降级.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cloudera_solr_%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.pdf">cloudera_solr_集群搭建.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cloudera_%E6%94%B6%E8%B4%B9%E7%89%88%E5%8A%9F%E8%83%BD.pdf">cloudera_收费版功能.pdf</a></li>
<li><a href="http://www.ask3.cn/files/css_url_%E7%9B%B8%E5%AF%B9%E8%B7%AF%E5%BE%84.pdf">css_url_相对路径.pdf</a></li>
<li><a href="http://www.ask3.cn/files/dbvisualizer_pro_64%E4%BD%8D%E7%A0%B4%E8%A7%A3%E7%89%88_v10.0.20%E4%B8%93%E4%B8%9A%E7%89%88.pdf">dbvisualizer_pro_64位破解版_v10.0.20专业版.pdf</a></li>
<li><a href="http://www.ask3.cn/files/django%E5%81%9A%E4%B8%80%E4%B8%AA%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E5%BA%97%E5%95%86%E7%BD%91%E7%AB%99.pdf">django做一个最简单的店商网站.pdf</a></li>
<li><a href="http://www.ask3.cn/files/dokuwiki_%E5%AE%89%E8%A3%85%E4%B8%8E%E8%AE%BE%E7%BD%AE%E5%92%8C%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9.pdf">dokuwiki_安装与设置和注意事项.pdf</a></li>
<li><a href="http://www.ask3.cn/files/elasticsearch6.4.2%E8%AE%B8%E5%8F%AF%E8%AF%81%E8%BF%87%E6%9C%9F%E4%BA%86_es%E7%A0%B4%E8%A7%A3.pdf">elasticsearch6.4.2许可证过期了_es破解.pdf</a></li>
<li><a href="http://www.ask3.cn/files/elasticsearch%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD%E4%B8%8E%E8%BF%81%E7%A7%BB.pdf">elasticsearch数据备份与迁移.pdf</a></li>
<li><a href="http://www.ask3.cn/files/es-sql_(elasticsearch-sql)%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85.pdf">es-sql_(elasticsearch-sql)插件安装.pdf</a></li>
<li><a href="http://www.ask3.cn/files/es%E6%98%A0%E5%B0%84%E5%88%B0hive_%E7%B1%BB%E5%9E%8B%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E5%9C%B0%E6%96%B9.pdf">es映射到hive_类型需要注意的地方.pdf</a></li>
<li><a href="http://www.ask3.cn/files/filebeat.yml_%E9%85%8D%E7%BD%AE.pdf">filebeat.yml_配置.pdf</a></li>
<li><a href="http://www.ask3.cn/files/filebeat%E8%AF%A6%E8%A7%A3.pdf">filebeat详解.pdf</a></li>
<li><a href="http://www.ask3.cn/files/fuser%E5%91%BD%E4%BB%A4.pdf">fuser命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/github%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2.pdf">github搭建hexo博客.pdf</a></li>
<li><a href="http://www.ask3.cn/files/gpu%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95.pdf">gpu压力测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hadoop3.0%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%BA%A0%E5%88%A0%E7%A0%81.pdf">hadoop3.0中使用纠删码.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hadoop%E7%94%9F%E6%80%81%E5%9C%88.pdf">hadoop生态圈.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hadoop%E8%B7%A8%E9%9B%86%E7%BE%A4%E4%B9%8B%E9%97%B4%E8%BF%81%E7%A7%BBhive%E6%95%B0%E6%8D%AE.pdf">hadoop跨集群之间迁移hive数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hadoop%E9%9B%86%E7%BE%A4tmp%E7%9B%AE%E5%BD%95%E8%AE%B8%E5%A4%9ADATANODE_.hprof%E6%96%87%E4%BB%B6.pdf">hadoop集群tmp目录许多DATANODE_.hprof文件.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hadoop%E9%9B%86%E7%BE%A4%E6%8F%90%E9%AB%98%E7%A3%81%E7%9B%98_IO_%E7%9A%84%E6%95%88%E7%8E%87%E3%80%81%E6%8F%90%E5%8D%87%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%82.pdf">hadoop集群提高磁盘_IO_的效率、提升文件系统的性能。.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase2.0__master.pdf">hbase2.0__master.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase_jdbc%E8%BF%9E%E6%8E%A5.pdf">hbase_jdbc连接.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase_shel_l%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%BF%87%E6%BB%A4%E5%99%A8.pdf">hbase_shel_l中常用的过滤器.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase_%E7%A0%B4%E5%9D%8F%E6%80%A7%E6%B5%8B%E8%AF%95_%E5%8F%8Cmaster%E6%8C%82%E6%8E%89.pdf">hbase_破坏性测试_双master挂掉.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase%E5%BF%AB%E7%85%A7.pdf">hbase快照.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4%E8%A1%A8%E6%95%B0%E6%8D%AE.pdf">hbase批量删除表数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD%E5%AE%9E%E6%88%98.pdf">hbase数据备份实战.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hdfs_balancer.pdf">hdfs_balancer.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hdfs_%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%A4%B9%E5%90%8D%E7%A7%B0(1).pdf">hdfs_批量修改文件夹名称(1).pdf</a></li>
<li><a href="http://www.ask3.cn/files/hdfs_%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%A4%B9%E5%90%8D%E7%A7%B0.pdf">hdfs_批量修改文件夹名称.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hdfs%E5%9D%97%E4%B8%A2%E5%A4%B1%E5%9D%97%E5%AF%BC%E8%87%B4%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%A7%A3%E5%86%B3.pdf">hdfs块丢失块导致的异常问题排查解决.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive,impala%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8.pdf">hive,impala客户端使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive_jdbc_%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95.pdf">hive_jdbc_压力测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive_%E7%94%9F%E4%BA%A7%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE.pdf">hive_生产测试数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive_%E8%A7%A3%E9%94%81.pdf">hive_解锁.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive_%E8%BF%87%E6%BB%A4%E7%89%B9%E6%AE%8A%E5%AD%97%E7%AC%A6.pdf">hive_过滤特殊字符.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%85%B3%E8%81%94es_,json%E5%B5%8C%E5%A5%97%EF%BC%8Cstruct%EF%BC%8Carry_%E7%AD%89%E7%89%B9%E6%AE%8A%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86.pdf">hive关联es_,json嵌套，struct，arry_等特殊类型数据处理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%88%9B%E5%BB%BA%E6%B0%B8%E4%B9%85%E5%87%BD%E6%95%B0%E5%92%8C%E4%B8%B4%E6%97%B6%E5%87%BD%E6%95%B0.pdf">hive创建永久函数和临时函数.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%8F%AF%E8%A7%86%E5%8C%96%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%E6%80%BB%E7%BB%93.pdf">hive可视化权限控制总结.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%A4%84%E7%90%86json%E5%92%8C%E6%95%B0%E7%BB%84%E6%95%B0%E6%8D%AE.pdf">hive处理json和数组数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%AF%BC%E6%95%B0%E6%8D%AE%E5%88%B0neo4j.pdf">hive导数据到neo4j.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%BF%AB%E9%80%9F%E5%A4%8D%E5%88%B6%E4%B8%80%E5%BC%A0%E5%88%86%E5%8C%BA%E8%A1%A8.pdf">hive快速复制一张分区表.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.pdf">hive性能优化.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E6%89%A7%E8%A1%8C%E6%97%A5%E5%BF%97%E8%A7%A3%E6%9E%90%EF%BC%8Cjob%E6%97%A5%E5%BF%97%E8%A7%A3%E6%9E%90.pdf">hive执行日志解析，job日志解析.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E6%98%A0%E5%B0%84hbase.pdf">hive映射hbase.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E6%98%A0%E5%B0%84phoenix_&amp;&amp;_cdh_hive%E7%BB%84%E4%BB%B6%E5%8D%87%E7%BA%A7.pdf">hive映射phoenix_&amp;&amp;_cdh_hive组件升级.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6.pdf">hive权限控制.pdf</a></li>
<li><a href="http://www.ask3.cn/files/http%E4%B8%8B%E8%BD%BDhdfsd.pdf">http下载hdfsd.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hue4.2_%E8%BF%9E%E6%8E%A5hbase_Api_Error_timed_out.pdf">hue4.2_连接hbase_Api_Error_timed_out.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hue%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEHbase.pdf">hue远程访问Hbase.pdf</a></li>
<li><a href="http://www.ask3.cn/files/impala.pdf">impala.pdf</a></li>
<li><a href="http://www.ask3.cn/files/impala%E4%BC%98%E5%8C%96.pdf">impala优化.pdf</a></li>
<li><a href="http://www.ask3.cn/files/impala%E4%BD%BF%E7%94%A8udf%E5%87%BD%E6%95%B0.pdf">impala使用udf函数.pdf</a></li>
<li><a href="http://www.ask3.cn/files/impala%E5%90%8C%E6%AD%A5%E5%85%83%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8E%9F%E5%88%99.pdf">impala同步元数据使用原则.pdf</a></li>
<li><a href="http://www.ask3.cn/files/impala%E9%A9%B1%E5%8A%A8%E7%9A%84%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F.pdf">impala驱动的连接方式.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ipython%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD,_iPython_%E5%AE%89%E8%A3%85_%E6%AF%94shell%E5%A5%BD%E7%94%A8.pdf">ipython常用功能,_iPython_安装_比shell好用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/janusgraph%E9%83%A8%E7%BD%B2.pdf">janusgraph部署.pdf</a></li>
<li><a href="http://www.ask3.cn/files/js_%E8%8E%B7%E5%8F%96%E5%BD%93%E5%A4%A9%E5%87%8C%E6%99%A8%E7%9A%84%E6%97%B6%E9%97%B4%E6%88%B3%EF%BC%8C%E5%87%A0%E5%A4%A9%E5%89%8D%E5%87%8C%E6%99%A8%E7%9A%84%E6%97%B6%E9%97%B4%E6%88%B3.pdf">js_获取当天凌晨的时间戳，几天前凌晨的时间戳.pdf</a></li>
<li><a href="http://www.ask3.cn/files/json_%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5hive_%E5%88%A9%E7%94%A8_get_json_object_%E5%92%8Cjson_tuple_%E5%87%BD%E6%95%B0.pdf">json_数据导入hive_利用_get_json_object_和json_tuple_函数.pdf</a></li>
<li><a href="http://www.ask3.cn/files/jstat_-gcutil_%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8.pdf">jstat_-gcutil_命令使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/jvm%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%E8%B0%83%E4%BC%98.pdf">jvm垃圾回收器调优.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kafka_%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3.pdf">kafka_参数详解.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kerberos%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93.pdf">kerberos使用总结.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kubeflow_%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2.pdf">kubeflow_安装部署.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kubernetes_pv_pvc%E4%B8%8Enfs_%E6%B5%8B%E8%AF%95.pdf">kubernetes_pv_pvc与nfs_测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kudu_%EF%BC%8CNot_enough_live_tablet_servers_to_create_a_table.pdf">kudu_，Not_enough_live_tablet_servers_to_create_a_table.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kudu%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.pdf">kudu常见问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kudu%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.pdf">kudu性能调优.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kylin%E5%AE%89%E8%A3%85_%E5%92%8Ckylin%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86.pdf">kylin安装_和kylin用户权限管理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ldap_%E6%B7%BB%E5%8A%A0%E7%B4%A2%E5%BC%95.pdf">ldap_添加索引.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ldap%E5%AE%9E%E7%8E%B0Linux%E7%99%BB%E5%BD%95%E8%B4%A6%E5%8F%B7%E7%BB%9F%E4%B8%80%E7%AE%A1%E7%90%86.pdf">ldap实现Linux登录账号统一管理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ldap%E8%BF%87%E6%BB%A4%E5%99%A8.pdf">ldap过滤器.pdf</a></li>
<li><a href="http://www.ask3.cn/files/linux_%E7%94%A8%E6%88%B7%E8%BF%81%E7%A7%BB.pdf">linux_用户迁移.pdf</a></li>
<li><a href="http://www.ask3.cn/files/linux%E4%B8%8B%E8%A7%A3%E5%8E%8B%E7%BC%A9rar%E6%A0%BC%E5%BC%8F%E7%9A%84%E5%8E%8B%E7%BC%A9%E5%8C%85.pdf">linux下解压缩rar格式的压缩包.pdf</a></li>
<li><a href="http://www.ask3.cn/files/linux%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5_NTP%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8.pdf">linux时间同步_NTP配置与开机自启动.pdf</a></li>
<li><a href="http://www.ask3.cn/files/linux%E6%9F%A5%E7%9C%8B%E6%9F%90%E4%B8%AA%E6%97%B6%E9%97%B4%E6%AE%B5%E7%9A%84%E6%97%A5%E5%BF%97.pdf">linux查看某个时间段的日志.pdf</a></li>
<li><a href="http://www.ask3.cn/files/linux%E6%9F%A5%E7%9C%8B%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%97%A5%E5%BF%97%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E7%94%A8%E6%93%8D%E4%BD%9C.pdf">linux查看系统的日志的一些实用操作.pdf</a></li>
<li><a href="http://www.ask3.cn/files/messagehive.metastore.sasl.enabled_can%E2%80%99t_be_false_in_non-testing_mode.pdf">messagehive.metastore.sasl.enabled_can’t_be_false_in_non-testing_mode.pdf</a></li>
<li><a href="http://www.ask3.cn/files/messages%E6%97%A5%E5%BF%97%E8%BF%87%E6%BB%A4%E6%8E%89ldapd%E7%9A%84%E9%94%99%E8%AF%AF.pdf">messages日志过滤掉ldapd的错误.pdf</a></li>
<li><a href="http://www.ask3.cn/files/mt5__K%E7%BA%BF%E5%9B%BE_%E6%9C%80%E5%8F%B3%E8%BE%B9%E7%95%99%E7%82%B9%E7%A9%BA%E7%99%BD.pdf">mt5__K线图_最右边留点空白.pdf</a></li>
<li><a href="http://www.ask3.cn/files/mysql%E4%B8%8Ehive_sql_%E5%AF%B9%E6%AF%94.pdf">mysql与hive_sql_对比.pdf</a></li>
<li><a href="http://www.ask3.cn/files/nfs%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4.pdf">nfs常用操作命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/nfs%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86.pdf">nfs问题处理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/nohup%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8Cscp,ftp%E7%AD%89%E9%9C%80%E8%A6%81%E8%BE%93%E5%85%A5%E5%AF%86%E7%A0%81%E7%9A%84%E5%91%BD%E4%BB%A4.pdf">nohup后台运行scp,ftp等需要输入密码的命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/nxlog%E9%85%8D%E7%BD%AE%E5%B8%AE%E5%8A%A9.pdf">nxlog配置帮助.pdf</a></li>
<li><a href="http://www.ask3.cn/files/oozie_%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C.pdf">oozie_命令行操作.pdf</a></li>
<li><a href="http://www.ask3.cn/files/oozie%E8%B0%83%E5%BA%A6ssh%E6%89%A7%E8%A1%8Cshell.pdf">oozie调度ssh执行shell.pdf</a></li>
<li><a href="http://www.ask3.cn/files/openldap_ssl%E9%85%8D%E7%BD%AE.pdf">openldap_ssl配置.pdf</a></li>
<li><a href="http://www.ask3.cn/files/openldap%E5%AE%89%E8%A3%85%EF%BC%8Chue%E3%80%81hive%E3%80%81impala%E9%9B%86%E6%88%90ldap.pdf">openldap安装，hue、hive、impala集成ldap.pdf</a></li>
<li><a href="http://www.ask3.cn/files/openldap%E8%AE%BE%E7%BD%AE%E7%94%A8%E6%88%B7%E4%BF%AE%E6%94%B9%E5%AF%86%E7%A0%81%E6%9D%83%E9%99%90_%E5%AF%86%E7%A0%81%E8%BF%87%E6%9C%9F.pdf">openldap设置用户修改密码权限_密码过期.pdf</a></li>
<li><a href="http://www.ask3.cn/files/oracle_%E5%92%8CSAS%E4%B9%8B%E9%97%B4%E4%BC%A0%E9%80%92%E6%95%B0%E6%8D%AE_sas%E4%B8%AD%E6%96%87%E6%97%A5%E6%9C%9F%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA.pdf">oracle_和SAS之间传递数据_sas中文日期格式输出.pdf</a></li>
<li><a href="http://www.ask3.cn/files/pgsql%E4%BD%BF%E7%94%A8%E5%B8%AE%E5%8A%A9.pdf">pgsql使用帮助.pdf</a></li>
<li><a href="http://www.ask3.cn/files/pip.pdf">pip.pdf</a></li>
<li><a href="http://www.ask3.cn/files/pip_%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8_%E4%BB%A3%E7%90%86%E5%AE%89%E8%A3%85.pdf">pip_代理服务器_代理安装.pdf</a></li>
<li><a href="http://www.ask3.cn/files/pm2.pdf">pm2.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto-elasticsearch.pdf">presto-elasticsearch.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto_admin_%E5%AE%89%E8%A3%85.pdf">presto_admin_安装.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto_%E6%9B%B4%E6%96%B0hive%E6%95%B0%E6%8D%AE_insert_owerwrite__table.pdf">presto_更新hive数据_insert_owerwrite__table.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto%E6%9F%A5%E8%AF%A2%E5%8C%BA%E5%88%86%E5%A4%A7%E5%B0%8F%E5%86%99%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93(mysql%EF%BC%8Cmongo)%E8%A1%A8%EF%BC%8C%E6%8A%A5%E9%94%99%E8%A1%A8%E5%90%8D%E4%B8%8D%E5%AD%98%E5%9C%A8.pdf">presto查询区分大小写的数据库(mysql，mongo)表，报错表名不存在.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto%E9%85%8D%E7%BD%AEldap%E7%94%A8%E4%BA%8E%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81.pdf">presto配置ldap用于用户认证.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86.pdf">presto集群管理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python-module_'pymysql'_has_no_attribute_'connect'.pdf">python-module_‘pymysql’<em>has_no_attribute</em>’connect’.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python3%E6%93%8D%E4%BD%9Chive.pdf">python3操作hive.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python_%E6%93%8D%E4%BD%9Cneo4j.pdf">python_操作neo4j.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python%E4%B8%AD%E7%9A%84urlencode%E4%B8%8Eurldecode.pdf">python中的urlencode与urldecode.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python%E6%93%8D%E4%BD%9Chive.pdf">python操作hive.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python%E6%93%8D%E4%BD%9Cimpala.pdf">python操作impala.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%B0%86%E7%BD%91%E9%A1%B5%E4%B8%AD%E6%89%80%E6%9C%89img_src=XXX_%E5%BD%A2%E5%BC%8F%E4%B8%AD%E7%9A%84XXX%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8F%90%E5%8F%96%E5%87%BA.pdf">python用正则表达式将网页中所有img_src=XXX_形式中的XXX的字符串提取出.pdf</a></li>
<li><a href="http://www.ask3.cn/files/solr_%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96.pdf">solr_查询优化.pdf</a></li>
<li><a href="http://www.ask3.cn/files/spark%E5%81%9Aetl%E6%B8%85%E6%B4%97json%E6%95%B0%E6%8D%AE.pdf">spark做etl清洗json数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/spark%E8%B0%83%E4%BC%98.pdf">spark调优.pdf</a></li>
<li><a href="http://www.ask3.cn/files/sqoop%E8%BF%9E%E6%8E%A5oralce.pdf">sqoop连接oralce.pdf</a></li>
<li><a href="http://www.ask3.cn/files/top%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97.pdf">top命令指南.pdf</a></li>
<li><a href="http://www.ask3.cn/files/visionapp_Remote_Desktop_2010.pdf">visionapp_Remote_Desktop_2010.pdf</a></li>
<li><a href="http://www.ask3.cn/files/win10_%E4%BD%BF%E7%94%A8Tor_%E5%88%9B%E5%BB%BA%E5%8C%BF%E5%90%8D%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%8C%BF%E5%90%8D%E7%BD%91%E7%AB%99%EF%BC%88.onion.pdf">win10_使用Tor_创建匿名服务和匿名网站（.onion.pdf</a></li>
<li><a href="http://www.ask3.cn/files/win10%E4%B8%8B%E5%8F%8C%E7%B3%BB%E7%BB%9F_%E4%BD%BF%E7%94%A8%E5%B8%AE%E5%8A%A9.pdf">win10下双系统_使用帮助.pdf</a></li>
<li><a href="http://www.ask3.cn/files/windows%E4%B8%AD%E7%B1%BB%E4%BC%BClinux%E7%9A%84ln%E5%91%BD%E4%BB%A4.pdf">windows中类似linux的ln命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/window%E4%B8%8Bpython%E8%BF%9E%E6%8E%A5hive.pdf">window下python连接hive.pdf</a></li>
<li><a href="http://www.ask3.cn/files/wordpress_%E4%B8%BB%E9%A2%98.pdf">wordpress_主题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/zeppelin.pdf">zeppelin.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%B8%8A%E6%B5%B7%E5%8D%81%E6%9D%A1%E9%AA%91%E8%BD%A6%E8%B7%AF%E7%BA%BF%E6%8E%A8%E8%8D%90.pdf">上海十条骑车路线推荐.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8HiveServer2%E7%AE%A1%E7%90%86udf%E5%87%BD%E6%95%B0.pdf">使用HiveServer2管理udf函数.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8Hive%E8%AF%BB%E5%86%99ElasticSearch%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE.pdf">使用Hive读写ElasticSearch中的数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8PHP%E7%9A%84mail%E5%87%BD%E6%95%B0%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6.pdf">使用PHP的mail函数发送邮件.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8dd%E6%B5%8B%E8%AF%95%E7%A1%AC%E7%9B%98%E8%AF%BB%E5%86%99%E9%80%9F%E5%BA%A6%EF%BC%8C%E5%AE%9E%E6%B5%8B%E8%85%BE%E8%AE%AF%E4%BA%91%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8.pdf">使用dd测试硬盘读写速度，实测腾讯云阿里云服务器.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8python%E6%9F%A5%E8%AF%A2Elasticsearch%E5%B9%B6%E5%AF%BC%E5%87%BA%E6%89%80%E6%9C%89%E6%95%B0%E6%8D%AE.pdf">使用python查询Elasticsearch并导出所有数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8scp%E4%BF%9D%E7%95%99%E6%9D%83%E9%99%90.pdf">使用scp保留权限.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BF%AE%E6%94%B9cdh5%E9%9B%86%E7%BE%A4%E4%B8%AD%E4%B8%BB%E6%9C%BAhostName.pdf">修改cdh5集群中主机hostName.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%85%A5%E8%81%8C%E6%8C%87%E5%8D%97.pdf">入职指南.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%88%86%E5%8C%BA%E8%A1%A8%E5%A2%9E%E5%8A%A0%E5%AD%97%E6%AE%B5%E6%8A%A5%E9%94%99_Unable_to_alter_partition._alter_is_not_possible.pdf">分区表增加字段报错_Unable_to_alter_partition._alter_is_not_possible.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%9B%9B%E7%A7%8D%E6%96%B9%E6%B3%95%E6%8A%8AmongDB%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E5%88%B0hive%E6%88%96Hbase.pdf">四种方法把mongDB迁移数据到hive或Hbase.pdf</a></li>
<li>[图数据库JanusGraph实战<a href="http://www.ask3.cn/files/%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93JanusGraph%E5%AE%9E%E6%88%98%5B5%5D_JanusGraph%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BGephi.pdf">5]_JanusGraph可视化之Gephi.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%9C%A8HBase%E9%9B%86%E7%BE%A4%E8%BF%90%E8%A1%8C%E7%9A%84%E6%97%B6%E5%80%99%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB.pdf">在HBase集群运行的时候进行数据迁移.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%9C%A8Hadoop_%E4%B8%8A%E8%BF%90%E8%A1%8CTensorflow.pdf">在Hadoop_上运行Tensorflow.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%E7%9F%A5%E8%AF%86.pdf">堆外内存知识.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%B5%B7%E9%87%8F%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%A6%82%E4%BD%95%E8%BF%90%E7%BB%B4.pdf">大数据环境下海量服务器如何运维.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%A6%82%E4%BD%95%E4%B8%BAPresto%E9%9B%86%E6%88%90Kerberos%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Hive.pdf">如何为Presto集成Kerberos环境下的Hive.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8StreamSets%E5%AE%9E%E7%8E%B0MySQL%E4%B8%AD%E5%8F%98%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%86%99%E5%85%A5Kudu.pdf">如何使用StreamSets实现MySQL中变化数据实时写入Kudu.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEimpala%E8%87%AA%E5%8A%A8%E5%90%8C%E6%AD%A5HMS%E5%85%83%E6%95%B0%E6%8D%AE.pdf">如何配置impala自动同步HMS元数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%AE%89%E8%A3%85kafka%E6%8E%A7%E5%88%B6%E5%8F%B0kafka_web_console.pdf">安装kafka控制台kafka_web_console.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%AE%9D%E4%BB%98%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8_.pdf">宝付业务数据库表_.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%B0%86%E7%B3%BB%E7%BB%9F%E7%94%A8%E6%88%B7%E5%AF%BC%E5%85%A5FreeIPA%E4%B8%AD.pdf">将系统用户导入FreeIPA中.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%B8%A6kerberos%E8%AE%A4%E8%AF%81%E7%9A%84hdfs%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C.pdf">带kerberos认证的hdfs文件操作.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%BC%80%E9%80%9A%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%E5%90%8Ecsv%E5%AF%BC%E5%85%A5hive%E5%BB%BA%E8%A1%A8%E9%97%AE%E9%A2%98.pdf">开通权限管理后csv导入hive建表问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%BC%82%E5%B8%B8%E6%97%A5%E5%BF%97%EF%BC%9A_No_data_or_no_sasl_data_in_the_stream.pdf">异常日志：_No_data_or_no_sasl_data_in_the_stream.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4hive%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93.pdf">强制删除hive的数据库.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%89%93%E5%8D%B0Spark_RDD%E4%B8%AD%E7%9A%84top_n_%E5%86%85%E5%AE%B9.pdf">打印Spark_RDD中的top_n_内容.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%8A%8Ahdfs%E4%B8%8A%E7%9A%84%E5%A4%9A%E4%B8%AA%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6%E4%B8%BA%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6.pdf">把hdfs上的多个目录下的文件合并为一个文件.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%8B%9B%E8%81%98%E9%9D%A2%E8%AF%95.pdf">招聘面试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%90%AD%E5%BB%BAOpenLDAP%E8%87%AA%E5%8A%A9%E4%BF%AE%E6%94%B9%E5%AF%86%E7%A0%81%E7%B3%BB%E7%BB%9FSelf_Service_Password.pdf">搭建OpenLDAP自助修改密码系统Self_Service_Password.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%90%AD%E5%BB%BAtensorflow-gpu%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83,GPU%E6%B5%8B%E8%AF%95.pdf">搭建tensorflow-gpu深度学习环境,GPU测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%95%88%E7%8E%87%E7%AC%94%E8%AE%B0_2019.04.18.pdf">效率笔记_2019.04.18.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%96%B0%E7%A3%81%E7%9B%98%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%B8%8E%E6%8C%82%E8%BD%BD.pdf">新磁盘格式化与挂载.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%97%B6%E9%92%9F%E5%81%8F%E5%B7%AE.pdf">时钟偏差.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%9D%83%E9%99%90_sentry%E8%BF%81%E7%A7%BB.pdf">权限_sentry迁移.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%9F%A5%E7%9C%8Byarn%E6%97%A5%E5%BF%97%E6%8A%A5%E9%94%99Error_getting_logs_at_hostname8041.pdf">查看yarn日志报错Error_getting_logs_at_hostname8041.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%9F%A5%E8%AF%A2cloudera__manager_%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9.pdf">查询cloudera__manager_配置修改.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%A0%B9%E6%8D%AEmapreduce_%E6%A0%B9%E6%8D%AE_job_id_%E5%BF%AB%E9%80%9F%E6%9F%A5%E5%8E%9F%E5%9B%A0.pdf">根据mapreduce_根据_job_id_快速查原因.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8%E6%9C%89%E9%81%93%E4%BA%91%E7%AC%94%E8%AE%B0.pdf">欢迎使用有道云笔记.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E5%85%A8%E6%96%B0%E7%9A%84%E6%9C%89%E9%81%93%E4%BA%91%E7%AC%94%E8%AE%B0.pdf">欢迎来到全新的有道云笔记.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B5%8B%E8%AF%95cpu%E5%92%8Cgpu%E7%9A%84%E9%80%9F%E5%BA%A6%E5%B7%AE%E8%B7%9D.pdf">测试cpu和gpu的速度差距.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83Kerberos%E3%80%81LDAP%E5%AE%89%E8%A3%85.pdf">测试环境Kerberos、LDAP安装.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B7%BB%E5%8A%A0pgsql%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE_&amp;&amp;_%E9%87%8D%E5%90%AFcloudera_manager%E7%9A%84pgsql.pdf">添加pgsql远程访问_&amp;&amp;_重启cloudera_manager的pgsql.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B9%98%E4%B9%A1%E8%AF%9D%E6%96%B9%E8%A8%80%E7%BF%BB%E8%AF%91.pdf">湘乡话方言翻译.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B9%98%E6%BD%AD%E7%AB%9E%E4%BB%B7%E5%89%8D%E5%8F%B0%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.pdf">湘潭竞价前台管理系统.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B9%98%E6%BD%AD%E7%AB%9E%E4%BB%B7%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.pdf">湘潭竞价后台管理系统.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%94%A8StreamSets%E5%AE%9E%E7%8E%B0MySQL%E4%B8%AD%E5%8F%98%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%86%99%E5%85%A5Kudu.pdf">用StreamSets实现MySQL中变化数据实时写入Kudu.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%94%A8cloudera_manager_%E5%AE%89%E8%A3%85kudu%E6%97%B6%E6%8A%A5%E9%94%99%E8%AF%AF.pdf">用cloudera_manager_安装kudu时报错误.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%9B%91%E5%90%AC%E7%AB%AF%E5%8F%A3_%E5%8F%91%E7%8E%B0%E7%A8%8B%E5%BA%8F%E5%AE%95%E4%BA%86%E3%80%82%E9%87%8D%E5%90%AF%E7%A8%8B%E5%BA%8F.pdf">监听端口_发现程序宕了。重启程序.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%9B%B4%E6%8E%A5%E4%BB%8Ehdfs%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE.pdf">直接从hdfs下载数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%A3%81%E7%9B%98%E4%B8%8D%E8%83%BD%E8%AF%BB%E5%86%99.pdf">磁盘不能读写.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%A6%81%E7%94%A8Hive.pdf">禁用Hive.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%A6%81%E7%94%A8_ssh.pdf">禁用_ssh.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%80%81%E9%9B%86%E7%BE%A4%E8%A1%A5%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE.pdf">老集群补历史数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7_Ansible_%E5%9C%A8%E9%83%A8%E7%BD%B2%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E4%B8%8B%E7%9A%84%E5%BA%94%E7%94%A8.pdf">自动化运维工具_Ansible_在部署大数据平台下的应用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%A7%A3%E5%86%B3RegionServer_%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E6%95%B0%E6%8A%A5%E8%AD%A6.pdf">解决RegionServer_打开文件描述符数报警.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%A7%A3%E5%86%B3_wordpress_ftp%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5.pdf">解决_wordpress_ftp无法连接.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%A7%A3%E5%86%B3hive_comment_%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98.pdf">解决hive_comment_中文乱码问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%A7%A3%E5%86%B3parquet%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8FImpala%E4%B8%8EHive%E6%97%A5%E6%9C%9F%E6%97%B6%E9%97%B4%E4%B8%8D%E5%90%8C.pdf">解决parquet文件格式Impala与Hive日期时间不同.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%A7%A3%E5%86%B3spark_streaming%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97%E4%B8%8D%E6%96%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98.pdf">解决spark_streaming长时间运行日志不断增长问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%BA%BF%E4%B8%8A%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%A2%91%E7%B9%81%E5%AE%95%E6%9C%BA.pdf">记一次线上服务器频繁宕机.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%AE%BE%E7%BD%AE%E6%B5%8F%E8%A7%88%E5%99%A8%E5%85%81%E8%AE%B8Kerberos%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81.pdf">设置浏览器允许Kerberos身份验证.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%B0%88%E8%B0%88spark%E4%B8%AD%E5%AF%B9RDD%E7%9A%84%E8%AE%A4%E8%AF%86%E3%80%82.pdf">谈谈spark中对RDD的认识。.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%B7%A8%E7%89%88%E6%9C%ACdistcp%E6%8A%A5Check-sum%E9%94%99%E8%AF%AF.pdf">跨版本distcp报Check-sum错误.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%BF%9C%E7%A8%8B%E5%8A%9E%E5%85%AC%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88_%E6%90%AD%E5%BB%BA%E5%B1%80%E5%9F%9F%E7%BD%91vpn%E6%9C%8D%E5%8A%A1%E5%99%A8.pdf">远程办公解决方案_搭建局域网vpn服务器.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%BF%9E%E6%8E%A5HiveServer2%E4%BC%A0%E9%80%92%E5%8F%82%E6%95%B0%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F.pdf">连接HiveServer2传递参数的几种方式.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%83%A8%E7%BD%B2Harbor%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93.pdf">部署Harbor私有镜像仓库.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%85%8D%E7%BD%AEOpenLDAP%E7%9A%84%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6.pdf">配置OpenLDAP的日志文件.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%98%BF%E9%87%8Capi%E4%BD%BF%E7%94%A8.pdf">阿里api使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%9B%86%E5%9B%A2%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E6%96%B9%E6%A1%88%E8%A7%84%E5%88%92.pdf">集团大数据平台项目整体方案规划.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%9B%86%E5%9B%A2%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E6%95%B4%E6%94%B9%E6%96%B9%E6%A1%88%E8%A7%84%E5%88%92.pdf">集团大数据平台项目整改方案规划.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%9B%86%E7%BE%A4%E6%8B%86%E5%88%86%E9%A1%B9%E7%9B%AE%E5%B7%A5%E4%BD%9C%E8%BF%9B%E5%BA%A6.pdf">集群拆分项目工作进度.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%9B%86%E7%BE%A4%E6%96%B0%E5%8A%A0%E8%8A%82%E7%82%B9%EF%BC%8C%E5%AE%89%E8%A3%85tensorflow.pdf">集群新加节点，安装tensorflow.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%9D%9E%E5%B8%B8%E7%AE%80%E5%8D%95%E7%9A%84PYTHON_HTTP%E6%9C%8D%E5%8A%A1.pdf">非常简单的PYTHON_HTTP服务.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%AA%91%E8%A1%8C%E9%81%82%E6%98%8C.pdf">骑行遂昌.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%B1%BC%E5%84%BF%E8%80%81%E5%B8%88%E7%9A%84%E7%AC%94%E8%AE%B0.pdf">鱼儿老师的笔记.pdf</a></li>
<li></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>IPFS音乐播放器</title>
    <url>/2020/09/30/IPFS%E9%9F%B3%E4%B9%90%E6%92%AD%E6%94%BE%E5%99%A8/</url>
    <content><![CDATA[<h1>IPFS音乐播放器</h1>
<h2 id="IPFS相关">IPFS相关</h2>
<h3 id="IPFS第一次亲密接触">IPFS第一次亲密接触</h3>
<ul>
<li>什么是IPFS</li>
<li>IPFS对比HTTP/FTP等协议的优势</li>
<li>IPFS应用场景</li>
</ul>
<p>-移动数据 交易 路由 网络</p>
<ul>
<li>定义数据 命名</li>
<li>使用数据</li>
</ul>
<p>具体场景;<br>
挂载全球文件<br>
版本管理功能<br>
数据库<br>
加密平台<br>
各种类型cdn<br>
永久访问的链接</p>
<h3 id="ipfs入门">ipfs入门</h3>
<ul>
<li>官网地址：<a href="https://ipfs.io/" target="_blank" rel="noopener">https://ipfs.io</a></li>
<li>下载安装：<a href="https://dist.ipfs.io/#go-ipfs" target="_blank" rel="noopener">https://dist.ipfs.io/#go-ipfs</a></li>
<li>节点初始化
<ul>
<li><code>ipfs init</code></li>
<li>如果出现<code>Error: cannot acquire lock: can't lock file</code>删除其后边给出的repo.lock文件即可</li>
</ul>
</li>
<li>节点配置
<ul>
<li><code>ipfs id</code> 查看当前节点id等信息</li>
<li><code>ipfs config show</code> ipfs配置信息</li>
</ul>
</li>
<li>节点服务器daemon
<ul>
<li><code>ipfs daemon</code></li>
</ul>
</li>
<li>修改IPFS默认路径</li>
<li>开放API请求
<ul>
<li>ipfs config --json Addresses.API ‘&quot;/ip4/0.0.0.0/tcp/5001&quot;’</li>
</ul>
</li>
<li>开放公共网关
<ul>
<li>ipfs config --json Addresses.Gateway ‘&quot;/ip4/0.0.0.0/tcp/8080&quot;’</li>
</ul>
</li>
</ul>
<h3 id="发布数据">发布数据</h3>
<ul>
<li>
<p>上传文件</p>
<p><code>ipfs add haha.txt</code>文本</p>
<p><code>ipfs add cat.jpg</code>图片</p>
<p><code>ipfs add -q cat.jpg</code> 只输出hash结果</p>
<p>hash记录返回的文件路径信息</p>
</li>
<li>
<p>上传/查看目录</p>
<p>添加目录: <code>ipfs add -r dir</code></p>
<p>查看目录: <code>ipfs ls Qmej3u92BHJVkkiC8F5uL1J4a98no76TnXmQxHGXCHGE7t</code></p>
</li>
<li>
<p>发布网站</p>
<p>把包含index.html的文件夹add到ipfs网络即可</p>
</li>
</ul>
<h3 id="获取数据">获取数据</h3>
<p>命令行获取 cat/get</p>
<p><code>ipfs cat QmaoahyA1UejtgPifPvjpsviePsDnJn8vHrYAN9nfF7w9N</code> 查看文本</p>
<p><code>ipfs cat QmaoahyA1UejtgPifPvjpsviePsDnJn8vHrYAN9nfF7w9N &gt; new-haha.txt</code> 保存文本</p>
<p><code>ipfs cat Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u &gt; new-cat.jpg</code>保存图片</p>
<p><code>ipfs get QmZSwQghczB41omjfJipFPQ3FovYyLvXR9oa4Y9LQS7Urp -o tomcat.jpg</code>获取并保存文件</p>
<p><code>ipfs get QmZSwQghczB41omjfJipFPQ3FovYyLvXR9oa4Y9LQS7Urp -Cao tomcat-go</code> 压缩并下载文件</p>
<p><code>ipfs get QmYtrQVXatZGm1WRKZ29vUt5tFa24xnTvhv6D71DSNVttZ -o ipfs-day01</code>获取文件夹</p>
<p>浏览器访问获取</p>
<ol>
<li>
<p>在http://localhost:5001/webui下搜索hash值，可以raw查看原数据，download下载文件</p>
</li>
<li>
<p>在8080端口访问：<a href="http://localhost:8080/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u" target="_blank" rel="noopener">http://localhost:8080/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u</a></p>
</li>
<li>
<p>在ipfs.io访问：<a href="https://ipfs.io/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u" target="_blank" rel="noopener">https://ipfs.io/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u</a></p>
<p>不需要翻墙的网关：<a href="https://ipfs.infura.io/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u" target="_blank" rel="noopener">https://ipfs.infura.io/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u</a></p>
</li>
</ol>
<blockquote>
<p>文本数据<br>
图片数据<br>
音频数据</p>
</blockquote>
<h3 id="files文件-文件夹操作">files文件/文件夹操作</h3>
<p><code>ipfs files cp &lt;文件/文件夹的hash&gt; &lt;目标文件/文件夹&gt;</code> 拷贝文件</p>
<p><code>ipfs files ls -l</code> 查看目录</p>
<p><code>ipfs files mkdir</code> 创建目录</p>
<p><code>ipfs files cp</code> 拷贝</p>
<p><code>ipfs files mv</code> 移动</p>
<p><code>ipfs files stat</code> 状态</p>
<p><code>ipfs files read</code> 读取</p>
<h2 id="React音乐播放器">React音乐播放器</h2>
<h3 id="音乐列表数据">音乐列表数据</h3>
<ul>
<li>音乐名称/歌手等文本信息</li>
<li>专辑图片展示</li>
</ul>
<h3 id="播放音频数据">播放音频数据</h3>
<ul>
<li>
<p>网易音乐api地址：</p>
<p><a href="https://github.com/Binaryify/NeteaseCloudMusicApi" target="_blank" rel="noopener">https://github.com/Binaryify/NeteaseCloudMusicApi</a></p>
</li>
<li>
<p>ipfs-api：<a href="https://github.com/ipfs/js-ipfs-api" target="_blank" rel="noopener">https://github.com/ipfs/js-ipfs-api</a></p>
<ul>
<li>初始化环境端口号开启服务：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Show the ipfs config API port to check it is correct</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> ipfs config Addresses.API</span></span><br><span class="line">/ip4/127.0.0.1/tcp/5001</span><br><span class="line"><span class="meta">#</span><span class="bash"> Set it <span class="keyword">if</span> it does not match the above output</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> ipfs config Addresses.API /ip4/127.0.0.1/tcp/5001</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Restart the daemon after changing the config</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run the daemon</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> ipfs daemon</span></span><br></pre></td></tr></table></figure>
<ul>
<li>cat获取数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipfs.files.cat(&quot;QmY4NqRyr9SebC3P6W3pzg22UK3QsJNDKGzDHqQZsEyPi3&quot;, function (err, file) &#123;</span><br><span class="line">    if (err) &#123;</span><br><span class="line">        throw err</span><br><span class="line">    &#125;</span><br><span class="line">    const json &#x3D; file.toString(&#39;utf8&#39;);</span><br><span class="line">    console.log(json)</span><br><span class="line">    that.setState(&#123;</span><br><span class="line">        songInfo: JSON.parse(json)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>add 添加数据</li>
</ul>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> ipfsAPI = <span class="built_in">require</span>(<span class="string">'ipfs-api'</span>)</span><br><span class="line"><span class="keyword">const</span> ipfs = ipfsAPI(<span class="string">'localhost'</span>, <span class="string">'5001'</span>, &#123;<span class="attr">protocol</span>: <span class="string">'http'</span>&#125;)</span><br><span class="line"><span class="keyword">const</span> buffer = Buffer.from(<span class="string">'hello ipfs-api!'</span>)</span><br><span class="line">ipfs.add(buffer)</span><br><span class="line">    .then( <span class="function"><span class="params">rsp</span> =&gt;</span> <span class="built_in">console</span>.log(rsp[<span class="number">0</span>].hash))</span><br><span class="line">	.catch(<span class="function"><span class="params">e</span> =&gt;</span> <span class="built_in">console</span>.error(e))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>设置cors</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ipfs config --json API.HTTPHeaders.Access-Control-Allow-Methods '["PUT", "GET", "POST", "OPTIONS"]'</span><br><span class="line">ipfs config --json API.HTTPHeaders.Access-Control-Allow-Origin '["*"]'</span><br><span class="line">ipfs config --json API.HTTPHeaders.Access-Control-Allow-Credentials '["true"]'</span><br><span class="line">ipfs config --json API.HTTPHeaders.Access-Control-Allow-Headers '["Authorization"]'</span><br><span class="line">ipfs config --json API.HTTPHeaders.Access-Control-Expose-Headers '["Location"]'</span><br></pre></td></tr></table></figure>
<h3 id="ipns">ipns</h3>
<ul>
<li>绑定ipfs节点, 把一个文件/文件夹的hash发布到自己的ID下</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipfs name publish QmSx37PT8iV2XxzfHLMRYSxZEt87uE3jdQwCyz7otd5ktP</span><br></pre></td></tr></table></figure>
<ul>
<li>查看节点绑定的ipfs路径</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipfs name resolve [peerId]</span><br></pre></td></tr></table></figure>
<ul>
<li>离线客户端框架： <a href="https://github.com/electron/electron" target="_blank" rel="noopener">https://github.com/electron/electron</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>hbase常用命令操作实战</title>
    <url>/2020/09/11/hbase%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C%E5%AE%9E%E6%88%98/</url>
    <content><![CDATA[<h2 id="hbase-hbck2">hbase-hbck2</h2>
<p>从官网下载hbck2 执行文件</p>
<p><a href="http://hbase.apache.org/downloads.html" target="_blank" rel="noopener">http://hbase.apache.org/downloads.html</a></p>
<p>或 wget  <a href="https://www.apache.org/dyn/closer.lua/hbase/hbase-operator-tools-1.0.0/hbase-operator-tools-1.0.0-bin.tar.gz" target="_blank" rel="noopener">https://www.apache.org/dyn/closer.lua/hbase/hbase-operator-tools-1.0.0/hbase-operator-tools-1.0.0-bin.tar.gz</a></p>
<p>cdh官网的使用帮助</p>
<p><a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hbase_hbck.html#concept_hkk_q25_llb" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hbase_hbck.html#concept_hkk_q25_llb</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /opt/hbase-operator-tools-1.0.0/hbase-hbck2</span><br><span class="line"></span><br><span class="line">hbase hbck -j hbase-hbck2-1.0.0.jar -s  assigns 1588230740</span><br><span class="line"></span><br><span class="line">hbase hbck -j hbase-hbck2-1.0.0.jar -s  assigns hbase:namespace,,1594264903686.db55eec81d86ac0ae26eba718518ce26</span><br><span class="line"></span><br><span class="line">hbase hbck -j hbase-hbck2-1.0.0.jar -s addFsRegionsMissingInMeta default:test n1:tbl_2 n2</span><br></pre></td></tr></table></figure>
<h2 id="hbase-shell-实战">hbase shell  实战</h2>
<p>移动表的数据到另外一个服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 语法：move &#39;encodeRegionName&#39;, &#39;ServerName&#39;&#96;&#96;</span><br><span class="line"># encodeRegionName指的regioName后面的编码，ServerName指的是master-status的Region Servers列表</span><br><span class="line">move &#39;61a647884b761a785daf3b38049aaa27&#39;,&#39;bigdata-5.baofoo.cn,16020,1595233360076&#39;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="如何查看HBase的HFile">如何查看HBase的HFile</h3>
<p><a href="https://www.jianshu.com/p/49043e99795a" target="_blank" rel="noopener">https://www.jianshu.com/p/49043e99795a</a></p>
<h3 id="跨集群备份数据">跨集群备份数据</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">正则查询快照</span></span><br><span class="line">list_snapshots 'snapshot.*_20200902'</span><br><span class="line">echo "list_snapshots 'snapshot.*_`date -d "1 days ago" +%Y%m%d`' " |sudo -u hbase hbase shell </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">同步快照到备份集群</span></span><br><span class="line">curl "http://cdh85-49:20550/" &gt; hbase_tbls.txt</span><br><span class="line">cat hbase_tbls.txt | tr ':' '-' | while read tb; do </span><br><span class="line">sudo -u hbase hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \</span><br><span class="line">-snapshot "snapshot_$&#123;tb&#125;_20200902" \</span><br><span class="line">-copy-from hdfs://cdh85-49:8020/hbase \</span><br><span class="line">-copy-to hdfs://cdh85-106:8020/hbase ;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">还原快照</span></span><br><span class="line">cat hbase_tbls.txt | tr ':' '-' | while read tb; do </span><br><span class="line">echo  "restore_snapshot 'snapshot_$&#123;tb&#125;_20200902'" | sudo -u hbase hbase shell ;  </span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<h3 id="定时备份-批量脚本程序">定时备份  批量脚本程序</h3>
<h3 id="hbase-snapshots-sh-root-cdh85-55">hbase <a href="http://snapshots.sh/" target="_blank" rel="noopener">snapshots.sh</a> root@cdh85-55</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">PRG="$&#123;0&#125;"</span><br><span class="line">BASEDIR=`dirname $&#123;PRG&#125;`</span><br><span class="line">BASEDIR=`cd $&#123;BASEDIR&#125;/;pwd`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 0. 列出所有hbase表 准备做快照</span></span></span><br><span class="line">echo "######################### job start: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line">echo "######################### 0.list hbase tables;"</span><br><span class="line">curl "http://bigdata-5.baofoo.cn:20550/" &gt; $&#123;BASEDIR&#125;/tbls.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1. 生成新的当日的快照  快照命名方式  snapshot_NAMESPARCE-TABLE_YYYYMMDD</span></span></span><br><span class="line">echo "######################### 1.create hbase table snapshots;"</span><br><span class="line">cat  $&#123;BASEDIR&#125;/tbls.txt | while read tbls; do</span><br><span class="line">echo  "######################### create '$&#123;tbls&#125;' snapshot:`date '+%Y-%m%-d %H:%M:%S'`" </span><br><span class="line">echo  "snapshot '$&#123;tbls&#125;','snapshot_$&#123;tbls&#125;_`date +%Y%m%d`' " | awk -F, '&#123; gsub(":","-",$2) ;print $1","$2&#125;' | sudo -u hbase hbase shell ;  done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2.删除指定hbase集群快照， 入参 &#123; $1:n , 表示n天前的快照，比如 0：当天 1：一天前 2：两天前&#125;</span></span></span><br><span class="line">echo "######################### 2.delete hbase history snapshots;"</span><br><span class="line">find_snapshot_list()&#123;</span><br><span class="line">echo "list_snapshots 'snapshot.*_`date -d "$&#123;1&#125; days ago" +%Y%m%d`' " |sudo -u hbase hbase shell  ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">delete_snapshot_list()&#123;</span><br><span class="line">find_snapshot_list $1 | grep "\\[.*\\]"  | sed 's/[]["]//g' | tr ',' '\n' | while read word; do</span><br><span class="line">echo  "delete_snapshot '$&#123;word&#125;'" |sudo -u hbase hbase shell</span><br><span class="line">done</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">删除1天前快照</span></span><br><span class="line">delete_snapshot_list 0</span><br><span class="line"></span><br><span class="line">echo "######################### job end: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">cd /root/hbase_snapshots/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 0. 列出所有hbase表 准备做快照</span></span></span><br><span class="line">echo "######################### job start: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line">echo "######################### 0.list hbase tables;"</span><br><span class="line">curl "http://cdh85-49:20550/" &gt; tbls.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1. 生成新的当日的快照  快照命名方式  snapshot_NAMESPARCE-TABLE_YYYYMMDD</span></span></span><br><span class="line">echo "######################### 1.create hbase table snapshots;"</span><br><span class="line">cat tbls.txt | while read tbls; do</span><br><span class="line">echo  "######################### create '$&#123;tbls&#125;' snapshot:`date '+%Y-%m%-d %H:%M:%S'`" </span><br><span class="line">echo  "snapshot '$&#123;tbls&#125;','snapshot_$&#123;tbls&#125;_`date +%Y%m%d`'; sleep 1; " | awk -F, '&#123; gsub(":","-",$2) ;print $1","$2&#125;' | sudo -u hbase hbase shell ;  done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2.删除指定hbase集群快照， 入参 &#123; $1:n , 表示n天前的快照，比如 0：当天 1：一天前 2：两天前&#125;</span></span></span><br><span class="line">echo "######################### 2.delete hbase history snapshots;"</span><br><span class="line">find_snapshot_list()&#123;</span><br><span class="line">echo "list_snapshots 'snapshot.*_`date -d "$&#123;1&#125; days ago" +%Y%m%d`' " |sudo -u hbase hbase shell  ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">delete_snapshot_list()&#123;</span><br><span class="line">find_snapshot_list $1 | grep "\\[.*\\]"  | sed 's/[]["]//g' | tr ',' '\n' | while read word; do</span><br><span class="line">echo  "delete_snapshot '$&#123;word&#125;'" |sudo -u hbase hbase shell</span><br><span class="line">done</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">删除1天前快照</span></span><br><span class="line">delete_snapshot_list 1</span><br><span class="line"></span><br><span class="line">echo "######################### job end: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 0. 列出所有hbase表 准备做快照</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"list"</span> | sudo -u hbase hbase shell |grep <span class="string">"\\[.*\\]"</span> &gt; a.log</span></span><br><span class="line">curl "http://bigdata-5.baofoo.cn:20550/" &gt; hbase_tbls.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1. 删除当前集群历史快照</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"delete_all_snapshot 'snapshot.*' "</span> |sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase shell  ;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">cat a.log   | sed <span class="string">'s/[]["]//g'</span> | tr <span class="string">','</span> <span class="string">'\n'</span> | <span class="keyword">while</span> <span class="built_in">read</span> word; <span class="keyword">do</span> <span class="built_in">echo</span>  <span class="string">"delete_snapshot   'snapshot_<span class="variable">$&#123;word&#125;</span>_`date -d '1 days ago' +%Y%m%d`' "</span> | awk -F<span class="string">"   "</span> <span class="string">'&#123; gsub(":","-",$2) ;print $1" "$2&#125;'</span> |sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase shell  ; <span class="keyword">done</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2. 生成新的当日的快照  快照命名方式  snapshot_NAMESPARCE-TABLE_YYYYMMDD</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">cat a.log   | sed <span class="string">'s/[]["]//g'</span> | tr <span class="string">','</span> <span class="string">'\n'</span> | <span class="keyword">while</span> <span class="built_in">read</span> word; <span class="keyword">do</span> \</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span>  <span class="string">"snapshot '<span class="variable">$&#123;word&#125;</span>' ,  'snapshot_<span class="variable">$&#123;word&#125;</span>_`date +%Y%m%d`' "</span> | awk -F, <span class="string">'&#123; gsub(":","-",$2) ;print $1","$2&#125;'</span> | sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase shell ;  <span class="keyword">done</span></span></span><br><span class="line"></span><br><span class="line">cat hbase_tbls.txt | while read word; do \</span><br><span class="line">echo  "snapshot '$&#123;word&#125;' ,  'snapshot_$&#123;word&#125;_`date +%Y%m%d`' " | awk -F, '&#123; gsub(":","-",$2) ;print $1","$2&#125;' | sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase shell ;  done</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  3. 删除备份集群历史快照</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">cat a.log |sed <span class="string">'s/"//g'</span>   | sed <span class="string">'s/[][]//g'</span> | tr <span class="string">','</span> <span class="string">'\n'</span> | <span class="keyword">while</span> <span class="built_in">read</span> word; <span class="keyword">do</span> <span class="built_in">echo</span>  <span class="string">"delete_snapshot   'snapshot_<span class="variable">$&#123;word&#125;</span>_`date -d '1 days ago' +%Y%m%d`' "</span> | awk -F<span class="string">"   "</span> <span class="string">'&#123; gsub(":","-",$2) ;print $1" "$2&#125;'</span> |sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase2 shell  ; <span class="keyword">done</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">" delete_all_snapshot 'snapshot.*' "</span> |sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase2 shell  </span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 4. 同步快照到备份集群</span></span></span><br><span class="line">cat a.log |sed 's/"//g'   | sed 's/[][]//g' | tr ',' '\n' | tr ':' '-'  | while read word;do </span><br><span class="line">sudo -u hbase hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \</span><br><span class="line">-Dmapreduce.job.queuename=bf_yarn_pool.development \</span><br><span class="line">--snapshot 'snapshot_$&#123;word&#125;_`date +%Y%m%d`'  \</span><br><span class="line">--copy-from hdfs://ns1/hbase3  \</span><br><span class="line">--copy-to hdfs://ns1/hbase9 \</span><br><span class="line">--chuser hbase -chgroup hbase --overwrite</span><br><span class="line">;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 查找历史快照，   入参 2个 &#123; $1:n , 表示n天前的快照，比如 0：当天 1：一天前 2：两天前；$2: hbase config路径&#125;</span></span></span><br><span class="line"></span><br><span class="line">find_snapshot_list()&#123;</span><br><span class="line">echo "list_snapshots 'snapshot.*_`date -d "$&#123;1&#125; days ago" +%Y%m%d`' " |sudo -u hbase hbase --config  $2 shell  ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 查找一天前快照 hbase1集群</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">find_snapshot_list 1 /etc/hbase/conf.cloudera.hbase</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 删除指定hbase集群快照， 入参 2个，&#123; $1:n , 表示n天前的快照，比如 0：当天 1：一天前 2：两天前；$2: hbase config路径 &#125;</span></span></span><br><span class="line">delete_snapshot_list()&#123;</span><br><span class="line">find_snapshot_list $1 $2 | grep "\\[.*\\]"  | sed 's/[]["]//g' | tr ',' '\n' | while read word; do</span><br><span class="line">echo  "delete_snapshot '$&#123;word&#125;'" |sudo -u hbase hbase --config $2  shell</span><br><span class="line">done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">delete_snapshot_list 1 /etc/hbase/conf.cloudera.hbase</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cat a.log   | sed 's/[]["]//g' | tr ',' '\n' | while read word; do echo   '$&#123;word&#125;' ;done</span><br></pre></td></tr></table></figure>
<h3 id="hbase-restore-snapshot-sh"><a href="http://hbase-restore-snapshot.sh/" target="_blank" rel="noopener">hbase-restore-snapshot.sh</a></h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">PRG="$&#123;0&#125;"</span><br><span class="line">BASEDIR=`dirname $&#123;PRG&#125;`</span><br><span class="line">BASEDIR=`cd $&#123;BASEDIR&#125;/;pwd`</span><br><span class="line"></span><br><span class="line">usage()&#123;</span><br><span class="line">read -p "DO YOU WANT RESTORE ALL HBASE TABLES? PLEASE INPUT[yes OR no]:" yn</span><br><span class="line">case $yn in</span><br><span class="line">        [Yy]* ) continue;;</span><br><span class="line">        [Nn]* ) exit 1;;</span><br><span class="line">        * ) echo "Please answer yes or no.";exit 2;;</span><br><span class="line">esac</span><br><span class="line">&#125;</span><br><span class="line">usage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 0. 列出所有hbase表 准备还原快照</span></span></span><br><span class="line">echo "######################### job start: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line">echo "######################### 0.list hbase tables;"</span><br><span class="line">curl "http://bigdata-3.baofoo.cn:20550/" &gt; $&#123;BASEDIR&#125;/tbls.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1.disable  hbase tables.</span></span></span><br><span class="line">echo "######################### 1.disable hbase  table; "</span><br><span class="line">cat  $&#123;BASEDIR&#125;/tbls.txt | while read tbls; do </span><br><span class="line">echo  "######################### disable '$&#123;tbls&#125;' " </span><br><span class="line">echo  "disable '$&#123;tbls&#125;'  " | sudo -u hbase hbase shell ;  done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2.恢复指定hbase集群快照， 入参 &#123; $1:n , 表示n天前的快照，比如 0：当天 1：一天前 2：两天前&#125;</span></span></span><br><span class="line">echo "######################### 2.restore_snapshot "</span><br><span class="line">find_snapshot_list()&#123;</span><br><span class="line">echo "list_snapshots 'snapshot.*_`date -d "$&#123;1&#125; days ago" +%Y%m%d`' " |sudo -u hbase hbase shell  ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">restore_snapshot_()&#123;</span><br><span class="line">find_snapshot_list $1 | grep "\\[.*\\]"  | sed 's/[]["]//g' | tr ',' '\n' | while read word; do</span><br><span class="line">echo "######################### 2.restore_snapshot '$&#123;word&#125;'; "</span><br><span class="line">echo  "restore_snapshot  '$&#123;word&#125;'" |sudo -u hbase hbase shell</span><br><span class="line">done</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">恢复1天前快照</span></span><br><span class="line">restore_snapshot_ 1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 3.enable  hbase tables.</span></span></span><br><span class="line">echo "######################### 3.enable hbase  table; "</span><br><span class="line">cat  $&#123;BASEDIR&#125;/tbls.txt | while read tbls; do </span><br><span class="line">echo  "######################### enable '$&#123;tbls&#125;' " </span><br><span class="line">echo  "enable '$&#123;tbls&#125;'  "| sudo -u hbase hbase shell ;  done</span><br><span class="line"></span><br><span class="line">echo "######################### job end: `date '+%Y-%m%-d %H:%M:%S'`"</span><br></pre></td></tr></table></figure>
<h3 id="hbase-restore-snashot-1-sh">hbase_restore_snashot_1.sh</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">PRG="$&#123;0&#125;"</span><br><span class="line">BASEDIR=`dirname $&#123;PRG&#125;`</span><br><span class="line">BASEDIR=`cd $&#123;BASEDIR&#125;/;pwd`</span><br><span class="line"></span><br><span class="line">usage()&#123;</span><br><span class="line">echo "RESTORE ALL HBASE TABLES,steps:1.disable hbase tables;2.restore_snapshot;3.enable hbase tables."</span><br><span class="line">read -p "DO YOU WANT RESTORE ALL HBASE TABLES? PLEASE INPUT[yes OR no]:" yn</span><br><span class="line">case $yn in</span><br><span class="line">        [Yy]* ) continue;;</span><br><span class="line">        [Nn]* ) exit 1;;</span><br><span class="line">        * ) echo "Please answer yes or no.";exit 2;;</span><br><span class="line">esac</span><br><span class="line">&#125;</span><br><span class="line">usage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 0. 列出所有hbase表 准备还原快照</span></span></span><br><span class="line">echo "######################### job start: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line">echo "######################### 0.list hbase tables;"</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#curl "http://bigdata-3.baofoo.cn:20550/" &gt; $&#123;BASEDIR&#125;/tbls.txt</span></span></span><br><span class="line">curl "http://cdh85-49:20550/" &gt; $&#123;BASEDIR&#125;/tbls.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1.disable  hbase tables;  2.restore_snapshot;   3.enable hbase tables;</span></span></span><br><span class="line"></span><br><span class="line">cat  $&#123;BASEDIR&#125;/tbls.txt | while read tbls; do </span><br><span class="line">echo  "######################### 1. disable '$&#123;tbls&#125;' ：`date '+%Y-%m%-d %H:%M:%S'` " </span><br><span class="line">echo  "disable '$&#123;tbls&#125;'" | sudo -u hbase hbase shell ;  </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">deal_date=`date -d <span class="string">"0 days ago"</span> +%Y%m%d`</span></span><br><span class="line">deal_date=`date -d "1 days ago" +%Y%m%d`</span><br><span class="line">tbls_r=`echo $&#123;tbls&#125;|tr ':' '-'`</span><br><span class="line">echo  "######################### 2. restore_snapshot 'snapshot_$&#123;tbls_r&#125;_$&#123;deal_date&#125;'：`date '+%Y-%m%-d %H:%M:%S'`  "</span><br><span class="line">echo  "restore_snapshot 'snapshot_$&#123;tbls_r&#125;_$&#123;deal_date&#125;'" | sudo -u hbase hbase shell ;  </span><br><span class="line"></span><br><span class="line">echo  "######################### 3. enable '$&#123;tbls&#125;' ：`date '+%Y-%m%-d %H:%M:%S'` " </span><br><span class="line">echo  "enable '$&#123;tbls&#125;' " | sudo -u hbase hbase shell ;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">done</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>CDH-HBase 使用 HBCK2 运维</title>
    <url>/2020/08/27/CDH-HBase%20%E4%BD%BF%E7%94%A8%20HBCK2%20%E8%BF%90%E7%BB%B4/</url>
    <content><![CDATA[<h4 id="前言">前言</h4>
<p>周末 CDH6.3 的集群断电，导致 HBase 出现 RIT 状态。</p>
<p>赶紧把之前学的 hbck2 的知识实践顺便回顾下</p>
<h4 id="过程">过程</h4>
<p>将项目拉取到本地 <code>git clone https://github.com/apache/hbase-operator-tools.git --depth 1</code></p>
<p>编译出jar包上传到集群上 <code>mvn clean package -Dmaven.skip.test=true</code></p>
<p>CDH 集群的话将其上传至<code>/opt/cloudera/parcels/CDH/lib/hbase/lib</code>路径下</p>
<h4 id="使用">使用</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hbase.HBCK2 &lt;命令&gt;</span><br><span class="line"></span><br><span class="line"># 验证是否可以使用</span><br><span class="line">hbase org.apache.hbase.HBCK2 -v</span><br></pre></td></tr></table></figure>
<p>结果当头一棒，不支持 2.1.0-cdh6.3.1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">12:48:02.084 [main] INFO  org.apache.hadoop.hbase.client.ConnectionImplementation - Closing master protocol: MasterService</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.UnsupportedOperationException: bypass not supported on server version&#x3D;2.1.0-cdh6.3.1; needs at least a server that matches or exceeds [2.0.3, 2.1.1, 2.2.0, 3.0.0]</span><br><span class="line">        at org.apache.hbase.HBCK2.checkHBCKSupport(HBCK2.java:134)</span><br><span class="line">        at org.apache.hbase.HBCK2.bypass(HBCK2.java:335)</span><br><span class="line">        at org.apache.hbase.HBCK2.doCommandLine(HBCK2.java:686)</span><br><span class="line">        at org.apache.hbase.HBCK2.run(HBCK2.java:631)</span><br><span class="line">        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)</span><br><span class="line">        at org.apache.hbase.HBCK2.main(HBCK2.java:865)</span><br></pre></td></tr></table></figure>
<p>版本不支持，翻阅文档使用<code>-s</code>跳过版本检查，即 <code>hbase org.apache.hbase.HBCK2 -s</code></p>
<p>更多的命令参考可以参阅下方链接</p>
<ul>
<li><a href="https://github.com/apache/hbase-operator-tools/tree/master/hbase-hbck2" target="_blank" rel="noopener">Apache HBase HBCK2 Tool</a></li>
<li><a href="https://mp.weixin.qq.com/s/GVMWwB1WsKcdvZGfvX1lcA" target="_blank" rel="noopener">HBase 2.0之修复工具HBCK2运维指南</a></li>
</ul>
<h4 id="其他">其他</h4>
<p>Region in transition 的信息是在 active hmater WEB UI 页面上查看，<br>
如果没有RIT状态的 region，其不会显示，所以正常 HBase 集群是看不到 Region in transition 的内容</p>
]]></content>
  </entry>
  <entry>
    <title>Linux(CentOS7)修改mysql默认数据文件目录</title>
    <url>/2020/02/28/Linux(CentOS7)%E4%BF%AE%E6%94%B9mysql%E9%BB%98%E8%AE%A4%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95/</url>
    <content><![CDATA[<p>默认情况下<code>mysql</code>的数据路径应该在</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd /var/lib/mysql</span></span><br></pre></td></tr></table></figure>
<p>现在我们要将它转移到<code>/data</code></p>
<ol>
<li>停掉mysql服务</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># service mysql stop</span></span><br></pre></td></tr></table></figure>
<ol>
<li>将原数据目录转移到<code>data</code>目录下</li>
</ol>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line"># mv /<span class="keyword">var</span>/lib/mysql /<span class="keyword">data</span>/</span><br></pre></td></tr></table></figure>
<ol>
<li>修改<code># vi /etc/my.cnf</code>文件,增加以下行</li>
</ol>
<figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line">datadir = <span class="regexp">/data/my</span>sql</span><br><span class="line">socket = <span class="regexp">/data/my</span>sql/mysql.sock</span><br></pre></td></tr></table></figure>
<ol>
<li>修改<code># vi /etc/init.d/mysql</code>文件,增加以下行</li>
</ol>
<figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line">datadir = <span class="regexp">/data/my</span>sql</span><br></pre></td></tr></table></figure>
<ol>
<li>如果你的 <code># vi /usr/bin/mysqld_safe</code>里面也有指定mysql的数据目录，那么也请按照上面修改</li>
<li>重启<code>mysql</code></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># service mysql restart</span></span><br></pre></td></tr></table></figure>
<ol>
<li>如果没有成功，重启报错：</li>
</ol>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">尴尬。。忘了提示，差不多就是说找不到/<span class="keyword">var</span>/lib/mysql/mysql.sock</span><br></pre></td></tr></table></figure>
<p>给mysql.sock做个链接</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line"># ln -s /<span class="keyword">data</span>/mysql/mysql.sock /<span class="keyword">var</span>/lib/mysql/mysql.sock</span><br></pre></td></tr></table></figure>
<p>如果提示该链接已经存在，辣就到<code># /var/lib/mysql/</code> <code># rm mysql.sock</code>再进行以上操作。</p>
<ol>
<li>再重启<code>mysql</code>，祝你成功。</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>hexo 常用命令</title>
    <url>/2020/02/25/hexo%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>hexo 常用命令 ：</p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line">$ hexo generate (hexo g) 生成静态文件</span><br><span class="line">$ hexo server (hexo s) 启动本地服务</span><br><span class="line">$ hexo deploy (hexo d) 提交到远程仓库</span><br><span class="line">$ hexo new page <span class="string">"xx"</span>(hexo n page) 创建页面 </span><br><span class="line">$ hexo new <span class="string">"xx"</span> (hexo n <span class="string">""</span>) 创建文章</span><br><span class="line">$ hexo d -g 生成静态并提交到远程仓库</span><br><span class="line">$ hexo s -g 生成静态文件并启动本地预览</span><br><span class="line">$ hexo clean (hexo cl)清除本地 public 文件</span><br></pre></td></tr></table></figure>
<p>其他参考：</p>
<p>5分钟搞定个人博客-hexo     <a href="https://www.jianshu.com/p/390f202c5b0e" target="_blank" rel="noopener">https://www.jianshu.com/p/390f202c5b0e</a></p>
<p>换终端更新hexo博客  <a href="https://www.jianshu.com/p/6a29f5243ab4" target="_blank" rel="noopener">https://www.jianshu.com/p/6a29f5243ab4</a></p>
]]></content>
  </entry>
  <entry>
    <title>Linux查找含有某字符串的所有文件</title>
    <url>/2020/02/25/Linux%E6%9F%A5%E6%89%BE%E5%90%AB%E6%9C%89%E6%9F%90%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%89%80%E6%9C%89%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<p>如果你想在当前目录下 查找&quot;hello,world!&quot;字符串,可以这样:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep -rn &quot;hello,world!&quot; *</span><br></pre></td></tr></table></figure>
<p><code>*</code> : 表示当前目录所有文件，也可以是某个文件名</p>
<ul>
<li>-r 是递归查找</li>
<li>-n 是显示行号</li>
<li>-R 查找所有文件包含子目录</li>
<li>-i 忽略大小写</li>
</ul>
<h4 id="下面是一些有意思的命令行参数：">下面是一些有意思的命令行参数：</h4>
<p>grep -i pattern files ：不区分大小写地搜索。默认情况区分大小写，<br>
grep -l pattern files ：只列出匹配的文件名，<br>
grep -L pattern files ：列出不匹配的文件名，<br>
grep -w pattern files ：只匹配整个单词，而不是字符串的一部分（如匹配‘magic’，而不是‘magical’），</p>
<p>grep -C number pattern files ：匹配的上下文分别显示[number]行，<br>
grep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的行，<br>
grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。</p>
<h4 id="这里还有些用于搜索的特殊符号：">这里还有些用于搜索的特殊符号：</h4>
<p>&lt; 和 &gt; 分别标注单词的开始与结尾。</p>
<h4 id="例如：">例如：</h4>
<p>grep man  *会匹配 ‘Batman’、‘manic’、‘man’等，<br>
grep '\ 匹配‘manic’和‘man’，但不是‘Batman’，<br>
grep ‘’ 只匹配‘man’，而不是‘Batman’或‘manic’等其他的字符串。</p>
<p><code>'^'</code>：指匹配的字符串在行首，<br>
<code>'$'</code>：指匹配的字符串在行尾，</p>
<h2 id="xargs配合grep查找">xargs配合grep查找</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find -type f -name &#39;*.php&#39;|xargs grep &#39;GroupRecord&#39;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>CDH6 新加节点，不能关掉 Auto-TLS 的解决办法</title>
    <url>/2020/01/16/CDH6%20%E6%96%B0%E5%8A%A0%E8%8A%82%E7%82%B9%EF%BC%8C%E4%B8%8D%E8%83%BD%E5%85%B3%E6%8E%89%20Auto-TLS%20%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    <content><![CDATA[<h1>CDH6 新加节点，不能关掉 Auto-TLS 的解决办法</h1>
<p>坑一：新加节点 必须关掉TLS ，但是关不到</p>
<p>参考官网</p>
<p><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html</a></p>
<p>使用“添加主机向导”添加主机</p>
<p>您可以使用“添加主机”向导在主机上安装CDH，Impala和Cloudera Manager Agent。</p>
<ol>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__title_214" target="_blank" rel="noopener">禁用TLS加密或身份验证</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__section_t4z_zyk_kcb" target="_blank" rel="noopener">在不禁用TLS的情况下安装Cloudera Manager Agent的替代方法</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__title_215" target="_blank" rel="noopener">使用“添加主机向导”</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__title_217" target="_blank" rel="noopener">启用TLS加密或身份验证</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__section_ejz_fdw_yr" target="_blank" rel="noopener">为CDH组件启用TLS / SSL</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__section_ny1_bxv_ls" target="_blank" rel="noopener">启用Kerberos</a></li>
</ol>
<p>分析：</p>
<p><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/install_cm_server.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/6.1/topics/install_cm_server.html</a></p>
<p>由于安装的时候设置了</p>
<p>sudo JAVA_HOME=/usr/java/jdk1.8.0_141-cloudera /opt/cloudera/cm-agent/bin/certmanager setup --configure-services</p>
<p>导致不能关掉 Auto-TLS</p>
<p>解决办法：</p>
<p>1.cm后台关掉</p>
<p><img src="/images/image-20200116143338485.png" alt="image-20200116143338485"></p>
<ol start="2">
<li>备份cm_init.txt， 然后清空这个文件的内容</li>
</ol>
<p>cp /var/lib/cloudera-scm-server/certmanager/cm_init.txt</p>
<p>3.修改每一个节点的 agent的config.ini</p>
<p>vi /etc/cloudera-scm-agent/config.ini</p>
<p>use_tls = 1 改成 use_tls = 0</p>
<p>4.重启服务</p>
<p>systemctl restart cloudera-scm-server</p>
<p>systemctl restart cloudera-scm-agent</p>
]]></content>
  </entry>
  <entry>
    <title>CDH集群部署</title>
    <url>/2020/01/08/CDH%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h2 id="一、集群规划">一、集群规划</h2>
<p>如果你正准备从0开始搭建一套CDH集群应用于生产环境，那么此时需要做的事情应该是 <strong>结合当前的数据、业务、硬件、节点、服务等对集群做合理的规划</strong>，而不是马上动手去安装软件。</p>
<p>合理的集群规划应该做到以下几点：</p>
<ul>
<li>充分了解当前的数据现状</li>
<li>与业务方深入沟通，了解将会在集群上运行的业务，集群将会为业务提供什么服务</li>
<li>结合数据现状与业务，合理预估未来的数据量增长</li>
<li>盘点当前可用的硬件资源，包括机柜机架、服务器、交换机等</li>
<li>当前硬件资源不充足的情况下，根据数据评估情况作出采购建议</li>
<li>根据业务属性与组成，合理规划集群的部署架构</li>
<li>根据可用硬件资源，对集群节点的服务角色进行合理划分</li>
</ul>
<p>以上步骤完成之后才是动手进行安装与部署。</p>
<p>你将会对集群的架构模式、应用方向与业务场景了然于胸，并确保这个集群（或者是集群组）能够<strong>提供稳定、高效、高性能的服务</strong>，为业务保驾护航。</p>
<p>并有能力能够提供 <strong>集群建设目标</strong>：</p>
<ul>
<li>
<p>性能需求</p>
</li>
<li>
<ul>
<li>简单查询100G数据量时，耗时上限</li>
<li>复杂查询（join）时，耗时上限</li>
<li>历史数据导入时，耗时上限</li>
<li>增量数据导入时，耗时上限</li>
</ul>
</li>
<li>
<p>可靠性需求：每月宕机次数（&lt;1），每月宕机时间（&lt;10min）</p>
</li>
<li>
<p>可用性：每台机器每月的宕机时间</p>
</li>
<li>
<p>容错性：机器故障，数据不丢失</p>
</li>
</ul>
<h3 id="1-1-硬件规划">1.1 硬件规划</h3>
<p>硬件规划决定集群将使用多少硬件资源，以及什么配置的硬件资源。</p>
<p>可以从以下几个维度进行评估：</p>
<ul>
<li>
<p>数据现状</p>
</li>
<li>
<ul>
<li>盘点所有数据情况，包括数据源、数据量、数据大小、数据维度等信息</li>
</ul>
</li>
<li>
<p>工作负载</p>
</li>
<li>
<ul>
<li>评估在集群与数据之上将执行的任务类型</li>
<li>如实时计算、离线计算、图像处理、关系网络等应用场景以及是否提供OLTP服务等</li>
</ul>
</li>
<li>
<p>未来数据量预估</p>
</li>
<li>
<ul>
<li>根据数据源与业务应用场景可以对未来衍生的数据总量与数据增量做大致评估</li>
<li>评估的时间范围视业务场景而定，建议做不少于一年的规划</li>
</ul>
</li>
<li>
<p>硬件资源现状</p>
</li>
<li>
<ul>
<li>盘点目前可用的硬件资源，确认是否满足所评估的规模及要求</li>
<li>机房机柜空间、电源（双）等是否充足（需考虑后续扩容问题）</li>
<li>网络交换机性能是否满足要求（建议万兆网卡（双））</li>
<li>查看服务器磁盘、内存、CPU等资源是否需要补充</li>
</ul>
</li>
<li>
<p>硬件选择</p>
</li>
<li>
<ul>
<li>现有硬件资源不满足的需求的情况下，结合运维建议提出需要增加或者新采购的硬件型号、配置等</li>
<li>确认所需服务器数量</li>
</ul>
</li>
</ul>
<p>示例主机列表：</p>
<ul>
<li>cdh2-1</li>
<li>cdh2-2</li>
<li>cdh2-3</li>
<li>cdh2-4</li>
<li>cdh2-5</li>
<li>cdh2-6</li>
<li>cdh2-7</li>
<li>cdh2-8</li>
</ul>
<p>服务器硬件情况如下：</p>
<ul>
<li>数量：8</li>
<li>CPU：10</li>
<li>内存：64G</li>
<li>硬盘：3.3T</li>
</ul>
<h3 id="1-2-集群架构">1.2 集群架构</h3>
<h3 id="混合型集群">混合型集群</h3>
<p>指由一个统一的大集群提供所有大数据服务，所有组件集中安装在同一个集群中，有部署简单、运维方便、易于使用等优点。</p>
<p>但是由于混合型集群集群承载了所有功能，职能繁多，网络带宽、磁盘IO等为集群共享，会因大型离线任务占用大量网络或磁盘IO峰值，对线上业务会造成短暂延迟。</p>
<p>且集群环境较为复杂，有较多对线上业务造成影响的风险。</p>
<h3 id="专用型集群">专用型集群</h3>
<p>专用型集群指根据不同的需求与功能职责对集群进行划分，由多个职责不同、硬件隔离的集群组成集群组环境提供服务。</p>
<p>子集群各司其职，根据自身业务最大化利用硬件资源，互相独立互不影响。部署较为复杂，运维难度增加。</p>
<p>专用型集群根据业务与应用场景可以划分如下：</p>
<ul>
<li>离线计算集群</li>
<li>实时计算集群</li>
<li>数据服务集群</li>
<li>GPU深度学习集群</li>
<li>图数据库集群</li>
</ul>
<p>等等。</p>
<p>HBase提供实时读写服务的生产环境下<strong>建议将HBase集群独立部署为数据服务集群</strong>，参考：<a href="https://zhuanlan.zhihu.com/p/72150364" target="_blank" rel="noopener">HBase最佳实践</a> - 「集群部署」小节。</p>
<h3 id="1-3-节点规划">1.3 节点规划</h3>
<p>进行节点角色划分时尽可能遵守以下原则：</p>
<ul>
<li>
<p>CM监控服务在小集群下可以部署在同一主机，大集群下需要独立部署</p>
</li>
<li>
<p>集群主节点与子节点独立部署（HDFS/HBase/Yarn），且各自子节点部署在相同主机上</p>
</li>
<li>
<ul>
<li>独立部署可以避免子节点大量读写、计算引起IO、CPU、网络等资源阻塞而导致主节点异常甚至宕机</li>
<li>HDFS/HBase/Yarn部署在相同主机上可以最大化利用数据本地化特性</li>
<li>如果数据量巨大，而集群存储空间不足的时候忽视以上两点，满足业务需求放在第一位</li>
</ul>
</li>
<li>
<p>Hive、Hue、Impala、Sentry等服务/元数据服务需要部署在同一主机</p>
</li>
<li>
<ul>
<li>Hue+Sentry对Hive与Impala进行权限控制的时候需要读取Linux主机的用户与用户组进行判别，如果部署在不同主机上则需要在每个主机上创建相同的用户与用户组</li>
<li>或者使用LDAP进行账号管理</li>
<li>条件允许情况下，独立主机或者压力小的主机</li>
</ul>
</li>
<li>
<p>Zookeeper尽量使用5个节点，且条件允许下最好在不同的物理主机上</p>
</li>
<li>
<ul>
<li>5个节点的zk可以保证leader的快速选举</li>
<li>在不同的物理主机上可以最大限度保证安全</li>
</ul>
</li>
</ul>
<p>示例集群主要进程分布如下：</p>
<ul>
<li>CM服务(monitors)：cdh2-4</li>
<li>HDFS主节点（NameNode）：cdh2-1,cdh2-2</li>
<li>HDFS容灾节点(JournalNode)：cdh2-[1:3]</li>
<li>HDFS数据节点（DataNode）：cdh2-[3:8]</li>
<li>HBase主节点(HMaster)：cdh2-1,cdh2-2</li>
<li>HBase数据节点(RegionServer)：cdh2-[3:8]</li>
<li>Yarn主节点(ResourceManager)：cdh2-1,cdh2-2</li>
<li>Yarn子节点(NodeManager)：cdh2-[3:8]</li>
<li>Hive元数据服务（Metastore、HiveServer2）：cdh2-3</li>
<li>Hue服务(HueServer)：cdh2-3</li>
<li>Impala服务(CatalogServer、StateStore)：cdh2-3</li>
<li>Sentry权限验证服务（SentryServer）：cdh2-3</li>
<li>Oozie服务(OozieServer)：cdh2-3</li>
<li>Zookeeper服务(Server)：cdh2-[1:5]</li>
<li>Solr服务(SolrServer)：cdh2-4</li>
<li>Spark服务(HistoryServer)：cdh2-4</li>
<li>Kakfa服务(KafkaBroker)：cdh2-[6:8]</li>
<li>Flume(Agent)：cdh2-[6-8]</li>
</ul>
<h2 id="二、集群安装与部署">二、集群安装与部署</h2>
<h3 id="2-1-打开系统网络">2.1 打开系统网络</h3>
<p>操作系统安装初始，如果无法ping通内部服务，则检查 <strong>/etc/sysconfig/network-scripts/ifcfg-ens33</strong> 文件，确认 <strong>ONBOOT</strong> 的值如果为no需要修改为yes（Centos7.5虚拟机安装初始默认为no），否则网络无法连通。</p>
<p>手动检查各个主机上的网络设置，如果有问题则修改配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ONBOOT&#x3D;no 改成 ONBOOT&#x3D;yes</span><br><span class="line">vim &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33</span><br><span class="line">reboot</span><br></pre></td></tr></table></figure>
<h3 id="2-2-硬盘挂载">2.2 硬盘挂载</h3>
<p>如果服务器硬盘已插入还未挂载则需要先载入硬盘：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 格式化硬盘为xfs</span><br><span class="line">mkfs -t xfs &#x2F;dev&#x2F;sdb</span><br><span class="line"></span><br><span class="line"># 硬盘挂载</span><br><span class="line"># 主节点</span><br><span class="line">mount  &#x2F;dev&#x2F;sdb  &#x2F;opt</span><br><span class="line"># 计算节点</span><br><span class="line">mount  &#x2F;dev&#x2F;sdb  &#x2F;opt&#x2F;data1</span><br><span class="line">mount  &#x2F;dev&#x2F;sdc  &#x2F;opt&#x2F;data2</span><br><span class="line"></span><br><span class="line"># 查看挂载结果</span><br><span class="line">df -HT</span><br></pre></td></tr></table></figure>
<h3 id="2-3-自动化安装">2.3 自动化安装</h3>
<p>使用自动化脚本工具进行安装操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取安装脚本，上传相关安装软件包至服务器（JDK、MySQL、CM、CDH等）</span><br><span class="line">yum install -y git</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;chubbyjiang&#x2F;cdh-deploy-robot.git</span><br><span class="line">cd cdh-deploy-robot</span><br><span class="line"></span><br><span class="line"># 编辑节点主机名</span><br><span class="line">vi hosts</span><br><span class="line"># 修改安装配置项</span><br><span class="line">vi deploy_robot.config</span><br><span class="line"># 配置ssh免密登录</span><br><span class="line">sh deploy_robot.sh init_ssh</span><br><span class="line"># 执行安装</span><br><span class="line">sh deploy_robot.sh install_all</span><br></pre></td></tr></table></figure>
<p>安装脚本将会执行 配置SSH免密登录、安装软件、操作系统优化、Java等开发环境初始化、MySQL安装、CM服务安装、操作系统性能测试等过程。</p>
<p>脚本操作说明见：<a href="https://link.zhihu.com/?target=https%3A//github.com/chubbyjiang/cdh-deploy-robot">CDH集群自动化部署工具</a> 。</p>
<p>等待cloudera-scm-server进程起来后，在浏览器输入 ip:7180 进入CM管理界面部署CDH组件。</p>
<p>第三部分将详细描述集群手动安装过程，<strong>与自动安装达成的效果一致</strong>，如已通过自动脚本完成CM服务安装可直接前往第四部分CDH部署。</p>
<h2 id="三、手动安装">三、手动安装</h2>
<p>以下操作均为Centos7.5操作系统上进行。</p>
<h3 id="3-1-系统设置">3.1 系统设置</h3>
<h3 id="主机名配置">主机名配置</h3>
<p>设置集群机器主机名，并加入各自hosts文件中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 设置各主机主机名</span><br><span class="line">hostnamectl set-hostname cdh2-1</span><br><span class="line"></span><br><span class="line"># 更新hosts文件</span><br><span class="line">echo &quot;192.168.2.1 cdh2-1&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.2 cdh2-2&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.3 cdh2-3&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.4 cdh2-4&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.5 cdh2-5&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.6 cdh2-6&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.7 cdh2-7&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.8 cdh2-8&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line"></span><br><span class="line"># 拷贝</span><br><span class="line">scp &#x2F;etc&#x2F;hosts root@cdh2-2:&#x2F;etc</span><br></pre></td></tr></table></figure>
<h3 id="SSH免密登录">SSH免密登录</h3>
<p>需要有root权限的用户（root或者sudo权限）设置免密登录。</p>
<p>在各个主机上操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成密钥</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line">cat ~&#x2F;.ssh&#x2F;id_rsa.pub &gt;&gt; ~&#x2F;.ssh&#x2F;authorized_keys</span><br><span class="line"># 拷贝密钥到登录主机</span><br><span class="line">ssh-copy-id -i ~&#x2F;.ssh&#x2F;id_rsa.pub root@cdh2-1</span><br></pre></td></tr></table></figure>
<p>从登录主机上复制到其他主机：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 同步密钥</span><br><span class="line">scp ~&#x2F;.ssh&#x2F;authorized_keys root@cdh2-2:~&#x2F;.ssh&#x2F;</span><br><span class="line"># 测试</span><br><span class="line">ssh cdh2-2 date</span><br></pre></td></tr></table></figure>
<h3 id="安装Ansible">安装Ansible</h3>
<p>安装ansible批量管理主机：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 控制机器上安装ansible</span><br><span class="line">yum install -y ansible</span><br></pre></td></tr></table></figure>
<p>安装完毕后配置修改 <strong>/etc/ansible/hosts</strong> 对需要管理的主机进行配置，默认配置需要修改编辑 <strong>/etc/ansible/ansible.cfg</strong>。</p>
<p>ansible使用配置参考 <a href="https://link.zhihu.com/?target=https%3A//docs.ansible.com/ansible/latest/installation_guide/intro_installation.html%23managed-node-requirements">Ansible官网</a>。</p>
<p>host示例配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[except1]</span><br><span class="line">cdh2-[2:8]</span><br><span class="line"></span><br><span class="line">[all]</span><br><span class="line">cdh2-[1:8]</span><br></pre></td></tr></table></figure>
<h3 id="关闭selinux">关闭selinux</h3>
<p>查看selinux状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Enforcing 开启状态</span><br><span class="line"># Disabled 关闭状态</span><br><span class="line">ansible all -a &quot;getenforce&quot;</span><br></pre></td></tr></table></figure>
<p>修改为关闭状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在控制机器上修改内容</span><br><span class="line">vim &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">SELINUX&#x3D;disable </span><br><span class="line"># 同步到其他机器</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;selinux&#x2F;config dest&#x3D;&#x2F;etc&#x2F;selinux&#x2F;config&quot;</span><br><span class="line"># 需要重启时候启用</span><br></pre></td></tr></table></figure>
<h3 id="禁用IPv6">禁用IPv6</h3>
<p>Centos7.5默认开启IPv6，CM组件明确说明不支持系统的IPv6功能，IPv6开启状态下可能会<strong>出现不可预料的错误</strong>，需要提前关闭。</p>
<p>查看IPv6启用状态可以通过以下几种方式：</p>
<ul>
<li>ifconfig：查看是否有IPv6的地址（inet6）</li>
<li>lsmod：查看是否有ipv6关键字</li>
<li>disable_ipv6：查看/proc/sys/net/ipv6/conf/all/disable_ipv6文件内容，0为开启，1为关闭</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看IPv6当前状态，有值则为打开，空则为关闭</span><br><span class="line">ansible all -a &quot;lsmod | grep ipv6&quot;</span><br></pre></td></tr></table></figure>
<p>IPv6打开的情况下如何关闭：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 第六行添加</span><br><span class="line">vim &#x2F;etc&#x2F;default&#x2F;grub</span><br><span class="line">GRUB_CMDLINE_LUNUX&#x3D;&quot;ipv6.disable&#x3D;1 ....&quot;</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;default&#x2F;grub dest&#x3D;&#x2F;etc&#x2F;default&#x2F;grub&quot;</span><br><span class="line"># 重启</span><br><span class="line">ansible all -a &quot;reboot&quot;</span><br><span class="line"># 验证</span><br><span class="line">ansible all -a &quot;lsmod | grep ipv6&quot;</span><br></pre></td></tr></table></figure>
<h3 id="防火墙设置">防火墙设置</h3>
<p>局域网内部安全情况下最好关闭防火墙，因为CM管理组件和CDH组件有大量的端口进行通讯，需要配置很多防火墙策略。</p>
<p>需要开放的端口可参考 <a href="https://link.zhihu.com/?target=https%3A//www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_ig_ports.html">官网说明</a>，如果不能确保开放所有所需端口，则需要关闭防火墙。</p>
<p>关闭防火墙：</p>
<p><strong>firewall</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;systemctl status firewalld&quot;</span><br><span class="line">ansible all -a &quot;systemctl stop firewalld&quot;</span><br><span class="line">ansible all -a &quot;systemctl disable firewalld&quot;</span><br></pre></td></tr></table></figure>
<p><strong>iptables</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;chkconfig iptables off&quot;</span><br><span class="line">ansible all -a &quot;service iptables status&quot;</span><br><span class="line">ansible all -a &quot;service iptables stop&quot;</span><br></pre></td></tr></table></figure>
<h3 id="DNS服务器">DNS服务器</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加dns，有则略过</span><br><span class="line">echo &quot;nameserver 114.114.114.114&quot; &gt;&gt; &#x2F;etc&#x2F;resolv.conf</span><br><span class="line">echo &quot;nameserver 8.8.8.8&quot; &gt;&gt; &#x2F;etc&#x2F;resolv.conf</span><br><span class="line"># 文件同步</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;resolv.conf dest&#x3D;&#x2F;etc&#x2F;resolv.conf&quot;</span><br></pre></td></tr></table></figure>
<h3 id="NTP配置">NTP配置</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 调整时区</span><br><span class="line">ansible all -a &quot;ln -sf &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime&quot;</span><br><span class="line"># 安装ntp</span><br><span class="line">ansible all -a &quot;yum install ntp -y&quot;</span><br><span class="line"># 手动同步时间（ntpd服务关闭的情况下），避免时间差距过大导致同步失败</span><br><span class="line">ansible all -a &quot;ntpdate -u 0.cn.pool.ntp.org&quot;</span><br><span class="line"></span><br><span class="line"># 配置ntp服务器地址</span><br><span class="line">vim &#x2F;etc&#x2F;ntp.conf</span><br><span class="line"># 有外网的情况下可直接配置外部ntp服务器</span><br><span class="line">echo &quot;server ntp1.aliyun.com&quot; &gt;&gt; &#x2F;etc&#x2F;ntp.conf</span><br><span class="line"># 其他备用ntp服务器</span><br><span class="line"># server 0.pool.ntp.org</span><br><span class="line"># server 1.pool.ntp.org</span><br><span class="line"># server 2.pool.ntp.org</span><br><span class="line"># server 0.pool.ntp.org  # 有域名负载均衡</span><br><span class="line"># server 0.cn.pool.ntp.org  # 有域名负载均衡</span><br><span class="line"># server ntp.tuna.tsinghua.edu.cn # 清华大学</span><br><span class="line"></span><br><span class="line"># 启动ntp</span><br><span class="line">ansible all -a &quot;systemctl start ntpd&quot;</span><br><span class="line"># 开机启动</span><br><span class="line">ansible all -a &quot;systemctl enable ntpd&quot;</span><br><span class="line"></span><br><span class="line"># 查看系统硬件时间</span><br><span class="line">hwclock --systohc</span><br></pre></td></tr></table></figure>
<p><strong>附：NTP内网服务器搭建</strong></p>
<p>配置内网NTP-Server(管理节点)。</p>
<p>ntp.conf配置文件内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#新增：日志文件</span><br><span class="line">logfile &#x2F;var&#x2F;log&#x2F;ntpd.log</span><br><span class="line">restrict default kod nomodify notrap nopeer noquery</span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict ::1</span><br><span class="line"></span><br><span class="line">#授权192.168.1.0网段上所有机器可以从这台机器上查询和时间同步</span><br><span class="line">restrict 192.168.0.0 mask 255.255.0.0 nomodify notrap</span><br><span class="line"></span><br><span class="line">#新增：时间服务器列表</span><br><span class="line">#server 0.cn.pool.ntp.org iburst</span><br><span class="line">#server 1.cn.pool.ntp.org iburst</span><br><span class="line">#server 2.cn.pool.ntp.org iburst</span><br><span class="line">#server 3.cn.pool.ntp.org iburst</span><br><span class="line"></span><br><span class="line">#新增：当外部时间不可用时，使用本地时间</span><br><span class="line">server 127.127.1.0 biurst</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br><span class="line"></span><br><span class="line">#新增：允许上层时间服务器主动修改本机时间</span><br><span class="line">#restrict 0.cn.pool.ntp.org nomodify notrap noquery</span><br><span class="line">#restrict 1.cn.pool.ntp.org nomodify notrap noquery</span><br><span class="line">#restrict 2.cn.pool.ntp.org nomodify notrap noquery</span><br><span class="line"></span><br><span class="line">includefile &#x2F;etc&#x2F;ntp&#x2F;crypto&#x2F;pw</span><br><span class="line">keys &#x2F;etc&#x2F;ntp&#x2F;keys</span><br><span class="line">disable monitor</span><br></pre></td></tr></table></figure>
<h3 id="3-2-软件包安装">3.2 软件包安装</h3>
<h3 id="修改yum源">修改yum源</h3>
<p>默认国外的yum源下载速度缓慢，替换为国内阿里云的yum源。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 备份</span><br><span class="line">ansible all -a &quot;mv &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo.backup&quot;</span><br><span class="line"># 下载</span><br><span class="line">ansible all -a &quot;wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo&quot;</span><br><span class="line"># 更新</span><br><span class="line">ansible all -m shell -a &quot;yum clean all &amp;&amp; yum makecache&quot;</span><br><span class="line">ansible all -a &quot;yum -y update&quot;</span><br></pre></td></tr></table></figure>
<h3 id="系统软件安装">系统软件安装</h3>
<p>安装后续需要用到的系统软件，以备日后服务器无外网无法下载的情况。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 包括不限于</span><br><span class="line">ansible all -a &quot;yum install -y expect bc net-tools iotop zip unzip telnet wget iperf3 fio ntfs-3g lzo iftop vim&quot;</span><br></pre></td></tr></table></figure>
<h3 id="JDK与Scala">JDK与Scala</h3>
<p>Java安装</p>
<p>JDK下载地址：<a href="https://link.zhihu.com/?target=https%3A//archive.cloudera.com/cm6/">Cloudera Archive CM</a>， 根据对应的cm版本选择下载。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以jdk1.8示例</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;tmp&#x2F;oracle-j2sdk1.8 dest&#x3D;&#x2F;tmp&#x2F;oracle-j2sdk1.8&quot;</span><br><span class="line">ansible all -m shell -a &quot;yum localinstall -y oracle-j2sdk1.8 &amp;&amp; ln -s &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_141-cloudera &#x2F;usr&#x2F;java&#x2F;default&quot;</span><br></pre></td></tr></table></figure>
<p>Scala安装</p>
<p>下载地址：<a href="https://link.zhihu.com/?target=https%3A//www.scala-lang.org/download/all.html">Scala</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以2.11.8示例</span><br><span class="line">ansible all -m shell -a &quot;rm -rf &#x2F;usr&#x2F;scala &amp;&amp; mkdir -p &#x2F;usr&#x2F;scala&quot;</span><br><span class="line">cp &#x2F;tmp&#x2F;scala-2.11.8.tgz &#x2F;usr&#x2F;scala</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;usr&#x2F;scala&#x2F;scala-2.11.8.tgz dest&#x3D;&#x2F;usr&#x2F;scala&#x2F;scala-2.11.8.tgz&quot;</span><br><span class="line">ansible all -m shell -a &quot;cd &#x2F;usr&#x2F;scala &amp;&amp; tar -zxvf scala-2.11.8.tgz &amp;&amp; rm -rf scala-2.11.8.tgz&quot;</span><br></pre></td></tr></table></figure>
<p>配置环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;echo JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_141-cloudera &gt;&gt; &#x2F;etc&#x2F;profile&quot;</span><br><span class="line">ansible all -a &quot;echo SCALA_HOME&#x3D;&#x2F;usr&#x2F;scala&#x2F;scala-2.11.8 &gt;&gt; &#x2F;etc&#x2F;profile&quot;</span><br><span class="line">ansible all -a &quot;echo CLASSPATH&#x3D;$JAVA_HOME&#x2F;bin:$SCALA_HOME&#x2F;bin &gt;&gt; &#x2F;etc&#x2F;profile&quot;</span><br><span class="line">ansible all -a &quot;echo export PATH&#x3D;$JAVA_HOME:$SCALA_HOME:$CLASSPATH:$PATH &gt;&gt; &#x2F;etc&#x2F;profile&quot;</span><br><span class="line">ansible all -a &quot;source &#x2F;etc&#x2F;profile&quot;</span><br></pre></td></tr></table></figure>
<h3 id="Python与Python包">Python与Python包</h3>
<p>Centos7自带python2.7，Centos6自带python2.6需要升级。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -V</span><br><span class="line"># Python 2.7.5</span><br></pre></td></tr></table></figure>
<p>安装python3.6</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 依赖包安装</span><br><span class="line">ansible all -a &quot;yum install -y openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel gcc-c++ python36-devel cyrus-sasl-lib.x86_64 cyrus-sasl-devel.x86_64 libgsasl-devel.x86_64 epel-release&quot;</span><br><span class="line"># yum源下载</span><br><span class="line">ansible all -a &quot;yum install https:&#x2F;&#x2F;centos7.iuscommunity.org&#x2F;ius-release.rpm -y&quot;</span><br><span class="line"># 安装python3.6</span><br><span class="line">ansible all -a &quot;yum install python36 -y&quot;</span><br><span class="line"># 安装setuptools</span><br><span class="line">ansible all -a &quot;wget -P &#x2F;tmp --no-check-certificate https:&#x2F;&#x2F;pypi.python.org&#x2F;packages&#x2F;source&#x2F;s&#x2F;setuptools&#x2F;setuptools-19.6.tar.gz#md5&#x3D;c607dd118eae682c44ed146367a17e26&quot;</span><br><span class="line">ansible all -a &quot;tar -zxvf &#x2F;tmp&#x2F;setuptools-19.6.tar.gz&quot;</span><br><span class="line">ansible all -m shell -a &quot;cd &#x2F;tmp&#x2F;setuptools-19.6 &amp;&amp; python3.6 setup.py build &amp;&amp; python3.6 setup.py install&quot;</span><br><span class="line"># 安装pip3.6</span><br><span class="line">ansible all -a &quot;wget -P &#x2F;tmp --no-check-certificate https:&#x2F;&#x2F;pypi.python.org&#x2F;packages&#x2F;source&#x2F;p&#x2F;pip&#x2F;pip-8.0.2.tar.gz#md5&#x3D;3a73c4188f8dbad6a1e6f6d44d117eeb&quot;</span><br><span class="line">ansible all -a &quot;tar -zxvf &#x2F;tmp&#x2F;pip-8.0.2.tar.gz&quot;</span><br><span class="line">ansible all -m shell -a &quot;cd &#x2F;tmp&#x2F;pip-8.0.2 &amp;&amp; python3.6 setup.py build &amp;&amp; python3.6 setup.py install&quot;</span><br></pre></td></tr></table></figure>
<p>修改pip源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;mkdir ~&#x2F;.pip&quot;</span><br><span class="line">vim ~&#x2F;.pip&#x2F;pip.conf</span><br><span class="line"></span><br><span class="line"># 内容修改如下</span><br><span class="line">[global]</span><br><span class="line">index-url &#x3D; https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br><span class="line"></span><br><span class="line"># 文件同步</span><br><span class="line">ansibe all -m copy -a &quot;src&#x3D;&#x2F;root&#x2F;.pip&#x2F;pip.conf dest&#x3D;&#x2F;root&#x2F;.pip&#x2F;pip.conf&quot;</span><br><span class="line"></span><br><span class="line"># 更新</span><br><span class="line">ansible all -m shell -a &quot;pip3.6 install --upgrade setuptools &amp;&amp; easy_install-3.6 -U setuptools &amp;&amp; pip3.6 install --upgrade pip&quot;</span><br></pre></td></tr></table></figure>
<p>安装python所需依赖包：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansibe all -m copy -a &quot;src&#x3D;&#x2F;tmp&#x2F;requirements.txt dest&#x3D;&#x2F;tmp&#x2F;requirements.txt&quot;</span><br><span class="line">ansible all -a &quot;pip3.6 install -r &#x2F;tmp&#x2F;requirements.txt&quot;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-其他系统设置">3.3 其他系统设置</h3>
<h3 id="中文乱码">中文乱码</h3>
<p>安装操作系统时选择了中文语言，使用时发现部分中文会有乱码的情况，解决方案如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;locale.conf</span><br><span class="line"># 内容</span><br><span class="line">LANG&#x3D;zh_CN.UTF-8</span><br><span class="line">LC_CTYPE&#x3D;zh_CN.UTF-8</span><br><span class="line">LC_NUMERIC&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_TIME&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_COLLATE&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_MONETARY&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_MESSAGES&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_PAPER&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_NAME&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_ADDRESS&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_TELEPHONE&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_MEASUREMENT&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_IDENTIFICATION&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_ALL&#x3D;</span><br><span class="line"></span><br><span class="line"># 更新</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;locale.conf dest&#x3D;&#x2F;etc&#x2F;locale.conf&quot;</span><br><span class="line">ansible all -a &quot;source &#x2F;etc&#x2F;locale.conf&quot;</span><br></pre></td></tr></table></figure>
<h3 id="tuned">tuned</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;systemctl start tuned&quot;</span><br><span class="line">ansible all -a &quot;systemctl status tuned&quot;</span><br><span class="line"># 显示No current active profile</span><br><span class="line">ansible all -a &quot;tuned-adm off&quot;</span><br><span class="line">ansible all -a &quot;tuned-adm list&quot;</span><br><span class="line"># 关闭tuned服务</span><br><span class="line">ansible all -a &quot;systemctl stop tuned&quot;</span><br><span class="line">ansible all -a &quot;systemctl disable tuned&quot;</span><br></pre></td></tr></table></figure>
<h3 id="大页面关闭">大页面关闭</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 输出[always] never意味着THP已启用，always [never]意味着THP未启用</span><br><span class="line">ansible all -a &quot;cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled&quot;</span><br><span class="line">ansible all -a &quot;cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag&quot;</span><br><span class="line"></span><br><span class="line"># 关闭</span><br><span class="line">ansible all -m shell -a &quot;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled&quot;</span><br><span class="line">ansible all -m shell -a &quot;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag&quot;</span><br><span class="line"># 设置开机关闭</span><br><span class="line">echo &quot;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag&quot; &gt;&gt; &#x2F;etc&#x2F;rc.local</span><br><span class="line">echo &quot;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled&quot; &gt;&gt; &#x2F;etc&#x2F;rc.local</span><br><span class="line">chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br><span class="line"></span><br><span class="line"># 在GRUB_CMDLINE_LINUX项目后面添加一个参数：transparent_hugepage&#x3D;never</span><br><span class="line">vim &#x2F;etc&#x2F;default&#x2F;grub</span><br><span class="line">ansile all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;default&#x2F;grub dest&#x3D;&#x2F;etc&#x2F;default&#x2F;grub&quot;</span><br><span class="line"># 重新生成gurb.cfg文件</span><br><span class="line">ansible all -a &quot;grub2-mkconfig -o &#x2F;boot&#x2F;grub2&#x2F;grub.cfg&quot;</span><br></pre></td></tr></table></figure>
<h3 id="swappiness">swappiness</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;swappiness&quot;</span><br><span class="line">ansible all -a &quot;sysctl -w vm.swappiness&#x3D;1&quot;</span><br><span class="line">echo &quot;vm.swappiness&#x3D;1&quot; &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;sysctl.conf dest&#x3D;&#x2F;etc&#x2F;sysctl.conf&quot;</span><br></pre></td></tr></table></figure>
<h3 id="会话超时">会话超时</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;TMOUT&#x3D;900&quot;&gt;&gt;&#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>
<h3 id="内核优化">内核优化</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo -e &quot;\nnet.ipv4.tcp_tw_reuse &#x3D; 1</span><br><span class="line">\nnet.ipv4.tcp_tw_recycle &#x3D; 1</span><br><span class="line">\nnet.ipv4.tcp_keepalive_time &#x3D; 1200</span><br><span class="line">\nnet.ipv4.ip_local_port_range &#x3D; 10000 65000</span><br><span class="line">\nnet.ipv4.tcp_max_syn_backlog &#x3D; 8192</span><br><span class="line">\nnet.ipv4.tcp_max_tw_buckets &#x3D; 5000</span><br><span class="line">\nfs.file-max &#x3D; 655350</span><br><span class="line">\nnet.ipv4.route.gc_timeout &#x3D; 100</span><br><span class="line">\nnet.ipv4.tcp_syn_retries &#x3D; 1</span><br><span class="line">\nnet.ipv4.tcp_synack_retries &#x3D; 1</span><br><span class="line">\nnet.core.netdev_max_backlog &#x3D; 16384</span><br><span class="line">\nnet.ipv4.tcp_max_orphans &#x3D; 16384</span><br><span class="line">\nnet.ipv4.tcp_fin_timeout &#x3D; 2</span><br><span class="line">\net.core.somaxconn&#x3D;32768</span><br><span class="line">\kernel.threads-max&#x3D;196605</span><br><span class="line">\kernel.pid_max&#x3D;196605</span><br><span class="line">\vm.max_map_count&#x3D;393210&quot;  &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br></pre></td></tr></table></figure>
<h3 id="最大打开文件数">最大打开文件数</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ulimit -a</span><br><span class="line">sed -i &#39;$ a\* soft nofile 196605&#39; &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br><span class="line">sed -i &#39;$ a\* hard nofile 196605&#39; &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br><span class="line">echo &quot;* soft nproc 196605&quot; &gt;&gt; &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br><span class="line">echo &quot;* hard nproc 196605&quot; &gt;&gt; &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br></pre></td></tr></table></figure>
<h3 id="3-4-系统性能测试">3.4 系统性能测试</h3>
<h3 id="网络测试">网络测试</h3>
<p>使用iperf测试主机之间的网络传输效率。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在一个主机上启动iperf 服务端</span><br><span class="line">iperf3 -s -p 12345 -i 1</span><br><span class="line"># 另外一个主机启动iperf 客户端连接服务端</span><br><span class="line">iperf3 -c cdh85-19 -p 12345 -i 1 -t 10 -w 100K</span><br></pre></td></tr></table></figure>
<h3 id="磁盘IO测试">磁盘IO测试</h3>
<p>使用fio工具对io进行各个场景的读写性能测试。</p>
<p><strong>随机读</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fio -filename&#x3D;&#x2F;dev&#x2F;sda -direct&#x3D;1 -iodepth 1 -thread -rw&#x3D;randread -ioengine&#x3D;psync -bs&#x3D;4k -size&#x3D;60G -numjobs&#x3D;64 -runtime&#x3D;10 -group_reporting -name&#x3D;file -allow_mounted_write&#x3D;1</span><br></pre></td></tr></table></figure>
<p><strong>顺序读</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fio -filename&#x3D;&#x2F;dev&#x2F;sda -direct&#x3D;1 -iodepth 1 -thread -rw&#x3D;read -ioengine&#x3D;psync -bs&#x3D;4k -size&#x3D;60G -numjobs&#x3D;64 -runtime&#x3D;10 -group_reporting -name&#x3D;file -allow_mounted_write&#x3D;1</span><br></pre></td></tr></table></figure>
<p><strong>随机写</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fio -filename&#x3D;&#x2F;dev&#x2F;sda -direct&#x3D;1 -iodepth 1 -thread -rw&#x3D;randwrite -ioengine&#x3D;psync -bs&#x3D;4k -size&#x3D;60G -numjobs&#x3D;64 -runtime&#x3D;10 -group_reporting -name&#x3D;file -allow_mounted_write&#x3D;1</span><br></pre></td></tr></table></figure>
<p><strong>顺序写</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fio -filename&#x3D;&#x2F;dev&#x2F;sda -direct&#x3D;1 -iodepth 1 -thread -rw&#x3D;write -ioengine&#x3D;psync -bs&#x3D;4k -size&#x3D;60G -numjobs&#x3D;64 -runtime&#x3D;10 -group_reporting -name&#x3D;file -allow_mounted_write&#x3D;1</span><br></pre></td></tr></table></figure>
<p><strong>混合随机读写</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fio -filename&#x3D;&#x2F;dev&#x2F;sda -direct&#x3D;1 -iodepth 1 -thread -rw&#x3D;randrw -rwmixread&#x3D;30 -ioengine&#x3D;psync -bs&#x3D;4k -size&#x3D;60G -numjobs&#x3D;64 -runtime&#x3D;10 -group_reporting -name&#x3D;file -ioscheduler&#x3D;noop -allow_mounted_write&#x3D;1</span><br></pre></td></tr></table></figure>
<p>todo:</p>
<ul>
<li>内存性能测试</li>
<li>操作系统性能测试</li>
</ul>
<h3 id="3-5-MySQL数据库">3.5 MySQL数据库</h3>
<p>卸载已有mariadb数据库。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -qa|grep -i mariadb</span><br><span class="line">rpm -e mariadb-libs-5.5.60-1.el7_5.x86_64 --nodeps</span><br></pre></td></tr></table></figure>
<p>下载mysql安装包。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mysql官网下载bundle安装包并上传至服务器</span><br><span class="line"></span><br><span class="line"># 安装</span><br><span class="line">rpm -ivh &#x2F;tmp&#x2F;mysql-community-common-5.7.20-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh &#x2F;tmp&#x2F;mysql-community-libs-5.7.20-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh &#x2F;tmp&#x2F;mysql-community-client-5.7.20-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh &#x2F;tmp&#x2F;mysql-community-server-5.7.20-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>创建MySQL数据目录（非默认盘）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;opt&#x2F;mysql&#x2F;data</span><br></pre></td></tr></table></figure>
<p>修改mysql配置文件内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;my.conf</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"># datadir修改为&#x2F;opt&#x2F;mysql&#x2F;data</span><br><span class="line">datadir&#x3D;&#x2F;opt&#x2F;mysql&#x2F;data</span><br><span class="line">socket&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql.sock</span><br><span class="line"></span><br><span class="line"># set the isolation level to READ-COMMITTED</span><br><span class="line">transaction-isolation &#x3D; READ-COMMITTED</span><br><span class="line"></span><br><span class="line"># Disabling symbolic-links is recommended to prevent assorted security risks;</span><br><span class="line"># to do so, uncomment this line:</span><br><span class="line">symbolic-links &#x3D; 0</span><br><span class="line"></span><br><span class="line">key_buffer_size &#x3D; 32M</span><br><span class="line">max_allowed_packet &#x3D; 32M</span><br><span class="line">thread_stack &#x3D; 256K</span><br><span class="line">thread_cache_size &#x3D; 64</span><br><span class="line">query_cache_limit &#x3D; 8M</span><br><span class="line">query_cache_size &#x3D; 64M</span><br><span class="line">query_cache_type &#x3D; 1</span><br><span class="line"></span><br><span class="line"># max_connections - Allow 100 maximum connections for each database and then add 50 extra connections. </span><br><span class="line">max_connections &#x3D; 550</span><br><span class="line">#expire_logs_days &#x3D; 10</span><br><span class="line">#max_binlog_size &#x3D; 100M</span><br><span class="line"></span><br><span class="line">#log_bin should be on a disk with enough free space.</span><br><span class="line">#Replace &#39;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql_binary_log&#39; with an appropriate path for your</span><br><span class="line">#system and chown the specified folder to the mysql user.</span><br><span class="line"># 修改binlog存储路径</span><br><span class="line">log_bin&#x3D;&#x2F;opt&#x2F;mysql&#x2F;binlog&#x2F;mysql_binary_log</span><br><span class="line"></span><br><span class="line">#In later versions of MySQL, if you enable the binary log and do not set</span><br><span class="line">#a server_id, MySQL will not start. The server_id must be unique within</span><br><span class="line">#the replicating group.</span><br><span class="line">server_id&#x3D;1</span><br><span class="line"></span><br><span class="line">binlog_format &#x3D; mixed</span><br><span class="line"></span><br><span class="line">read_buffer_size &#x3D; 2M</span><br><span class="line">read_rnd_buffer_size &#x3D; 16M</span><br><span class="line">sort_buffer_size &#x3D; 8M</span><br><span class="line">join_buffer_size &#x3D; 8M</span><br><span class="line"></span><br><span class="line"># InnoDB settings</span><br><span class="line">innodb_file_per_table &#x3D; 1</span><br><span class="line">innodb_flush_log_at_trx_commit  &#x3D; 2</span><br><span class="line">innodb_log_buffer_size &#x3D; 64M</span><br><span class="line">innodb_buffer_pool_size &#x3D; 4G</span><br><span class="line">innodb_thread_concurrency &#x3D; 8</span><br><span class="line"># set the innodb_flush_method property to O_DIRECT.</span><br><span class="line">innodb_flush_method &#x3D; O_DIRECT</span><br><span class="line">innodb_log_file_size &#x3D; 512M</span><br><span class="line"></span><br><span class="line">[mysqld_safe]</span><br><span class="line">log-error&#x3D;&#x2F;var&#x2F;log&#x2F;mysqld.log</span><br><span class="line">pid-file&#x3D;&#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.pid</span><br><span class="line"></span><br><span class="line">sql_mode&#x3D;STRICT_ALL_TABLES</span><br></pre></td></tr></table></figure>
<p>备份文件logfile文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;ib_logfile0 &#x2F;tmp&#x2F;hadoop&#x2F;ib_logfile0.ba</span><br><span class="line">cp &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;ib_logfile1 &#x2F;tmp&#x2F;hadoop&#x2F;ib_logfile1.ba</span><br></pre></td></tr></table></figure>
<p>安装mysql驱动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 官网下载mysql驱动并上传至服务器</span><br><span class="line"># Installing the MySQL JDBC Driver</span><br><span class="line">tar zxvf &#x2F;tmp&#x2F;mysql-connector-java-5.1.46.tar.gz</span><br><span class="line"></span><br><span class="line">ansible all -a &quot;mkdir -p &#x2F;usr&#x2F;share&#x2F;java&#x2F;&quot;</span><br><span class="line"># 注意去掉版本号，否则cm无法使用</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;tmp&#x2F;mysql-connector-java-5.1.46&#x2F;mysql-connector-java-5.1.46-bin.jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;usr&#x2F;share&#x2F;java&#x2F;mysql-connector-java.jar&quot;</span><br><span class="line">rm -rf &#x2F;tmp&#x2F;mysql-connector-java-5.1.46*</span><br></pre></td></tr></table></figure>
<p>启动mysql服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start mysqld</span><br><span class="line">systemctl status mysqld</span><br><span class="line">systemctl enable mysqld</span><br></pre></td></tr></table></figure>
<p>设置mysql账号密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 初始密码</span><br><span class="line">grep &quot;temporary password&quot; &#x2F;var&#x2F;log&#x2F;mysqld.log</span><br><span class="line"># 重置</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;mysql_secure_installation</span><br></pre></td></tr></table></figure>
<p>mysql5.7以上强制密码策略不满足可以通过以下方式修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 默认1，中等强度策略</span><br><span class="line">mysql -u root -p&quot;$init_passwd&quot; --connect-expired-password -e &quot;set global validate_password_policy&#x3D;0&quot;</span><br><span class="line"># 默认8长度</span><br><span class="line">mysql -u root -p&quot;$init_passwd&quot; --connect-expired-password -e &quot;set global validate_password_length&#x3D;1&quot;</span><br></pre></td></tr></table></figure>
<p>进入mysql并创建数据库：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line"># Configure the Cloudera Manager Server, Activity Monitor, Reports Manager, Cloudera Navigator Audit Server, and Cloudera Navigator Metadata Server databases to support the utf8mb4 character set encoding.</span><br><span class="line"># Configure all other databases to use the utf8 character set.</span><br><span class="line">CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON scm.* TO &#39;scm&#39;@&#39;%&#39; IDENTIFIED BY &#39;scm@DW&#39;;</span><br><span class="line">CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON amon.* TO &#39;amon&#39;@&#39;%&#39; IDENTIFIED BY &#39;amon@DW&#39;;</span><br><span class="line">CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON rman.* TO &#39;rman&#39;@&#39;%&#39; IDENTIFIED BY &#39;rman@DW&#39;;</span><br><span class="line">CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON hue.* TO &#39;hue&#39;@&#39;%&#39; IDENTIFIED BY &#39;hue@DW&#39;;</span><br><span class="line">CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON metastore.* TO &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;hive@DW&#39;;</span><br><span class="line">CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON sentry.* TO &#39;sentry&#39;@&#39;%&#39; IDENTIFIED BY &#39;sentry@DW&#39;;</span><br><span class="line">CREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON nav.* TO &#39;nav&#39;@&#39;%&#39; IDENTIFIED BY &#39;nav@DW&#39;;</span><br><span class="line">CREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON navms.* TO &#39;navms&#39;@&#39;%&#39; IDENTIFIED BY &#39;navms@DW&#39;;</span><br><span class="line">CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON oozie.* TO &#39;oozie&#39;@&#39;%&#39; IDENTIFIED BY &#39;oozie@DW&#39;;</span><br><span class="line">SHOW DATABASES;</span><br></pre></td></tr></table></figure>
<p><strong>注意事项：</strong></p>
<p>在大型集群中Activity Monitor 与 Service Monitor 使用的数据库应该分配不同的磁盘卷来进行读写。</p>
<p>在超过50个节点的集群中，不要将所有服务的数据库装在一个节点中，否则该节点的数据库压力会很大。最好能为每个服务配置不同位于不同节点上的数据库。</p>
<p>不需要使用专门的数据库服务器，但是每个服务的数据库应该分散在不同的节点上。</p>
<p>如果集群节点超过1000个，将mysql的max_allowed_packet值设置为16M。</p>
<p>For MySQL 5.6 and 5.7, you must install the MySQL-shared-compat or MySQL-shared package. This is required for the Cloudera Manager Agent package installation.</p>
<h3 id="3-6-安装Cloudera-Manager服务">3.6 安装Cloudera Manager服务</h3>
<p>本次过程<strong>不启用auto-ssl。</strong></p>
<h3 id="安装CM软件包">安装CM软件包</h3>
<p>创建免密root权限用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;sudoers</span><br><span class="line">cloudera-scm    ALL&#x3D;(ALL)    NOPASSWD:ALL </span><br><span class="line"></span><br><span class="line"># 同步</span><br><span class="line">ansible except1 -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;sudoers dest&#x3D;&#x2F;etc&#x2F;sudoers&quot;</span><br></pre></td></tr></table></figure>
<p>从 <a href="https://link.zhihu.com/?target=https%3A//archive.cloudera.com/cm6/">这里</a> 下载rpm离线安装包，所需文件及软件列表如下（以6.1版本为例）：</p>
<ul>
<li>allkeys.asc</li>
<li>cloudera-manager.repo</li>
<li>oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm</li>
<li>cloudera-manager-server-db-2-6.1.0-853290.el7.x86_64.rpm</li>
<li>cloudera-manager-server-6.1.0-853290.el7.x86_64.rpm</li>
<li>cloudera-manager-daemons-6.1.0-853290.el7.x86_64.rpm</li>
<li>cloudera-manager-agent-6.1.0-853290.el7.x86_64.rpm</li>
</ul>
<p>上传并安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 主节点安装所有服务</span><br><span class="line">rpm -ivh --force --nodeps &#x2F;tmp&#x2F;cm&#x2F;*.rpm</span><br><span class="line"># 子节点安装daemons和agent</span><br><span class="line">rpm -ivh --force --nodeps &#x2F;tmp&#x2F;cm&#x2F;daemons*.rpm</span><br><span class="line">rpm -ivh --force --nodeps &#x2F;tmp&#x2F;cm&#x2F;agent*.rpm</span><br></pre></td></tr></table></figure>
<p><strong>附：yum方式安装</strong></p>
<p>安装yum源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;tmp&#x2F;cm&#x2F;cloudera-manager.repo dest&#x3D;&#x2F;etc&#x2F;yum.repos.d&quot;</span><br><span class="line">ansible all -m shell -a &quot;yum clean all &amp;&amp; yum makecache&quot;</span><br><span class="line"></span><br><span class="line"># 将会下载很多依赖包，需要联网</span><br><span class="line">ansible all -m shell -a &quot;yum localinstall cloudera-manager-daemons-6.1.0-769885.el7.x86_64.rpm cloudera-manager-agent-6.1.0-769885.el7.x86_64.rpm cloudera-manager-server-6.1.0-769885.el7.x86_64.rpm&quot;</span><br></pre></td></tr></table></figure>
<h3 id="CDH-parcel包">CDH parcel包</h3>
<p>在线安装耗时太长且不稳定，本次只考虑离线安装方式。</p>
<p>从 <a href="https://link.zhihu.com/?target=https%3A//archive.cloudera.com/cdh6/">这里</a> 下载以下文件及软件：</p>
<ul>
<li>manifest.json</li>
<li>CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel</li>
</ul>
<p>创建安装路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo</span><br><span class="line">mv &#x2F;tmp&#x2F;cm&#x2F;CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo</span><br><span class="line">mv &#x2F;tmp&#x2F;cm&#x2F;manifest.json &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo</span><br><span class="line"># 创建签名文件</span><br><span class="line">cd &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo &amp;&amp; sha1sum CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel | awk &#39;&#123; print $1 &#125;&#39; &gt; CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel.sha</span><br></pre></td></tr></table></figure>
<h3 id="初始化Cloudera-Manager">初始化Cloudera Manager</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cm数据库配置</span><br><span class="line">cat &#x2F;etc&#x2F;cloudera-scm-server&#x2F;db.properties</span><br><span class="line"># 初始化数据库</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;schema&#x2F;scm_prepare_database.sh -h cdh2-3 mysql scm scm</span><br><span class="line"># 启动server</span><br><span class="line">systemctl start cloudera-scm-server</span><br><span class="line"># 查看日志</span><br><span class="line">tail -f &#x2F;var&#x2F;log&#x2F;cloudera-scm-server&#x2F;cloudera-scm-server.log</span><br><span class="line"></span><br><span class="line"># 各个节点上启动agent</span><br><span class="line">ansible all -a &quot;systemctl start cloudera-scm-agent&quot;</span><br></pre></td></tr></table></figure>
<h3 id="3-7-附：卸载Cloudera-Manager服务">3.7 附：卸载Cloudera Manager服务</h3>
<p>删除mysql数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">drop database scm;</span><br><span class="line">drop database amon;</span><br><span class="line">drop database rman;</span><br><span class="line">drop database hue;</span><br><span class="line">drop database metastore;</span><br><span class="line">drop database sentry;</span><br><span class="line">drop database nav;</span><br><span class="line">drop database navms;</span><br><span class="line">drop database oozie;</span><br></pre></td></tr></table></figure>
<p>卸载软件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;yum -y remove cloudera-manager-*&quot;</span><br><span class="line">ansible all -a &quot;umount &#x2F;var&#x2F;run&#x2F;cloudera-scm-agent&#x2F;process&quot;</span><br><span class="line"># 删除数据目录</span><br><span class="line">ansible all -a &quot;rm -Rf &#x2F;var&#x2F;lib&#x2F;cloudera* &#x2F;var&#x2F;log&#x2F;cloudera* &#x2F;var&#x2F;run&#x2F;cloudera* &#x2F;etc&#x2F;cloudera* &#x2F;tmp&#x2F;.scm_prepare_node.lock&quot;</span><br></pre></td></tr></table></figure>
<h2 id="四、CDH组件部署">四、CDH组件部署</h2>
<h3 id="4-1-界面安装">4.1 界面安装</h3>
<p>进入Web界面，根据节点角色划分安装CDH组件，并启动集群。</p>
<p>正常启动集群后需要根据业务与应用场景对各个组件的配置属性进行调优工作。</p>
<h3 id="4-2-组件配置调优">4.2 组件配置调优</h3>
<p>如果对组件配置调优无从下手，可以参考以下配置的调整。</p>
<h3 id="HBase">HBase</h3>
<p>根据使用的业务场景不同，HBase配置优化选项有很大的差异。</p>
<p>有关HBase的性能、配置优化与配置项计算说明可以参考：<a href="https://zhuanlan.zhihu.com/p/72150364" target="_blank" rel="noopener">HBase最佳实践</a> - 「性能优化」小节。</p>
<h3 id="Yarn">Yarn</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 开启压缩</span><br><span class="line">mapreduce.output.fileoutputformat.compress&#x3D;已启用</span><br><span class="line">mapreduce.output.fileoutputformat.compress.type&#x3D;BLOCK</span><br><span class="line">mapreduce.output.fileoutputformat.compress.codec&#x3D;org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">mapreduce.map.output.compress.codec&#x3D;org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">mapreduce.map.output.compress&#x3D;已启用</span><br><span class="line">zlib.compress.level&#x3D;DEFAULT_COMPRESSION</span><br><span class="line"></span><br><span class="line"># 设置资源队列</span><br><span class="line">yarn.nodemanager.resource.memory-mb：每台主机上能够被Yarn使用的内存大小</span><br><span class="line">yarn.app.mapreduce.am.resource.cpu-vcores：每台主机上能够被Yarn使用的CPU核心数</span><br><span class="line">yarn.scheduler.minimum-allocation-mb：Container最小申请的内存大小</span><br><span class="line">yarn.scheduler.maximum-allocation-mb：Container最大可申请的内存大小</span><br><span class="line"></span><br><span class="line"># Service Monitor 客户端配置替代</span><br><span class="line">&lt;property&gt;&lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;&#x2F;name&gt;&lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;&#x2F;name&gt;&lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;io.compression.codecs&lt;&#x2F;name&gt;&lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line"># YARN 服务 MapReduce 高级配置代码段（安全阀）</span><br><span class="line">&lt;property&gt;&lt;name&gt;mapreduce.map.output.compress&lt;&#x2F;name&gt;&lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;mapred.map.output.compress.codec&lt;&#x2F;name&gt;&lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line"># 以上用户需要包含在yarn acl控制的用户中，最好和hue超级用户一致（可以直接访问hdfs页面的路径，但是允许所有人访问，有风险）</span><br><span class="line">&lt;property&gt;&lt;name&gt;hadoop.http.staticuser.user&lt;&#x2F;name&gt;&lt;value&gt;yarn&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>资源行管参数细节解释说明：</p>
<ul>
<li>AM参数mapreduce.map.memory.mb=1536MB，表示AM要为map Container申请1536MB资源，但RM实际分配的内存却是2048MB，因为yarn.scheduler.mininum-allocation-mb=1024MB，这定义了RM最小要分配1024MB，1536MB超过了这个值，所以实际分配给AM的值为2048MB。</li>
<li>AM参数mapreduce.map.java.opts=-Xmx 1024m，表示运行map任务的jvm内存为1024MB,因为map任务要运行在Container里面，所以这个参数的值略微小于mapreduce.map.memory.mb=1536MB这个值。</li>
<li>NM参数yarn.nodemanager.vmem-pmem-radio=2.1,这表示NodeManager可以分配给map/reduce Container 2.1倍的虚拟内存，安照上面的配置，实际分配给map Container容器的虚拟内存大小为2048 * 2.1=3225.6MB，若实际用到的内存超过这个值，NM就会kill掉这个map Container,任务执行过程就会出现异常。</li>
<li>AM参数mapreduce.reduce.memory.mb=3072MB，表示分配给reduce Container的容器大小为3072MB,而map Container的大小分配的是1536MB，从这也看出，reduce Container容器的大小最好是map Container大小的两倍。</li>
<li>NM参数yarn.nodemanager.resource.mem.mb=24576MB,这个值表示节点分配给NodeManager的可用内存，也就是节点用来执行yarn任务的内存大小。这个值要根据实际服务器内存大小来配置，比如我们hadoop集群机器内存是128GB，我们可以分配其中的80%给yarn，也就是102GB。</li>
</ul>
<p><strong>Yarn动态资源队列设置</strong></p>
<p>在Cloudera Manager界面点击集群选项卡，进入动态资源池配置，设置任务队列、队列资源、访问控制等信息。</p>
<h3 id="Spark">Spark</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># spark-conf&#x2F;spark-defaults.conf 的 Spark 客户端高级配置代码段（安全阀）</span><br><span class="line">spark.driver.extraJavaOptions&#x3D;-Dfile.encoding&#x3D;UTF-8</span><br><span class="line">spark.executor.extraJavaOptions&#x3D;-Dfile.encoding&#x3D;UTF-8</span><br><span class="line">spark.hadoop.mapred.output.compress&#x3D;true</span><br><span class="line">spark.hadoop.mapred.output.compression.codec&#x3D;org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">spark.hadoop.mapred.output.compression.type&#x3D;BLOCK</span><br><span class="line"></span><br><span class="line"># 修改spark默认的python版本</span><br><span class="line"># spark2-conf&#x2F;spark-env.sh 的 Spark 2 客户端高级配置代码段（安全阀）</span><br><span class="line">PYSPARK_PYTHON&#x3D;&#x2F;usr&#x2F;bin&#x2F;python3.6</span><br><span class="line"># spark-shell或者spark-submit使用</span><br><span class="line">spark.pyspark.python</span><br></pre></td></tr></table></figure>
<h3 id="hdfs">hdfs</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 压缩设置</span><br><span class="line">io.compression.codecs&#x3D;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec</span><br><span class="line"></span><br><span class="line"># 存量数据导入后可进行rebalance操作</span><br><span class="line">sudo -u hdfs hadoop balancer -threshold 10 -policy datanode</span><br><span class="line"></span><br><span class="line"># 启用HA</span><br><span class="line"># HDFS服务配置中开启HA设置</span><br></pre></td></tr></table></figure>
<h3 id="Impala">Impala</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hue启用impala</span><br><span class="line"></span><br><span class="line">hue_safety_value.ini</span><br><span class="line">[impala]</span><br><span class="line">server_host&#x3D;</span><br><span class="line">server_port&#x3D;</span><br><span class="line"></span><br><span class="line"># hue管理用户勾选组的impala权限</span><br></pre></td></tr></table></figure>
<h3 id="Kafka">Kafka</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kafka默认分区数修改为8</span><br><span class="line">num.partitions&#x3D;8</span><br></pre></td></tr></table></figure>
<h3 id="使用Sentry进行权限管理">使用Sentry进行权限管理</h3>
<p>Hue、Hive启用Sentry设置：</p>
<ul>
<li>hue开启sentry服务</li>
<li>hive 启用数据库中的存储通知</li>
<li>hue添加hue用户组和用户，超级管理权限</li>
<li>security添加super role给hue组，所有权限</li>
</ul>
<p>添加普通用户流程：</p>
<ul>
<li>linux添加用户和用户组</li>
<li>hue添加用户和用户组</li>
<li>security添加role给到组</li>
</ul>
<p><strong>建议结合LDAP进行账号体系管理。</strong></p>
<h3 id="4-3-其他配置">4.3 其他配置</h3>
<h3 id="CM集群警报配置">CM集群警报配置</h3>
<p>进入Cloudera Manager Service配置界面，搜索alert对邮件警报进行配置。</p>
<ul>
<li>启用电子邮件警报</li>
<li>邮件服务器协议：smtp</li>
<li>邮箱服务器主机名称：邮箱服务器地址</li>
<li>邮箱服务器用户名：邮箱登录用户名</li>
<li>邮箱服务器密码：邮箱登录用户密码</li>
<li>邮件发件人地址：同登录用户名</li>
<li>邮件收件人地址：多个使用逗号隔开</li>
</ul>
<h3 id="使用hdfs纠错码">使用hdfs纠错码</h3>
<p>HDFS纠错码参考 <a href="https://link.zhihu.com/?target=https%3A//cloud.tencent.com/developer/article/1363393">这里</a>。</p>
<h3 id="4-4-集群性能测试">4.4 集群性能测试</h3>
<h3 id="HDFS性能测试">HDFS性能测试</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 写</span><br><span class="line">hadoop jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-tests.jar TestDFSIO -D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark -write -nrFiles 1000 -fileSize 100</span><br><span class="line"># 读</span><br><span class="line">hadoop jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-tests.jar TestDFSIO -D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark -read -nrFiles 1000 -fileSize 100</span><br><span class="line"># 清理数据</span><br><span class="line">hadoop jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-tests.jar TestDFSIO -D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark -clean</span><br></pre></td></tr></table></figure>
<h3 id="hbase性能测试">hbase性能测试</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows&#x3D;100000 --presplit&#x3D;100 sequentialWrite 10</span><br><span class="line">19&#x2F;04&#x2F;11 09:08:19 INFO hbase.PerformanceEvaluation: [SequentialWriteTest]   Min: 14083ms    Max: 14549ms    Avg: 14270ms</span><br><span class="line"></span><br><span class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows&#x3D;100000 --presplit&#x3D;100 randomWrite 10</span><br><span class="line">19&#x2F;04&#x2F;11 09:09:59 INFO hbase.PerformanceEvaluation: [RandomWriteTest]   Min: 20766ms    Max: 21968ms    Avg: 21383ms</span><br><span class="line"></span><br><span class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows&#x3D;100000 sequentialRead 10</span><br><span class="line">19&#x2F;04&#x2F;11 09:12:07 INFO hbase.PerformanceEvaluation: [SequentialReadTest]    Min: 50383ms    Max: 52429ms    Avg: 51691ms</span><br><span class="line"></span><br><span class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows&#x3D;100000 randomRead 10</span><br><span class="line">19&#x2F;04&#x2F;11 09:13:46 INFO hbase.PerformanceEvaluation: [RandomReadTest]    Min: 73645ms    Max: 74517ms    Avg: 74130ms</span><br><span class="line"></span><br><span class="line">sudo -u hdfs hbase pe sequentialWrite 1</span><br><span class="line">sudo -u hdfs hbase pe sequentialRead 1</span><br><span class="line">sudo -u hdfs hbase pe randomWrite 1</span><br><span class="line">sudo -u hdfs hbase pe randomRead 1</span><br><span class="line"></span><br><span class="line">count &#39;TestTable&#39;, &#123;INTERVAL &#x3D;&gt; 100000, CACHE &#x3D;&gt; 50000&#125;</span><br></pre></td></tr></table></figure>
<p>查看输出的测试报告，Done！</p>
]]></content>
  </entry>
  <entry>
    <title>Typora轻便简洁的Markdown编辑器</title>
    <url>/2020/01/07/Typora%E8%BD%BB%E4%BE%BF%E7%AE%80%E6%B4%81%E7%9A%84Markdown%E7%BC%96%E8%BE%91%E5%99%A8/</url>
    <content><![CDATA[<p>Typora是一款轻便简洁的Markdown编辑器，支持即时渲染技术，这也是与其他Markdown编辑器最显著的区别。即时渲染使得你写Markdown就想是写Word文档一样流畅自如，不像其他编辑器的有编辑栏和显示栏。</p>
<h1>对文字的特殊标注</h1>
<p>标题</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一阶标题  或者快捷键Ctrl+1</span></span><br><span class="line"><span class="comment">## 二阶标题 或者快捷键Ctrl+2</span></span><br><span class="line"><span class="comment">### 三阶标题    或者快捷键Ctrl+3</span></span><br><span class="line"><span class="comment">#### 四阶标题   或者快捷键Ctrl+4</span></span><br><span class="line"><span class="comment">##### 五阶标题  或者快捷键Ctrl+5</span></span><br><span class="line"><span class="comment">###### 六阶标题 或者快捷键Ctrl+6</span></span><br></pre></td></tr></table></figure>
<p>下划线</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">u</span>&gt;</span>下划线的内容<span class="tag">&lt;/<span class="name">u</span>&gt;</span> 或按快捷键Ctrl+U</span><br></pre></td></tr></table></figure>
<p>字体加粗</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**加粗内容**    或按快捷键Ctrl+B</span><br></pre></td></tr></table></figure>
<p>斜体</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">*倾斜内容*  或按快捷键Ctrl+I</span><br></pre></td></tr></table></figure>
<p>删除线</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~~删除线的内容~~  或按快捷键Alt+Shift+5</span><br></pre></td></tr></table></figure>
<p>文字高亮</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x3D;&#x3D;我是最重要的&#x3D;&#x3D;</span><br></pre></td></tr></table></figure>
<p>角标</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x^2^    H~2~O</span><br></pre></td></tr></table></figure>
<p>文本居中</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span>这是要居中的文本内容<span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>list<br>
有序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">数字+英文小数点(.)+空格</span><br></pre></td></tr></table></figure>
<p>1.策划目标<br>
2.战前准备<br>
3.开始行动<br>
无序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+ 、- 、* 创建无序列，任意数字开始+空格创建有序列表</span><br></pre></td></tr></table></figure>
<p>猪<br>
兔<br>
马<br>
Todolist</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">-</span> <span class="selector-attr">[ ]</span> 参加会议</span><br><span class="line"><span class="selector-tag">-</span> <span class="selector-attr">[x]</span> 中超足球赛</span><br></pre></td></tr></table></figure>
<p>[ ] 参加会议<br>
[x] 中超足球赛<br>
Table</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">快捷键Ctrl+T弹出对话框</span><br></pre></td></tr></table></figure>
<p>分割线</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">***+回车  </span><br><span class="line">---+回车</span><br></pre></td></tr></table></figure>
<h1>插入</h1>
<p>图片</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">![图片内容](http:<span class="comment">//t10.baidu.com/it/u=1069603383,3074552113&amp;fm=170&amp;s=771B15C75C12D8D61C3C69FB0300501F&amp;w=640&amp;h=426&amp;img.JPEG)</span></span><br><span class="line"> 也可使用快捷键Ctrl+K</span><br><span class="line">PS：也可将图片直接拖拽进来，自动生成链接</span><br></pre></td></tr></table></figure>
<p>链接<br>
内行式</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">[<span class="meta">百度一下，你就知道</span>](https:<span class="comment">//www.baidu.com/)</span></span><br></pre></td></tr></table></figure>
<p>参考式</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">[百度一下，你就知道][]https:<span class="comment">//www.baidu.com/          # 第二个括号内可任意填写(不显)</span></span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">[百度一下，你就知道][]https://www.baidu.com/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">快速链接</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">&lt;http:<span class="comment">//www.baidu.com&gt;</span></span><br><span class="line">PS：按住Ctrl点击链接可直接打开。</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#数学公式（简）</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">Typora支持加入用LaTeX写成的数学公式，并且在软件界面下用MathJax直接渲染。</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">*1.行内公式(inline math)，可以在偏好设置中单独打开，由一个美元符号将公式围起来；name=\prod \frac&#123;1&#125;&#123;i^2&#125;$</span></span><br><span class="line"><span class="string">*2.行外公式，直接按Ctrl+Shift+M；(双$+回车也可做到)</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">注：上标和下标可以使用数学表达式来获取</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">#其余</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">引用</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">&gt;+空格    或按快捷键Ctrl+Shift+Q</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">注释</span></span><br><span class="line"><span class="string">要添加注释的文字</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">这是我们的标号[^<span class="number">1</span>]</span><br><span class="line">[^<span class="number">1</span>]:标号的含义</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">表情</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">:单词:</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">目录</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">[TOC]</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">Typora快捷键整合</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">Ctrl+<span class="number">1</span>  一阶标题    Ctrl+B  字体加粗</span><br><span class="line">Ctrl+<span class="number">2</span>  二阶标题    Ctrl+I  字体倾斜</span><br><span class="line">Ctrl+<span class="number">3</span>  三阶标题    Ctrl+U  下划线</span><br><span class="line">Ctrl+<span class="number">4</span>  四阶标题    Ctrl+Home   返回Typora顶部</span><br><span class="line">Ctrl+<span class="number">5</span>  五阶标题    Ctrl+End    返回Typora底部</span><br><span class="line">Ctrl+<span class="number">6</span>  六阶标题    Ctrl+T  创建表格</span><br><span class="line">Ctrl+L  选中某句话   Ctrl+K  创建超链接</span><br><span class="line">Ctrl+D  选中某个单词  Ctrl+F  搜索</span><br><span class="line">Ctrl+E  选中相同格式的文字   Ctrl+H  搜索并替换</span><br><span class="line">Alt+Shift+<span class="number">5</span> 删除线 Ctrl+Shift+I    插入图片</span><br><span class="line">Ctrl+Shift+M    公式块 Ctrl+Shift+Q    引用</span><br><span class="line"></span><br><span class="line">注：一些实体符号需要在实体符号之前加”\”才能够显示</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>为hexo添加hexo-admin组件</title>
    <url>/2020/01/07/%E4%B8%BAhexo%E6%B7%BB%E5%8A%A0hexo-admin%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<h4 id="准备工作">准备工作</h4>
<blockquote>
<p>已安装好<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a>，选择好自己的<a href="https://hexo.io/themes/" target="_blank" rel="noopener">主题</a>(我选择的主题是<a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener">melody</a>)，并部署到<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>等静态托管服务器上。</p>
</blockquote>
<h4 id="插件介绍">插件介绍</h4>
<blockquote>
<p><a href="https://github.com/jaredly/hexo-admin" target="_blank" rel="noopener"><strong>hexo-admin</strong></a> 是一个Hexo博客引擎的管理用户界面插件。这个插件最初是作为本地编辑器设计的，在本地运行<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a>使用<strong>hexo-admin</strong>编写文章，然后通过<code>hexo g</code>或<code>hexo d</code>（<code>hexo g</code>是本地渲染，<code>hexo d</code>是将渲染的静态页面发布到<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>）将生成的静态页面发布到<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>等静态服务器。如果你使用的是非静态托管服务器，比如自己买的主机搭建的<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a>，那么一定要设置<a href="https://github.com/jaredly/hexo-admin" target="_blank" rel="noopener"><strong>hexo-admin</strong></a>  的密码，否则谁都可以编辑你的文章。</p>
</blockquote>
<h4 id="插件安装">插件安装</h4>
<ol>
<li>
<p>首先进入hexo创建的博客项目的根目录下，执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install --save hexo-admin</span><br></pre></td></tr></table></figure>
<p>mac可能需要root权限，前面加个<code>sudo</code> 就可以了。如果报错缺少组件，则缺少什么安装什么，<code>npm install</code> 加缺少的组件。</p>
</li>
<li>
<p>运行下列命令启动<a href="https://github.com/jaredly/hexo-admin" target="_blank" rel="noopener"><strong>hexo-admin</strong></a> ：</p>
<p>hexo server -d<br>
打开 <a href="http://localhost:4000/admin/" target="_blank" rel="noopener">http://localhost:4000/admin/</a>  就可以访问到hexo-admin管理页面了。</p>
</li>
</ol>
<h4 id="密码保护">密码保护</h4>
<p>打开<code>setting</code>，点击<code>Setup authentification here</code>输入用户名，密码，密钥，下面会自动生成配置文件，复制加在<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a>根目录下的<code>_config.yml</code>中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">admin:</span><br><span class="line">  username: myfavoritename</span><br><span class="line">  password_hash: be121740bf988b2225a313fa1f107ca1</span><br><span class="line">  secret: a secret something</span><br></pre></td></tr></table></figure>
<p>重启<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a>，就可以看到登录页面了</p>
<h4 id="发布文章">发布文章</h4>
<p>进入后台之后点击<code>Deploy</code>，里面的Deploy按钮是用来执行发布脚本的，所以我们先在博客根目录下新建个目录<code>admin_script</code>，然后在目录中新建一个脚本<code>hexo-g.sh</code>，里面写下下面代码然后保存，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure>
<p>然后给hexo-g.sh加入可执行权限</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">chmod</span> +<span class="selector-tag">x</span> <span class="selector-tag">hexo-d</span><span class="selector-class">.sh</span></span><br></pre></td></tr></table></figure>
<p>然后在<code>_config.yml</code>中的admin下添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">admin:</span><br><span class="line">  username: myfavoritename</span><br><span class="line">  password_hash: be121740bf988b2225a313fa1f107ca1</span><br><span class="line">  secret: a secret something</span><br><span class="line">  deployCommand: .&#x2F;admin_script&#x2F;hexo-d.sh</span><br></pre></td></tr></table></figure>
<p>设置发布执行的脚本，点击<code>Deploy</code>就会执行这个命令并提交到<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>上。</p>
]]></content>
  </entry>
  <entry>
    <title>Shell数组</title>
    <url>/2020/01/07/Shell%E6%95%B0%E7%BB%84/</url>
    <content><![CDATA[<p>Shell在编程方面比Windows批处理强大很多，无论是在循环、运算。</p>
<p>bash支持一维数组（不支持多维数组），并且没有限定数组的大小。类似与C语言，数组元素的下标由0开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于0。<br>
定义数组</p>
<p>在Shell中，用括号来表示数组，数组元素用“空格”符号分割开。定义数组的一般形式为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array_name&#x3D;(value1 ... valuen)</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array_name&#x3D;(value0 value1 value2 value3)</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array_name&#x3D;(</span><br><span class="line">value0</span><br><span class="line">value1</span><br><span class="line">value2</span><br><span class="line">value3</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>还可以单独定义数组的各个分量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array_name[0]&#x3D;value0</span><br><span class="line">array_name[1]&#x3D;value1</span><br><span class="line">array_name[2]&#x3D;value2</span><br></pre></td></tr></table></figure>
<p>可以不使用连续的下标，而且下标的范围没有限制。</p>
<h4 id="读取数组">读取数组</h4>
<p>读取数组元素值的一般格式是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;array_name[index]&#125;</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">valuen&#x3D;$&#123;array_name[2]&#125;</span><br></pre></td></tr></table></figure>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">NAME[0]&#x3D;&quot;Zara&quot;</span><br><span class="line">NAME[1]&#x3D;&quot;Qadir&quot;</span><br><span class="line">NAME[2]&#x3D;&quot;Mahnaz&quot;</span><br><span class="line">NAME[3]&#x3D;&quot;Ayan&quot;</span><br><span class="line">NAME[4]&#x3D;&quot;Daisy&quot;</span><br><span class="line">echo &quot;First Index: $&#123;NAME[0]&#125;&quot;</span><br><span class="line">echo &quot;Second Index: $&#123;NAME[1]&#125;&quot;</span><br></pre></td></tr></table></figure>
<p>运行脚本，输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$.&#x2F;test.sh</span><br><span class="line">First Index: Zara</span><br><span class="line">Second Index: Qadir</span><br></pre></td></tr></table></figure>
<p>使用@ 或 * 可以获取数组中的所有元素，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;array_name[*]&#125;</span><br><span class="line">$&#123;array_name[@]&#125;</span><br></pre></td></tr></table></figure>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">NAME[0]&#x3D;&quot;Zara&quot;</span><br><span class="line">NAME[1]&#x3D;&quot;Qadir&quot;</span><br><span class="line">NAME[2]&#x3D;&quot;Mahnaz&quot;</span><br><span class="line">NAME[3]&#x3D;&quot;Ayan&quot;</span><br><span class="line">NAME[4]&#x3D;&quot;Daisy&quot;</span><br><span class="line">echo &quot;First Method: $&#123;NAME[*]&#125;&quot;</span><br><span class="line">echo &quot;Second Method: $&#123;NAME[@]&#125;&quot;</span><br></pre></td></tr></table></figure>
<p>运行脚本，输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$.&#x2F;test.sh</span><br><span class="line">First Method: Zara Qadir Mahnaz Ayan Daisy</span><br><span class="line">Second Method: Zara Qadir Mahnaz Ayan Daisy</span><br></pre></td></tr></table></figure>
<h4 id="获取数组的长度">获取数组的长度</h4>
<p>获取数组长度的方法与获取字符串长度的方法相同，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 取得数组元素的个数</span><br><span class="line">length&#x3D;$&#123;#array_name[@]&#125;</span><br><span class="line"># 或者</span><br><span class="line">length&#x3D;$&#123;#array_name[*]&#125;</span><br><span class="line"># 取得数组单个元素的长度</span><br><span class="line">lengthn&#x3D;$&#123;#array_name[n]&#125;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>liunx查看内存</title>
    <url>/2020/01/07/liunx%E6%9F%A5%E7%9C%8B%E5%86%85%E5%AD%98/</url>
    <content><![CDATA[<h3 id="监控内存">监控内存</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">top -d 1</span><br></pre></td></tr></table></figure>
<p>然后使用<code>shift + m</code>以内存排列。<a href="https://www.orchome.com/100" target="_blank" rel="noopener">top命令详解</a></p>
<h3 id="查看内存的使用">查看内存的使用</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">free -g</span><br><span class="line">free -m</span><br></pre></td></tr></table></figure>
<p>可参考：<a href="https://www.orchome.com/296" target="_blank" rel="noopener">free命令详解</a></p>
<h3 id="查看内存">查看内存</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep MemTotal &#x2F;proc&#x2F;meminfo</span><br><span class="line">grep MemTotal &#x2F;proc&#x2F;meminfo | cut -f2 -d:</span><br><span class="line">free -m |grep &quot;Mem&quot; | awk &#39;&#123;print $2&#125;’</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>kubernetes命令大全</title>
    <url>/2020/01/07/kubernetes%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/</url>
    <content><![CDATA[<h2 id="状态查询">状态查询</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看集群信息</span><br><span class="line">kubectl cluster-info</span><br><span class="line"></span><br><span class="line">systemctl status kube-apiserver</span><br><span class="line">systemctl status kubelet</span><br><span class="line">systemctl status kube-proxy</span><br><span class="line">systemctl status kube-scheduler</span><br><span class="line">systemctl status kube-controller-manager</span><br><span class="line">systemctl status docker</span><br></pre></td></tr></table></figure>
<h2 id="node相关">node相关</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看namespaces</span><br><span class="line">kubectl get namespaces</span><br><span class="line"></span><br><span class="line"># 为节点增加lable</span><br><span class="line">kubectl label nodes 10.126.72.31 points&#x3D;test</span><br><span class="line"></span><br><span class="line"># 查看节点和lable</span><br><span class="line">kubectl get nodes --show-labels</span><br><span class="line"></span><br><span class="line"># 查看状态</span><br><span class="line">kubectl get componentstatuses</span><br><span class="line"></span><br><span class="line"># Node的隔离与恢复</span><br><span class="line">## 隔离</span><br><span class="line">kubectl cordon k8s-node1</span><br><span class="line"></span><br><span class="line">## 恢复</span><br><span class="line">kubectl uncordon k8s-node1</span><br></pre></td></tr></table></figure>
<h2 id="查询">查询</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看nodes节点</span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"># 通过yaml文件查询</span><br><span class="line">kubectl get -f xxx-yaml&#x2F;</span><br><span class="line"></span><br><span class="line"># 查询资源</span><br><span class="line">kubectl get resourcequota</span><br><span class="line"></span><br><span class="line"># endpoints端</span><br><span class="line">kubectl get endpoints</span><br><span class="line"></span><br><span class="line"># 查看pods</span><br><span class="line"></span><br><span class="line"># 查看指定空间&#96;kube-system&#96;的pods</span><br><span class="line">kubectl get po -n kube-system</span><br><span class="line"></span><br><span class="line"># 查看所有空间的</span><br><span class="line">kubectl get pods -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"># 其他的写法</span><br><span class="line">kubectl get pod -o wide --namespace&#x3D;kube-system</span><br><span class="line"></span><br><span class="line"># 获取svc</span><br><span class="line">kubectl get svc --all-namespaces</span><br><span class="line"></span><br><span class="line"># 其他写法</span><br><span class="line">kubectl get services --all-namespaces</span><br><span class="line"></span><br><span class="line"># 通过lable查询</span><br><span class="line">kubectl get pods -l app&#x3D;nginx -o yaml|grep podIP</span><br><span class="line"></span><br><span class="line"># 当我们发现一个pod迟迟无法创建时，描述一个pods</span><br><span class="line">kubectl describe pod xxx</span><br></pre></td></tr></table></figure>
<h2 id="删除所有pod">删除所有pod</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除所有pods</span><br><span class="line">kubectl delete pods --all</span><br><span class="line"></span><br><span class="line"># 删除所有包含某个lable的pod和serivce</span><br><span class="line">kubectl delete pods,services -l name&#x3D;&lt;lable-name&gt;</span><br><span class="line"></span><br><span class="line"># 删除ui server,然后重建</span><br><span class="line">kubectl delete deployments kubernetes-dashboard --namespace&#x3D;kube-system</span><br><span class="line">kubectl delete services kubernetes-dashboard --namespace&#x3D;kube-system</span><br><span class="line"></span><br><span class="line"># 强制删除部署</span><br><span class="line">kubectl delete deployment kafka-1</span><br><span class="line"></span><br><span class="line"># 删除rc</span><br><span class="line">kubectl delete rs --all &amp;&amp; kubectl delete rc --all</span><br><span class="line"></span><br><span class="line">## 强制删除Terminating状态的pod</span><br><span class="line">kubectl delete deployment kafka-1 --grace-period&#x3D;0 --force</span><br></pre></td></tr></table></figure>
<h2 id="滚动">滚动</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 升级</span><br><span class="line">kubectl apply -f xxx.yaml --record</span><br><span class="line"></span><br><span class="line"># 回滚</span><br><span class="line">kubectl rollout undo deployment javademo</span><br><span class="line"></span><br><span class="line"># 查看滚动升级记录</span><br><span class="line">kubectl rollout history deployment &#123;名称&#125;</span><br></pre></td></tr></table></figure>
<h2 id="查看日志">查看日志</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看指定镜像的日志</span><br><span class="line">kubectl logs -f kube-dns-699984412-vz1q6 -n kube-system</span><br><span class="line"></span><br><span class="line">kubectl logs --tail&#x3D;10 nginx  </span><br><span class="line"></span><br><span class="line">#指定其中一个查看日志</span><br><span class="line">kubectl logs kube-dns-699984412-n5zkz -c kubedns --namespace&#x3D;kube-system</span><br><span class="line">kubectl logs kube-dns-699984412-vz1q6 -c dnsmasq --namespace&#x3D;kube-system</span><br><span class="line">kubectl logs kube-dns-699984412-mqb14 -c sidecar --namespace&#x3D;kube-system</span><br><span class="line"></span><br><span class="line"># 看日志</span><br><span class="line">journalctl -f</span><br></pre></td></tr></table></figure>
<h2 id="扩展">扩展</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 扩展副本</span><br><span class="line">kubectl scale rc xxxx --replicas&#x3D;3</span><br><span class="line">kubectl scale rc mysql --replicas&#x3D;1</span><br><span class="line">kubectl scale --replicas&#x3D;3 -f foo.yaml</span><br></pre></td></tr></table></figure>
<h2 id="执行">执行</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动</span><br><span class="line">nohup kubectl proxy --address&#x3D;&#39;10.1.70.247&#39; --port&#x3D;8001 --accept-hosts&#x3D;&#39;^*$&#39; &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"># 进入镜像</span><br><span class="line">kubectl exec kube-dns-699984412-vz1q6 -n kube-system -c kubedns ifconfig</span><br><span class="line">kubectl exec kube-dns-699984412-vz1q6 -n kube-system -c kubedns ifconfig &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line"># 执行镜像内命令</span><br><span class="line">kubectl exec kube-dns-4140740281-pfjhr -c etcd --namespace&#x3D;kube-system etcdctl get &#x2F;skydns&#x2F;local&#x2F;cluster&#x2F;default&#x2F;redis-master</span><br></pre></td></tr></table></figure>
<h2 id="无限循环命令">无限循环命令</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while true; do sleep 1; done</span><br></pre></td></tr></table></figure>
<h2 id="资源管理">资源管理</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 暂停资源更新（资源变更不会生效）</span><br><span class="line">kubectl rollout pause deployment xxx</span><br><span class="line"></span><br><span class="line"># 恢复资源更新</span><br><span class="line">kubectl rollout resume deployment xxx</span><br><span class="line"></span><br><span class="line"># 设置内存、cpu限制</span><br><span class="line">kubectl set resources deployment xxx -c&#x3D;xxx --limits&#x3D;cpu&#x3D;200m,memory&#x3D;512Mi --requests&#x3D;cpu&#x3D;1m,memory&#x3D;1Mi</span><br><span class="line"></span><br><span class="line"># 设置storageclass为默认</span><br><span class="line">kubectl patch storageclass &lt;your-class-name&gt; -p &#39;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io&#x2F;is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#39;</span><br></pre></td></tr></table></figure>
<h2 id="其他">其他</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建和删除</span><br><span class="line">kubectl create -f dashboard-controller.yaml</span><br><span class="line">kubectl delete -f dashboard-dashboard.yaml</span><br><span class="line"></span><br><span class="line"># 查看指定pods的环境变量</span><br><span class="line">kubectl exec xxx env</span><br><span class="line"></span><br><span class="line"># 判断dns是否通</span><br><span class="line">kubectl exec busybox -- nslookup kube-dns.kube-system</span><br><span class="line"></span><br><span class="line"># kube-proxy状态</span><br><span class="line">systemctl status kube-proxy -l</span><br><span class="line"></span><br><span class="line"># token的</span><br><span class="line">kubectl get serviceaccount&#x2F;kube-dns --namespace&#x3D;kube-system -o yaml|grep token</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>数据平台规划方案</title>
    <url>/2020/01/07/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E8%A7%84%E5%88%92%E6%96%B9%E6%A1%88/</url>
    <content><![CDATA[<h2 id="大数据平台现状">大数据平台现状</h2>
<ul>
<li>总可用节点：36个（各节点内存512G，CPU 56核）</li>
<li>总可用内存：9.38T（平均每个节点开放260G）</li>
<li>总可用CPU：1120个（平均每个节点开放31个）</li>
</ul>
<p>包含了文件存储、计算、数据库等集群服务。</p>
<p><strong>现有集群职能包括：</strong></p>
<ul>
<li>离线OLAP数据调度同步（原始数据）</li>
<li>各产品线离线生产任务（雷达、探针、定制产品、回溯测试等业务）</li>
<li>离线OLTP数据更新（HBase数据更新）</li>
<li>线下模型、数据测试（商户定制与联合建模）</li>
<li>模型训练、迭代与更新（评分卡、推荐、联合模型等）</li>
<li>基础数据实时流（底层实时数据处理）</li>
<li>各产品线实时流计算（雷达、探针等上层业务）</li>
<li>日常数据分析任务（大量数据分析、行业分析等需求）</li>
<li>OLTP数据库服务（HBase）</li>
<li>监控报表任务（BI、数据监控等）</li>
<li>数据仓库（数据整合、清洗、调度等）</li>
<li>宝付大数据平台相关任务（Spark、Hive、Impala等）</li>
</ul>
<p>由于当前集群职能繁多，网络带宽、磁盘IO等为集群共享，<strong>会因大型离线任务占用大量网络或磁盘IO峰值，对线上业务会造成短暂延迟。且集群环境较为复杂，有较多对线上业务造成影响的风险。</strong></p>
<h2 id="规划方案">规划方案</h2>
<p>鉴于后续业务发展，大数据平台的使用人数和执行任务将会快速上升，为了避免对线上业务的直接影响，提议部署一个次规模（20个节点内）的生产集群（以下简称在线集群）承接部分职能，减轻现有集群（以下简称离线离线计算集群）的压力，同时规划与隔离不同等级的任务。</p>
<p>在线集群<strong>主要职能为OLTP数据库服务（HBase）</strong>，将业务主库迁移至在线集群可保证业务不受任何大规模计算任务（或者计算量比较集中的情况）所带来的延迟影响。</p>
<p>除此之外，<strong>在线集群将不会进行其他任何程序与任务以保证线上服务的稳定性。</strong></p>
<p>将业务主库分离出去之后，离线计算集群的HBase将作为中间件为流式计算等系统提供服务（同时兼业务备库使用，在需要重启等情况下可以相互切换）。</p>
<p>此时离线计算集群的定位为执行大规模、密集的数据分析、计算、模型等计算资源消耗巨大的任务，在线集群的定位为稳定的业务数据服务。</p>
<p>由于离线计算集群承载了大部分任务，随着业务发展，后续会有更多计算需求产生（如设备指纹与爬虫数据的分析与计算），<strong>需要离线计算集群能够方便、快速的进行横向扩容。</strong></p>
<h2 id="迁移方案">迁移方案</h2>
<p>将使用新采购机器（50台）在新机房部署两个集群（在线+离线计算），<strong>使用的节点个数待定。</strong></p>
<p><strong>在线集群的HBase服务对部署参数、分区配置策略进行优化调整，同时确认跨集群数据同步方案与程序。</strong></p>
<p>集群准备就绪后：</p>
<p>1.同步老机房线上所有业务数据至新机房的在线集群。<br>
2.将老机房集群上数据同步、生产调度任务等相关程序、脚本和配置迁移至新机房的离线计算集群。<br>
3.老机房业务与任务确认迁移完成后，停机下线（分批）调整至新机房并入离线计算集群中，在线集群视情况调整一些新机器扩容。</p>
<p><strong>以上步骤需要整理详细操作过程和确认完成时间点。</strong></p>
<h2 id="离线计算集群资源池">离线计算集群资源池</h2>
<p>当前离线计算集群的资源池划分为宝付的production、development，新颜的production、development，在集群承载了比较多的职能的时候此划分方案已经不满足需求，建议对新颜的资源池进行更详细的划分：</p>
<ul>
<li>stream：流式计算系统使用，占比10%</li>
<li>model：模型训练与标签系统使用，占比30%</li>
<li>experiment：联合建模与商户线下测试使用，占比20%</li>
<li>production：离线生产任务使用，占比20%</li>
<li>development：测试任务与日常分析使用，占比20%</li>
</ul>
<p>以上分配占比仅为预估，需要根据实际情况（包括新采购需求添加之后的集群情况）重新调整。</p>
]]></content>
  </entry>
  <entry>
    <title>12.清理集群</title>
    <url>/2020/01/07/12.%E6%B8%85%E7%90%86%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>tags: clean</p>
<h1>12.清理集群</h1>
<h2 id="清理-Node-节点">清理 Node 节点</h2>
<p>停相关进程：</p>
<pre><code>$ sudo systemctl stop kubelet kube-proxy flanneld docker
$
</code></pre>
<p>清理文件：</p>
<pre><code>$ # umount kubelet 挂载的目录
$ mount | grep '/var/lib/kubelet'| awk '{print $3}'|xargs sudo umount
$ # 删除 kubelet 工作目录
$ sudo rm -rf /var/lib/kubelet
$ # 删除 docker 工作目录
$ sudo rm -rf /var/lib/docker
$ # 删除 flanneld 写入的网络配置文件
$ sudo rm -rf /var/run/flannel/
$ # 删除 docker 的一些运行文件
$ sudo rm -rf /var/run/docker/
$ # 删除 systemd unit 文件
$ sudo rm -rf /etc/systemd/system/{kubelet,docker,flanneld}.service
$ # 删除程序文件
$ sudo rm -rf /opt/k8s/bin/*
$ # 删除证书文件
$ sudo rm -rf /etc/flanneld/cert /etc/kubernetes/cert
$
</code></pre>
<p>清理 kube-proxy 和 docker 创建的 iptables：</p>
<pre><code>$ sudo iptables -F &amp;&amp; sudo iptables -X &amp;&amp; sudo iptables -F -t nat &amp;&amp; sudo iptables -X -t nat
$
</code></pre>
<p>删除 flanneld 和 docker 创建的网桥：</p>
<pre><code>$ ip link del flannel.1
$ ip link del docker0
$
</code></pre>
<h2 id="清理-Master-节点">清理 Master 节点</h2>
<p>停相关进程：</p>
<pre><code>$ sudo systemctl stop kube-apiserver kube-controller-manager kube-scheduler
$
</code></pre>
<p>清理文件：</p>
<pre><code>$ # 删除 kube-apiserver 工作目录
$ sudo rm -rf /var/run/kubernetes
$ # 删除 systemd unit 文件
$ sudo rm -rf /etc/systemd/system/{kube-apiserver,kube-controller-manager,kube-scheduler}.service
$ # 删除程序文件
$ sudo rm -rf /opt/k8s/bin/{kube-apiserver,kube-controller-manager,kube-scheduler}
$ # 删除证书文件
$ sudo rm -rf /etc/flanneld/cert /etc/kubernetes/cert
$
</code></pre>
<h2 id="清理-etcd-集群">清理 etcd 集群</h2>
<p>停相关进程：</p>
<pre><code>$ sudo systemctl stop etcd
$
</code></pre>
<p>清理文件：</p>
<pre><code>$ # 删除 etcd 的工作目录和数据目录
$ sudo rm -rf /var/lib/etcd
$ # 删除 systemd unit 文件
$ sudo rm -rf /etc/systemd/system/etcd.service
$ # 删除程序文件
$ sudo rm -rf /opt/k8s/bin/etcd
$ # 删除 x509 证书文件
$ sudo rm -rf /etc/etcd/cert/*
$</code></pre>
]]></content>
  </entry>
  <entry>
    <title>11.部署 harbor 私有仓库</title>
    <url>/2020/01/07/11.%E9%83%A8%E7%BD%B2%20harbor%20%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/</url>
    <content><![CDATA[<p>tags: registry, harbor</p>
<h1>11.部署 harbor 私有仓库</h1>
<p>本文档介绍使用 docker-compose 部署 harbor 私有仓库的步骤，你也可以使用 docker 官方的 registry 镜像部署私有仓库(<a href="https://www.orchome.com/663" target="_blank" rel="noopener">部署 Docker Registry</a>)。</p>
<h2 id="使用的变量">使用的变量</h2>
<p>本文档用到的变量定义如下：</p>
<pre><code>$ export NODE_IP=10.64.3.7 # 当前部署 harbor 的节点 IP
$
</code></pre>
<h2 id="下载文件">下载文件</h2>
<p>从 docker compose <a href="https://github.com/docker/compose/releases" target="_blank" rel="noopener">发布页面</a>下载最新的 <code>docker-compose</code> 二进制文件</p>
<pre><code>$ wget https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64
$ mv ~/docker-compose-Linux-x86_64 /opt/k8s/bin/docker-compose
$ chmod a+x  /opt/k8s/bin/docker-compose
$ export PATH=/opt/k8s/bin:$PATH
$
</code></pre>
<p>从 harbor <a href="https://github.com/vmware/harbor/releases" target="_blank" rel="noopener">发布页面</a>下载最新的 harbor 离线安装包</p>
<pre><code>$ wget  --continue https://storage.googleapis.com/harbor-releases/release-1.5.0/harbor-offline-installer-v1.5.1.tgz
$ tar -xzvf harbor-offline-installer-v1.5.1.tgz
$
</code></pre>
<h2 id="导入-docker-images">导入 docker images</h2>
<p>导入离线安装包中 harbor 相关的 docker images：</p>
<pre><code>$ cd harbor
$ docker load -i harbor.v1.5.1.tar.gz
$
</code></pre>
<h2 id="创建-harbor-nginx-服务器使用的-x509-证书">创建 harbor nginx 服务器使用的 x509 证书</h2>
<p>创建 harbor 证书签名请求：</p>
<pre><code>$ cat &gt; harbor-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;harbor&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;${NODE_IP}&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>hosts 字段指定授权使用该证书的当前部署节点 IP，如果后续使用域名访问 harbor 则还需要添加域名；</li>
</ul>
<p>生成 harbor 证书和私钥：</p>
<pre><code>$ cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes harbor-csr.json | cfssljson -bare harbor

$ ls harbor*
harbor.csr  harbor-csr.json  harbor-key.pem harbor.pem

$ sudo mkdir -p /etc/harbor/ssl
$ sudo mv harbor*.pem /etc/harbor/ssl
$ rm harbor.csr  harbor-csr.json
</code></pre>
<h2 id="修改-harbor-cfg-文件">修改 harbor.cfg 文件</h2>
<pre><code>$ cp harbor.cfg{,.bak}
$ vim harbor.cfg
$ diff harbor.cfg{,.bak}
7c7
&lt; hostname = 172.27.129.81
---
&gt; hostname = reg.mydomain.com
11c11
&lt; ui_url_protocol = https
---
&gt; ui_url_protocol = http
23,24c23,24
&lt; ssl_cert =  /etc/harbor/ssl/harbor.pem
&lt; ssl_cert_key = /etc/harbor/ssl/harbor-key.pem
---
&gt; ssl_cert = /data/cert/server.crt
&gt; ssl_cert_key = /data/cert/server.key

$ cp prepare{,.bak}
$ vim prepare
$ diff prepare{,.bak}
453a454
&gt;         print(&quot;%s %w&quot;, args, kw)
490c491
&lt;     empty_subj = &quot;/&quot;
---
&gt;     empty_subj = &quot;/C=/ST=/L=/O=/CN=/&quot;
</code></pre>
<ul>
<li>
<p>需要修改 prepare 脚本的 empyt_subj 参数，否则后续 install 时出错退出：</p>
<p>Fail to generate key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crt</p>
</li>
</ul>
<p>参考：<a href="https://github.com/vmware/harbor/issues/2920" target="_blank" rel="noopener">https://github.com/vmware/harbor/issues/2920</a></p>
<h2 id="加载和启动-harbor-镜像">加载和启动 harbor 镜像</h2>
<pre><code>$ sudo mkdir /data
$ sudo chmod 777 /var/run/docker.sock /data
$ sudo apt-get install python
$ ./install.sh

[Step 0]: checking installation environment ...

Note: docker version: 18.03.0

Note: docker-compose version: 1.21.2

[Step 1]: loading Harbor images ...
Loaded image: vmware/clair-photon:v2.0.1-v1.5.1
Loaded image: vmware/postgresql-photon:v1.5.1
Loaded image: vmware/harbor-adminserver:v1.5.1
Loaded image: vmware/registry-photon:v2.6.2-v1.5.1
Loaded image: vmware/photon:1.0
Loaded image: vmware/harbor-migrator:v1.5.1
Loaded image: vmware/harbor-ui:v1.5.1
Loaded image: vmware/redis-photon:v1.5.1
Loaded image: vmware/nginx-photon:v1.5.1
Loaded image: vmware/mariadb-photon:v1.5.1
Loaded image: vmware/notary-signer-photon:v0.5.1-v1.5.1
Loaded image: vmware/harbor-log:v1.5.1
Loaded image: vmware/harbor-db:v1.5.1
Loaded image: vmware/harbor-jobservice:v1.5.1
Loaded image: vmware/notary-server-photon:v0.5.1-v1.5.1


[Step 2]: preparing environment ...
loaded secret from file: /data/secretkey
Generated configuration file: ./common/config/nginx/nginx.conf
Generated configuration file: ./common/config/adminserver/env
Generated configuration file: ./common/config/ui/env
Generated configuration file: ./common/config/registry/config.yml
Generated configuration file: ./common/config/db/env
Generated configuration file: ./common/config/jobservice/env
Generated configuration file: ./common/config/jobservice/config.yml
Generated configuration file: ./common/config/log/logrotate.conf
Generated configuration file: ./common/config/jobservice/config.yml
Generated configuration file: ./common/config/ui/app.conf
Generated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crt
The configuration files are ready, please use docker-compose to start the service.


[Step 3]: checking existing instance of Harbor ...


[Step 4]: starting Harbor ...
Creating network &quot;harbor_harbor&quot; with the default driver
Creating harbor-log ... done
Creating redis              ... done
Creating harbor-adminserver ... done
Creating harbor-db          ... done
Creating registry           ... done
Creating harbor-ui          ... done
Creating harbor-jobservice  ... done
Creating nginx              ... done

✔ ----Harbor has been installed and started successfully.----

Now you should be able to visit the admin portal at https://172.27.129.81.
For more details, please visit https://github.com/vmware/harbor .
</code></pre>
<h2 id="访问管理界面">访问管理界面</h2>
<p>确认所有组件都工作正常：</p>
<pre><code>$ docker-compose  ps
       Name                     Command                  State                                    Ports
-------------------------------------------------------------------------------------------------------------------------------------
harbor-adminserver   /harbor/start.sh                 Up (healthy)
harbor-db            /usr/local/bin/docker-entr ...   Up (healthy)   3306/tcp
harbor-jobservice    /harbor/start.sh                 Up
harbor-log           /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514-&gt;10514/tcp
harbor-ui            /harbor/start.sh                 Up (healthy)
nginx                nginx -g daemon off;             Up (healthy)   0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcp
redis                docker-entrypoint.sh redis ...   Up             6379/tcp
registry             /entrypoint.sh serve /etc/ ...   Up (healthy)   5000/tcp
</code></pre>
<p>浏览器访问 <code>https://${NODE_IP}</code>，示例的是 <code>https://172.27.129.81</code>；</p>
<p>由于是在 virtualbox 虚机 kube-node2 中运行，所以需要做下端口转发，Vagrant 文件中已经指定 host 端口为 4443，也可以在 virtualbox 的 GUI 中直接添加端口转发：</p>
<p><img src="https://img.orchome.com/group1/M00/00/03/dr5oXFutksSACh_VAAZX5IF_yio990.png" alt="screenshot"></p>
<p>浏览器访问 <code>https://127.0.0.1:443</code>，用账号 <code>admin</code> 和 harbor.cfg 配置文件中的默认密码 <code>Harbor12345</code> 登陆系统。</p>
<p><img src="https://img.orchome.com/group1/M00/00/03/dr5oXFutkuWAGD1XAAOBIWs7iWE167.png" alt="screenshot"></p>
<h2 id="harbor-运行时产生的文件、目录">harbor 运行时产生的文件、目录</h2>
<p>harbor 将日志打印到 /var/log/harbor 的相关目录下，使用 docker logs XXX 或 docker-compose logs XXX 将看不到容器的日志。</p>
<pre><code>$ # 日志目录
$ ls /var/log/harbor
adminserver.log  jobservice.log  mysql.log  proxy.log  registry.log  ui.log
$ # 数据目录，包括数据库、镜像仓库
$ ls /data/
ca_download  config  database  job_logs registry  secretkey
</code></pre>
<h2 id="docker-客户端登陆">docker 客户端登陆</h2>
<p>将签署 harbor 证书的 CA 证书拷贝到 <code>/etc/docker/certs.d/172.27.129.81</code> 目录下</p>
<pre><code>$ sudo mkdir -p /etc/docker/certs.d/172.27.129.81
$ sudo cp /etc/kubernetes/cert/ca.pem /etc/docker/certs.d/172.27.129.81/ca.crt
$
</code></pre>
<p>登陆 harbor</p>
<pre><code>$ docker login 172.27.129.81
Username: admin
Password:
</code></pre>
<p>认证信息自动保存到 <code>~/.docker/config.json</code> 文件。</p>
<h2 id="其它操作">其它操作</h2>
<p>下列操作的工作目录均为 解压离线安装文件后 生成的 harbor 目录。</p>
<pre><code>$ # 停止 harbor
$ docker-compose down -v
$ # 修改配置
$ vim harbor.cfg
$ # 更修改的配置更新到 docker-compose.yml 文件
$ ./prepare
Clearing the configuration file: ./common/config/ui/app.conf
Clearing the configuration file: ./common/config/ui/env
Clearing the configuration file: ./common/config/ui/private_key.pem
Clearing the configuration file: ./common/config/db/env
Clearing the configuration file: ./common/config/registry/root.crt
Clearing the configuration file: ./common/config/registry/config.yml
Clearing the configuration file: ./common/config/jobservice/app.conf
Clearing the configuration file: ./common/config/jobservice/env
Clearing the configuration file: ./common/config/nginx/cert/admin.pem
Clearing the configuration file: ./common/config/nginx/cert/admin-key.pem
Clearing the configuration file: ./common/config/nginx/nginx.conf
Clearing the configuration file: ./common/config/adminserver/env
loaded secret from file: /data/secretkey
Generated configuration file: ./common/config/nginx/nginx.conf
Generated configuration file: ./common/config/adminserver/env
Generated configuration file: ./common/config/ui/env
Generated configuration file: ./common/config/registry/config.yml
Generated configuration file: ./common/config/db/env
Generated configuration file: ./common/config/jobservice/env
Generated configuration file: ./common/config/jobservice/app.conf
Generated configuration file: ./common/config/ui/app.conf
Generated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crt
The configuration files are ready, please use docker-compose to start the service.
$ sudo chmod -R 666 common ## 防止容器进程没有权限读取生成的配置
$ # 启动 harbor
$ docker-compose up -d</code></pre>
]]></content>
  </entry>
  <entry>
    <title>10.部署私有 docker registry</title>
    <url>/2020/01/07/10.%E9%83%A8%E7%BD%B2%E7%A7%81%E6%9C%89%20docker%20registry/</url>
    <content><![CDATA[<p>tags: registry, ceph</p>
<h1>10.部署私有 docker registry</h1>
<p>注意：本文档介绍使用 docker 官方的 registry v2 镜像部署私有仓库的步骤，你也可以部署 Harbor 私有仓库（<a href="https://www.orchome.com/664" target="_blank" rel="noopener">部署 Harbor 私有仓库</a>）。</p>
<p>本文档讲解部署一个 TLS 加密、HTTP Basic 认证、用 ceph rgw 做后端存储的私有 docker registry 步骤，如果使用其它类型的后端存储，则可以从 “创建 docker registry” 节开始；</p>
<p>示例两台机器 IP 如下：</p>
<ul>
<li>ceph rgw: 172.27.132.66</li>
<li>docker registry: 172.27.132.67</li>
</ul>
<h2 id="部署-ceph-RGW-节点">部署 ceph RGW 节点</h2>
<pre><code>$ ceph-deploy rgw create 172.27.132.66 # rgw 默认监听7480端口
$
</code></pre>
<h2 id="创建测试账号-demo">创建测试账号 demo</h2>
<pre><code>$ radosgw-admin user create --uid=demo --display-name=&quot;ceph rgw demo user&quot;
$
</code></pre>
<h2 id="创建-demo-账号的子账号-swift">创建 demo 账号的子账号 swift</h2>
<p>当前 registry 只支持使用 swift 协议访问 ceph rgw 存储，暂时不支持 s3 协议；</p>
<pre><code>$ radosgw-admin subuser create --uid demo --subuser=demo:swift --access=full --secret=secretkey --key-type=swift
$
</code></pre>
<h2 id="创建-demo-swift-子账号的-sercret-key">创建 demo:swift 子账号的 sercret key</h2>
<pre><code>$ radosgw-admin key create --subuser=demo:swift --key-type=swift --gen-secret
{
    &quot;user_id&quot;: &quot;demo&quot;,
    &quot;display_name&quot;: &quot;ceph rgw demo user&quot;,
    &quot;email&quot;: &quot;&quot;,
    &quot;suspended&quot;: 0,
    &quot;max_buckets&quot;: 1000,
    &quot;auid&quot;: 0,
    &quot;subusers&quot;: [
        {
            &quot;id&quot;: &quot;demo:swift&quot;,
            &quot;permissions&quot;: &quot;full-control&quot;
        }
    ],
    &quot;keys&quot;: [
        {
            &quot;user&quot;: &quot;demo&quot;,
            &quot;access_key&quot;: &quot;5Y1B1SIJ2YHKEHO5U36B&quot;,
            &quot;secret_key&quot;: &quot;nrIvtPqUj7pUlccLYPuR3ntVzIa50DToIpe7xFjT&quot;
        }
    ],
    &quot;swift_keys&quot;: [
        {
            &quot;user&quot;: &quot;demo:swift&quot;,
            &quot;secret_key&quot;: &quot;ttQcU1O17DFQ4I9xzKqwgUe7WIYYX99zhcIfU9vb&quot;
        }
    ],
    &quot;caps&quot;: [],
    &quot;op_mask&quot;: &quot;read, write, delete&quot;,
    &quot;default_placement&quot;: &quot;&quot;,
    &quot;placement_tags&quot;: [],
    &quot;bucket_quota&quot;: {
        &quot;enabled&quot;: false,
        &quot;max_size_kb&quot;: -1,
        &quot;max_objects&quot;: -1
    },
    &quot;user_quota&quot;: {
        &quot;enabled&quot;: false,
        &quot;max_size_kb&quot;: -1,
        &quot;max_objects&quot;: -1
    },
        &quot;temp_url_keys&quot;: []
}
</code></pre>
<ul>
<li><code>ttQcU1O17DFQ4I9xzKqwgUe7WIYYX99zhcIfU9vb</code> 为子账号 demo:swift 的 secret key；</li>
</ul>
<h2 id="创建-docker-registry">创建 docker registry</h2>
<p>创建 registry 使用的 x509 证书</p>
<pre><code>$ mkdir -p registry/{auth,certs}
$ cat &gt; registry-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;registry&quot;,
  &quot;hosts&quot;: [
      &quot;127.0.0.1&quot;,
      &quot;172.27.132.67&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
$ cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
    -ca-key=/etc/kubernetes/cert/ca-key.pem \
    -config=/etc/kubernetes/cert/ca-config.json \
    -profile=kubernetes registry-csr.json | cfssljson -bare registry
$ cp registry.pem registry-key.pem registry/certs
$
</code></pre>
<ul>
<li>这里复用以前创建的 CA 证书和秘钥文件；</li>
<li>hosts 字段指定 registry 的 NodeIP；</li>
</ul>
<p>创建 HTTP Baisc 认证文件</p>
<pre><code>$ docker run --entrypoint htpasswd registry:2 -Bbn foo foo123  &gt; registry/auth/htpasswd
$ cat  registry/auth/htpasswd
foo:$2y$05$iZaM45Jxlcg0DJKXZMggLOibAsHLGybyU.CgU9AHqWcVDyBjiScN.
</code></pre>
<p>配置 registry 参数</p>
<pre><code>export RGW_AUTH_URL=&quot;https://172.27.132.66:7480/auth/v1&quot;
export RGW_USER=&quot;demo:swift&quot;
export RGW_SECRET_KEY=&quot;ttQcU1O17DFQ4I9xzKqwgUe7WIYYX99zhcIfU9vb&quot;
cat &gt; config.yml &lt;&lt; EOF
# https://docs.docker.com/registry/configuration/#list-of-configuration-options
version: 0.1
log:
  level: info
  fromatter: text
  fields:
    service: registry

storage:
  cache:
    blobdescriptor: inmemory
  delete:
    enabled: true
  swift:
    authurl: ${RGW_AUTH_URL}
    username: ${RGW_USER}
    password: ${RGW_SECRET_KEY}
    container: registry

auth:
  htpasswd:
    realm: basic-realm
    path: /auth/htpasswd

https:
  addr: 0.0.0.0:8000
  headers:
    X-Content-Type-Options: [nosniff]
  tls:
    certificate: /certs/registry.pem
    key: /certs/registry-key.pem

health:
  storagedriver:
    enabled: true
    interval: 10s
    threshold: 3
EOF
[k8s@kube-node1 cert]$ cp config.yml registry
[k8s@kube-node1 cert]$ scp -r registry 172.27.132.67:/opt/k8s
</code></pre>
<ul>
<li>storage.swift 指定后端使用 swfit 接口协议的存储，这里配置的是 ceph rgw 存储参数；</li>
<li>auth.htpasswd 指定了 HTTP Basic 认证的 token 文件路径；</li>
<li>http.tls 指定了 registry http 服务器的证书和秘钥文件路径；</li>
</ul>
<p>创建 docker registry</p>
<pre><code>ssh k8s@172.27.132.67
$ docker run -d -p 8000:8000 --privileged \
    -v /opt/k8s/registry/auth/:/auth \
    -v /opt/k8s/registry/certs:/certs \
    -v /opt/k8s/registry/config.yml:/etc/docker/registry/config.yml \
    --name registry registry:2
</code></pre>
<ul>
<li>执行该 docker run 命令的机器 IP 为 172.27.132.67；</li>
</ul>
<h2 id="向-registry-push-image">向 registry push image</h2>
<p>将签署 registry 证书的 CA 证书拷贝到 <code>/etc/docker/certs.d/172.27.132.67:8000</code> 目录下</p>
<pre><code>[k8s@kube-node1 cert]$ sudo mkdir -p /etc/docker/certs.d/172.27.132.67:8000
[k8s@kube-node1 cert]$ sudo cp /etc/kubernetes/cert/ca.pem /etc/docker/certs.d/172.27.132.67:8000/ca.crt
</code></pre>
<p>登陆私有 registry</p>
<pre><code>$ docker login 172.27.132.67:8000
Username: foo
Password:
Login Succeeded
</code></pre>
<p>登陆信息被写入 <code>~/.docker/config.json</code> 文件</p>
<pre><code>$ cat ~/.docker/config.json
{
        &quot;auths&quot;: {
                &quot;172.27.132.67:8000&quot;: {
                        &quot;auth&quot;: &quot;Zm9vOmZvbzEyMw==&quot;
                }
        }
}
</code></pre>
<p>将本地的 image 打上私有 registry 的 tag</p>
<pre><code>$ docker tag prom/node-exporter:v0.16.0 172.27.132.67:8000/prom/node-exporter:v0.16.0
$ docker images |grep pause
prom/node-exporter:v0.16.0                            latest              f9d5de079539        2 years ago         239.8 kB
172.27.132.67:8000/prom/node-exporter:v0.16.0                        latest              f9d5de079539        2 years ago         239.8 kB
</code></pre>
<p>将 image push 到私有 registry</p>
<pre><code>$ docker push 172.27.132.67:8000/prom/node-exporter:v0.16.0
The push refers to a repository [172.27.132.67:8000/prom/node-exporter:v0.16.0]
5f70bf18a086: Pushed
e16a89738269: Pushed
latest: digest: sha256:9a6b437e896acad3f5a2a8084625fdd4177b2e7124ee943af642259f2f283359 size: 916
</code></pre>
<p>查看 ceph 上是否已经有 push 的 pause 容器文件</p>
<pre><code>[k8s@kube-node1 ~]$ rados lspools
rbd
cephfs_data
cephfs_metadata
.rgw.root
k8s
default.rgw.control
default.rgw.meta
default.rgw.log
default.rgw.buckets.index
default.rgw.buckets.data

[k8s@kube-node1 ~]$  rados --pool default.rgw.buckets.data ls|grep node-exporter
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_layers/sha256/cdb7590af5f064887f3d6008d46be65e929c74250d747813d85199e04fc70463/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_manifests/revisions/sha256/55302581333c43d540db0e144cf9e7735423117a733cdec27716d87254221086/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_manifests/tags/v0.16.0/current/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_manifests/tags/v0.16.0/index/sha256/55302581333c43d540db0e144cf9e7735423117a733cdec27716d87254221086/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_layers/sha256/224a21997e8ca8514d42eb2ed98b19a7ee2537bce0b3a26b8dff510ab637f15c/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_layers/sha256/528dda9cf23d0fad80347749d6d06229b9a19903e49b7177d5f4f58736538d4e/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_layers/sha256/188af75e2de0203eac7c6e982feff45f9c340eaac4c7a0f59129712524fa2984/link
</code></pre>
<h2 id="私有-registry-的运维操作">私有 registry 的运维操作</h2>
<h3 id="查询私有镜像中的-images">查询私有镜像中的 images</h3>
<pre><code>[k8s@kube-node1 ~]$ curl  --user foo:foo123 --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/_catalog
{&quot;repositories&quot;:[&quot;prom/node-exporter&quot;]}
</code></pre>
<h3 id="查询某个镜像的-tags-列表">查询某个镜像的 tags 列表</h3>
<pre><code>[k8s@kube-node1 ~]$  curl  --user foo:foo123 --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/prom/node-exporter/tags/list
{&quot;name&quot;:&quot;prom/node-exporter&quot;,&quot;tags&quot;:[&quot;v0.16.0&quot;]}
</code></pre>
<h3 id="获取-image-或-layer-的-digest">获取 image 或 layer 的 digest</h3>
<p>向 <code>v2/&lt;repoName&gt;/manifests/&lt;tagName&gt;</code> 发 GET 请求，从响应的头部 <code>Docker-Content-Digest</code> 获取 image digest，从响应的 body 的 <code>fsLayers.blobSum</code> 中获取 layDigests;</p>
<p>注意，必须包含请求头：<code>Accept: application/vnd.docker.distribution.manifest.v2+json</code>：</p>
<pre><code>[k8s@kube-node1 ~]$ curl -v -H &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; --user foo:foo123 --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/prom/node-exporter/manifests/v0.16.0
* About to connect() to 172.27.132.67 port 8000 (#0)
*   Trying 172.27.132.67...
* Connected to 172.27.132.67 (172.27.132.67) port 8000 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
*   CAfile: /etc/docker/certs.d/172.27.132.67:8000/ca.crt
  CApath: none
* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
* Server certificate:
*       subject: CN=registry,OU=4Paradigm,O=k8s,L=BeiJing,ST=BeiJing,C=CN
*       start date: Jul 05 12:52:00 2018 GMT
*       expire date: Jul 02 12:52:00 2028 GMT
*       common name: registry
*       issuer: CN=kubernetes,OU=4Paradigm,O=k8s,L=BeiJing,ST=BeiJing,C=CN
* Server auth using Basic with user 'foo'
&gt; GET /v2/prom/node-exporter/manifests/v0.16.0 HTTP/1.1
&gt; Authorization: Basic Zm9vOmZvbzEyMw==
&gt; User-Agent: curl/7.29.0
&gt; Host: 172.27.132.67:8000
&gt; Accept: application/vnd.docker.distribution.manifest.v2+json
&gt;
&lt; HTTP/1.1 200 OK
&lt; Content-Length: 949
&lt; Content-Type: application/vnd.docker.distribution.manifest.v2+json
&lt; Docker-Content-Digest: sha256:55302581333c43d540db0e144cf9e7735423117a733cdec27716d87254221086
&lt; Docker-Distribution-Api-Version: registry/2.0
&lt; Etag: &quot;sha256:55302581333c43d540db0e144cf9e7735423117a733cdec27716d87254221086&quot;
&lt; X-Content-Type-Options: nosniff
&lt; Date: Fri, 06 Jul 2018 06:18:41 GMT
&lt;
{
   &quot;schemaVersion&quot;: 2,
   &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;,
   &quot;config&quot;: {
      &quot;mediaType&quot;: &quot;application/vnd.docker.container.image.v1+json&quot;,
      &quot;size&quot;: 3511,
      &quot;digest&quot;: &quot;sha256:188af75e2de0203eac7c6e982feff45f9c340eaac4c7a0f59129712524fa2984&quot;
   },
   &quot;layers&quot;: [
      {
         &quot;mediaType&quot;: &quot;application/vnd.docker.image.rootfs.diff.tar.gzip&quot;,
         &quot;size&quot;: 2392417,
         &quot;digest&quot;: &quot;sha256:224a21997e8ca8514d42eb2ed98b19a7ee2537bce0b3a26b8dff510ab637f15c&quot;
      },
      {
         &quot;mediaType&quot;: &quot;application/vnd.docker.image.rootfs.diff.tar.gzip&quot;,
         &quot;size&quot;: 560703,
         &quot;digest&quot;: &quot;sha256:cdb7590af5f064887f3d6008d46be65e929c74250d747813d85199e04fc70463&quot;
      },
      {
         &quot;mediaType&quot;: &quot;application/vnd.docker.image.rootfs.diff.tar.gzip&quot;,
         &quot;size&quot;: 5332460,
         &quot;digest&quot;: &quot;sha256:528dda9cf23d0fad80347749d6d06229b9a19903e49b7177d5f4f58736538d4e&quot;
      }
   ]
</code></pre>
<h3 id="删除-image">删除 image</h3>
<p>向 <code>/v2/&lt;name&gt;/manifests/&lt;reference&gt;</code> 发送 DELETE 请求，reference 为上一步返回的 Docker-Content-Digest 字段内容：</p>
<pre><code>$ curl -X DELETE  --user foo:foo123 --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/prom/node-exporter/manifests/sha256:68effe31a4ae8312e47f54bec52d1fc925908009ce7e6f734e1b54a4169081c5
$
</code></pre>
<h3 id="删除-layer">删除 layer</h3>
<p>向 <code>/v2/&lt;name&gt;/blobs/&lt;digest&gt;</code>发送 DELETE 请求，其中 digest 是上一步返回的 <code>fsLayers.blobSum</code> 字段内容：</p>
<pre><code>$ curl -X DELETE  --user foo:foo123 --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/prom/node-exporter/blobs/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4
$ curl -X DELETE  --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/prom/node-exporter/blobs/sha256:04176c8b224aa0eb9942af765f66dae866f436e75acef028fe44b8a98e045515
$
</code></pre>
<h2 id="常见问题">常见问题</h2>
<h3 id="login-失败-416">login 失败 416</h3>
<p>执行 <a href="https://docs.ceph.com/docs/master/install/install-ceph-gateway/" target="_blank" rel="noopener">https://docs.ceph.com/docs/master/install/install-ceph-gateway/</a> 里面的 s3 <a href="http://test.py" target="_blank" rel="noopener">test.py</a> 程序失败：</p>
<p>[k8s@kube-node1 cert]$ python <a href="http://s3test.py" target="_blank" rel="noopener">s3test.py</a><br>
Traceback (most recent call last):<br>
File “<a href="http://s3test.py" target="_blank" rel="noopener">s3test.py</a>”, line 12, in<br>
bucket = conn.create_bucket(‘my-new-bucket’)<br>
File “/usr/lib/python2.7/site-packages/boto/s3/connection.py”, line 625, in create_bucket<br>
response.status, response.reason, body)<br>
boto.exception.S3ResponseError: S3ResponseError: 416 Requested Range Not Satisfiable</p>
<p>解决版办法：</p>
<ol>
<li>在管理节点上修改 ceph.conf</li>
<li>ceph-deploy config push kube-node1 kube-node2 kube-node3</li>
<li>systemctl restart ‘ceph-mds@kube-node3.service’<br>
systemctl restart ceph-osd@0<br>
systemctl restart ‘ceph-mon@kube-node1.service’<br>
systemctl restart ‘ceph-mgr@kube-node1.service’</li>
</ol>
<p>For anyone who is hitting this issue<br>
set default pg_num and pgp_num to lower value(8 for example), or set mon_max_pg_per_osd to a high value in ceph.conf<br>
radosgw-admin doesn’ throw proper error when internal pool creation fails, hence the upper level error which is very confusing.</p>
<p><a href="https://tracker.ceph.com/issues/21497" target="_blank" rel="noopener">https://tracker.ceph.com/issues/21497</a></p>
<h3 id="login-失败-503">login 失败 503</h3>
<p>[root@kube-node1 ~]# docker login 172.27.132.67:8000<br>
Username: foo<br>
Password:<br>
Error response from daemon: login attempt to <a href="https://172.27.132.67:8000/v2/" target="_blank" rel="noopener">https://172.27.132.67:8000/v2/</a> failed with status: 503 Service Unavailable</p>
<p>原因： docker run 缺少 --privileged 参数；</p>
]]></content>
  </entry>
  <entry>
    <title>09-5.部署 EFK 插件</title>
    <url>/2020/01/07/09-5.%E9%83%A8%E7%BD%B2%20EFK%20%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<p>tags: addons, EFK, fluentd, elasticsearch, kibana</p>
<h1>09-5.部署 EFK 插件</h1>
<p>EFK 对应的目录：<code>kubernetes/cluster/addons/fluentd-elasticsearch</code></p>
<pre><code>$ cd /opt/k8s/kubernetes/cluster/addons/fluentd-elasticsearch
$ ls *.yaml
es-service.yaml  es-statefulset.yaml  fluentd-es-configmap.yaml  fluentd-es-ds.yaml  kibana-deployment.yaml  kibana-service.yaml
</code></pre>
<h2 id="修改定义文件">修改定义文件</h2>
<pre><code>$ cp es-statefulset.yaml{,.orig}
$ diff es-statefulset.yaml{,.orig}
76c76
&lt;       - image: longtds/elasticsearch:v5.6.4
---
&gt;       - image: k8s.gcr.io/elasticsearch:v5.6.4

$ cp fluentd-es-ds.yaml{,.orig}
$ diff fluentd-es-ds.yaml{,.orig}
79c79
&lt;         image: netonline/fluentd-elasticsearch:v2.0.4
---
&gt;         image: k8s.gcr.io/fluentd-elasticsearch:v2.0.4
</code></pre>
<h2 id="给-Node-设置标签">给 Node 设置标签</h2>
<p>DaemonSet <code>fluentd-es</code> 只会调度到设置了标签 <code>beta.kubernetes.io/fluentd-ds-ready=true</code> 的 Node，需要在期望运行 fluentd 的 Node 上设置该标签；</p>
<pre><code>$ kubectl get nodes
NAME         STATUS    ROLES     AGE       VERSION
kube-node1   Ready     &lt;none&gt;    3d        v1.10.4
kube-node2   Ready     &lt;none&gt;    3d        v1.10.4
kube-node3   Ready     &lt;none&gt;    3d        v1.10.4

$ kubectl label nodes kube-node3 beta.kubernetes.io/fluentd-ds-ready=true
node &quot;kube-node3&quot; labeled
</code></pre>
<h2 id="执行定义文件">执行定义文件</h2>
<pre><code>$ pwd
/opt/k8s/kubernetes/cluster/addons/fluentd-elasticsearch
$ ls *.yaml
es-service.yaml  es-statefulset.yaml  fluentd-es-configmap.yaml  fluentd-es-ds.yaml  kibana-deployment.yaml  kibana-service.yaml
$ kubectl create -f .
</code></pre>
<h2 id="检查执行结果">检查执行结果</h2>
<pre><code>$ kubectl get pods -n kube-system -o wide|grep -E 'elasticsearch|fluentd|kibana'
elasticsearch-logging-0                  1/1       Running   0          5m        172.30.81.7   kube-node1
elasticsearch-logging-1                  1/1       Running   0          2m        172.30.39.8   kube-node3
fluentd-es-v2.0.4-hntfp                  1/1       Running   0          5m        172.30.39.6   kube-node3
kibana-logging-7445dc9757-pvpcv          1/1       Running   0          5m        172.30.39.7   kube-node3

$ kubectl get service  -n kube-system|grep -E 'elasticsearch|kibana'
elasticsearch-logging   ClusterIP   10.254.50.198    &lt;none&gt;        9200/TCP        5m
kibana-logging          ClusterIP   10.254.255.190   &lt;none&gt;        5601/TCP        5m
</code></pre>
<p>kibana Pod 第一次启动时会用**较长时间(0-20分钟)**来优化和 Cache 状态页面，可以 tailf 该 Pod 的日志观察进度：</p>
<pre><code>[k8s@kube-node1 fluentd-elasticsearch]$ kubectl logs kibana-logging-7445dc9757-pvpcv -n kube-system -f
{&quot;type&quot;:&quot;log&quot;,&quot;@timestamp&quot;:&quot;2018-06-16T11:36:18Z&quot;,&quot;tags&quot;:[&quot;info&quot;,&quot;optimize&quot;],&quot;pid&quot;:1,&quot;message&quot;:&quot;Optimizing and caching bundles for graph, ml, kibana, stateSessionStorageRedirect, timelion and status_page. This may take a few minutes&quot;}
{&quot;type&quot;:&quot;log&quot;,&quot;@timestamp&quot;:&quot;2018-06-16T11:40:03Z&quot;,&quot;tags&quot;:[&quot;info&quot;,&quot;optimize&quot;],&quot;pid&quot;:1,&quot;message&quot;:&quot;Optimization of bundles for graph, ml, kibana, stateSessionStorageRedirect, timelion and status_page complete in 224.57 seconds&quot;}
</code></pre>
<p>注意：只有当的 Kibana pod 启动完成后，才能查看 kibana dashboard，否则会提示 refuse。</p>
<h2 id="访问-kibana">访问 kibana</h2>
<ol>
<li>
<p>通过 kube-apiserver 访问：</p>
<pre><code> $ kubectl cluster-info|grep -E 'Elasticsearch|Kibana'
 Elasticsearch is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy
 Kibana is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy
</code></pre>
<p>浏览器访问 URL： <code>https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy</code><br>
对于 virtuabox 做了端口映射： <code>https://127.0.0.1:8080/api/v1/namespaces/kube-system/services/kibana-logging/proxy</code></p>
</li>
<li>
<p>通过 kubectl proxy 访问：</p>
<p>创建代理</p>
<pre><code> $ kubectl proxy --address='172.27.129.105' --port=8086 --accept-hosts='^*$'
 Starting to serve on 172.27.129.80:8086
</code></pre>
<p>浏览器访问 URL：<code>https://172.27.129.105:8086/api/v1/namespaces/kube-system/services/kibana-logging/proxy</code><br>
对于 virtuabox 做了端口映射： <code>https://127.0.0.1:8086/api/v1/namespaces/kube-system/services/kibana-logging/proxy</code></p>
</li>
</ol>
<p>在 Settings -&gt; Indices 页面创建一个 index（相当于 mysql 中的一个 database），选中 <code>Index contains time-based events</code>，使用默认的 <code>logstash-*</code> pattern，点击 <code>Create</code> ;</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufGsGAXPDYAATDIwOmfzI253.png" alt="screenshot"></p>
<p>创建 Index 后，稍等几分钟就可以在 <code>Discover</code> 菜单下看到 ElasticSearch logging 中汇聚的日志；</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufGuGAfailAAn_vbTpJB0015.png" alt="screenshot"></p>
]]></content>
  </entry>
  <entry>
    <title>09-4.部署 metrics-server 插件</title>
    <url>/2020/01/07/09-4.%E9%83%A8%E7%BD%B2%20metrics-server%20%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<p>tags: addons, metrics, metrics-server</p>
<h1>09-4.部署 metrics-server 插件</h1>
<h2 id="创建-metrics-server-使用的证书">创建 metrics-server 使用的证书</h2>
<p>创建 metrics-server 证书签名请求:</p>
<pre><code>cat &gt; metrics-server-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;aggregator&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>注意： CN 名称为 aggregator，需要与 kube-apiserver 的 --requestheader-allowed-names 参数配置一致；</li>
</ul>
<p>生成 metrics-server 证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem  \
  -config=/etc/kubernetes/cert/ca-config.json  \
  -profile=kubernetes metrics-server-csr.json | cfssljson -bare metrics-server
</code></pre>
<p>将生成的证书和私钥文件拷贝到 kube-apiserver 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp metrics-server*.pem k8s@${node_ip}:/etc/kubernetes/cert/
  done
</code></pre>
<h2 id="修改-kubernetes-控制平面组件的配置以支持-metrics-server">修改 kubernetes 控制平面组件的配置以支持 metrics-server</h2>
<h3 id="kube-apiserver">kube-apiserver</h3>
<p>添加如下配置参数：</p>
<pre><code>--requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem
--requestheader-allowed-names=&quot;&quot;
--requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot;
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
--proxy-client-cert-file=/etc/kubernetes/cert/metrics-server.pem
--proxy-client-key-file=/etc/kubernetes/cert/metrics-server-key.pem
--runtime-config=api/all=true
</code></pre>
<ul>
<li><code>--requestheader-XXX</code>、<code>--proxy-client-XXX</code> 是 kube-apiserver 的 aggregator layer 相关的配置参数，metrics-server &amp; HPA 需要使用；</li>
<li><code>--requestheader-client-ca-file</code>：用于签名 <code>--proxy-client-cert-file</code> 和 <code>--proxy-client-key-file</code> 指定的证书；在启用了 metric aggregator 时使用；</li>
<li>如果 --requestheader-allowed-names 不为空，则–proxy-client-cert-file 证书的 CN 必须位于 allowed-names 中，默认为 aggregator;</li>
</ul>
<p>如果 kube-apiserver 机器<strong>没有</strong>运行 kube-proxy，则还需要添加 <code>--enable-aggregator-routing=true</code> 参数；</p>
<p>关于 <code>--requestheader-XXX</code> 相关参数，参考：</p>
<ul>
<li><a href="https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md</a></li>
<li><a href="https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/" target="_blank" rel="noopener">https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/</a></li>
</ul>
<p>注意：requestheader-client-ca-file 指定的 CA 证书，必须具有 client auth and server auth；</p>
<h3 id="kube-controllr-manager">kube-controllr-manager</h3>
<p>添加如下配置参数：</p>
<p>–horizontal-pod-autoscaler-use-rest-clients=true</p>
<p>用于配置 HPA 控制器使用 REST 客户端获取 metrics 数据。</p>
<h2 id="整体架构">整体架构</h2>
<p><img src="https://img.orchome.com/group1/M00/00/06/rBAABF4Nnk-Ab8mMAAErdjdFQWs425.png" alt="screenshot"></p>
<h2 id="修改插件配置文件配置文件">修改插件配置文件配置文件</h2>
<p>metrics-server 插件位于 kubernetes 的 <code>cluster/addons/metrics-server/</code> 目录下。</p>
<p>修改 metrics-server-deployment 文件：</p>
<pre><code>$ cp metrics-server-deployment.yaml{,.orig}
$ diff metrics-server-deployment.yaml.orig metrics-server-deployment.yaml
51c51
&lt;         image: mirrorgooglecontainers/metrics-server-amd64:v0.2.1
---
&gt;         image: k8s.gcr.io/metrics-server-amd64:v0.2.1
54c54
&lt;         - --source=kubernetes.summary_api:''
---
&gt;         - --source=kubernetes.summary_api:https://kubernetes.default?kubeletHttps=true&amp;kubeletPort=10250
60c60
&lt;         image: siriuszg/addon-resizer:1.8.1
---
&gt;         image: k8s.gcr.io/addon-resizer:1.8.1
</code></pre>
<ul>
<li>metrics-server 的参数格式与 heapster 类似。由于 kubelet 只在 10250 监听 https 请求，故添加相关参数；</li>
</ul>
<p>授予 kube-system:metrics-server ServiceAccount 访问 kubelet API 的权限：</p>
<pre><code>$ cat auth-kubelet.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: metrics-server:system:kubelet-api-admin
  labels:
    kubernetes.io/cluster-service: &quot;true&quot;
    addonmanager.kubernetes.io/mode: Reconcile
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kubelet-api-admin
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
</code></pre>
<ul>
<li>新建一个 ClusterRoleBindings 定义文件，授予相关权限；</li>
</ul>
<h2 id="创建-metrics-server">创建 metrics-server</h2>
<pre><code>$ pwd
/opt/k8s/kubernetes/cluster/addons/metrics-server
$ ls -l *.yaml
-rw-rw-r-- 1 k8s k8s  398 Jun  5 07:17 auth-delegator.yaml
-rw-rw-r-- 1 k8s k8s  404 Jun 16 18:02 auth-kubelet.yaml
-rw-rw-r-- 1 k8s k8s  419 Jun  5 07:17 auth-reader.yaml
-rw-rw-r-- 1 k8s k8s  393 Jun  5 07:17 metrics-apiservice.yaml
-rw-rw-r-- 1 k8s k8s 2640 Jun 16 17:54 metrics-server-deployment.yaml
-rw-rw-r-- 1 k8s k8s  336 Jun  5 07:17 metrics-server-service.yaml
-rw-rw-r-- 1 k8s k8s  801 Jun  5 07:17 resource-reader.yaml
$ kubectl create -f .
</code></pre>
<h2 id="查看运行情况">查看运行情况</h2>
<pre><code>$ kubectl get pods -n kube-system |grep metrics-server
metrics-server-v0.2.1-7486f5bd67-v95q2   2/2       Running   0          45s

$ kubectl get svc -n kube-system|grep metrics-server
metrics-server         ClusterIP   10.254.115.120   &lt;none&gt;        443/TCP         1m
</code></pre>
<h2 id="查看-metrcs-server-输出的-metrics">查看 metrcs-server 输出的 metrics</h2>
<p>metrics-server 输出的 APIs：<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md" target="_blank" rel="noopener">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md</a></p>
<ol>
<li>
<p>通过 kube-apiserver 或 kubectl proxy 访问：</p>
<p><a href="https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/nodes" target="_blank" rel="noopener">https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/nodes</a><br>
<a href="https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/nodes/" target="_blank" rel="noopener">https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/nodes/</a><br>
<a href="https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/pods" target="_blank" rel="noopener">https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/pods</a><br>
<a href="https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/namespace/" target="_blank" rel="noopener">https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/namespace/</a>/pods/</p>
</li>
<li>
<p>直接使用 kubectl 命令访问：</p>
<p>kubectl get --raw apis/metrics.k8s.io/v1beta1/nodes<br>
kubectl get --raw apis/metrics.k8s.io/v1beta1/pods<br>
kubectl get --raw apis/metrics.k8s.io/v1beta1/nodes/<br>
kubectl get --raw apis/metrics.k8s.io/v1beta1/namespace//pods/</p>
<p>$ kubectl get --raw “/apis/metrics.k8s.io/v1beta1” | jq .<br>
{<br>
“kind”: “APIResourceList”,<br>
“apiVersion”: “v1”,<br>
“groupVersion”: “<a href="http://metrics.k8s.io/v1beta1" target="_blank" rel="noopener">metrics.k8s.io/v1beta1</a>”,<br>
“resources”: [<br>
{<br>
“name”: “nodes”,<br>
“singularName”: “”,<br>
“namespaced”: false,<br>
“kind”: “NodeMetrics”,<br>
“verbs”: [<br>
“get”,<br>
“list”<br>
]<br>
},<br>
{<br>
“name”: “pods”,<br>
“singularName”: “”,<br>
“namespaced”: true,<br>
“kind”: “PodMetrics”,<br>
“verbs”: [<br>
“get”,<br>
“list”<br>
]<br>
}<br>
]<br>
}</p>
<p>$ kubectl get --raw “/apis/metrics.k8s.io/v1beta1/nodes” | jq .<br>
{<br>
“kind”: “NodeMetricsList”,<br>
“apiVersion”: “<a href="http://metrics.k8s.io/v1beta1" target="_blank" rel="noopener">metrics.k8s.io/v1beta1</a>”,<br>
“metadata”: {<br>
“selfLink”: “/apis/metrics.k8s.io/v1beta1/nodes”<br>
},<br>
“items”: [<br>
{<br>
“metadata”: {<br>
“name”: “kube-node3”,<br>
“selfLink”: “/apis/metrics.k8s.io/v1beta1/nodes/kube-node3”,<br>
“creationTimestamp”: “2018-06-16T10:24:03Z”<br>
},<br>
“timestamp”: “2018-06-16T10:23:00Z”,<br>
“window”: “1m0s”,<br>
“usage”: {<br>
“cpu”: “133m”,<br>
“memory”: “1115728Ki”<br>
}<br>
},<br>
{<br>
“metadata”: {<br>
“name”: “kube-node1”,<br>
“selfLink”: “/apis/metrics.k8s.io/v1beta1/nodes/kube-node1”,<br>
“creationTimestamp”: “2018-06-16T10:24:03Z”<br>
},<br>
“timestamp”: “2018-06-16T10:23:00Z”,<br>
“window”: “1m0s”,<br>
“usage”: {<br>
“cpu”: “221m”,<br>
“memory”: “6799908Ki”<br>
}<br>
},<br>
{<br>
“metadata”: {<br>
“name”: “kube-node2”,<br>
“selfLink”: “/apis/metrics.k8s.io/v1beta1/nodes/kube-node2”,<br>
“creationTimestamp”: “2018-06-16T10:24:03Z”<br>
},<br>
“timestamp”: “2018-06-16T10:23:00Z”,<br>
“window”: “1m0s”,<br>
“usage”: {<br>
“cpu”: “76m”,<br>
“memory”: “1130180Ki”<br>
}<br>
}<br>
]<br>
}</p>
</li>
</ol>
<ul>
<li>/apis/metrics.k8s.io/v1beta1/nodes 和 /apis/metrics.k8s.io/v1beta1/pods 返回的 usage 包含 CPU 和 Memory；</li>
</ul>
<h2 id="参考：">参考：</h2>
<ol>
<li><a href="https://kubernetes.feisky.xyz/zh/addons/metrics.html" target="_blank" rel="noopener">https://kubernetes.feisky.xyz/zh/addons/metrics.html</a></li>
<li>metrics-server RBAC：<a href="https://github.com/kubernetes-incubator/metrics-server/issues/40" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/metrics-server/issues/40</a></li>
<li>metrics-server 参数：<a href="https://github.com/kubernetes-incubator/metrics-server/issues/25" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/metrics-server/issues/25</a></li>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>09-3.部署 heapster 插件</title>
    <url>/2020/01/07/09-3.%E9%83%A8%E7%BD%B2%20heapster%20%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<p>tags: addons, heapster</p>
<h1>09-3.部署 heapster 插件</h1>
<p>Heapster是一个收集者，将每个Node上的cAdvisor的数据进行汇总，然后导到第三方工具(如InfluxDB)。</p>
<p>Heapster 是通过调用 kubelet 的 http API 来获取 cAdvisor 的 metrics 数据的。</p>
<p>由于 kublet 只在 10250 端口接收 https 请求，故需要修改 heapster 的 deployment 配置。同时，需要赋予 kube-system:heapster ServiceAccount 调用 kubelet API 的权限。</p>
<h2 id="下载-heapster-文件">下载 heapster 文件</h2>
<p>到 <a href="https://github.com/kubernetes/heapster/releases" target="_blank" rel="noopener">heapster release 页面</a> 下载最新版本的 heapster</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://github.com/kubernetes/heapster/archive/v1.5.3.tar.gz</span><br><span class="line">tar -xzvf v1.5.3.tar.gz</span><br><span class="line">mv v1.5.3.tar.gz heapster-1.5.3.tar.gz</span><br></pre></td></tr></table></figure>
<p>官方文件目录： <code>heapster-1.5.3/deploy/kube-config/influxdb</code></p>
<h2 id="修改配置">修改配置</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> heapster-1.5.3/deploy/kube-config/influxdb</span><br><span class="line">$ cp grafana.yaml&#123;,.orig&#125;</span><br><span class="line">$ diff grafana.yaml.orig grafana.yaml</span><br><span class="line">16c16</span><br><span class="line">&lt;         image: gcr.io/google_containers/heapster-grafana-amd64:v4.4.3</span><br><span class="line">---</span><br><span class="line">&gt;         image: wanghkkk/heapster-grafana-amd64-v4.4.3:v4.4.3</span><br><span class="line">67c67</span><br><span class="line">&lt;   <span class="comment"># type: NodePort</span></span><br><span class="line">---</span><br><span class="line">&gt;   <span class="built_in">type</span>: NodePort</span><br></pre></td></tr></table></figure>
<ul>
<li>开启 NodePort；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cp heapster.yaml&#123;,.orig&#125;</span><br><span class="line">$ diff heapster.yaml.orig heapster.yaml</span><br><span class="line">23c23</span><br><span class="line">&lt;         image: gcr.io/google_containers/heapster-amd64:v1.5.3</span><br><span class="line">---</span><br><span class="line">&gt;         image: fishchen/heapster-amd64:v1.5.3</span><br><span class="line">27c27</span><br><span class="line">&lt;         - --<span class="built_in">source</span>=kubernetes:https://kubernetes.default</span><br><span class="line">---</span><br><span class="line">&gt;         - --<span class="built_in">source</span>=kubernetes:https://kubernetes.default?kubeletHttps=<span class="literal">true</span>&amp;kubeletPort=10250</span><br></pre></td></tr></table></figure>
<ul>
<li>由于 kubelet 只在 10250 监听 https 请求，故添加相关参数；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cp influxdb.yaml&#123;,.orig&#125;</span><br><span class="line">$ diff influxdb.yaml.orig influxdb.yaml</span><br><span class="line">16c16</span><br><span class="line">&lt;         image: gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3</span><br><span class="line">---</span><br><span class="line">&gt;         image: fishchen/heapster-influxdb-amd64:v1.3.3</span><br></pre></td></tr></table></figure>
<h2 id="执行所有定义文件">执行所有定义文件</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">pwd</span></span><br><span class="line">/opt/k8s/heapster-1.5.2/deploy/kube-config/influxdb</span><br><span class="line">$ ls *.yaml</span><br><span class="line">grafana.yaml  heapster.yaml  influxdb.yaml</span><br><span class="line">$ kubectl create -f  .</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ../rbac/</span><br><span class="line">$ <span class="built_in">pwd</span></span><br><span class="line">/opt/k8s/heapster-1.5.2/deploy/kube-config/rbac</span><br><span class="line">$ ls</span><br><span class="line">heapster-rbac.yaml</span><br><span class="line">$ cp heapster-rbac.yaml&#123;,.orig&#125;</span><br><span class="line">$ diff heapster-rbac.yaml.orig heapster-rbac.yaml</span><br><span class="line">12a13,26</span><br><span class="line">&gt; ---</span><br><span class="line">&gt; kind: ClusterRoleBinding</span><br><span class="line">&gt; apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">&gt; metadata:</span><br><span class="line">&gt;   name: heapster-kubelet-api</span><br><span class="line">&gt; roleRef:</span><br><span class="line">&gt;   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">&gt;   kind: ClusterRole</span><br><span class="line">&gt;   name: system:kubelet-api-admin</span><br><span class="line">&gt; subjects:</span><br><span class="line">&gt; - kind: ServiceAccount</span><br><span class="line">&gt;   name: heapster</span><br><span class="line">&gt;   namespace: kube-system</span><br><span class="line">&gt;</span><br><span class="line"></span><br><span class="line">$ kubectl create -f heapster-rbac.yaml</span><br></pre></td></tr></table></figure>
<ul>
<li>将 serviceAccount kube-system:heapster 与 ClusterRole system:kubelet-api-admin 绑定，授予它调用 kubelet API 的权限；</li>
</ul>
<h2 id="检查执行结果">检查执行结果</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system | grep -E <span class="string">'heapster|monitoring'</span></span><br><span class="line">heapster-ddb6c4994-vnnrn                1/1       Running   0          1m</span><br><span class="line">monitoring-grafana-779bd4dd7b-xqkgk     1/1       Running   0          1m</span><br><span class="line">monitoring-influxdb-f75847d48-2lnz6     1/1       Running   0          1m</span><br></pre></td></tr></table></figure>
<p>检查 kubernets dashboard 界面，可以正确显示各 Nodes、Pods 的 CPU、内存、负载等统计数据和图表：</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufGEuACG4XAAZAW50ZRmU600.png" alt="screenshot"></p>
<h2 id="访问-grafana">访问 grafana</h2>
<ol>
<li>
<p>通过 kube-apiserver 访问：</p>
<p>获取 monitoring-grafana 服务 URL：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl cluster-info</span><br><span class="line">Kubernetes master is running at https://172.27.129.105:6443</span><br><span class="line">CoreDNS is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy</span><br><span class="line">Heapster is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/heapster/proxy</span><br><span class="line">kubernetes-dashboard is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</span><br><span class="line">monitoring-grafana is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</span><br><span class="line">monitoring-influxdb is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/monitoring-influxdb/proxy</span><br><span class="line">   </span><br><span class="line">To further debug and diagnose cluster problems, use <span class="string">'kubectl cluster-info dump'</span>.</span><br></pre></td></tr></table></figure>
<p>浏览器访问 URL： <code>https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</code><br>
对于 virtuabox 做了端口映射： <code>https://127.0.0.1:8080/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</code></p>
</li>
<li>
<p>通过 kubectl proxy 访问：</p>
<p>创建代理</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl proxy --address=<span class="string">'172.27.129.105'</span> --port=8086 --accept-hosts=<span class="string">'^*$'</span></span><br><span class="line">Starting to serve on 172.27.129.80:8086</span><br></pre></td></tr></table></figure>
<p>浏览器访问 URL：<code>https://172.27.129.105:8086/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy/?orgId=1</code><br>
对于 virtuabox 做了端口映射： <code>https://127.0.0.1:8086/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy/?orgId=1</code></p>
</li>
<li>
<p>通过 NodePort 访问：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get svc -n kube-system|grep -E <span class="string">'monitoring|heapster'</span></span><br><span class="line">heapster               ClusterIP   10.254.58.136    &lt;none&gt;        80/TCP          47m</span><br><span class="line">monitoring-grafana     NodePort    10.254.28.196    &lt;none&gt;        80:8452/TCP     47m</span><br><span class="line">monitoring-influxdb    ClusterIP   10.254.138.164   &lt;none&gt;        8086/TCP        47m</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>grafana 监听 NodePort 8452；</p>
<p>浏览器访问 URL：<code>https://172.27.129.105:8452/?orgId=1</code></p>
</li>
</ul>
</li>
</ol>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufGDOANkBEAAVj52nZSPQ139.png" alt="screenshot"><br>
参考：</p>
<ol>
<li>配置 heapster：<a href="https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md" target="_blank" rel="noopener">https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>09-2.部署 dashboard 插件</title>
    <url>/2020/01/07/09-2.%E9%83%A8%E7%BD%B2%20dashboard%20%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<h1>09-2.部署 dashboard 插件</h1>
<h2 id="修改配置文件">修改配置文件</h2>
<p>将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。</p>
<p>dashboard 对应的目录是：<code>cluster/addons/dashboard</code>。</p>
<pre><code>$ pwd
/opt/k8s/kubernetes/cluster/addons/dashboard

$ cp dashboard-controller.yaml{,.orig}

$ diff dashboard-controller.yaml{,.orig}
33c33
&lt;         image: siriuszg/kubernetes-dashboard-amd64:v1.8.3
---
&gt;         image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3

$ cp dashboard-service.yaml{,.orig}

$ diff dashboard-service.yaml.orig dashboard-service.yaml
10a11
&gt;   type: NodePort
</code></pre>
<ul>
<li>指定端口类型为 NodePort，这样外界可以通过地址 nodeIP:nodePort 访问 dashboard；</li>
</ul>
<h2 id="执行所有定义文件">执行所有定义文件</h2>
<pre><code>$ ls *.yaml
dashboard-configmap.yaml  dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-secret.yaml  dashboard-service.yaml

$ kubectl create -f  .
</code></pre>
<h2 id="查看分配的-NodePort">查看分配的 NodePort</h2>
<pre><code>$ kubectl get deployment kubernetes-dashboard  -n kube-system
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-dashboard   1         1         1            1           2m

$ kubectl --namespace kube-system get pods -o wide
NAME                                    READY     STATUS    RESTARTS   AGE       IP            NODE
coredns-77c989547b-6l6jr                1/1       Running   0          58m       172.30.39.3   kube-node3
coredns-77c989547b-d9lts                1/1       Running   0          58m       172.30.81.3   kube-node1
kubernetes-dashboard-65f7b4f486-wgc6j   1/1       Running   0          2m        172.30.81.5   kube-node1

$ kubectl get services kubernetes-dashboard -n kube-system
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   NodePort   10.254.96.204   &lt;none&gt;        443:8607/TCP   2m
</code></pre>
<ul>
<li>NodePort 8607 映射到 dashboard pod 443 端口；</li>
</ul>
<p>dashboard 的 --authentication-mode 支持 token、basic，默认为 token。如果使用 basic，则 kube-apiserver 必须配置 ‘–authorization-mode=ABAC’ 和 ‘–basic-auth-file’ 参数。</p>
<h2 id="查看-dashboard-支持的命令行参数">查看 dashboard 支持的命令行参数</h2>
<pre><code>$ kubectl exec --namespace kube-system -it kubernetes-dashboard-65f7b4f486-wgc6j  -- /dashboard --help
2018/06/13 15:17:44 Starting overwatch
Usage of /dashboard:
      --alsologtostderr                   log to standard error as well as files
      --apiserver-host string             The address of the Kubernetes Apiserver to connect to in the format of protocol://address:port, e.g., https://localhost:8080. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and local discovery is attempted.
      --authentication-mode stringSlice   Enables authentication options that will be reflected on login screen. Supported values: token, basic. Default: token.Note that basic option should only be used if apiserver has '--authorization-mode=ABAC' and '--basic-auth-file' flags set. (default [token])
      --auto-generate-certificates        When set to true, Dashboard will automatically generate certificates used to serve HTTPS. Default: false.
      --bind-address ip                   The IP address on which to serve the --secure-port (set to 0.0.0.0 for all interfaces). (default 0.0.0.0)
      --default-cert-dir string           Directory path containing '--tls-cert-file' and '--tls-key-file' files. Used also when auto-generating certificates flag is set. (default &quot;/certs&quot;)
      --disable-settings-authorizer       When enabled, Dashboard settings page will not require user to be logged in and authorized to access settings page.
      --enable-insecure-login             When enabled, Dashboard login view will also be shown when Dashboard is not served over HTTPS. Default: false.
      --heapster-host string              The address of the Heapster Apiserver to connect to in the format of protocol://address:port, e.g., https://localhost:8082. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and service proxy will be used.
      --insecure-bind-address ip          The IP address on which to serve the --port (set to 0.0.0.0 for all interfaces). (default 127.0.0.1)
      --insecure-port int                 The port to listen to for incoming HTTP requests. (default 9090)
      --kubeconfig string                 Path to kubeconfig file with authorization and master location information.
      --log_backtrace_at traceLocation    when logging hits line file:N, emit a stack trace (default :0)
      --log_dir string                    If non-empty, write log files in this directory
      --logtostderr                       log to standard error instead of files
      --metric-client-check-period int    Time in seconds that defines how often configured metric client health check should be run. Default: 30 seconds. (default 30)
      --port int                          The secure port to listen to for incoming HTTPS requests. (default 8443)
      --stderrthreshold severity          logs at or above this threshold go to stderr (default 2)
      --system-banner string              When non-empty displays message to Dashboard users. Accepts simple HTML tags. Default: ''.
      --system-banner-severity string     Severity of system banner. Should be one of 'INFO|WARNING|ERROR'. Default: 'INFO'. (default &quot;INFO&quot;)
      --tls-cert-file string              File containing the default x509 Certificate for HTTPS.
      --tls-key-file string               File containing the default x509 private key matching --tls-cert-file.
      --token-ttl int                     Expiration time (in seconds) of JWE tokens generated by dashboard. Default: 15 min. 0 - never expires (default 900)
  -v, --v Level                           log level for V logs
      --vmodule moduleSpec                comma-separated list of pattern=N settings for file-filtered logging
command terminated with exit code 2
$
</code></pre>
<h2 id="访问-dashboard">访问 dashboard</h2>
<p>为了集群安全，从 1.7 开始，dashboard 只允许通过 https 访问，如果使用 kube proxy 则必须监听 localhost 或 127.0.0.1，对于 NodePort 没有这个限制，但是仅建议在开发环境中使用。</p>
<p>对于不满足这些条件的登录访问，在登录成功后<strong>浏览器不跳转，始终停在登录界面</strong>。</p>
<p>参考：<br>
<a href="https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard---1.7.X-and-above" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard—1.7.X-and-above</a><br>
<a href="https://github.com/kubernetes/dashboard/issues/2540" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard/issues/2540</a></p>
<ol>
<li>kubernetes-dashboard 服务暴露了 NodePort，可以使用 <code>https://NodeIP:NodePort</code> 地址访问 dashboard；</li>
<li>通过 kube-apiserver 访问 dashboard；</li>
<li>通过 kubectl proxy 访问 dashboard：</li>
</ol>
<p>如果使用了 VirtualBox，需要启用 VirtualBox 的 ForworadPort 功能将虚机监听的端口和 Host 的本地端口绑定。</p>
<p>可以在 Vagrant 的配置中指定这些端口转发规则，对于正在运行的虚机，也可以通过 VirtualBox 的界面进行配置：</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufE_6ATXaNAAYSE23OlTM418.png" alt="screenshot"></p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufFBGANtysAAWK0mJBQZQ201.png" alt="screenshot"></p>
<h3 id="通过-kubectl-proxy-访问-dashboard">通过 kubectl proxy 访问 dashboard</h3>
<p>启动代理：</p>
<pre><code>$ kubectl proxy --address='localhost' --port=8086 --accept-hosts='^*$'
Starting to serve on 127.0.0.1:8086
</code></pre>
<ul>
<li>–address 必须为 localhost 或 127.0.0.1；</li>
<li>需要指定 <code>--accept-hosts</code> 选项，否则浏览器访问 dashboard 页面时提示 “Unauthorized”；</li>
</ul>
<p>浏览器访问 URL：<code>https://127.0.0.1:8086/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</code></p>
<h3 id="通过-kube-apiserver-访问-dashboard">通过 kube-apiserver 访问 dashboard</h3>
<p>获取集群服务地址列表：</p>
<pre><code>$ kubectl cluster-info
Kubernetes master is running at https://172.27.129.105:6443
CoreDNS is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy
kubernetes-dashboard is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</code></pre>
<p>必须通过 kube-apiserver 的安全端口(https)访问 dashbaord，访问时浏览器需要使用<strong>自定义证书</strong>，否则会被 kube-apiserver 拒绝访问。</p>
<p>创建和导入自定义证书的步骤，参考：<a href="/1204">A.浏览器访问kube-apiserver安全端口</a></p>
<p>浏览器访问 URL：<code>https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</code><br>
对于 virtuabox 做了端口映射： <code>https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</code></p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufFC2ASDpDAAFzzzBHFKY593.png" alt="screenshot"></p>
<h2 id="创建登录-Dashboard-的-token-和-kubeconfig-配置文件">创建登录 Dashboard 的 token 和 kubeconfig 配置文件</h2>
<p>上面提到，Dashboard 默认只支持 token 认证，所以如果使用 KubeConfig 文件，需要在该文件中指定 token，不支持使用 client 证书认证。</p>
<h3 id="创建登录-token">创建登录 token</h3>
<pre><code>kubectl create sa dashboard-admin -n kube-system
kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin
ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk '{print $1}')
DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system ${ADMIN_SECRET} | grep -E '^token' | awk '{print $2}')
echo ${DASHBOARD_LOGIN_TOKEN}
</code></pre>
<p>使用输出的 token 登录 Dashboard。</p>
<h3 id="创建使用-token-的-KubeConfig-文件">创建使用 token 的 KubeConfig 文件</h3>
<pre><code>source /opt/k8s/bin/environment.sh
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=dashboard.kubeconfig

# 设置客户端认证参数，使用上面创建的 Token
kubectl config set-credentials dashboard_user \
  --token=${DASHBOARD_LOGIN_TOKEN} \
  --kubeconfig=dashboard.kubeconfig

# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=dashboard_user \
  --kubeconfig=dashboard.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=dashboard.kubeconfig
</code></pre>
<p>用生成的 dashboard.kubeconfig 登录 Dashboard。</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufFKCAOkuDAAKWwIAdbSI128.png" alt="screenshot"></p>
<p>由于缺少 Heapster 插件，当前 dashboard 不能展示 Pod、Nodes 的 CPU、内存等统计数据和图表；</p>
<h2 id="参考">参考</h2>
<p><a href="https://github.com/kubernetes/dashboard/wiki/Access-control" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard/wiki/Access-control</a><br>
<a href="https://github.com/kubernetes/dashboard/issues/2558" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard/issues/2558</a><br>
<a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/</a></p>
]]></content>
  </entry>
  <entry>
    <title>09-1.部署 coredns 插件</title>
    <url>/2020/01/07/09-1.%E9%83%A8%E7%BD%B2%20coredns%20%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<h1>09-1.部署 coredns 插件</h1>
<h2 id="修改配置文件">修改配置文件</h2>
<p>将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。</p>
<p>coredns 对应的目录是：<code>cluster/addons/dns</code>。</p>
<pre><code>$ pwd
/opt/k8s/kubernetes/cluster/addons/dns

$ cp coredns.yaml.base coredns.yaml
$ diff coredns.yaml.base coredns.yaml
61c61
&lt;         kubernetes __PILLAR__DNS__DOMAIN__ in-addr.arpa ip6.arpa {
---
&gt;         kubernetes cluster.local. in-addr.arpa ip6.arpa {
153c153
&lt;   clusterIP: __PILLAR__DNS__SERVER__
---
&gt;   clusterIP: 10.254.0.2
</code></pre>
<h2 id="创建-coredns">创建 coredns</h2>
<pre><code>$ kubectl create -f coredns.yaml
</code></pre>
<h2 id="检查-coredns-功能">检查 coredns 功能</h2>
<pre><code>$ kubectl get all -n kube-system
NAME                           READY     STATUS    RESTARTS   AGE
pod/coredns-77c989547b-6l6jr   1/1       Running   0          3m
pod/coredns-77c989547b-d9lts   1/1       Running   0          3m

NAME              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
service/coredns   ClusterIP   10.254.0.2   &lt;none&gt;        53/UDP,53/TCP   3m

NAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2         2         2            2           3m

NAME                                 DESIRED   CURRENT   READY     AGE
replicaset.apps/coredns-77c989547b   2         2         2         3m
</code></pre>
<p>新建一个 Deployment</p>
<pre><code>$ cat &gt; my-nginx.yaml &lt;&lt;EOF
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
EOF
$ kubectl create -f my-nginx.yaml
</code></pre>
<p>Export 该 Deployment, 生成 <code>my-nginx</code> 服务：</p>
<pre><code>$ kubectl expose deploy my-nginx
service &quot;my-nginx&quot; exposed

$ kubectl get services --all-namespaces |grep my-nginx
default       my-nginx     ClusterIP   10.254.242.255   &lt;none&gt;        80/TCP          9s
</code></pre>
<p>创建另一个 Pod，查看 <code>/etc/resolv.conf</code> 是否包含 <code>kubelet</code> 配置的 <code>--cluster-dns</code> 和 <code>--cluster-domain</code>，是否能够将服务 <code>my-nginx</code> 解析到上面显示的 Cluster IP <code>10.254.242.255</code></p>
<pre><code>$ cat &gt; pod-nginx.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.7.9
    ports:
    - containerPort: 80
EOF

$ kubectl create -f pod-nginx.yaml
$ kubectl exec  nginx -i -t -- /bin/bash
root@nginx:/# cat /etc/resolv.conf
nameserver 10.254.0.2
search default.svc.cluster.local. svc.cluster.local. cluster.local. 4pd.io
options ndots:5

root@nginx:/#  ping my-nginx
PING my-nginx.default.svc.cluster.local (10.254.242.255): 48 data bytes
56 bytes from 10.254.242.255: icmp_seq=0 ttl=64 time=0.115 ms
^C--- my-nginx.default.svc.cluster.local ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.115/0.115/0.115/0.000 ms
</code></pre>
<p>​<br>
root@nginx:/# ping my-nginx<br>
PING my-nginx.default.svc.cluster.local (10.254.63.136): 48 data bytes<br>
^C— my-nginx.default.svc.cluster.local ping statistics —<br>
4 packets transmitted, 0 packets received, 100% packet loss</p>
<pre><code>root@nginx:/# ping kubernetes
PING kubernetes.default.svc.cluster.local (10.254.0.1): 48 data bytes
56 bytes from 10.254.0.1: icmp_seq=0 ttl=64 time=0.097 ms
56 bytes from 10.254.0.1: icmp_seq=1 ttl=64 time=0.123 ms
^C--- kubernetes.default.svc.cluster.local ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.097/0.110/0.123/0.000 ms

root@nginx:/# ping coredns.kube-system.svc.cluster.local
PING coredns.kube-system.svc.cluster.local (10.254.0.2): 48 data bytes
56 bytes from 10.254.0.2: icmp_seq=0 ttl=64 time=0.129 ms
^C--- coredns.kube-system.svc.cluster.local ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.129/0.129/0.129/0.000 ms
</code></pre>
<h2 id="参考">参考</h2>
<p><a href="https://community.infoblox.com/t5/Community-Blog/CoreDNS-for-Kubernetes-Service-Discovery/ba-p/8187" target="_blank" rel="noopener">https://community.infoblox.com/t5/Community-Blog/CoreDNS-for-Kubernetes-Service-Discovery/ba-p/8187</a><br>
<a href="https://coredns.io/2017/03/01/coredns-for-kubernetes-service-discovery-take-2/" target="_blank" rel="noopener">https://coredns.io/2017/03/01/coredns-for-kubernetes-service-discovery-take-2/</a><br>
<a href="https://www.cnblogs.com/boshen-hzb/p/7511432.html" target="_blank" rel="noopener">https://www.cnblogs.com/boshen-hzb/p/7511432.html</a><br>
<a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns</a></p>
]]></content>
  </entry>
  <entry>
    <title>08.验证集群功能</title>
    <url>/2020/01/07/08.%E9%AA%8C%E8%AF%81%E9%9B%86%E7%BE%A4%E5%8A%9F%E8%83%BD/</url>
    <content><![CDATA[<p>tags: verify</p>
<h1>08.验证集群功能</h1>
<p>本文档使用 daemonset 验证 master 和 worker 节点是否工作正常。</p>
<h2 id="检查节点状态">检查节点状态</h2>
<pre><code>$ kubectl get nodes
NAME         STATUS    ROLES     AGE       VERSION
kube-node1   Ready     &lt;none&gt;    3h        v1.10.4
kube-node2   Ready     &lt;none&gt;    3h        v1.10.4
kube-node3   Ready     &lt;none&gt;    3h        v1.10.4
</code></pre>
<p>都为 Ready 时正常。</p>
<h2 id="创建测试文件">创建测试文件</h2>
<pre><code>$ cat &gt; nginx-ds.yml &lt;&lt;EOF
apiVersion: v1
kind: Service
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
spec:
  type: NodePort
  selector:
    app: nginx-ds
  ports:
  - name: http
    port: 80
    targetPort: 80
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
EOF
</code></pre>
<h2 id="执行定义文件">执行定义文件</h2>
<pre><code>$ kubectl create -f nginx-ds.yml
service &quot;nginx-ds&quot; created
daemonset.extensions &quot;nginx-ds&quot; created
</code></pre>
<h2 id="检查各-Node-上的-Pod-IP-连通性">检查各 Node 上的 Pod IP 连通性</h2>
<pre><code>$ kubectl get pods  -o wide|grep nginx-ds
nginx-ds-dbn97   1/1       Running   0          2m        172.30.29.2   kube-node2
nginx-ds-rk777   1/1       Running   0          2m        172.30.81.2   kube-node1
nginx-ds-tr9g5   1/1       Running   0          2m        172.30.39.2   kube-node3
</code></pre>
<p>可见，nginx-ds 的 Pod IP 分别是 <code>172.30.39.2</code>、<code>172.30.81.2</code>、<code>172.30.29.2</code>，在所有 Node 上分别 ping 这三个 IP，看是否连通：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.39.2&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.81.2&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.29.2&quot;
  done
</code></pre>
<h2 id="检查服务-IP-和端口可达性">检查服务 IP 和端口可达性</h2>
<pre><code>$ kubectl get svc |grep nginx-ds
nginx-ds     NodePort    10.254.254.228   &lt;none&gt;        80:8900/TCP   4m
</code></pre>
<p>可见：</p>
<ul>
<li>Service Cluster IP：10.254.254.228</li>
<li>服务端口：80</li>
<li>NodePort 端口：8900</li>
</ul>
<p>在所有 Node 上 curl Service IP：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;curl 10.254.254.228&quot;
  done
</code></pre>
<p>预期输出 nginx 欢迎页面内容。</p>
<h2 id="检查服务的-NodePort-可达性">检查服务的 NodePort 可达性</h2>
<p>在所有 Node 上执行：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;curl ${node_ip}:8900&quot;
  done
</code></pre>
<p>预期输出 nginx 欢迎页面内容。</p>
]]></content>
  </entry>
  <entry>
    <title>07-3.部署 kube-proxy 组件</title>
    <url>/2020/01/07/07-3.%E9%83%A8%E7%BD%B2%20kube-proxy%20%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<h1>07-3.部署 kube-proxy 组件</h1>
<p>kube-proxy 运行在所有 worker 节点上，，它监听 apiserver 中 service 和 Endpoint 的变化情况，创建路由规则来进行服务负载均衡。</p>
<p>本文档讲解部署 kube-proxy 的部署，使用 ipvs 模式。</p>
<h2 id="下载和分发-kube-proxy-二进制文件">下载和分发 kube-proxy 二进制文件</h2>
<p>参考 <a href="https://www.orchome.com/657" target="_blank" rel="noopener">06-0.部署master节点.md</a></p>
<h2 id="安装依赖包">安装依赖包</h2>
<p>各节点需要安装 <code>ipvsadm</code> 和 <code>ipset</code> 命令，加载 <code>ip_vs</code> 内核模块。</p>
<p>参考 <a href="https://www.orchome.com/658" target="_blank" rel="noopener">07-0.部署worker节点.md</a></p>
<h2 id="创建-kube-proxy-证书">创建 kube-proxy 证书</h2>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; kube-proxy-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>CN：指定该证书的 User 为 <code>system:kube-proxy</code>；</li>
<li>预定义的 RoleBinding <code>system:node-proxier</code> 将User <code>system:kube-proxy</code> 与 Role <code>system:node-proxier</code> 绑定，该 Role 授予了调用 <code>kube-apiserver</code> Proxy 相关 API 的权限；</li>
<li>该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
</code></pre>
<h2 id="创建和分发-kubeconfig-文件">创建和分发 kubeconfig 文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
</code></pre>
<ul>
<li><code>--embed-certs=true</code>：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl-proxy.kubeconfig 文件中(不加时，写入的是证书文件路径)；</li>
</ul>
<p>分发 kubeconfig 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    scp kube-proxy.kubeconfig k8s@${node_name}:/etc/kubernetes/
  done
</code></pre>
<h2 id="创建-kube-proxy-配置文件">创建 kube-proxy 配置文件</h2>
<p>从 v1.10 开始，kube-proxy <strong>部分参数</strong>可以配置文件中配置。可以使用 <code>--write-config-to</code> 选项生成该配置文件，或者参考 kubeproxyconfig 的类型定义源文件 ：<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go</a></p>
<p>创建 kube-proxy config 文件模板：</p>
<pre><code>cat &gt;kube-proxy.config.yaml.template &lt;&lt;EOF
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: ##NODE_IP##
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: ${CLUSTER_CIDR}
healthzBindAddress: ##NODE_IP##:10256
hostnameOverride: ##NODE_NAME##
kind: KubeProxyConfiguration
metricsBindAddress: ##NODE_IP##:10249
mode: &quot;ipvs&quot;
EOF
</code></pre>
<ul>
<li><code>bindAddress</code>: 监听地址；</li>
<li><code>clientConnection.kubeconfig</code>: 连接 apiserver 的 kubeconfig 文件；</li>
<li><code>clusterCIDR</code>: kube-proxy 根据 <code>--cluster-cidr</code> 判断集群内部和外部流量，指定 <code>--cluster-cidr</code> 或 <code>--masquerade-all</code> 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</li>
<li><code>hostnameOverride</code>: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；</li>
<li><code>mode</code>: 使用 ipvs 模式；</li>
</ul>
<p>为各节点创建和分发 kube-proxy 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; 3; i++ ))
  do 
    echo &quot;&gt;&gt;&gt; ${NODE_NAMES[i]}&quot;
    sed -e &quot;s/##NODE_NAME##/${NODE_NAMES[i]}/&quot; -e &quot;s/##NODE_IP##/${NODE_IPS[i]}/&quot; kube-proxy.config.yaml.template &gt; kube-proxy-${NODE_NAMES[i]}.config.yaml
    scp kube-proxy-${NODE_NAMES[i]}.config.yaml root@${NODE_NAMES[i]}:/etc/kubernetes/kube-proxy.config.yaml
  done
</code></pre>
<h2 id="创建和分发-kube-proxy-systemd-unit-文件">创建和分发 kube-proxy systemd unit 文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kube-proxy.service &lt;&lt;EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/opt/k8s/bin/kube-proxy \\
  --config=/etc/kubernetes/kube-proxy.config.yaml \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<p>替换后的 unit 文件：<a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kube-proxy.service" target="_blank" rel="noopener">kube-proxy.service</a></p>
<p>分发 kube-proxy systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do 
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    scp kube-proxy.service root@${node_name}:/etc/systemd/system/
  done
</code></pre>
<h2 id="启动-kube-proxy-服务">启动 kube-proxy 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/lib/kube-proxy&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy&quot;
  done
</code></pre>
<ul>
<li>必须先创建工作和日志目录；</li>
</ul>
<h2 id="检查启动结果">检查启动结果</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status kube-proxy|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u kube-proxy
</code></pre>
<h2 id="查看监听端口和-metrics">查看监听端口和 metrics</h2>
<pre><code>[k8s@kube-node1 ~]$ sudo netstat -lnpt|grep kube-prox
tcp        0      0 172.27.129.105:10249    0.0.0.0:*               LISTEN      16847/kube-proxy
tcp        0      0 172.27.129.105:10256    0.0.0.0:*               LISTEN      16847/kube-proxy
</code></pre>
<ul>
<li>10249：http prometheus metrics port;</li>
<li>10256：http healthz port;</li>
</ul>
<h2 id="查看-ipvs-路由规则">查看 ipvs 路由规则</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;/usr/sbin/ipvsadm -ln&quot;
  done
</code></pre>
<p>预期输出：</p>
<pre><code>&gt;&gt;&gt; 172.27.129.105
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&gt; 172.27.129.105:6443          Masq    1      0          0
&gt;&gt;&gt; 172.27.129.111
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&gt; 172.27.129.105:6443          Masq    1      0          0
&gt;&gt;&gt; 172.27.129.112
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&gt; 172.27.129.105:6443          Masq    1      0          0
</code></pre>
<p>可见将所有到 kubernetes cluster ip 443 端口的请求都转发到 kube-apiserver 的 6443 端口；</p>
]]></content>
  </entry>
  <entry>
    <title>07-2.部署 kubelet 组件</title>
    <url>/2020/01/07/07-2-%E9%83%A8%E7%BD%B2-kubelet-%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<p>tags: worker, kubelet</p>
<h1>07-2.部署 kubelet 组件</h1>
<p>kublet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如 exec、run、logs 等。</p>
<p>kublet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况。</p>
<p>为确保安全，本文档只开启接收 https 请求的安全端口，对请求进行认证和授权，拒绝未授权的访问(如 apiserver、heapster)。</p>
<h2 id="下载和分发-kubelet-二进制文件">下载和分发 kubelet 二进制文件</h2>
<p>参考 <a href="https://www.orchome.com/657" target="_blank" rel="noopener">06-0.部署master节点.md</a></p>
<h2 id="安装依赖包">安装依赖包</h2>
<p>参考 <a href="https://www.orchome.com/658" target="_blank" rel="noopener">07-0.部署worker节点.md</a></p>
<h2 id="创建-kubelet-bootstrap-kubeconfig-文件">创建 kubelet bootstrap kubeconfig 文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;

    # 创建 token
    export BOOTSTRAP_TOKEN=$(kubeadm token create \
      --description kubelet-bootstrap-token \
      --groups system:bootstrappers:${node_name} \
      --kubeconfig ~/.kube/config)

    # 设置集群参数
    kubectl config set-cluster kubernetes \
      --certificate-authority=/etc/kubernetes/cert/ca.pem \
      --embed-certs=true \
      --server=${KUBE_APISERVER} \
      --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig

    # 设置客户端认证参数
    kubectl config set-credentials kubelet-bootstrap \
      --token=${BOOTSTRAP_TOKEN} \
      --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig

    # 设置上下文参数
    kubectl config set-context default \
      --cluster=kubernetes \
      --user=kubelet-bootstrap \
      --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig

    # 设置默认上下文
    kubectl config use-context default --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig
  done
</code></pre>
<ul>
<li>证书中写入 Token 而非证书，证书后续由 controller-manager 创建。</li>
</ul>
<p>查看 kubeadm 为各节点创建的 token：</p>
<pre><code>$ kubeadm token list --kubeconfig ~/.kube/config
TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS
k0s2bj.7nvw1zi1nalyz4gz   23h       2018-06-14T15:14:31+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:kube-node1
mkus5s.vilnjk3kutei600l   23h       2018-06-14T15:14:32+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:kube-node3
zkiem5.0m4xhw0jc8r466nk   23h       2018-06-14T15:14:32+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:kube-node2
</code></pre>
<ul>
<li>创建的 token 有效期为 1 天，超期后将不能再被使用，且会被 kube-controller-manager 的 tokencleaner 清理(如果启用该 controller 的话)；</li>
<li>kube-apiserver 接收 kubelet 的 bootstrap token 后，将请求的 user 设置为 system:bootstrap:，group 设置为 system:bootstrappers；</li>
</ul>
<p>各 token 关联的 Secret：</p>
<pre><code>$ kubectl get secrets  -n kube-system
NAME                     TYPE                                  DATA      AGE
bootstrap-token-k0s2bj   bootstrap.kubernetes.io/token         7         1m
bootstrap-token-mkus5s   bootstrap.kubernetes.io/token         7         1m
bootstrap-token-zkiem5   bootstrap.kubernetes.io/token         7         1m
default-token-99st7      kubernetes.io/service-account-token   3         2d
</code></pre>
<h2 id="分发-bootstrap-kubeconfig-文件到所有-worker-节点">分发 bootstrap kubeconfig 文件到所有 worker 节点</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    scp kubelet-bootstrap-${node_name}.kubeconfig k8s@${node_name}:/etc/kubernetes/kubelet-bootstrap.kubeconfig
  done
</code></pre>
<h2 id="创建和分发-kubelet-参数配置文件">创建和分发 kubelet 参数配置文件</h2>
<p>从 v1.10 开始，kubelet <strong>部分参数</strong>需在配置文件中配置，<code>kubelet --help</code> 会提示：</p>
<pre><code>DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag
</code></pre>
<p>创建 kubelet 参数配置模板文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kubelet.config.json.template &lt;&lt;EOF
{
  &quot;kind&quot;: &quot;KubeletConfiguration&quot;,
  &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;,
  &quot;authentication&quot;: {
    &quot;x509&quot;: {
      &quot;clientCAFile&quot;: &quot;/etc/kubernetes/cert/ca.pem&quot;
    },
    &quot;webhook&quot;: {
      &quot;enabled&quot;: true,
      &quot;cacheTTL&quot;: &quot;2m0s&quot;
    },
    &quot;anonymous&quot;: {
      &quot;enabled&quot;: false
    }
  },
  &quot;authorization&quot;: {
    &quot;mode&quot;: &quot;Webhook&quot;,
    &quot;webhook&quot;: {
      &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;,
      &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot;
    }
  },
  &quot;address&quot;: &quot;##NODE_IP##&quot;,
  &quot;port&quot;: 10250,
  &quot;readOnlyPort&quot;: 0,
  &quot;cgroupDriver&quot;: &quot;cgroupfs&quot;,
  &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;,
  &quot;serializeImagePulls&quot;: false,
  &quot;featureGates&quot;: {
    &quot;RotateKubeletClientCertificate&quot;: true,
    &quot;RotateKubeletServerCertificate&quot;: true
  },
  &quot;clusterDomain&quot;: &quot;${CLUSTER_DNS_DOMAIN}&quot;,
  &quot;clusterDNS&quot;: [&quot;${CLUSTER_DNS_SVC_IP}&quot;]
}
EOF
</code></pre>
<ul>
<li>address：API 监听地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API；</li>
<li>readOnlyPort=0：关闭只读端口(默认 10255)，等效为未指定；</li>
<li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li>
<li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTP 证书认证；</li>
<li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li>
<li>对于未通过 x509 证书和 webhook 认证的请求(kube-apiserver 或其他客户端)，将被拒绝，提示 Unauthorized；</li>
<li>authroization.mode=Webhook：kubelet 使用 SubjectAccessReview API 查询 kube-apiserver 某 user、group 是否具有操作资源的权限(RBAC)；</li>
<li>featureGates.RotateKubeletClientCertificate、featureGates.RotateKubeletServerCertificate：自动 rotate 证书，证书的有效期取决于 kube-controller-manager 的 --experimental-cluster-signing-duration 参数；</li>
<li>需要 root 账户运行；</li>
</ul>
<p>为各节点创建和分发 kubelet 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do 
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    sed -e &quot;s/##NODE_IP##/${node_ip}/&quot; kubelet.config.json.template &gt; kubelet.config-${node_ip}.json
    scp kubelet.config-${node_ip}.json root@${node_ip}:/etc/kubernetes/kubelet.config.json
  done
</code></pre>
<h2 id="创建和分发-kubelet-systemd-unit-文件">创建和分发 kubelet systemd unit 文件</h2>
<p>创建 kubelet systemd unit 文件模板：</p>
<pre><code>cat &gt; kubelet.service.template &lt;&lt;EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/opt/k8s/bin/kubelet \\
  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \\
  --cert-dir=/etc/kubernetes/cert \\
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\
  --config=/etc/kubernetes/kubelet.config.json \\
  --hostname-override=##NODE_NAME## \\
  --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\
  --allow-privileged=true \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li>如果设置了 <code>--hostname-override</code> 选项，则 <code>kube-proxy</code> 也需要设置该选项，否则会出现找不到 Node 的情况；</li>
<li><code>--bootstrap-kubeconfig</code>：指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；</li>
<li>K8S approve kubelet 的 csr 请求后，在 <code>--cert-dir</code> 目录创建证书和私钥文件，然后写入 <code>--kubeconfig</code> 文件；</li>
</ul>
<p>替换后的 unit 文件：<a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kubelet.service" target="_blank" rel="noopener">kubelet.service</a></p>
<p>为各节点创建和分发 kubelet systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do 
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    sed -e &quot;s/##NODE_NAME##/${node_name}/&quot; kubelet.service.template &gt; kubelet-${node_name}.service
    scp kubelet-${node_name}.service root@${node_name}:/etc/systemd/system/kubelet.service
  done
</code></pre>
<h2 id="Bootstrap-Token-Auth-和授予权限">Bootstrap Token Auth 和授予权限</h2>
<p>kublet 启动时查找配置的 --kubeletconfig 文件是否存在，如果不存在则使用 --bootstrap-kubeconfig 向 kube-apiserver 发送证书签名请求 (CSR)。</p>
<p>kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证（事先使用 kubeadm 创建的 token），认证通过后将请求的 user 设置为 system:bootstrap:，group 设置为 system:bootstrappers，这一过程称为 Bootstrap Token Auth。</p>
<p>默认情况下，这个 user 和 group 没有创建 CSR 的权限，kubelet 启动失败，错误日志如下：</p>
<pre><code>$ sudo journalctl -u kubelet -a |grep -A 2 'certificatesigningrequests'
May 06 06:42:36 kube-node1 kubelet[26986]: F0506 06:42:36.314378   26986 server.go:233] failed to run Kubelet: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &quot;system:bootstrap:lemy40&quot; cannot create certificatesigningrequests.certificates.k8s.io at the cluster scope
May 06 06:42:36 kube-node1 systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
May 06 06:42:36 kube-node1 systemd[1]: kubelet.service: Failed with result 'exit-code'.
</code></pre>
<p>解决办法是：创建一个 clusterrolebinding，将 group system:bootstrappers 和 clusterrole system:node-bootstrapper 绑定：</p>
<pre><code>$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
</code></pre>
<h2 id="启动-kubelet-服务">启动 kubelet 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/lib/kubelet&quot;
    ssh root@${node_ip} &quot;/usr/sbin/swapoff -a&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet&quot;
  done
</code></pre>
<ul>
<li>
<p>关闭 swap 分区，否则 kubelet 会启动失败；</p>
</li>
<li>
<p>必须先创建工作和日志目录；</p>
<p>$ journalctl -u kubelet |tail</p>
</li>
</ul>
<p>kubelet 启动后使用 --bootstrap-kubeconfig 向 kube-apiserver 发送 CSR 请求，当这个 CSR 被 approve 后，kube-controller-manager 为 kubelet 创建 TLS 客户端证书、私钥和 --kubeletconfig 文件。</p>
<p>注意：kube-controller-manager 需要配置 <code>--cluster-signing-cert-file</code> 和 <code>--cluster-signing-key-file</code> 参数，才会为 TLS Bootstrap 创建证书和私钥。</p>
<pre><code>$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk   43s       system:bootstrap:zkiem5   Pending
node-csr-oVbPmU-ikVknpynwu0Ckz_MvkAO_F1j0hmbcDa__sGA   27s       system:bootstrap:mkus5s   Pending
node-csr-u0E1-ugxgotO_9FiGXo8DkD6a7-ew8sX2qPE6KPS2IY   13m       system:bootstrap:k0s2bj   Pending

$ kubectl get nodes
No resources found.
</code></pre>
<ul>
<li>三个 work 节点的 csr 均处于 pending 状态；</li>
</ul>
<h2 id="approve-kubelet-CSR-请求">approve kubelet CSR 请求</h2>
<p>可以手动或自动 approve CSR 请求。推荐使用自动的方式，因为从 v1.8 版本开始，可以自动轮转approve csr 后生成的证书。</p>
<h3 id="手动-approve-CSR-请求">手动 approve CSR 请求</h3>
<p>查看 CSR 列表：</p>
<pre><code>$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk   43s       system:bootstrap:zkiem5   Pending
node-csr-oVbPmU-ikVknpynwu0Ckz_MvkAO_F1j0hmbcDa__sGA   27s       system:bootstrap:mkus5s   Pending
node-csr-u0E1-ugxgotO_9FiGXo8DkD6a7-ew8sX2qPE6KPS2IY   13m       system:bootstrap:k0s2bj   Pending
</code></pre>
<p>approve CSR：</p>
<pre><code>$ kubectl certificate approve node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk
certificatesigningrequest.certificates.k8s.io &quot;node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk&quot; approved
</code></pre>
<p>查看 Approve 结果：</p>
<pre><code>$ kubectl describe  csr node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk
Name:               node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk
Labels:             &lt;none&gt;
Annotations:        &lt;none&gt;
CreationTimestamp:  Wed, 13 Jun 2018 16:05:04 +0800
Requesting User:    system:bootstrap:zkiem5
Status:             Approved
Subject:
         Common Name:    system:node:kube-node2
         Serial Number:
         Organization:   system:nodes
Events:  &lt;none&gt;
</code></pre>
<ul>
<li><code>Requesting User</code>：请求 CSR 的用户，kube-apiserver 对它进行认证和授权；</li>
<li><code>Subject</code>：请求签名的证书信息；</li>
<li>证书的 CN 是 system:node:kube-node2， Organization 是 system:nodes，kube-apiserver 的 Node 授权模式会授予该证书的相关权限；</li>
</ul>
<h3 id="自动-approve-CSR-请求">自动 approve CSR 请求</h3>
<p>创建三个 ClusterRoleBinding，分别用于自动 approve client、renew client、renew server 证书：</p>
<pre><code>cat &gt; csr-crb.yaml &lt;&lt;EOF
 # Approve all CSRs for the group &quot;system:bootstrappers&quot;
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: auto-approve-csrs-for-group
 subjects:
 - kind: Group
   name: system:bootstrappers
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
   apiGroup: rbac.authorization.k8s.io
---
 # To let a node of the group &quot;system:nodes&quot; renew its own credentials
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: node-client-cert-renewal
 subjects:
 - kind: Group
   name: system:nodes
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
   apiGroup: rbac.authorization.k8s.io
---
# A ClusterRole which instructs the CSR approver to approve a node requesting a
# serving cert matching its client cert.
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: approve-node-server-renewal-csr
rules:
- apiGroups: [&quot;certificates.k8s.io&quot;]
  resources: [&quot;certificatesigningrequests/selfnodeserver&quot;]
  verbs: [&quot;create&quot;]
---
 # To let a node of the group &quot;system:nodes&quot; renew its own server credentials
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: node-server-cert-renewal
 subjects:
 - kind: Group
   name: system:nodes
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: approve-node-server-renewal-csr
   apiGroup: rbac.authorization.k8s.io
EOF
</code></pre>
<ul>
<li>auto-approve-csrs-for-group：自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrappers；</li>
<li>node-client-cert-renewal：自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes;</li>
<li>node-server-cert-renewal：自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes;</li>
</ul>
<p>生效配置：</p>
<pre><code>$ kubectl apply -f csr-crb.yaml
</code></pre>
<h2 id="查看-kublet-的情况">查看 kublet 的情况</h2>
<p>等待一段时间(1-10 分钟)，三个节点的 CSR 都被自动 approve：</p>
<pre><code>$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
csr-98h25                                              6m        system:node:kube-node2    Approved,Issued
csr-lb5c9                                              7m        system:node:kube-node3    Approved,Issued
csr-m2hn4                                              14m       system:node:kube-node1    Approved,Issued
node-csr-7q7i0q4MF_K2TSEJj16At4CJFLlJkHIqei6nMIAaJCU   28m       system:bootstrap:k0s2bj   Approved,Issued
node-csr-ND77wk2P8k2lHBtgBaObiyYw0uz1Um7g2pRvveMF-c4   35m       system:bootstrap:mkus5s   Approved,Issued
node-csr-Nysmrw55nnM48NKwEJuiuCGmZoxouK4N8jiEHBtLQso   6m        system:bootstrap:zkiem5   Approved,Issued
node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk   1h        system:bootstrap:zkiem5   Approved,Issued
node-csr-oVbPmU-ikVknpynwu0Ckz_MvkAO_F1j0hmbcDa__sGA   1h        system:bootstrap:mkus5s   Approved,Issued
node-csr-u0E1-ugxgotO_9FiGXo8DkD6a7-ew8sX2qPE6KPS2IY   1h        system:bootstrap:k0s2bj   Approved,Issued
</code></pre>
<p>所有节点均 ready：</p>
<pre><code>$ kubectl get nodes
NAME         STATUS    ROLES     AGE       VERSION
kube-node1   Ready     &lt;none&gt;    18m       v1.10.4
kube-node2   Ready     &lt;none&gt;    10m       v1.10.4
kube-node3   Ready     &lt;none&gt;    11m       v1.10.4
</code></pre>
<p>kube-controller-manager 为各 node 生成了 kubeconfig 文件和公私钥：</p>
<pre><code>$ ls -l /etc/kubernetes/kubelet.kubeconfig
-rw------- 1 root root 2293 Jun 13 17:07 /etc/kubernetes/kubelet.kubeconfig

$ ls -l /etc/kubernetes/cert/|grep kubelet
-rw-r--r-- 1 root root 1046 Jun 13 17:07 kubelet-client.crt
-rw------- 1 root root  227 Jun 13 17:07 kubelet-client.key
-rw------- 1 root root 1334 Jun 13 17:07 kubelet-server-2018-06-13-17-07-45.pem
lrwxrwxrwx 1 root root   58 Jun 13 17:07 kubelet-server-current.pem -&gt; /etc/kubernetes/cert/kubelet-server-2018-06-13-17-07-45.pem
</code></pre>
<ul>
<li>kubelet-server 证书会周期轮转；</li>
</ul>
<h2 id="kubelet-提供的-API-接口">kubelet 提供的 API 接口</h2>
<p>kublet 启动后监听多个端口，用于接收 kube-apiserver 或其它组件发送的请求：</p>
<pre><code>$ sudo netstat -lnpt|grep kubelet
tcp        0      0 172.27.129.111:4194     0.0.0.0:*               LISTEN      2490/kubelet
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      2490/kubelet
tcp        0      0 172.27.129.111:10250    0.0.0.0:*               LISTEN      2490/kubelet
</code></pre>
<ul>
<li>4194: cadvisor http 服务；</li>
<li>10248: healthz http 服务；</li>
<li>10250: https API 服务；注意：未开启只读端口 10255；</li>
</ul>
<p>例如执行 <code>kubectl ec -it nginx-ds-5rmws -- sh</code> 命令时，kube-apiserver 会向 kubelet 发送如下请求：</p>
<pre><code>POST /exec/default/nginx-ds-5rmws/my-nginx?command=sh&amp;input=1&amp;output=1&amp;tty=1
</code></pre>
<p>kubelet 接收 10250 端口的 https 请求：</p>
<ul>
<li>/pods、/runningpods</li>
<li>/metrics、/metrics/cadvisor、/metrics/probes</li>
<li>/spec</li>
<li>/stats、/stats/container</li>
<li>/logs</li>
<li>/run/、&quot;/exec/&quot;, “/attach/”, “/portForward/”, “/containerLogs/” 等管理；</li>
</ul>
<p>详情参考：<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3</a></p>
<p>由于关闭了匿名认证，同时开启了 webhook 授权，所有访问 10250 端口 https API 的请求都需要被认证和授权。</p>
<p>预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限：</p>
<pre><code>$ kubectl describe clusterrole system:kubelet-api-admin
Name:         system:kubelet-api-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources      Non-Resource URLs  Resource Names  Verbs
  ---------      -----------------  --------------  -----
  nodes          []                 []              [get list watch proxy]
  nodes/log      []                 []              [*]
  nodes/metrics  []                 []              [*]
  nodes/proxy    []                 []              [*]
  nodes/spec     []                 []              [*]
  nodes/stats    []                 []              [*]
</code></pre>
<h2 id="kublet-api-认证和授权">kublet api 认证和授权</h2>
<p>kublet 配置了如下认证参数：</p>
<ul>
<li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li>
<li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证；</li>
<li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li>
</ul>
<p>同时配置了如下授权参数：</p>
<ul>
<li>authroization.mode=Webhook：开启 RBAC 授权；</li>
</ul>
<p>kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized：</p>
<pre><code>$ curl -s --cacert /etc/kubernetes/cert/ca.pem https://172.27.129.111:10250/metrics
Unauthorized

$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer 123456&quot; https://172.27.129.111:10250/metrics
Unauthorized
</code></pre>
<p>通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)；</p>
<p>证书认证和授权：</p>
<pre><code>$ # 权限不足的证书；
$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /etc/kubernetes/cert/kube-controller-manager.pem --key /etc/kubernetes/cert/kube-controller-manager-key.pem https://172.27.129.111:10250/metrics
Forbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)

$ # 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；
$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert ./admin.pem --key ./admin-key.pem https://172.27.129.111:10250/metrics|head
# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;0&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;21600&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;43200&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;86400&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;172800&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;345600&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;604800&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;2.592e+06&quot;} 0
</code></pre>
<ul>
<li><code>--cacert</code>、<code>--cert</code>、<code>--key</code> 的参数值必须是文件路径，如上面的 <code>./admin.pem</code> 不能省略 <code>./</code>，否则返回 <code>401 Unauthorized</code>；</li>
</ul>
<p>bear token 认证和授权：</p>
<p>创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限：</p>
<pre><code>kubectl create sa kubelet-api-test
kubectl create clusterrolebinding kubelet-api-test --clusterrole=system:kubelet-api-admin --serviceaccount=default:kubelet-api-test
SECRET=$(kubectl get secrets | grep kubelet-api-test | awk '{print $1}')
TOKEN=$(kubectl describe secret ${SECRET} | grep -E '^token' | awk '{print $2}')
echo ${TOKEN}

$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer ${TOKEN}&quot; https://172.27.129.111:10250/metrics|head
# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;0&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;21600&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;43200&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;86400&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;172800&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;345600&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;604800&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;2.592e+06&quot;} 0
</code></pre>
<h3 id="cadvisor-和-metrics">cadvisor 和 metrics</h3>
<p>cadvisor 统计所在节点各容器的资源(CPU、内存、磁盘、网卡)使用情况，分别在自己的 http web 页面(4194 端口)和 10250 以 promehteus metrics 的形式输出。</p>
<p>浏览器访问 <a href="https://172.27.129.105:4194/containers/" target="_blank" rel="noopener">https://172.27.129.105:4194/containers/</a> 可以查看到 cadvisor 的监控页面：</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFuiOhaAUnECAAIXh1n8zAg265.png" alt="screenshot"></p>
<p>浏览器访问 <a href="https://172.27.129.80:10250/metrics" target="_blank" rel="noopener">https://172.27.129.80:10250/metrics</a> 和 <a href="https://172.27.129.80:10250/metrics/cadvisor" target="_blank" rel="noopener">https://172.27.129.80:10250/metrics/cadvisor</a> 分别返回 kublet 和 cadvisor 的 metrics。</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFuiOmuAISyfAAiaZ-dFVNs239.png" alt="screenshot"><br>
注意：</p>
<ul>
<li>kublet.config.json 设置 authentication.anonymous.enabled 为 false，不允许匿名证书访问 10250 的 https 服务；</li>
<li>参考<a href="A.%E6%B5%8F%E8%A7%88%E5%99%A8%E8%AE%BF%E9%97%AEkube-apiserver%E5%AE%89%E5%85%A8%E7%AB%AF%E5%8F%A3.md">A.浏览器访问kube-apiserver安全端口.md</a>，创建和导入相关证书，然后访问上面的 10250 端口；</li>
</ul>
<h2 id="获取-kublet-的配置">获取 kublet 的配置</h2>
<p>从 kube-apiserver 获取各 node 的配置：</p>
<pre><code>$ source /opt/k8s/bin/environment.sh
$ # 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；
$ curl -sSL --cacert /etc/kubernetes/cert/ca.pem --cert ./admin.pem --key ./admin-key.pem ${KUBE_APISERVER}/api/v1/nodes/kube-node1/proxy/configz | jq \
  '.kubeletconfig|.kind=&quot;KubeletConfiguration&quot;|.apiVersion=&quot;kubelet.config.k8s.io/v1beta1&quot;'
{
  &quot;syncFrequency&quot;: &quot;1m0s&quot;,
  &quot;fileCheckFrequency&quot;: &quot;20s&quot;,
  &quot;httpCheckFrequency&quot;: &quot;20s&quot;,
  &quot;address&quot;: &quot;172.27.129.80&quot;,
  &quot;port&quot;: 10250,
  &quot;readOnlyPort&quot;: 10255,
  &quot;authentication&quot;: {
    &quot;x509&quot;: {},
    &quot;webhook&quot;: {
      &quot;enabled&quot;: false,
      &quot;cacheTTL&quot;: &quot;2m0s&quot;
    },
    &quot;anonymous&quot;: {
      &quot;enabled&quot;: true
    }
  },
  &quot;authorization&quot;: {
    &quot;mode&quot;: &quot;AlwaysAllow&quot;,
    &quot;webhook&quot;: {
      &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;,
      &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot;
    }
  },
  &quot;registryPullQPS&quot;: 5,
  &quot;registryBurst&quot;: 10,
  &quot;eventRecordQPS&quot;: 5,
  &quot;eventBurst&quot;: 10,
  &quot;enableDebuggingHandlers&quot;: true,
  &quot;healthzPort&quot;: 10248,
  &quot;healthzBindAddress&quot;: &quot;127.0.0.1&quot;,
  &quot;oomScoreAdj&quot;: -999,
  &quot;clusterDomain&quot;: &quot;cluster.local.&quot;,
  &quot;clusterDNS&quot;: [
    &quot;10.254.0.2&quot;
  ],
  &quot;streamingConnectionIdleTimeout&quot;: &quot;4h0m0s&quot;,
  &quot;nodeStatusUpdateFrequency&quot;: &quot;10s&quot;,
  &quot;imageMinimumGCAge&quot;: &quot;2m0s&quot;,
  &quot;imageGCHighThresholdPercent&quot;: 85,
  &quot;imageGCLowThresholdPercent&quot;: 80,
  &quot;volumeStatsAggPeriod&quot;: &quot;1m0s&quot;,
  &quot;cgroupsPerQOS&quot;: true,
  &quot;cgroupDriver&quot;: &quot;cgroupfs&quot;,
  &quot;cpuManagerPolicy&quot;: &quot;none&quot;,
  &quot;cpuManagerReconcilePeriod&quot;: &quot;10s&quot;,
  &quot;runtimeRequestTimeout&quot;: &quot;2m0s&quot;,
  &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;,
  &quot;maxPods&quot;: 110,
  &quot;podPidsLimit&quot;: -1,
  &quot;resolvConf&quot;: &quot;/etc/resolv.conf&quot;,
  &quot;cpuCFSQuota&quot;: true,
  &quot;maxOpenFiles&quot;: 1000000,
  &quot;contentType&quot;: &quot;application/vnd.kubernetes.protobuf&quot;,
  &quot;kubeAPIQPS&quot;: 5,
  &quot;kubeAPIBurst&quot;: 10,
  &quot;serializeImagePulls&quot;: false,
  &quot;evictionHard&quot;: {
    &quot;imagefs.available&quot;: &quot;15%&quot;,
    &quot;memory.available&quot;: &quot;100Mi&quot;,
    &quot;nodefs.available&quot;: &quot;10%&quot;,
    &quot;nodefs.inodesFree&quot;: &quot;5%&quot;
  },
  &quot;evictionPressureTransitionPeriod&quot;: &quot;5m0s&quot;,
  &quot;enableControllerAttachDetach&quot;: true,
  &quot;makeIPTablesUtilChains&quot;: true,
  &quot;iptablesMasqueradeBit&quot;: 14,
  &quot;iptablesDropBit&quot;: 15,
  &quot;featureGates&quot;: {
    &quot;RotateKubeletClientCertificate&quot;: true,
    &quot;RotateKubeletServerCertificate&quot;: true
  },
  &quot;failSwapOn&quot;: true,
  &quot;containerLogMaxSize&quot;: &quot;10Mi&quot;,
  &quot;containerLogMaxFiles&quot;: 5,
  &quot;enforceNodeAllocatable&quot;: [
    &quot;pods&quot;
  ],
  &quot;kind&quot;: &quot;KubeletConfiguration&quot;,
  &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;
}
</code></pre>
<p>或者参考代码中的注释：<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go</a></p>
<h2 id="参考">参考</h2>
<ol>
<li>kubelet 认证和授权：<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/" target="_blank" rel="noopener">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>07-1.部署 docker 组件</title>
    <url>/2020/01/07/07-1.%E9%83%A8%E7%BD%B2%20docker%20%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<h1>07-1.部署 docker 组件</h1>
<p>docker 是容器的运行环境，管理它的生命周期。kubelet 通过 Container Runtime Interface (CRI) 与 docker 进行交互。</p>
<h2 id="安装依赖包">安装依赖包</h2>
<p>参考 <a href="https://www.orchome.com/658" target="_blank" rel="noopener">07-0.部署worker节点.md</a></p>
<h2 id="下载和分发-docker-二进制文件">下载和分发 docker 二进制文件</h2>
<p>到 <a href="https://download.docker.com/linux/static/stable/x86_64/" target="_blank" rel="noopener">https://download.docker.com/linux/static/stable/x86_64/</a> 页面下载最新发布包：</p>
<pre><code>wget https://download.docker.com/linux/static/stable/x86_64/docker-18.03.1-ce.tgz
tar -xvf docker-18.03.1-ce.tgz
</code></pre>
<p>分发二进制文件到所有 worker 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp docker/docker*  k8s@${node_ip}:/opt/k8s/bin/
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h2 id="创建和分发-systemd-unit-文件">创建和分发 systemd unit 文件</h2>
<pre><code>cat &gt; docker.service &lt;&lt;&quot;EOF&quot;
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.io

[Service]
Environment=&quot;PATH=/opt/k8s/bin:/bin:/sbin:/usr/bin:/usr/sbin&quot;
EnvironmentFile=-/run/flannel/docker
ExecStart=/opt/k8s/bin/dockerd --log-level=error $DOCKER_NETWORK_OPTIONS
ExecReload=/bin/kill -s HUP $MAINPID
Restart=on-failure
RestartSec=5
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li>
<p>EOF 前后有双引号，这样 bash 不会替换文档中的变量，如 $DOCKER_NETWORK_OPTIONS；</p>
</li>
<li>
<p>dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中；</p>
</li>
<li>
<p>flanneld 启动时将网络配置写入 <code>/run/flannel/docker</code> 文件中，dockerd 启动前读取该文件中的环境变量 <code>DOCKER_NETWORK_OPTIONS</code> ，然后设置 docker0 网桥网段；</p>
</li>
<li>
<p>如果指定了多个 <code>EnvironmentFile</code> 选项，则必须将 <code>/run/flannel/docker</code> 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；</p>
</li>
<li>
<p>docker 需要以 root 用于运行；</p>
</li>
<li>
<p>docker 从 1.13 版本开始，可能将 <strong>iptables FORWARD chain的默认策略设置为DROP</strong>，从而导致 ping 其它 Node 上的 Pod IP 失败，遇到这种情况时，需要手动设置策略为 <code>ACCEPT</code>：</p>
<pre><code>$ sudo iptables -P FORWARD ACCEPT
</code></pre>
<p>并且把以下命令写入 <code>/etc/rc.local</code> 文件中，防止节点重启<strong>iptables FORWARD chain的默认策略又还原为DROP</strong></p>
<pre><code>/sbin/iptables -P FORWARD ACCEPT
</code></pre>
</li>
</ul>
<p>分发 systemd unit 文件到所有 worker 机器:</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp docker.service root@${node_ip}:/etc/systemd/system/
  done
</code></pre>
<h2 id="配置和分发-docker-配置文件">配置和分发 docker 配置文件</h2>
<p>使用国内的仓库镜像服务器以加快 pull image 的速度，同时增加下载的并发数 (需要重启 dockerd 生效)：</p>
<pre><code>cat &gt; docker-daemon.json &lt;&lt;EOF
{
    &quot;registry-mirrors&quot;: [&quot;https://hub-mirror.c.163.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;],
    &quot;max-concurrent-downloads&quot;: 20
}
EOF
</code></pre>
<p>分发 docker 配置文件到所有 work 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p  /etc/docker/&quot;
    scp docker-daemon.json root@${node_ip}:/etc/docker/daemon.json
  done
</code></pre>
<h2 id="启动-docker-服务">启动 docker 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl stop firewalld &amp;&amp; systemctl disable firewalld&quot;
    ssh root@${node_ip} &quot;/usr/sbin/iptables -F &amp;&amp; /usr/sbin/iptables -X &amp;&amp; /usr/sbin/iptables -F -t nat &amp;&amp; /usr/sbin/iptables -X -t nat&quot;
    ssh root@${node_ip} &quot;/usr/sbin/iptables -P FORWARD ACCEPT&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker&quot;
    ssh root@${node_ip} 'for intf in /sys/devices/virtual/net/docker0/brif/*; do echo 1 &gt; $intf/hairpin_mode; done'
    ssh root@${node_ip} &quot;sudo sysctl -p /etc/sysctl.d/kubernetes.conf&quot;
  done
</code></pre>
<ul>
<li>关闭 firewalld(centos7)/ufw(ubuntu16.04)，否则可能会重复创建 iptables 规则；</li>
<li>清理旧的 iptables rules 和 chains 规则；</li>
<li>开启 docker0 网桥下虚拟网卡的 hairpin 模式;</li>
</ul>
<h2 id="检查服务运行状态">检查服务运行状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status docker|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>$ journalctl -u docker
</code></pre>
<h3 id="检查-docker0-网桥">检查 docker0 网桥</h3>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;/usr/sbin/ip addr show flannel.1 &amp;&amp; /usr/sbin/ip addr show docker0&quot;
  done
</code></pre>
<p>确认各 work 节点的 docker0 网桥和 flannel.1 接口的 IP 处于同一个网段中(如下 172.30.39.0 和 172.30.39.1)：</p>
<pre><code>3: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether ce:2f:d6:53:e5:f3 brd ff:ff:ff:ff:ff:ff
    inet 172.30.39.0/32 scope global flannel.1
      valid_lft forever preferred_lft forever
    inet6 fe80::cc2f:d6ff:fe53:e5f3/64 scope link
      valid_lft forever preferred_lft forever
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:bf:65:16:5c brd ff:ff:ff:ff:ff:ff
    inet 172.30.39.1/24 brd 172.30.39.255 scope global docker0
      valid_lft forever preferred_lft forever</code></pre>
]]></content>
  </entry>
  <entry>
    <title>07-0.部署 worker 节点</title>
    <url>/2020/01/07/07-0.%E9%83%A8%E7%BD%B2%20worker%20%E8%8A%82%E7%82%B9/</url>
    <content><![CDATA[<h1>07-0.部署 worker 节点</h1>
<p>kubernetes work 节点运行如下组件：</p>
<ul>
<li>docker</li>
<li>kubelet</li>
<li>kube-proxy</li>
</ul>
<h2 id="安装和配置-flanneld">安装和配置 flanneld</h2>
<p>参考 <a href="/656">05-部署flannel网络.md</a></p>
<h2 id="安装依赖包">安装依赖包</h2>
<p>CentOS:</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;yum install -y epel-release&quot;
    ssh root@${node_ip} &quot;yum install -y conntrack ipvsadm ipset jq iptables curl sysstat libseccomp &amp;&amp; /usr/sbin/modprobe ip_vs &quot;
  done
</code></pre>
<p>Ubuntu:</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;apt-get install -y conntrack ipvsadm ipset jq iptables curl sysstat libseccomp &amp;&amp; /usr/sbin/modprobe ip_vs &quot;
  done</code></pre>
]]></content>
  </entry>
  <entry>
    <title>06-4.部署高可用 kube-scheduler 集群</title>
    <url>/2020/01/07/06-4.%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8%20kube-scheduler%20%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h1>06-4.部署高可用 kube-scheduler 集群</h1>
<p>本文档介绍部署高可用 kube-scheduler 集群的步骤。</p>
<p>该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。</p>
<p>为保证通信安全，本文档先生成 x509 证书和私钥，kube-scheduler 在如下两种情况下使用该证书：</p>
<ol>
<li>与 kube-apiserver 的安全端口通信;</li>
<li>在<strong>安全端口</strong>(https，10251) 输出 prometheus 格式的 metrics；</li>
</ol>
<h2 id="准备工作">准备工作</h2>
<p>下载最新版本的二进制文件、安装和配置 flanneld 参考：<a href="https://www.orchome.com/657" target="_blank" rel="noopener">06-0.部署master节点</a></p>
<h2 id="创建-kube-scheduler-证书和私钥">创建 kube-scheduler 证书和私钥</h2>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; kube-scheduler-csr.json &lt;&lt;EOF
{
    &quot;CN&quot;: &quot;system:kube-scheduler&quot;,
    &quot;hosts&quot;: [
      &quot;127.0.0.1&quot;,
      &quot;172.27.129.105&quot;,
      &quot;172.27.129.111&quot;,
      &quot;172.27.129.112&quot;
    ],
    &quot;key&quot;: {
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    },
    &quot;names&quot;: [
      {
        &quot;C&quot;: &quot;CN&quot;,
        &quot;ST&quot;: &quot;BeiJing&quot;,
        &quot;L&quot;: &quot;BeiJing&quot;,
        &quot;O&quot;: &quot;system:kube-scheduler&quot;,
        &quot;OU&quot;: &quot;4Paradigm&quot;
      }
    ]
}
EOF
</code></pre>
<ul>
<li>hosts 列表包含<strong>所有</strong> kube-scheduler 节点 IP；</li>
<li>CN 为 system:kube-scheduler、O 为 system:kube-scheduler，kubernetes 内置的 ClusterRoleBindings system:kube-scheduler 将赋予 kube-scheduler 工作所需的权限。</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
</code></pre>
<h2 id="创建和分发-kubeconfig-文件">创建和分发 kubeconfig 文件</h2>
<p>kubeconfig 文件包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p>
<pre><code>source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context system:kube-scheduler \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
</code></pre>
<ul>
<li>上一步创建的证书、私钥以及 kube-apiserver 地址被写入到 kubeconfig 文件中；</li>
</ul>
<p>分发 kubeconfig 到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-scheduler.kubeconfig k8s@${node_ip}:/etc/kubernetes/
  done
</code></pre>
<h2 id="创建和分发-kube-scheduler-systemd-unit-文件">创建和分发 kube-scheduler systemd unit 文件</h2>
<pre><code>cat &gt; kube-scheduler.service &lt;&lt;EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/k8s/bin/kube-scheduler \\
  --address=127.0.0.1 \\
  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --leader-elect=true \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5
User=k8s

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li><code>--address</code>：在 127.0.0.1:10251 端口接收 http /metrics 请求；kube-scheduler 目前还不支持接收 https 请求；</li>
<li><code>--kubeconfig</code>：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；</li>
<li><code>--leader-elect=true</code>：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li>
<li><code>User=k8s</code>：使用 k8s 账户运行；</li>
</ul>
<p>完整 unit 见 <a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kube-scheduler.service" target="_blank" rel="noopener">kube-scheduler.service</a>。</p>
<p>分发 systemd unit 文件到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-scheduler.service root@${node_ip}:/etc/systemd/system/
  done
</code></pre>
<h2 id="启动-kube-scheduler-服务">启动 kube-scheduler 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler&quot;
  done
</code></pre>
<ul>
<li>必须先创建日志目录；</li>
</ul>
<h2 id="检查服务运行状态">检查服务运行状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status kube-scheduler|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u kube-scheduler
</code></pre>
<h2 id="查看输出的-metric">查看输出的 metric</h2>
<p>注意：以下命令在 kube-scheduler 节点上执行。</p>
<p>kube-scheduler 监听 10251 端口，接收 http 请求：</p>
<pre><code>$ sudo netstat -lnpt|grep kube-sche
tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      23783/kube-schedule


$ curl -s https://127.0.0.1:10251/metrics |head
# HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.
# TYPE apiserver_audit_event_total counter
apiserver_audit_event_total 0
# HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile=&quot;0&quot;} 9.7715e-05
go_gc_duration_seconds{quantile=&quot;0.25&quot;} 0.000107676
go_gc_duration_seconds{quantile=&quot;0.5&quot;} 0.00017868
go_gc_duration_seconds{quantile=&quot;0.75&quot;} 0.000262444
go_gc_duration_seconds{quantile=&quot;1&quot;} 0.001205223
</code></pre>
<h2 id="测试-kube-scheduler-集群的高可用">测试 kube-scheduler 集群的高可用</h2>
<p>随便找一个或两个 master 节点，停掉 kube-scheduler 服务，看其它节点是否获取了 leader 权限（systemd 日志）。</p>
<h2 id="查看当前的-leader">查看当前的 leader</h2>
<pre><code>$ kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{&quot;holderIdentity&quot;:&quot;kube-node3_61f34593-6cc8-11e8-8af7-5254002f288e&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2018-06-10T16:09:56Z&quot;,&quot;renewTime&quot;:&quot;2018-06-10T16:20:54Z&quot;,&quot;leaderTransitions&quot;:1}'
  creationTimestamp: 2018-06-10T16:07:33Z
  name: kube-scheduler
  namespace: kube-system
  resourceVersion: &quot;4645&quot;
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
  uid: 62382d98-6cc8-11e8-96fa-525400ba84c6
</code></pre>
<p>可见，当前的 leader 为 kube-node3 节点。</p>
]]></content>
  </entry>
  <entry>
    <title>06-3.部署高可用 kube-controller-manager 集群</title>
    <url>/2020/01/07/06-3.%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8%20kube-controller-manager%20%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h1>06-3.部署高可用 kube-controller-manager 集群</h1>
<p>本文档介绍部署高可用 kube-controller-manager 集群的步骤。</p>
<p>该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。</p>
<p>为保证通信安全，本文档先生成 x509 证书和私钥，kube-controller-manager 在如下两种情况下使用该证书：</p>
<ol>
<li>与 kube-apiserver 的安全端口通信时;</li>
<li>在<strong>安全端口</strong>(https，10252) 输出 prometheus 格式的 metrics；</li>
</ol>
<h2 id="准备工作">准备工作</h2>
<p>下载最新版本的二进制文件、安装和配置 flanneld 参考：<a href="https://www.orchome.com/657" target="_blank" rel="noopener">06-0.部署master节点.md</a></p>
<h2 id="创建-kube-controller-manager-证书和私钥">创建 kube-controller-manager 证书和私钥</h2>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF
{
    &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,
    &quot;key&quot;: {
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    },
    &quot;hosts&quot;: [
      &quot;127.0.0.1&quot;,
      &quot;172.27.129.105&quot;,
      &quot;172.27.129.111&quot;,
      &quot;172.27.129.112&quot;
    ],
    &quot;names&quot;: [
      {
        &quot;C&quot;: &quot;CN&quot;,
        &quot;ST&quot;: &quot;BeiJing&quot;,
        &quot;L&quot;: &quot;BeiJing&quot;,
        &quot;O&quot;: &quot;system:kube-controller-manager&quot;,
        &quot;OU&quot;: &quot;4Paradigm&quot;
      }
    ]
}
EOF
</code></pre>
<ul>
<li>hosts 列表包含<strong>所有</strong> kube-controller-manager 节点 IP；</li>
<li>CN 为 system:kube-controller-manager、O 为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:kube-controller-manager 赋予 kube-controller-manager 工作所需的权限。</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
</code></pre>
<p>将生成的证书和私钥分发到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-controller-manager*.pem k8s@${node_ip}:/etc/kubernetes/cert/
  done
</code></pre>
<h2 id="创建和分发-kubeconfig-文件">创建和分发 kubeconfig 文件</h2>
<p>kubeconfig 文件包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p>
<pre><code>source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context system:kube-controller-manager \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
</code></pre>
<p>分发 kubeconfig 到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-controller-manager.kubeconfig k8s@${node_ip}:/etc/kubernetes/
  done
</code></pre>
<h2 id="创建和分发-kube-controller-manager-systemd-unit-文件">创建和分发 kube-controller-manager systemd unit 文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kube-controller-manager.service &lt;&lt;EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/k8s/bin/kube-controller-manager \\
  --port=0 \\
  --secure-port=10252 \\
  --bind-address=127.0.0.1 \\
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --service-cluster-ip-range=${SERVICE_CIDR} \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/etc/kubernetes/cert/ca.pem \\
  --cluster-signing-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --experimental-cluster-signing-duration=8760h \\
  --root-ca-file=/etc/kubernetes/cert/ca.pem \\
  --service-account-private-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --leader-elect=true \\
  --feature-gates=RotateKubeletServerCertificate=true \\
  --controllers=*,bootstrapsigner,tokencleaner \\
  --horizontal-pod-autoscaler-use-rest-clients=true \\
  --horizontal-pod-autoscaler-sync-period=10s \\
  --tls-cert-file=/etc/kubernetes/cert/kube-controller-manager.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kube-controller-manager-key.pem \\
  --use-service-account-credentials=true \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on
Restart=on-failure
RestartSec=5
User=k8s

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li><code>--port=0</code>：关闭监听 http /metrics 的请求，同时 <code>--address</code> 参数无效，<code>--bind-address</code> 参数有效；</li>
<li><code>--secure-port=10252</code>、<code>--bind-address=0.0.0.0</code>: 在所有网络接口监听 10252 端口的 https /metrics 请求；</li>
<li><code>--kubeconfig</code>：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver；</li>
<li><code>--cluster-signing-*-file</code>：签名 TLS Bootstrap 创建的证书；</li>
<li><code>--experimental-cluster-signing-duration</code>：指定 TLS Bootstrap 证书的有效期；</li>
<li><code>--root-ca-file</code>：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验；</li>
<li><code>--service-account-private-key-file</code>：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 <code>--service-account-key-file</code> 指定的公钥文件配对使用；</li>
<li><code>--service-cluster-ip-range</code> ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致；</li>
<li><code>--leader-elect=true</code>：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li>
<li><code>--feature-gates=RotateKubeletServerCertificate=true</code>：开启 kublet server 证书的自动更新特性；</li>
<li><code>--controllers=*,bootstrapsigner,tokencleaner</code>：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token；</li>
<li><code>--horizontal-pod-autoscaler-*</code>：custom metrics 相关参数，支持 autoscaling/v2alpha1；</li>
<li><code>--tls-cert-file</code>、<code>--tls-private-key-file</code>：使用 https 输出 metrics 时使用的 Server 证书和秘钥；</li>
<li><code>--use-service-account-credentials=true</code>:</li>
<li><code>User=k8s</code>：使用 k8s 账户运行；</li>
</ul>
<p>kube-controller-manager 不对请求 https metrics 的 Client 证书进行校验，故不需要指定 <code>--tls-ca-file</code> 参数，而且该参数已被淘汰。</p>
<p>完整 unit 见 <a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kube-controller-manager.service" target="_blank" rel="noopener">kube-controller-manager.service</a></p>
<p>分发 systemd unit 文件到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-controller-manager.service root@${node_ip}:/etc/systemd/system/
  done
</code></pre>
<h2 id="kube-controller-manager-的权限">kube-controller-manager 的权限</h2>
<p>ClusteRole: system:kube-controller-manager 的<strong>权限很小</strong>，只能创建 secret、serviceaccount 等资源对象，各 controller 的权限分散到 ClusterRole system:controller:XXX 中。</p>
<p>需要在 kube-controller-manager 的启动参数中添加 <code>--use-service-account-credentials=true</code> 参数，这样 main controller 会为各 controller 创建对应的 ServiceAccount XXX-controller。</p>
<p>内置的 ClusterRoleBinding system:controller:XXX 将赋予各 XXX-controller ServiceAccount 对应的 ClusterRole system:controller:XXX 权限。</p>
<h2 id="启动-kube-controller-manager-服务">启动 kube-controller-manager 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager&quot;
  done
</code></pre>
<ul>
<li>必须先创建日志目录；</li>
</ul>
<h2 id="检查服务运行状态">检查服务运行状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status kube-controller-manager|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>$ journalctl -u kube-controller-manager
</code></pre>
<h2 id="查看输出的-metric">查看输出的 metric</h2>
<p>注意：以下命令在 kube-controller-manager 节点上执行。</p>
<p>kube-controller-manager 监听 10252 端口，接收 https 请求：</p>
<pre><code>$ sudo netstat -lnpt|grep kube-controll
tcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      18377/kube-controll


$ curl -s --cacert /etc/kubernetes/cert/ca.pem https://127.0.0.1:10252/metrics |head
# HELP ClusterRoleAggregator_adds Total number of adds handled by workqueue: ClusterRoleAggregator
# TYPE ClusterRoleAggregator_adds counter
ClusterRoleAggregator_adds 3
# HELP ClusterRoleAggregator_depth Current depth of workqueue: ClusterRoleAggregator
# TYPE ClusterRoleAggregator_depth gauge
ClusterRoleAggregator_depth 0
# HELP ClusterRoleAggregator_queue_latency How long an item stays in workqueueClusterRoleAggregator before being requested.
# TYPE ClusterRoleAggregator_queue_latency summary
ClusterRoleAggregator_queue_latency{quantile=&quot;0.5&quot;} 57018
ClusterRoleAggregator_queue_latency{quantile=&quot;0.9&quot;} 57268
</code></pre>
<ul>
<li>curl --cacert CA 证书用来验证 kube-controller-manager https server 证书；</li>
</ul>
<h2 id="测试-kube-controller-manager-集群的高可用">测试 kube-controller-manager 集群的高可用</h2>
<p>停掉一个或两个节点的 kube-controller-manager 服务，观察其它节点的日志，看是否获取了 leader 权限。</p>
<h2 id="查看当前的-leader">查看当前的 leader</h2>
<pre><code>$ kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{&quot;holderIdentity&quot;:&quot;kube-node2_084534e2-6cc4-11e8-a418-5254001f5b65&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2018-06-10T15:40:33Z&quot;,&quot;renewTime&quot;:&quot;2018-06-10T16:19:08Z&quot;,&quot;leaderTransitions&quot;:12}'
  creationTimestamp: 2018-06-10T13:59:42Z
  name: kube-controller-manager
  namespace: kube-system
  resourceVersion: &quot;4540&quot;
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager
  uid: 862cc048-6cb6-11e8-96fa-525400ba84c6
</code></pre>
<p>可见，当前的 leader 为 kube-node2 节点。</p>
<h2 id="参考">参考</h2>
<ol>
<li>关于 controller 权限和 use-service-account-credentials 参数：<a href="https://github.com/kubernetes/kubernetes/issues/48208" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/48208</a></li>
<li>kublet 认证和授权：<a href="https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet-authorization" target="_blank" rel="noopener">https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet-authorization</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>06-2.部署 kube-apiserver 组件</title>
    <url>/2020/01/07/06-2.%E9%83%A8%E7%BD%B2%20kube-apiserver%20%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<h1>06-2.部署 kube-apiserver 组件</h1>
<p>本文档讲解使用 keepalived 和 haproxy 部署一个 3 节点高可用 master 集群的步骤，对应的 LB VIP 为环境变量 ${MASTER_VIP}。</p>
<h2 id="准备工作">准备工作</h2>
<p>下载最新版本的二进制文件、安装和配置 flanneld 参考：<a href="https://www.orchome.com/657" target="_blank" rel="noopener">06-0.部署master节点.md</a></p>
<h2 id="创建-kubernetes-证书和私钥">创建 kubernetes 证书和私钥</h2>
<p>创建证书签名请求：</p>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kubernetes-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;172.27.129.105&quot;,
    &quot;172.27.129.111&quot;,
    &quot;172.27.129.112&quot;,
    &quot;${MASTER_VIP}&quot;,
    &quot;${CLUSTER_KUBERNETES_SVC_IP}&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>
<p>hosts 字段指定授权使用该证书的 <strong>IP 或域名列表</strong>，这里列出了 VIP 、apiserver 节点 IP、kubernetes 服务 IP 和域名；</p>
</li>
<li>
<p>域名最后字符不能是 <code>.</code>(如不能为 <code>kubernetes.default.svc.cluster.local.</code>)，否则解析时失败，提示： <code>x509: cannot parse dnsName &quot;kubernetes.default.svc.cluster.local.&quot;</code>；</p>
</li>
<li>
<p>如果使用非 <code>cluster.local</code> 域名，如 <code>opsnull.com</code>，则需要修改域名列表中的最后两个域名为：<code>kubernetes.default.svc.opsnull</code>、<code>kubernetes.default.svc.opsnull.com</code></p>
</li>
<li>
<p>kubernetes 服务 IP 是 apiserver 自动创建的，一般是 <code>--service-cluster-ip-range</code> 参数指定的网段的<strong>第一个IP</strong>，后续可以通过如下命令获取：</p>
<pre><code>$ kubectl get svc kubernetes
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.254.0.1   &lt;none&gt;        443/TCP   1d
</code></pre>
</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
ls kubernetes*pem
</code></pre>
<p>将生成的证书和私钥文件拷贝到 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/kubernetes/cert/ &amp;&amp; sudo chown -R k8s /etc/kubernetes/cert/&quot;
    scp kubernetes*.pem k8s@${node_ip}:/etc/kubernetes/cert/
  done
</code></pre>
<ul>
<li>k8s 账户可以读写 /etc/kubernetes/cert/ 目录；</li>
</ul>
<h2 id="创建加密配置文件">创建加密配置文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; encryption-config.yaml &lt;&lt;EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
</code></pre>
<p>将加密配置文件拷贝到 master 节点的 <code>/etc/kubernetes</code> 目录下：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp encryption-config.yaml root@${node_ip}:/etc/kubernetes/
  done
</code></pre>
<p>替换后的 encryption-config.yaml 文件：<a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/encryption-config.yaml" target="_blank" rel="noopener">encryption-config.yaml</a></p>
<h2 id="创建-kube-apiserver-systemd-unit-模板文件">创建 kube-apiserver systemd unit 模板文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kube-apiserver.service.template &lt;&lt;EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/opt/k8s/bin/kube-apiserver \\
  --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --anonymous-auth=false \\
  --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\
  --advertise-address=##NODE_IP## \\
  --bind-address=##NODE_IP## \\
  --insecure-port=0 \\
  --authorization-mode=Node,RBAC \\
  --runtime-config=api/all \\
  --enable-bootstrap-token-auth \\
  --service-cluster-ip-range=${SERVICE_CIDR} \\
  --service-node-port-range=${NODE_PORT_RANGE} \\
  --tls-cert-file=/etc/kubernetes/cert/kubernetes.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kubernetes-key.pem \\
  --client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --kubelet-client-certificate=/etc/kubernetes/cert/kubernetes.pem \\
  --kubelet-client-key=/etc/kubernetes/cert/kubernetes-key.pem \\
  --service-account-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --etcd-cafile=/etc/kubernetes/cert/ca.pem \\
  --etcd-certfile=/etc/kubernetes/cert/kubernetes.pem \\
  --etcd-keyfile=/etc/kubernetes/cert/kubernetes-key.pem \\
  --etcd-servers=${ETCD_ENDPOINTS} \\
  --enable-swagger-ui=true \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/kube-apiserver-audit.log \\
  --event-ttl=1h \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
User=k8s
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li><code>--experimental-encryption-provider-config</code>：启用加密特性；</li>
<li><code>--authorization-mode=Node,RBAC</code>： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求；</li>
<li><code>--enable-admission-plugins</code>：启用 <code>ServiceAccount</code> 和 <code>NodeRestriction</code>；</li>
<li><code>--service-account-key-file</code>：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 <code>--service-account-private-key-file</code> 指定私钥文件，两者配对使用；</li>
<li><code>--tls-*-file</code>：指定 apiserver 使用的证书、私钥和 CA 文件。<code>--client-ca-file</code> 用于验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书；</li>
<li><code>--kubelet-client-certificate</code>、<code>--kubelet-client-key</code>：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes*.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API 时提示未授权；</li>
<li><code>--bind-address</code>： 不能为 <code>127.0.0.1</code>，否则外界不能访问它的安全端口 6443；</li>
<li><code>--insecure-port=0</code>：关闭监听非安全端口(8080)；</li>
<li><code>--service-cluster-ip-range</code>： 指定 Service Cluster IP 地址段；</li>
<li><code>--service-node-port-range</code>： 指定 NodePort 的端口范围；</li>
<li><code>--runtime-config=api/all=true</code>： 启用所有版本的 APIs，如 autoscaling/v2alpha1；</li>
<li><code>--enable-bootstrap-token-auth</code>：启用 kubelet bootstrap 的 token 认证；</li>
<li><code>--apiserver-count=3</code>：指定集群运行模式，多台 kube-apiserver 会通过 leader 选举产生一个工作节点，其它节点处于阻塞状态；</li>
<li><code>User=k8s</code>：使用 k8s 账户运行；</li>
</ul>
<h2 id="为各节点创建和分发-kube-apiserver-systemd-unit-文件">为各节点创建和分发 kube-apiserver systemd unit 文件</h2>
<p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; 3; i++ ))
  do
    sed -e &quot;s/##NODE_NAME##/${NODE_NAMES[i]}/&quot; -e &quot;s/##NODE_IP##/${NODE_IPS[i]}/&quot; kube-apiserver.service.template &gt; kube-apiserver-${NODE_IPS[i]}.service 
  done
ls kube-apiserver*.service
</code></pre>
<ul>
<li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li>
</ul>
<p>分发生成的 systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    scp kube-apiserver-${node_ip}.service root@${node_ip}:/etc/systemd/system/kube-apiserver.service
  done
</code></pre>
<ul>
<li>必须先创建日志目录；</li>
<li>文件重命名为 kube-apiserver.service;</li>
</ul>
<h2 id="启动-kube-apiserver-服务">启动 kube-apiserver 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver&quot;
  done
</code></pre>
<h2 id="检查-kube-apiserver-运行状态">检查 kube-apiserver 运行状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl status kube-apiserver |grep 'Active:'&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则到 master 节点查看日志，确认原因：</p>
<pre><code>journalctl -u kube-apiserver
</code></pre>
<h2 id="打印-kube-apiserver-写入-etcd-的数据">打印 kube-apiserver 写入 etcd 的数据</h2>
<pre><code>source /opt/k8s/bin/environment.sh
ETCDCTL_API=3 etcdctl \
    --endpoints=${ETCD_ENDPOINTS} \
    --cacert=/etc/kubernetes/cert/ca.pem \
    --cert=/etc/etcd/cert/etcd.pem \
    --key=/etc/etcd/cert/etcd-key.pem \
    get /registry/ --prefix --keys-only
</code></pre>
<h2 id="检查集群信息">检查集群信息</h2>
<pre><code>$ kubectl cluster-info
Kubernetes master is running at https://172.27.129.253:8443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

$ kubectl get all --all-namespaces
NAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
default     service/kubernetes   ClusterIP   10.254.0.1   &lt;none&gt;        443/TCP   35m

$ kubectl get componentstatuses
NAME                 STATUS      MESSAGE                                                                                        ERROR
controller-manager   Unhealthy   Get https://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: getsockopt: connection refused
scheduler            Unhealthy   Get https://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused
etcd-1               Healthy     {&quot;health&quot;:&quot;true&quot;}
etcd-0               Healthy     {&quot;health&quot;:&quot;true&quot;}
etcd-2               Healthy     {&quot;health&quot;:&quot;true&quot;}
</code></pre>
<p>注意：</p>
<ol>
<li>
<p>如果执行 kubectl 命令式时输出如下错误信息，则说明使用的 <code>~/.kube/config</code> 文件不对，请切换到正确的账户后再执行该命令：</p>
<p><code>The connection to the server localhost:8080 was refused - did you specify the right host or port?</code></p>
</li>
<li>
<p>执行 <code>kubectl get componentstatuses</code> 命令时，apiserver 默认向 127.0.0.1 发送请求。当 controller-manager、scheduler 以集群模式运行时，有可能和 kube-apiserver <strong>不在一台机器</strong>上，这时 controller-manager 或 scheduler 的状态为 Unhealthy，但实际上它们工作<strong>正常</strong>。</p>
</li>
</ol>
<h2 id="检查-kube-apiserver-监听的端口">检查 kube-apiserver 监听的端口</h2>
<pre><code>$ sudo netstat -lnpt|grep kube
tcp        0      0 172.27.129.105:6443     0.0.0.0:*               LISTEN      13075/kube-apiserve
</code></pre>
<ul>
<li>6443: 接收 https 请求的安全端口，对所有请求做认证和授权；</li>
<li>由于关闭了非安全端口，故没有监听 8080；</li>
</ul>
<h2 id="授予-kubernetes-证书访问-kubelet-API-的权限">授予 kubernetes 证书访问 kubelet API 的权限</h2>
<p>在执行 kubectl exec、run、logs 等命令时，apiserver 会转发到 kubelet。这里定义 RBAC 规则，授权 apiserver 调用 kubelet API。</p>
<pre><code>$ kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
</code></pre>
<h2 id="参考">参考</h2>
<ol>
<li>关于证书域名最后字符不能是 <code>.</code> 的问题，实际和 Go 的版本有关，1.9 不支持这种类型的证书：<a href="https://github.com/kubernetes/ingress-nginx/issues/2188" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/issues/2188</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>06-1.部署高可用组件</title>
    <url>/2020/01/07/06-1.%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<h1>06-1.部署高可用组件</h1>
<p>本文档讲解使用 keepalived 和 haproxy 实现 kube-apiserver 高可用的步骤：</p>
<ul>
<li>keepalived 提供 kube-apiserver 对外服务的 VIP；</li>
<li>haproxy 监听 VIP，后端连接所有 kube-apiserver 实例，提供健康检查和负载均衡功能；</li>
</ul>
<p>运行 keepalived 和 haproxy 的节点称为 LB 节点。由于 keepalived 是一主多备运行模式，故至少两个 LB 节点。</p>
<p>本文档复用 master 节点的三台机器，haproxy 监听的端口(8443) 需要与 kube-apiserver 的端口 6443 不同，避免冲突。</p>
<p>keepalived 在运行过程中周期检查本机的 haproxy 进程状态，如果检测到 haproxy 进程异常，则触发重新选主的过程，VIP 将飘移到新选出来的主节点，从而实现 VIP 的高可用。</p>
<p>所有组件（如 kubeclt、apiserver、controller-manager、scheduler 等）都通过 VIP 和 haproxy 监听的 8443 端口访问 kube-apiserver 服务。</p>
<h2 id="安装软件包">安装软件包</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;yum install -y keepalived haproxy&quot;
  done
</code></pre>
<h2 id="配置和下发-haproxy-配置文件">配置和下发 haproxy 配置文件</h2>
<p>haproxy 配置文件：</p>
<pre><code>cat &gt; haproxy.cfg &lt;&lt;EOF
global
    log /dev/log    local0
    log /dev/log    local1 notice
    chroot /var/lib/haproxy
    stats socket /var/run/haproxy-admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon
    nbproc 1

defaults
    log     global
    timeout connect 5000
    timeout client  10m
    timeout server  10m

listen  admin_stats
    bind 0.0.0.0:10080
    mode http
    log 127.0.0.1 local0 err
    stats refresh 30s
    stats uri /status
    stats realm welcome login\ Haproxy
    stats auth admin:123456
    stats hide-version
    stats admin if TRUE

listen kube-master
    bind 0.0.0.0:8443
    mode tcp
    option tcplog
    balance source
    server 172.27.129.105 172.27.129.105:6443 check inter 2000 fall 2 rise 2 weight 1
    server 172.27.129.111 172.27.129.111:6443 check inter 2000 fall 2 rise 2 weight 1
    server 172.27.129.112 172.27.129.112:6443 check inter 2000 fall 2 rise 2 weight 1
EOF
</code></pre>
<ul>
<li>haproxy 在 10080 端口输出 status 信息；</li>
<li>haproxy 监听<strong>所有接口</strong>的 8443 端口，该端口与环境变量 ${KUBE_APISERVER} 指定的端口必须一致；</li>
<li>server 字段列出所有 kube-apiserver 监听的 IP 和端口；</li>
</ul>
<p>下发 haproxy.cfg 到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp haproxy.cfg root@${node_ip}:/etc/haproxy
  done
</code></pre>
<h2 id="起-haproxy-服务">起 haproxy 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl restart haproxy&quot;
  done
</code></pre>
<h2 id="检查-haproxy-服务状态">检查 haproxy 服务状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl status haproxy|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u haproxy
</code></pre>
<p>检查 haproxy 是否监听 8443 端口：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;netstat -lnpt|grep haproxy&quot;
  done
</code></pre>
<p>确保输出类似于:</p>
<pre><code>tcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN      120583/haproxy
</code></pre>
<h2 id="配置和下发-keepalived-配置文件">配置和下发 keepalived 配置文件</h2>
<p>keepalived 是一主（master）多备（backup）运行模式，故有两种类型的配置文件。master 配置文件只有一份，backup 配置文件视节点数目而定，对于本文档而言，规划如下：</p>
<ul>
<li>master: 172.27.129.105</li>
<li>backup：172.27.129.111、172.27.129.112</li>
</ul>
<p>master 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
cat  &gt; keepalived-master.conf &lt;&lt;EOF
global_defs {
    router_id lb-master-105
}

vrrp_script check-haproxy {
    script &quot;killall -0 haproxy&quot;
    interval 5
    weight -30
}

vrrp_instance VI-kube-master {
    state MASTER
    priority 120
    dont_track_primary
    interface ${VIP_IF}
    virtual_router_id 68
    advert_int 3
    track_script {
        check-haproxy
    }
    virtual_ipaddress {
        ${MASTER_VIP}
    }
}
EOF
</code></pre>
<ul>
<li>VIP 所在的接口（interface ${VIP_IF}）为 <code>eth0</code>；</li>
<li>使用 <code>killall -0 haproxy</code> 命令检查所在节点的 haproxy 进程是否正常。如果异常则将权重减少（-30）,从而触发重新选主过程；</li>
<li>router_id、virtual_router_id 用于标识属于该 HA 的 keepalived 实例，如果有多套 keepalived HA，则必须各不相同；</li>
</ul>
<p>backup 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
cat  &gt; keepalived-backup.conf &lt;&lt;EOF
global_defs {
    router_id lb-backup-105
}

vrrp_script check-haproxy {
    script &quot;killall -0 haproxy&quot;
    interval 5
    weight -30
}

vrrp_instance VI-kube-master {
    state BACKUP
    priority 110
    dont_track_primary
    interface ${VIP_IF}
    virtual_router_id 68
    advert_int 3
    track_script {
        check-haproxy
    }
    virtual_ipaddress {
        ${MASTER_VIP}
    }
}
EOF
</code></pre>
<ul>
<li>VIP 所在的接口（interface ${VIP_IF}）为 <code>eth0</code>；</li>
<li>使用 <code>killall -0 haproxy</code> 命令检查所在节点的 haproxy 进程是否正常。如果异常则将权重减少（-30）,从而触发重新选主过程；</li>
<li>router_id、virtual_router_id 用于标识属于该 HA 的 keepalived 实例，如果有多套 keepalived HA，则必须各不相同；</li>
<li>priority 的值必须小于 master 的值；</li>
</ul>
<h2 id="下发-keepalived-配置文件">下发 keepalived 配置文件</h2>
<p>下发 master 配置文件：</p>
<pre><code>scp keepalived-master.conf root@172.27.129.105:/etc/keepalived/keepalived.conf
</code></pre>
<p>下发 backup 配置文件：</p>
<pre><code>scp keepalived-backup.conf root@172.27.129.111:/etc/keepalived/keepalived.conf
scp keepalived-backup.conf root@172.27.129.112:/etc/keepalived/keepalived.conf
</code></pre>
<h2 id="起-keepalived-服务">起 keepalived 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl restart keepalived&quot;
  done
</code></pre>
<h2 id="检查-keepalived-服务">检查 keepalived 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl status keepalived|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u keepalived
</code></pre>
<p>查看 VIP 所在的节点，确保可以 ping 通 VIP：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;/usr/sbin/ip addr show ${VIP_IF}&quot;
    ssh ${node_ip} &quot;ping -c 1 ${MASTER_VIP}&quot;
  done
</code></pre>
<h2 id="查看-haproxy-状态页面">查看 haproxy 状态页面</h2>
<p>浏览器访问 ${MASTER_VIP}:10080/status 地址，查看 haproxy 状态页面：</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufDj2AOO3kAALXua6sc-M561.png" alt="screenshot"></p>
]]></content>
  </entry>
  <entry>
    <title>06-0.部署 master 节点</title>
    <url>/2020/01/07/06-0-%E9%83%A8%E7%BD%B2-master-%E8%8A%82%E7%82%B9/</url>
    <content><![CDATA[<h1>06-0.部署 master 节点</h1>
<p>kubernetes master 节点运行如下组件：</p>
<ul>
<li>kube-apiserver</li>
<li>kube-scheduler</li>
<li>kube-controller-manager</li>
</ul>
<p>kube-scheduler 和 kube-controller-manager 可以以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。</p>
<p>对于 kube-apiserver，可以运行多个实例（本文档是 3 实例），但对其它组件需要提供统一的访问地址，该地址需要高可用。本文档使用 keepalived 和 haproxy 实现 kube-apiserver VIP 高可用和负载均衡。</p>
<h2 id="下载最新版本的二进制文件">下载最新版本的二进制文件</h2>
<p>从 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md" target="_blank" rel="noopener"><code>CHANGELOG</code>页面</a> 下载 server tarball 文件。</p>
<pre><code>wget https://dl.k8s.io/v1.10.4/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
cd kubernetes
tar -xzvf  kubernetes-src.tar.gz
</code></pre>
<p>将二进制文件拷贝到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp server/bin/* k8s@${node_ip}:/opt/k8s/bin/
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done</code></pre>
]]></content>
      <tags>
        <tag>master, kube-apiserver, kube-scheduler, kube-controller-manager</tag>
      </tags>
  </entry>
  <entry>
    <title>05.部署 flannel 网络</title>
    <url>/2020/01/07/05-%E9%83%A8%E7%BD%B2-flannel-%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>tags: flanneld</p>
<h1>05.部署 flannel 网络</h1>
<p>kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络。</p>
<p>flaneel 第一次启动时，从 etcd 获取 Pod 网段信息，为本节点分配一个未使用的 <code>/24</code> 段地址，然后创建 <code>flannedl.1</code>（也可能是其它名称，如 flannel1 等） 接口。</p>
<p>flannel 将分配的 Pod 网段信息写入 <code>/run/flannel/docker</code> 文件，docker 后续使用这个文件中的环境变量设置 <code>docker0</code> 网桥。</p>
<h2 id="下载和分发-flanneld-二进制文件">下载和分发 flanneld 二进制文件</h2>
<p>到 <a href="https://github.com/coreos/flannel/releases" target="_blank" rel="noopener">https://github.com/coreos/flannel/releases</a> 页面下载最新版本的发布包：</p>
<pre><code>mkdir flannel
wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz
tar -xzvf flannel-v0.10.0-linux-amd64.tar.gz -C flannel
</code></pre>
<p>分发 flanneld 二进制文件到集群所有节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp  flannel/{flanneld,mk-docker-opts.sh} k8s@${node_ip}:/opt/k8s/bin/
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h2 id="创建-flannel-证书和私钥">创建 flannel 证书和私钥</h2>
<p>flannel 从 etcd 集群存取网段分配信息，而 etcd 集群启用了双向 x509 证书认证，所以需要为 flanneld 生成证书和私钥。</p>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; flanneld-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;flanneld&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld
ls flanneld*pem
</code></pre>
<p>将生成的证书和私钥分发到<strong>所有节点</strong>（master 和 worker）：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/flanneld/cert &amp;&amp; chown -R k8s /etc/flanneld&quot;
    scp flanneld*.pem k8s@${node_ip}:/etc/flanneld/cert
  done
</code></pre>
<h2 id="向-etcd-写入集群-Pod-网段信息">向 etcd 写入集群 Pod 网段信息</h2>
<p>注意：本步骤<strong>只需执行一次</strong>。</p>
<pre><code>source /opt/k8s/bin/environment.sh
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/cert/ca.pem \
  --cert-file=/etc/flanneld/cert/flanneld.pem \
  --key-file=/etc/flanneld/cert/flanneld-key.pem \
  set ${FLANNEL_ETCD_PREFIX}/config '{&quot;Network&quot;:&quot;'${CLUSTER_CIDR}'&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}}'
</code></pre>
<ul>
<li>flanneld <strong>当前版本 (v0.10.0) 不支持 etcd v3</strong>，故使用 etcd v2 API 写入配置 key 和网段数据；</li>
<li>写入的 Pod 网段 ${CLUSTER_CIDR} 必须是 /16 段地址，必须与 kube-controller-manager 的 <code>--cluster-cidr</code> 参数值一致；</li>
</ul>
<h2 id="创建-flanneld-的-systemd-unit-文件">创建 flanneld 的 systemd unit 文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
export IFACE=eth0
cat &gt; flanneld.service &lt;&lt; EOF
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
ExecStart=/opt/k8s/bin/flanneld \\
  -etcd-cafile=/etc/kubernetes/cert/ca.pem \\
  -etcd-certfile=/etc/flanneld/cert/flanneld.pem \\
  -etcd-keyfile=/etc/flanneld/cert/flanneld-key.pem \\
  -etcd-endpoints=${ETCD_ENDPOINTS} \\
  -etcd-prefix=${FLANNEL_ETCD_PREFIX} \\
  -iface=${IFACE}
ExecStartPost=/opt/k8s/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
EOF
</code></pre>
<ul>
<li><code>mk-docker-opts.sh</code> 脚本将分配给 flanneld 的 Pod 子网网段信息写入 <code>/run/flannel/docker</code> 文件，后续 docker 启动时使用这个文件中的环境变量配置 docker0 网桥；</li>
<li>flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 <code>-iface</code> 参数指定通信接口，如上面的 eth0 接口;</li>
<li>flanneld 运行时需要 root 权限；</li>
</ul>
<h2 id="分发-flanneld-systemd-unit-文件到所有节点">分发 flanneld systemd unit 文件到<strong>所有节点</strong></h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp flanneld.service root@${node_ip}:/etc/systemd/system/
  done
</code></pre>
<h2 id="启动-flanneld-服务">启动 flanneld 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld&quot;
  done
</code></pre>
<h2 id="检查启动结果">检查启动结果</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status flanneld|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>$ journalctl -u flanneld
</code></pre>
<h2 id="检查分配给各-flanneld-的-Pod-网段信息">检查分配给各 flanneld 的 Pod 网段信息</h2>
<p>查看集群 Pod 网段(/16)：</p>
<pre><code>source /opt/k8s/bin/environment.sh
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/cert/ca.pem \
  --cert-file=/etc/flanneld/cert/flanneld.pem \
  --key-file=/etc/flanneld/cert/flanneld-key.pem \
  get ${FLANNEL_ETCD_PREFIX}/config
</code></pre>
<p>输出：</p>
<p><code>{&quot;Network&quot;:&quot;172.30.0.0/16&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}}</code></p>
<p>查看已分配的 Pod 子网段列表(/24):</p>
<pre><code>source /opt/k8s/bin/environment.sh
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/cert/ca.pem \
  --cert-file=/etc/flanneld/cert/flanneld.pem \
  --key-file=/etc/flanneld/cert/flanneld-key.pem \
  ls ${FLANNEL_ETCD_PREFIX}/subnets
</code></pre>
<p>输出：</p>
<pre><code>/kubernetes/network/subnets/172.30.81.0-24
/kubernetes/network/subnets/172.30.29.0-24
/kubernetes/network/subnets/172.30.39.0-24
</code></pre>
<p>查看某一 Pod 网段对应的节点 IP 和 flannel 接口地址:</p>
<pre><code>source /opt/k8s/bin/environment.sh
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/cert/ca.pem \
  --cert-file=/etc/flanneld/cert/flanneld.pem \
  --key-file=/etc/flanneld/cert/flanneld-key.pem \
  get ${FLANNEL_ETCD_PREFIX}/subnets/172.30.81.0-24
</code></pre>
<p>输出：</p>
<p><code>{&quot;PublicIP&quot;:&quot;172.27.129.105&quot;,&quot;BackendType&quot;:&quot;vxlan&quot;,&quot;BackendData&quot;:{&quot;VtepMAC&quot;:&quot;12:21:93:9e:b1:eb&quot;}}</code></p>
<h2 id="验证各节点能通过-Pod-网段互通">验证各节点能通过 Pod 网段互通</h2>
<p>在<strong>各节点上部署</strong> flannel 后，检查是否创建了 flannel 接口(名称可能为 flannel0、flannel.0、flannel.1 等)：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;/usr/sbin/ip addr show flannel.1|grep -w inet&quot;
  done
</code></pre>
<p>输出：</p>
<pre><code>inet 172.30.81.0/32 scope global flannel.1
inet 172.30.29.0/32 scope global flannel.1
inet 172.30.39.0/32 scope global flannel.1
</code></pre>
<p>在各节点上 ping 所有 flannel 接口 IP，确保能通：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.81.0&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.29.0&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.39.0&quot;
  done</code></pre>
]]></content>
  </entry>
  <entry>
    <title>04.部署 etcd 集群</title>
    <url>/2020/01/07/04-%E9%83%A8%E7%BD%B2-etcd-%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>tags: etcd</p>
<h1>04.部署 etcd 集群</h1>
<p>etcd 是基于 Raft 的分布式 key-value 存储系统，由 CoreOS 开发，常用于服务发现、共享配置以及并发控制（如 leader 选举、分布式锁等）。kubernetes 使用 etcd 存储所有运行数据。</p>
<p>本文档介绍部署一个三节点高可用 etcd 集群的步骤：</p>
<ul>
<li>下载和分发 etcd 二进制文件；</li>
<li>创建 etcd 集群各节点的 x509 证书，用于加密客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的数据流；</li>
<li>创建 etcd 的 systemd unit 文件，配置服务参数；</li>
<li>检查集群工作状态；</li>
</ul>
<p>etcd 集群各节点的名称和 IP 如下：</p>
<ul>
<li>kube-node1：172.27.129.105</li>
<li>kube-node2：172.27.129.111</li>
<li>kube-node3：172.27.129.112</li>
</ul>
<h2 id="下载和分发-etcd-二进制文件">下载和分发 etcd 二进制文件</h2>
<p>到 <a href="https://github.com/coreos/etcd/releases" target="_blank" rel="noopener">https://github.com/coreos/etcd/releases</a> 页面下载最新版本的发布包：</p>
<pre><code>wget https://github.com/coreos/etcd/releases/download/v3.3.7/etcd-v3.3.7-linux-amd64.tar.gz
tar -xvf etcd-v3.3.7-linux-amd64.tar.gz
</code></pre>
<p>分发二进制文件到集群所有节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp etcd-v3.3.7-linux-amd64/etcd* k8s@${node_ip}:/opt/k8s/bin
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h2 id="创建-etcd-证书和私钥">创建 etcd 证书和私钥</h2>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; etcd-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;etcd&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;172.27.129.105&quot;,
    &quot;172.27.129.111&quot;,
    &quot;172.27.129.112&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>hosts 字段指定授权使用该证书的 etcd 节点 IP 或域名列表，这里将 etcd 集群的三个节点 IP 都列在其中；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
    -ca-key=/etc/kubernetes/cert/ca-key.pem \
    -config=/etc/kubernetes/cert/ca-config.json \
    -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
ls etcd*
</code></pre>
<p>分发生成的证书和私钥到各 etcd 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/etcd/cert &amp;&amp; chown -R k8s /etc/etcd/cert&quot;
    scp etcd*.pem k8s@${node_ip}:/etc/etcd/cert/
  done
</code></pre>
<h2 id="创建-etcd-的-systemd-unit-模板文件">创建 etcd 的 systemd unit 模板文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; etcd.service.template &lt;&lt;EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
User=k8s
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/opt/k8s/bin/etcd \\
  --data-dir=/var/lib/etcd \\
  --name=##NODE_NAME## \\
  --cert-file=/etc/etcd/cert/etcd.pem \\
  --key-file=/etc/etcd/cert/etcd-key.pem \\
  --trusted-ca-file=/etc/kubernetes/cert/ca.pem \\
  --peer-cert-file=/etc/etcd/cert/etcd.pem \\
  --peer-key-file=/etc/etcd/cert/etcd-key.pem \\
  --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --listen-peer-urls=https://##NODE_IP##:2380 \\
  --initial-advertise-peer-urls=https://##NODE_IP##:2380 \\
  --listen-client-urls=https://##NODE_IP##:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls=https://##NODE_IP##:2379 \\
  --initial-cluster-token=etcd-cluster-0 \\
  --initial-cluster=${ETCD_NODES} \\
  --initial-cluster-state=new
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li><code>User</code>：指定以 k8s 账户运行；</li>
<li><code>WorkingDirectory</code>、<code>--data-dir</code>：指定工作目录和数据目录为 <code>/var/lib/etcd</code>，需在启动服务前创建这个目录；</li>
<li><code>--name</code>：指定节点名称，当 <code>--initial-cluster-state</code> 值为 <code>new</code> 时，<code>--name</code> 的参数值必须位于 <code>--initial-cluster</code> 列表中；</li>
<li><code>--cert-file</code>、<code>--key-file</code>：etcd server 与 client 通信时使用的证书和私钥；</li>
<li><code>--trusted-ca-file</code>：签名 client 证书的 CA 证书，用于验证 client 证书；</li>
<li><code>--peer-cert-file</code>、<code>--peer-key-file</code>：etcd 与 peer 通信使用的证书和私钥；</li>
<li><code>--peer-trusted-ca-file</code>：签名 peer 证书的 CA 证书，用于验证 peer 证书；</li>
</ul>
<h2 id="为各节点创建和分发-etcd-systemd-unit-文件">为各节点创建和分发 etcd systemd unit 文件</h2>
<p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; 3; i++ ))
  do
    sed -e &quot;s/##NODE_NAME##/${NODE_NAMES[i]}/&quot; -e &quot;s/##NODE_IP##/${NODE_IPS[i]}/&quot; etcd.service.template &gt; etcd-${NODE_IPS[i]}.service 
  done
ls *.service
</code></pre>
<ul>
<li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li>
</ul>
<p>分发生成的 systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/lib/etcd &amp;&amp; chown -R k8s /var/lib/etcd&quot; 
    scp etcd-${node_ip}.service root@${node_ip}:/etc/systemd/system/etcd.service
  done
</code></pre>
<ul>
<li>必须先创建 etcd 数据目录和工作目录;</li>
<li>文件重命名为 etcd.service;</li>
</ul>
<p>完整 unit 文件见：<a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/etcd.service" target="_blank" rel="noopener">etcd.service</a></p>
<h2 id="启动-etcd-服务">启动 etcd 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl restart etcd &amp;&quot;
  done
</code></pre>
<ul>
<li>etcd 进程首次启动时会等待其它节点的 etcd 加入集群，命令 <code>systemctl start etcd</code> 会卡住一段时间，为正常现象。</li>
</ul>
<h2 id="检查启动结果">检查启动结果</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status etcd|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>$ journalctl -u etcd
</code></pre>
<h2 id="验证服务状态">验证服务状态</h2>
<p>部署完 etcd 集群后，在任一 etc 节点上执行如下命令：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ETCDCTL_API=3 /opt/k8s/bin/etcdctl \
    --endpoints=https://${node_ip}:2379 \
    --cacert=/etc/kubernetes/cert/ca.pem \
    --cert=/etc/etcd/cert/etcd.pem \
    --key=/etc/etcd/cert/etcd-key.pem endpoint health;
  done
</code></pre>
<p>预期输出：</p>
<pre><code>https://172.27.129.105:2379 is healthy: successfully committed proposal: took = 2.192932ms
https://172.27.129.111:2379 is healthy: successfully committed proposal: took = 3.546896ms
https://172.27.129.112:2379 is healthy: successfully committed proposal: took = 3.013667ms
</code></pre>
<p>输出均为 <code>healthy</code> 时表示集群服务正常。</p>
]]></content>
  </entry>
  <entry>
    <title>03.部署 kubectl 命令行工具</title>
    <url>/2020/01/07/03-%E9%83%A8%E7%BD%B2-kubectl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<p>tags: kubectl</p>
<h1>03.部署 kubectl 命令行工具</h1>
<p>kubectl 是 kubernetes 集群的命令行管理工具，本文档介绍安装和配置它的步骤。</p>
<p>kubectl 默认从 <code>~/.kube/config</code> 文件读取 kube-apiserver 地址、证书、用户名等信息，如果没有配置，执行 kubectl 命令时可能会出错：</p>
<pre><code>$ kubectl get pods
The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre>
<p>本文档只需要<strong>部署一次</strong>，生成的 kubeconfig 文件与机器无关。</p>
<h2 id="下载和分发-kubectl-二进制文件">下载和分发 kubectl 二进制文件</h2>
<p>下载和解压：</p>
<pre><code>wget https://dl.k8s.io/v1.10.4/kubernetes-client-linux-amd64.tar.gz
tar -xzvf kubernetes-client-linux-amd64.tar.gz
</code></pre>
<p>分发到所有使用 kubectl 的节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kubernetes/client/bin/kubectl k8s@${node_ip}:/opt/k8s/bin/
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h2 id="创建-admin-证书和私钥">创建 admin 证书和私钥</h2>
<p>kubectl 与 apiserver https 安全端口通信，apiserver 对提供的证书进行认证和授权。</p>
<p>kubectl 作为集群的管理工具，需要被授予最高权限。这里创建具有<strong>最高权限</strong>的 admin 证书。</p>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; admin-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>O 为 <code>system:masters</code>，kube-apiserver 收到该证书后将请求的 Group 设置为 system:masters；</li>
<li>预定义的 ClusterRoleBinding <code>cluster-admin</code> 将 Group <code>system:masters</code> 与 Role <code>cluster-admin</code> 绑定，该 Role 授予<strong>所有 API</strong>的权限；</li>
<li>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes admin-csr.json | cfssljson -bare admin
ls admin*
</code></pre>
<h2 id="创建-kubeconfig-文件">创建 kubeconfig 文件</h2>
<p>kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p>
<pre><code>source /opt/k8s/bin/environment.sh
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kubectl.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=kubectl.kubeconfig

# 设置上下文参数
kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin \
  --kubeconfig=kubectl.kubeconfig

# 设置默认上下文
kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig
</code></pre>
<ul>
<li><code>--certificate-authority</code>：验证 kube-apiserver 证书的根证书；</li>
<li><code>--client-certificate</code>、<code>--client-key</code>：刚生成的 <code>admin</code> 证书和私钥，连接 kube-apiserver 时使用；</li>
<li><code>--embed-certs=true</code>：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径)；</li>
</ul>
<h2 id="分发-kubeconfig-文件">分发 kubeconfig 文件</h2>
<p>分发到所有使用 <code>kubectl</code> 命令的节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;mkdir -p ~/.kube&quot;
    scp kubectl.kubeconfig k8s@${node_ip}:~/.kube/config
    ssh root@${node_ip} &quot;mkdir -p ~/.kube&quot;
    scp kubectl.kubeconfig root@${node_ip}:~/.kube/config
  done
</code></pre>
<ul>
<li>保存到用户的 <code>~/.kube/config</code> 文件；</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>02.创建 CA 证书和秘钥</title>
    <url>/2020/01/07/02-%E5%88%9B%E5%BB%BA-CA-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%98%E9%92%A5/</url>
    <content><![CDATA[<p>tags: TLS, CA, x509</p>
<h1>02.创建 CA 证书和秘钥</h1>
<p>为确保安全，<code>kubernetes</code> 系统各组件需要使用 <code>x509</code> 证书对通信进行加密和认证。</p>
<p>CA (Certificate Authority) 是自签名的根证书，用来签名后续创建的其它证书。</p>
<p>本文档使用 <code>CloudFlare</code> 的 PKI 工具集 <a href="https://github.com/cloudflare/cfssl" target="_blank" rel="noopener">cfssl</a> 创建所有证书。</p>
<h2 id="安装-cfssl-工具集">安装 cfssl 工具集</h2>
<pre><code>sudo mkdir -p /opt/k8s/cert &amp;&amp; sudo chown -R k8s /opt/k8s &amp;&amp; cd /opt/k8s
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
mv cfssl_linux-amd64 /opt/k8s/bin/cfssl

wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
mv cfssljson_linux-amd64 /opt/k8s/bin/cfssljson

wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
mv cfssl-certinfo_linux-amd64 /opt/k8s/bin/cfssl-certinfo

chmod +x /opt/k8s/bin/*
export PATH=/opt/k8s/bin:$PATH
</code></pre>
<h2 id="创建根证书-CA">创建根证书 (CA)</h2>
<p>CA 证书是集群所有节点共享的，<strong>只需要创建一个 CA 证书</strong>，后续创建的所有证书都由它签名。</p>
<h3 id="创建配置文件">创建配置文件</h3>
<p>CA 配置文件用于配置根证书的使用场景 (profile) 和具体参数 (usage，过期时间、服务端认证、客户端认证、加密等)，后续在签名其它证书时需要指定特定场景。</p>
<pre><code>cat &gt; ca-config.json &lt;&lt;EOF
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;87600h&quot;
      }
    }
  }
}
EOF
</code></pre>
<ul>
<li><code>signing</code>：表示该证书可用于签名其它证书，生成的 <code>ca.pem</code> 证书中 <code>CA=TRUE</code>；</li>
<li><code>server auth</code>：表示 client 可以用该该证书对 server 提供的证书进行验证；</li>
<li><code>client auth</code>：表示 server 可以用该该证书对 client 提供的证书进行验证；</li>
</ul>
<h3 id="创建证书签名请求文件">创建证书签名请求文件</h3>
<pre><code>cat &gt; ca-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>CN：<code>Common Name</code>，kube-apiserver 从证书中提取该字段作为请求的<strong>用户名 (User Name)</strong>，浏览器使用该字段验证网站是否合法；</li>
<li>O：<code>Organization</code>，kube-apiserver 从证书中提取该字段作为请求用户所属的<strong>组 (Group)</strong>；</li>
<li>kube-apiserver 将提取的 User、Group 作为 <code>RBAC</code> 授权的用户标识；</li>
</ul>
<h3 id="生成-CA-证书和私钥">生成 CA 证书和私钥</h3>
<pre><code>cfssl gencert -initca ca-csr.json | cfssljson -bare ca
ls ca*
</code></pre>
<h2 id="分发证书文件">分发证书文件</h2>
<p>将生成的 CA 证书、秘钥文件、配置文件拷贝到<strong>所有节点</strong>的 <code>/etc/kubernetes/cert</code> 目录下：</p>
<pre><code>source /opt/k8s/bin/environment.sh # 导入 NODE_IPS 环境变量
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/kubernetes/cert &amp;&amp; chown -R k8s /etc/kubernetes&quot;
    scp ca*.pem ca-config.json k8s@${node_ip}:/etc/kubernetes/cert
  done
</code></pre>
<ul>
<li>k8s 账户需要有读写 /etc/kubernetes 目录及其子目录文件的权限；</li>
</ul>
<h2 id="参考">参考</h2>
<ol>
<li>各种 CA 证书类型：<a href="https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>01.系统初始化和全局变量</title>
    <url>/2020/01/07/01.%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F/</url>
    <content><![CDATA[<p>tags: environment</p>
<h1>01.系统初始化和全局变量</h1>
<p>集群机器</p>
<hr>
<ul>
<li>kube-node1：172.27.129.105</li>
<li>kube-node2：172.27.129.111</li>
<li>kube-node3：172.27.129.112</li>
</ul>
<p>本着测试的目的，etcd 集群、kubernetes master 集群、kubernetes node 均使用这三台机器。</p>
<blockquote>
<p>若有安装 Vagrant 与 Virtualbox，这三台机器可以用本着提供的 Vagrantfile 来建置：</p>
</blockquote>
<pre><code>$ cd vagrant
$ vagrant up
</code></pre>
<p>主机名</p>
<hr>
<p>设置永久主机名称，然后重新登录:</p>
<pre><code>$ sudo hostnamectl set-hostname kube-node1 # 将 kube-node1 替换为当前主机名
</code></pre>
<ul>
<li>设置的主机名保存在 <code>/etc/hostname</code> 文件中；</li>
</ul>
<p>修改每台机器的 <code>/etc/hosts</code> 文件，添加主机名和 IP 的对应关系：</p>
<pre><code>$ grep kube-node /etc/hosts
172.27.129.105 kube-node1    kube-node1
172.27.129.111 kube-node2    kube-node2
172.27.129.112 kube-node3    kube-node3
</code></pre>
<h2 id="添加-k8s-和-docker-账户">添加 k8s 和 docker 账户</h2>
<p>在每台机器上添加 k8s 账户，可以无密码 sudo：</p>
<pre><code>$ sudo useradd -m k8s
$ sudo sh -c 'echo 123456 | passwd k8s --stdin' # 为 k8s 账户设置密码
$ sudo visudo
$ sudo grep '%wheel.*NOPASSWD: ALL' /etc/sudoers
%wheel    ALL=(ALL)    NOPASSWD: ALL
$ sudo gpasswd -a k8s wheel
</code></pre>
<p>在每台机器上添加 docker 账户，将 k8s 账户添加到 docker 组中，同时配置 dockerd 参数：</p>
<pre><code>$ sudo useradd -m docker
$ sudo gpasswd -a k8s docker
$ sudo mkdir -p  /etc/docker/
$ cat /etc/docker/daemon.json
{
    &quot;registry-mirrors&quot;: [&quot;https://hub-mirror.c.163.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;],
    &quot;max-concurrent-downloads&quot;: 20
}
</code></pre>
<h2 id="无密码-ssh-登录其它节点">无密码 ssh 登录其它节点</h2>
<p>如果没有特殊指明，本文档的所有操作<strong>均在 kube-node1 节点上执行</strong>，然后远程分发文件和执行命令。</p>
<p>设置 kube-node1 可以无密码登录<strong>所有节点</strong>的 k8s 和 root 账户：</p>
<pre><code>[k8s@kube-node1 k8s]$ ssh-keygen -t rsa
[k8s@kube-node1 k8s]$ ssh-copy-id root@kube-node1
[k8s@kube-node1 k8s]$ ssh-copy-id root@kube-node2
[k8s@kube-node1 k8s]$ ssh-copy-id root@kube-node3

[k8s@kube-node1 k8s]$ ssh-copy-id k8s@kube-node1
[k8s@kube-node1 k8s]$ ssh-copy-id k8s@kube-node2
[k8s@kube-node1 k8s]$ ssh-copy-id k8s@kube-node3
</code></pre>
<h2 id="将可执行文件路径-opt-k8s-bin-添加到-PATH-变量中">将可执行文件路径 /opt/k8s/bin 添加到 PATH 变量中</h2>
<p>在每台机器上添加环境变量：</p>
<pre><code>$ sudo sh -c &quot;echo 'PATH=/opt/k8s/bin:$PATH:$HOME/bin:$JAVA_HOME/bin' &gt;&gt;/root/.bashrc&quot;
$ echo 'PATH=/opt/k8s/bin:$PATH:$HOME/bin:$JAVA_HOME/bin' &gt;&gt;~/.bashrc
</code></pre>
<h2 id="安装依赖包">安装依赖包</h2>
<p>在每台机器上安装依赖包：</p>
<p>CentOS:</p>
<pre><code>$ sudo yum install -y epel-release
$ sudo yum install -y conntrack ipvsadm ipset jq sysstat curl iptables libseccomp
</code></pre>
<p>Ubuntu:</p>
<pre><code>$ sudo apt-get install -y conntrack ipvsadm ipset jq sysstat curl iptables libseccomp
</code></pre>
<ul>
<li>ipvs 依赖 ipset；</li>
</ul>
<h2 id="关闭防火墙">关闭防火墙</h2>
<p>在每台机器上关闭防火墙：</p>
<pre><code>$ sudo systemctl stop firewalld
$ sudo systemctl disable firewalld
$ sudo iptables -F &amp;&amp; sudo iptables -X &amp;&amp; sudo iptables -F -t nat &amp;&amp; sudo iptables -X -t nat
$ sudo sudo iptables -P FORWARD ACCEPT
</code></pre>
<h2 id="关闭-swap-分区">关闭 swap 分区</h2>
<p>如果开启了 swap 分区，kubelet 会启动失败(可以通过将参数 --fail-swap-on 设置为 false 来忽略 swap on)，故需要在每台机器上关闭 swap 分区：</p>
<pre><code>$ sudo swapoff -a
</code></pre>
<p>为了防止开机自动挂载 swap 分区，可以注释 <code>/etc/fstab</code> 中相应的条目：</p>
<pre><code>$ sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
</code></pre>
<h2 id="关闭-SELinux">关闭 SELinux</h2>
<p>关闭 SELinux，否则后续 K8S 挂载目录时可能报错 <code>Permission denied</code>：</p>
<pre><code>$ sudo setenforce 0
$ grep SELINUX /etc/selinux/config 
SELINUX=disabled
</code></pre>
<ul>
<li>修改配置文件，永久生效；</li>
</ul>
<h2 id="关闭-dnsmasq">关闭 dnsmasq</h2>
<p>linux 系统开启了 dnsmasq 后(如 GUI 环境)，将系统 DNS Server 设置为 127.0.0.1，这会导致 docker 容器无法解析域名，需要关闭它：</p>
<pre><code>$ sudo service dnsmasq stop
$ sudo systemctl disable dnsmasq
</code></pre>
<h2 id="设置系统参数">设置系统参数</h2>
<pre><code>$ cat &gt; kubernetes.conf &lt;&lt;EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
EOF
$ sudo cp kubernetes.conf  /etc/sysctl.d/kubernetes.conf
$ sudo sysctl -p /etc/sysctl.d/kubernetes.conf
$ sudo mount -t cgroup -o cpu,cpuacct none /sys/fs/cgroup/cpu,cpuacct
</code></pre>
<h2 id="加载内核模块">加载内核模块</h2>
<pre><code>$ sudo modprobe br_netfilter
$ sudo modprobe ip_vs
</code></pre>
<h2 id="设置系统时区">设置系统时区</h2>
<pre><code>$ # 调整系统 TimeZone
$ sudo timedatectl set-timezone Asia/Shanghai

$ # 将当前的 UTC 时间写入硬件时钟
$ sudo timedatectl set-local-rtc 0

$ # 重启依赖于系统时间的服务
$ sudo systemctl restart rsyslog 
$ sudo systemctl restart crond
</code></pre>
<h2 id="创建目录">创建目录</h2>
<p>在每台机器上创建目录：</p>
<pre><code>$ sudo mkdir -p /opt/k8s/bin
$ sudo chown -R k8s /opt/k8s

$ sudo sudo mkdir -p /etc/kubernetes/cert
$ sudo chown -R k8s /etc/kubernetes

$ sudo mkdir -p /etc/etcd/cert
$ sudo chown -R k8s /etc/etcd/cert

$ sudo mkdir -p /var/lib/etcd &amp;&amp; chown -R k8s /etc/etcd/cert
</code></pre>
<h2 id="检查系统内核和模块是否适合运行-docker-仅适用于-linux-系统">检查系统内核和模块是否适合运行 docker (仅适用于 linux 系统)</h2>
<pre><code>$ curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh &gt; check-config.sh
$ bash ./check-config.sh
</code></pre>
<h2 id="集群环境变量">集群环境变量</h2>
<p>后续的部署步骤将使用下面定义的全局环境变量，请根据<strong>自己的机器、网络情况</strong>修改：</p>
<pre><code>#!/usr/bin/bash

# 生成 EncryptionConfig 所需的加密 key
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

# 最好使用 当前未用的网段 来定义服务网段和 Pod 网段

# 服务网段，部署前路由不可达，部署后集群内路由可达(kube-proxy 和 ipvs 保证)
SERVICE_CIDR=&quot;10.254.0.0/16&quot;

# Pod 网段，建议 /16 段地址，部署前路由不可达，部署后集群内路由可达(flanneld 保证)
CLUSTER_CIDR=&quot;172.30.0.0/16&quot;

# 服务端口范围 (NodePort Range)
export NODE_PORT_RANGE=&quot;8400-9000&quot;

# 集群各机器 IP 数组
export NODE_IPS=(172.27.129.105 172.27.129.111 172.27.129.112)

# 集群各 IP 对应的 主机名数组
export NODE_NAMES=(kube-node1 kube-node2 kube-node3)

# kube-apiserver 的 VIP（HA 组件 keepalived 发布的 IP）
export MASTER_VIP=172.27.129.253

# kube-apiserver VIP 地址（HA 组件 haproxy 监听 8443 端口）
export KUBE_APISERVER=&quot;https://${MASTER_VIP}:8443&quot;

# HA 节点，VIP 所在的网络接口名称
export VIP_IF=&quot;eth0&quot;

# etcd 集群服务地址列表
export ETCD_ENDPOINTS=&quot;https://172.27.129.105:2379,https://172.27.129.111:2379,https://172.27.129.112:2379&quot;

# etcd 集群间通信的 IP 和端口
export ETCD_NODES=&quot;kube-node1=https://172.27.129.105:2380,kube-node2=https://172.27.129.111:2380,kube-node3=https://172.27.129.112:2380&quot;

# flanneld 网络配置前缀
export FLANNEL_ETCD_PREFIX=&quot;/kubernetes/network&quot;

# kubernetes 服务 IP (一般是 SERVICE_CIDR 中第一个IP)
export CLUSTER_KUBERNETES_SVC_IP=&quot;10.254.0.1&quot;

# 集群 DNS 服务 IP (从 SERVICE_CIDR 中预分配)
export CLUSTER_DNS_SVC_IP=&quot;10.254.0.2&quot;

# 集群 DNS 域名
export CLUSTER_DNS_DOMAIN=&quot;cluster.local.&quot;

# 将二进制目录 /opt/k8s/bin 加到 PATH 中
export PATH=/opt/k8s/bin:$PATH
</code></pre>
<ul>
<li>打包后的变量定义见 <a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/manifests/environment.sh" target="_blank" rel="noopener">environment.sh</a>，后续部署时会<strong>提示导入</strong>该脚本；</li>
</ul>
<h2 id="分发集群环境变量定义脚本">分发集群环境变量定义脚本</h2>
<p>把全局变量定义脚本拷贝到<strong>所有</strong>节点的 <code>/opt/k8s/bin</code> 目录：</p>
<pre><code>source environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp environment.sh k8s@${node_ip}:/opt/k8s/bin/
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h2 id="参考">参考</h2>
<ol>
<li>系统内核相关参数参考：<a href="https://docs.openshift.com/enterprise/3.2/admin_guide/overcommit.html" target="_blank" rel="noopener">https://docs.openshift.com/enterprise/3.2/admin_guide/overcommit.html</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>00.组件版本和配置策略</title>
    <url>/2020/01/07/00.%E7%BB%84%E4%BB%B6%E7%89%88%E6%9C%AC%E5%92%8C%E9%85%8D%E7%BD%AE%E7%AD%96%E7%95%A5/</url>
    <content><![CDATA[<p>tags: version</p>
<h1>00.组件版本和配置策略</h1>
<h2 id="组件版本">组件版本</h2>
<ul>
<li>Kubernetes 1.10.4</li>
<li>Docker 18.03.1-ce</li>
<li>Etcd 3.3.7</li>
<li>Flanneld 0.10.0</li>
<li>插件：
<ul>
<li>Coredns
<ul>
<li>Dashboard</li>
<li>Heapster (influxdb、grafana)</li>
<li>Metrics-Server</li>
<li>EFK (elasticsearch、fluentd、kibana)</li>
</ul>
</li>
</ul>
</li>
<li>镜像仓库：
<ul>
<li>docker registry
<ul>
<li>harbor</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="主要配置策略">主要配置策略</h2>
<p>kube-apiserver：</p>
<ul>
<li>使用 keepalived 和 haproxy 实现 3 节点高可用；</li>
<li>关闭非安全端口 8080 和匿名访问；</li>
<li>在安全端口 6443 接收 https 请求；</li>
<li>严格的认证和授权策略 (x509、token、RBAC)；</li>
<li>开启 bootstrap token 认证，支持 kubelet TLS bootstrapping；</li>
<li>使用 https 访问 kubelet、etcd，加密通信；</li>
</ul>
<p>kube-controller-manager：</p>
<ul>
<li>3 节点高可用；</li>
<li>关闭非安全端口，在安全端口 10252 接收 https 请求；</li>
<li>使用 kubeconfig 访问 apiserver 的安全端口；</li>
<li>自动 approve kubelet 证书签名请求 (CSR)，证书过期后自动轮转；</li>
<li>各 controller 使用自己的 ServiceAccount 访问 apiserver；</li>
</ul>
<p>kube-scheduler：</p>
<ul>
<li>3 节点高可用；</li>
<li>使用 kubeconfig 访问 apiserver 的安全端口；</li>
</ul>
<p>kubelet：</p>
<ul>
<li>使用 kubeadm 动态创建 bootstrap token，而不是在 apiserver 中静态配置；</li>
<li>使用 TLS bootstrap 机制自动生成 client 和 server 证书，过期后自动轮转；</li>
<li>在 KubeletConfiguration 类型的 JSON 文件配置主要参数；</li>
<li>关闭只读端口，在安全端口 10250 接收 https 请求，对请求进行认证和授权，拒绝匿名访问和非授权访问；</li>
<li>使用 kubeconfig 访问 apiserver 的安全端口；</li>
</ul>
<p>kube-proxy：</p>
<ul>
<li>使用 kubeconfig 访问 apiserver 的安全端口；</li>
<li>在 KubeProxyConfiguration 类型的 JSON 文件配置主要参数；</li>
<li>使用 ipvs 代理模式；</li>
</ul>
<p>集群插件：</p>
<ul>
<li>DNS：使用功能、性能更好的 coredns；</li>
<li>Dashboard：支持登录认证；</li>
<li>Metric：heapster、metrics-server，使用 https 访问 kubelet 安全端口；</li>
<li>Log：Elasticsearch、Fluend、Kibana；</li>
<li>Registry 镜像库：docker-registry、harbor；</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>有道markdown离线测试</title>
    <url>/2020/01/07/%E6%9C%89%E9%81%93markdown%E7%A6%BB%E7%BA%BF%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th>header 1</th>
<th>header 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>row 1 col 1</td>
<td>row 1 col 2</td>
</tr>
<tr>
<td>row 2 col 1</td>
<td>row 2 col 2</td>
</tr>
</tbody>
</table>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">E &#x3D; mc^2</span><br></pre></td></tr></table></figure>
<html>
<!--在这里插入内容-->
</html>
<ul>
<li>
<ol>
<li></li>
</ol>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>solr添加ik分词器</title>
    <url>/2020/01/07/solr%E6%B7%BB%E5%8A%A0ik%E5%88%86%E8%AF%8D%E5%99%A8/</url>
    <content><![CDATA[<h3 id="查找solr服务器web地址">查找solr服务器web地址</h3>
<p>find / -name WEB-INF</p>
<p>[root@bigdata-3 lib]# pwd<br>
/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/solr/server/solr-webapp/webapp/WEB-INF/lib</p>
<p>/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/solr/server/solr-webapp/webapp/WEB-INF/lib</p>
<p>/opt/cloudera/parcels/CDH/lib/solr/server/solr-webapp/webapp/WEB-INF/lib</p>
<h3 id="添加ik-jar包到指定位置-并修改权限">添加ik jar包到指定位置 并修改权限</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-rwxr-xr-x 1 root root 1184820 May  7 10:29 ik-analyzer-7.5.0.jar</span><br><span class="line">[root@bigdata-3 lib]#</span><br></pre></td></tr></table></figure>
<h3 id="WEB-INF-创建classes-我们把IKAnalyzer-cfg-xml、stopword-dic拷贝到需要使用分词器的core的conf下面，">WEB-INF 创建classes  我们把IKAnalyzer.cfg.xml、stopword.dic拷贝到需要使用分词器的core的conf下面，</h3>
<p>将resources目录下的5个配置文件放入solr服务的Jetty或Tomcat的webapp/WEB-INF/classes/目录下；</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">① IKAnalyzer.cfg.xml</span><br><span class="line">② ext.dic</span><br><span class="line">③ stopword.dic</span><br><span class="line">④ ik.conf</span><br><span class="line">⑤ dynamicdic.txt</span><br></pre></td></tr></table></figure>
<h3 id="配置文档：">配置文档：</h3>
<p><a href="https://github.com/magese/ik-analyzer-solr7" target="_blank" rel="noopener">https://github.com/magese/ik-analyzer-solr7</a></p>
<h3 id="后面开发人员执行。。。。">后面开发人员执行。。。。</h3>
<p>和core的schema.xml文件一个目录。<br>
修改core的schema.xml:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;fieldType name&#x3D;&quot;text_ik&quot; class&#x3D;&quot;solr.TextField&quot;&gt;   </span><br><span class="line"></span><br><span class="line">        &lt;analyzer type&#x3D;&quot;index&quot; isMaxWordLength&#x3D;&quot;false&quot; class&#x3D;&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;&#x2F;&gt;  </span><br><span class="line"></span><br><span class="line">        &lt;analyzer type&#x3D;&quot;query&quot; isMaxWordLength&#x3D;&quot;true&quot; class&#x3D;&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;&#x2F;&gt;  </span><br><span class="line"></span><br><span class="line">&lt;&#x2F;fieldType&gt;</span><br></pre></td></tr></table></figure>
<h3 id="配置测试字段：">配置测试字段：</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;field name&#x3D;&quot;quesContent&quot; type&#x3D;&quot;text_ik&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure>
<h3 id="参考">参考</h3>
<p><a href="https://my.oschina.net/u/2293326/blog/515883" target="_blank" rel="noopener">https://my.oschina.net/u/2293326/blog/515883</a></p>
<p><a href="https://github.com/magese/ik-analyzer-solr7" target="_blank" rel="noopener">https://github.com/magese/ik-analyzer-solr7</a></p>
<p><a href="https://blog.csdn.net/u011967615/article/details/69400263" target="_blank" rel="noopener">https://blog.csdn.net/u011967615/article/details/69400263</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp cdh85-42:&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;solr&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;lib&#x2F;ik-analyzer-*.jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;solr&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;lib&#x2F;</span><br><span class="line">mkdir &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;solr&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;classes&#x2F;</span><br><span class="line">scp cdh85-42:&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;solr&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;classes&#x2F;* &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;solr&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;classes&#x2F;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>solr  command</title>
    <url>/2020/01/07/solr%20%20command/</url>
    <content><![CDATA[<h3 id="生成本地配置">生成本地配置</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl instancedir --generate $HOME&#x2F;test_collection_config</span><br></pre></td></tr></table></figure>
<h3 id="上传到zk">上传到zk</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl instancedir --create test_collection_config $HOME&#x2F;test_collection_config</span><br></pre></td></tr></table></figure>
<h3 id="创建collection">创建collection</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl collection --create test_collection -s 1 -c test_collection_config</span><br></pre></td></tr></table></figure>
<h3 id="4-post数据">4. post数据</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;share&#x2F;doc&#x2F;solr-doc*&#x2F;example&#x2F;exampledocs</span><br><span class="line">java -Durl&#x3D;http:&#x2F;&#x2F;bigdata-3.baofoo.cn:8983&#x2F;solr&#x2F;test_collection&#x2F;update -jar post.jar *.xml</span><br></pre></td></tr></table></figure>
<h3 id="查看-hdfs-dir">查看 hdfs dir</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -ls -R &#x2F;solr&#x2F;test_co*&#x2F;</span><br><span class="line"> </span><br><span class="line">drwxr-xr-x   - solr solr          0 2019-05-08 22:07 &#x2F;solr&#x2F;test_collection&#x2F;core_node2</span><br><span class="line">drwxr-xr-x   - solr solr          0 2019-05-08 22:07 &#x2F;solr&#x2F;test_collection&#x2F;core_node2&#x2F;data</span><br><span class="line">drwxr-xr-x   - solr solr          0 2019-05-08 22:12 &#x2F;solr&#x2F;test_collection&#x2F;core_node2&#x2F;data&#x2F;index</span><br><span class="line">-rwxr-xr-x   3 solr solr         82 2019-05-08 22:12 &#x2F;solr&#x2F;test_collection&#x2F;core_node2&#x2F;data&#x2F;index&#x2F;_0.dii</span><br></pre></td></tr></table></figure>
<h3 id="查看-zk-dir">查看 zk dir</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;lib&#x2F;zookeeper&#x2F;bin&#x2F;zkCli.sh -server localhost:2181</span><br></pre></td></tr></table></figure>
<h3 id="配置-config">配置 config</h3>
<h4 id="Manager-Configs">Manager Configs</h4>
<p>The solrctl config command syntax is as follows:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl config [--create &lt;name&gt; &lt;baseConfig&gt; [-p &lt;name&gt;&#x3D;&lt;value&gt;]...]</span><br><span class="line">               [--delete &lt;name&gt;]</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>–create <name> <baseConfig>: Creates a new config based on an existing config. The config is created with the specified <name>, using <baseConfig> as the template. For more information about config templates, see Config Templates.</p>
</li>
<li>
<p>-p <name>=<value>: Overrides a <baseConfig> setting. The only config property that you can override is immutable, so the possible options are -p immutable=true and -p immutable=false. If you are copying an immutable config, such as a template, use -p immutable=false to make sure that you can edit the new config.</p>
</li>
<li>
<p>–delete <name>: Deletes the specified config. You cannot delete an immutable config without accessing ZooKeeper directly as the solr super user.</p>
</li>
<li>
<p>example:</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl config --create newConfig test_collection_config -p immutable&#x3D;false</span><br></pre></td></tr></table></figure>
<h4 id="Managing-Instance-Directories">Managing Instance Directories</h4>
<ul>
<li>An instance directory is a named set of configuration files. You can generate an instance directory template locally, edit the configuration, and then upload the directory to ZooKeeper as a named configuration set. You can then reference this named configuration set when creating a collection.</li>
</ul>
<p>The solrctl instancedir command syntax is as follows:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl instancedir [--generate &lt;path&gt; [-schemaless]]</span><br><span class="line">                    [--create &lt;name&gt; &lt;path&gt;]</span><br><span class="line">                    [--update &lt;name&gt; &lt;path&gt;]</span><br><span class="line">                    [--get &lt;name&gt; &lt;path&gt;]</span><br><span class="line">                    [--delete &lt;name&gt;]</span><br><span class="line">                    [--list]</span><br></pre></td></tr></table></figure>
<ul>
<li>–generate <path>: Generates an instance directory template on the local filesystem at <path>. The configuration files are located in the conf subdirectory under <path>.</li>
<li>-schemaless: Generates a schemaless instance directory template. For more information on schemaless support, see Schemaless Mode Overview and Best Practices.</li>
<li>–create <name> <path>: Uploads a copy of the instance directory from <path> on the local filesystem to ZooKeeper. If an instance directory with the specified <name> already exists, this command fails. Use --update to modify existing instance directories.</li>
<li>–update <name> <path>: Overwrites an existing instance directory in ZooKeeper using the specified files on the local filesystem. This command is analogous to first running --delete <name> followed by --create <name> <path>.</li>
<li>–get <name> <path>: Downloads the specified instance directory from ZooKeeper to the specified path on the local filesystem. You can then edit the configuration and then re-upload it using --update.</li>
<li>–delete <name>: Deletes the specified instance directory from ZooKeeper.</li>
<li>–list: Lists existing instance directories as well as configs created by the solrctl config command.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl instancedir --get test_collection_config  &#x2F;tmp&#x2F;test_collection_config</span><br><span class="line"></span><br><span class="line"> cd &#x2F;tmp&#x2F;test_collection_config&#x2F;conf&#x2F;</span><br><span class="line">vim managed-schema</span><br><span class="line"></span><br><span class="line">solrctl instancedir --update test_collection_config  &#x2F;tmp&#x2F;test_collection_config</span><br></pre></td></tr></table></figure>
<h3 id="schema-api">schema api</h3>
<ul>
<li>Add a New Field</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST -H &#39;Content-type:application&#x2F;json&#39; --data-binary &#39;&#123;</span><br><span class="line">  &quot;add-field&quot;:&#123;</span><br><span class="line">     &quot;name&quot;:&quot;sell-by&quot;,</span><br><span class="line">     &quot;type&quot;:&quot;pdate&quot;,</span><br><span class="line">     &quot;stored&quot;:true &#125;</span><br><span class="line">&#125;&#39; http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br></pre></td></tr></table></figure>
<ul>
<li>Delete a Field</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST -H &#39;Content-type:application&#x2F;json&#39; --data-binary &#39;&#123;</span><br><span class="line">  &quot;delete-field&quot; : &#123; &quot;name&quot;:&quot;sell-by&quot; &#125;</span><br><span class="line">&#125;&#39; http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br></pre></td></tr></table></figure>
<ul>
<li>Replace a Field</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST -H &#39;Content-type:application&#x2F;json&#39; --data-binary &#39;&#123;</span><br><span class="line">  &quot;replace-field&quot;:&#123;</span><br><span class="line">     &quot;name&quot;:&quot;sell-by&quot;,</span><br><span class="line">     &quot;type&quot;:&quot;date&quot;,</span><br><span class="line">     &quot;stored&quot;:false &#125;</span><br><span class="line">&#125;&#39; http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br></pre></td></tr></table></figure>
<ul>
<li>Add a Dynamic Field Rule</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST -H &#39;Content-type:application&#x2F;json&#39; --data-binary &#39;&#123;</span><br><span class="line">  &quot;add-dynamic-field&quot;:&#123;</span><br><span class="line">     &quot;name&quot;:&quot;*_s&quot;,</span><br><span class="line">     &quot;type&quot;:&quot;string&quot;,</span><br><span class="line">     &quot;stored&quot;:true &#125;</span><br><span class="line">&#125;&#39; http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br></pre></td></tr></table></figure>
<ul>
<li>Multiple Commands in a Single POST</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST -H &#39;Content-type:application&#x2F;json&#39; --data-binary &#39;&#123;</span><br><span class="line">  &quot;add-field-type&quot;:&#123;</span><br><span class="line">     &quot;name&quot;:&quot;myNewTxtField&quot;,</span><br><span class="line">     &quot;class&quot;:&quot;solr.TextField&quot;,</span><br><span class="line">     &quot;positionIncrementGap&quot;:&quot;100&quot;,</span><br><span class="line">     &quot;analyzer&quot;:&#123;</span><br><span class="line">        &quot;charFilters&quot;:[&#123;</span><br><span class="line">           &quot;class&quot;:&quot;solr.PatternReplaceCharFilterFactory&quot;,</span><br><span class="line">           &quot;replacement&quot;:&quot;$1$1&quot;,</span><br><span class="line">           &quot;pattern&quot;:&quot;([a-zA-Z])\\\\1+&quot; &#125;],</span><br><span class="line">        &quot;tokenizer&quot;:&#123;</span><br><span class="line">           &quot;class&quot;:&quot;solr.WhitespaceTokenizerFactory&quot; &#125;,</span><br><span class="line">        &quot;filters&quot;:[&#123;</span><br><span class="line">           &quot;class&quot;:&quot;solr.WordDelimiterFilterFactory&quot;,</span><br><span class="line">           &quot;preserveOriginal&quot;:&quot;0&quot; &#125;]&#125;&#125;,</span><br><span class="line">   &quot;add-field&quot; : &#123;</span><br><span class="line">      &quot;name&quot;:&quot;sell-by&quot;,</span><br><span class="line">      &quot;type&quot;:&quot;myNewTxtField&quot;,</span><br><span class="line">      &quot;stored&quot;:true &#125;</span><br><span class="line">&#125;&#39; http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br></pre></td></tr></table></figure>
<ul>
<li>Get the entire schema in JSON.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;responseHeader&quot;:&#123;</span><br><span class="line">    &quot;status&quot;:0,</span><br><span class="line">    &quot;QTime&quot;:5&#125;,</span><br><span class="line">  &quot;schema&quot;:&#123;</span><br><span class="line">    &quot;name&quot;:&quot;example&quot;,</span><br><span class="line">    &quot;version&quot;:1.5,</span><br><span class="line">    &quot;uniqueKey&quot;:&quot;id&quot;,</span><br><span class="line">    &quot;fieldTypes&quot;:[&#123;</span><br><span class="line">        &quot;name&quot;:&quot;alphaOnlySort&quot;,</span><br><span class="line">        &quot;class&quot;:&quot;solr.TextField&quot;,</span><br><span class="line">        &quot;sortMissingLast&quot;:true,</span><br><span class="line">        &quot;omitNorms&quot;:true,</span><br><span class="line">        &quot;analyzer&quot;:&#123;</span><br><span class="line">          &quot;tokenizer&quot;:&#123;</span><br><span class="line">            &quot;class&quot;:&quot;solr.KeywordTokenizerFactory&quot;&#125;,</span><br><span class="line">          &quot;filters&quot;:[&#123;</span><br><span class="line">              &quot;class&quot;:&quot;solr.LowerCaseFilterFactory&quot;&#125;,</span><br><span class="line">            &#123;</span><br><span class="line">              &quot;class&quot;:&quot;solr.TrimFilterFactory&quot;&#125;,</span><br><span class="line">            &#123;</span><br><span class="line">              &quot;class&quot;:&quot;solr.PatternReplaceFilterFactory&quot;,</span><br><span class="line">              &quot;replace&quot;:&quot;all&quot;,</span><br><span class="line">              &quot;replacement&quot;:&quot;&quot;,</span><br><span class="line">              &quot;pattern&quot;:&quot;([^a-z])&quot;&#125;]&#125;&#125;],</span><br><span class="line">    &quot;fields&quot;:[&#123;</span><br><span class="line">        &quot;name&quot;:&quot;_version_&quot;,</span><br><span class="line">        &quot;type&quot;:&quot;long&quot;,</span><br><span class="line">        &quot;indexed&quot;:true,</span><br><span class="line">        &quot;stored&quot;:true&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;name&quot;:&quot;author&quot;,</span><br><span class="line">        &quot;type&quot;:&quot;text_general&quot;,</span><br><span class="line">        &quot;indexed&quot;:true,</span><br><span class="line">        &quot;stored&quot;:true&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;name&quot;:&quot;cat&quot;,</span><br><span class="line">        &quot;type&quot;:&quot;string&quot;,</span><br><span class="line">        &quot;multiValued&quot;:true,</span><br><span class="line">        &quot;indexed&quot;:true,</span><br><span class="line">        &quot;stored&quot;:true&#125;],</span><br><span class="line">    &quot;copyFields&quot;:[&#123;</span><br><span class="line">        &quot;source&quot;:&quot;author&quot;,</span><br><span class="line">        &quot;dest&quot;:&quot;text&quot;&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;source&quot;:&quot;cat&quot;,</span><br><span class="line">        &quot;dest&quot;:&quot;text&quot;&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;source&quot;:&quot;content&quot;,</span><br><span class="line">        &quot;dest&quot;:&quot;text&quot;&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;source&quot;:&quot;author&quot;,</span><br><span class="line">        &quot;dest&quot;:&quot;author_s&quot;&#125;]&#125;&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema?wt&#x3D;xml</span><br><span class="line"></span><br><span class="line">&lt;response&gt;</span><br><span class="line">&lt;lst name&#x3D;&quot;responseHeader&quot;&gt;</span><br><span class="line">  &lt;int name&#x3D;&quot;status&quot;&gt;0&lt;&#x2F;int&gt;</span><br><span class="line">  &lt;int name&#x3D;&quot;QTime&quot;&gt;5&lt;&#x2F;int&gt;</span><br><span class="line">&lt;&#x2F;lst&gt;</span><br><span class="line">&lt;lst name&#x3D;&quot;schema&quot;&gt;</span><br><span class="line">  &lt;str name&#x3D;&quot;name&quot;&gt;example&lt;&#x2F;str&gt;</span><br><span class="line">  &lt;float name&#x3D;&quot;version&quot;&gt;1.5&lt;&#x2F;float&gt;</span><br><span class="line">  &lt;str name&#x3D;&quot;uniqueKey&quot;&gt;id&lt;&#x2F;str&gt;</span><br><span class="line">  &lt;arr name&#x3D;&quot;fieldTypes&quot;&gt;</span><br><span class="line">    &lt;lst&gt;</span><br><span class="line">      &lt;str name&#x3D;&quot;name&quot;&gt;alphaOnlySort&lt;&#x2F;str&gt;</span><br><span class="line">      &lt;str name&#x3D;&quot;class&quot;&gt;solr.TextField&lt;&#x2F;str&gt;</span><br><span class="line">      &lt;bool name&#x3D;&quot;sortMissingLast&quot;&gt;true&lt;&#x2F;bool&gt;</span><br><span class="line">      &lt;bool name&#x3D;&quot;omitNorms&quot;&gt;true&lt;&#x2F;bool&gt;</span><br><span class="line">      &lt;lst name&#x3D;&quot;analyzer&quot;&gt;</span><br><span class="line">        &lt;lst name&#x3D;&quot;tokenizer&quot;&gt;</span><br><span class="line">          &lt;str name&#x3D;&quot;class&quot;&gt;solr.KeywordTokenizerFactory&lt;&#x2F;str&gt;</span><br><span class="line">        &lt;&#x2F;lst&gt;</span><br><span class="line">        &lt;arr name&#x3D;&quot;filters&quot;&gt;</span><br><span class="line">          &lt;lst&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;class&quot;&gt;solr.LowerCaseFilterFactory&lt;&#x2F;str&gt;</span><br><span class="line">          &lt;&#x2F;lst&gt;</span><br><span class="line">          &lt;lst&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;class&quot;&gt;solr.TrimFilterFactory&lt;&#x2F;str&gt;</span><br><span class="line">          &lt;&#x2F;lst&gt;</span><br><span class="line">          &lt;lst&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;class&quot;&gt;solr.PatternReplaceFilterFactory&lt;&#x2F;str&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;replace&quot;&gt;all&lt;&#x2F;str&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;replacement&quot;&#x2F;&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;pattern&quot;&gt;([^a-z])&lt;&#x2F;str&gt;</span><br><span class="line">          &lt;&#x2F;lst&gt;</span><br><span class="line">        &lt;&#x2F;arr&gt;</span><br><span class="line">      &lt;&#x2F;lst&gt;</span><br><span class="line">    &lt;&#x2F;lst&gt;</span><br><span class="line">...</span><br><span class="line">    &lt;lst&gt;</span><br><span class="line">      &lt;str name&#x3D;&quot;source&quot;&gt;author&lt;&#x2F;str&gt;</span><br><span class="line">      &lt;str name&#x3D;&quot;dest&quot;&gt;author_s&lt;&#x2F;str&gt;</span><br><span class="line">    &lt;&#x2F;lst&gt;</span><br><span class="line">  &lt;&#x2F;arr&gt;</span><br><span class="line">&lt;&#x2F;lst&gt;</span><br><span class="line">&lt;&#x2F;response&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema?wt&#x3D;schema.xml</span><br><span class="line"></span><br><span class="line">&lt;schema name&#x3D;&quot;example&quot; version&#x3D;&quot;1.5&quot;&gt;</span><br><span class="line">  &lt;uniqueKey&gt;id&lt;&#x2F;uniqueKey&gt;</span><br><span class="line">  &lt;types&gt;</span><br><span class="line">    &lt;fieldType name&#x3D;&quot;alphaOnlySort&quot; class&#x3D;&quot;solr.TextField&quot; sortMissingLast&#x3D;&quot;true&quot; omitNorms&#x3D;&quot;true&quot;&gt;</span><br><span class="line">      &lt;analyzer&gt;</span><br><span class="line">        &lt;tokenizer class&#x3D;&quot;solr.KeywordTokenizerFactory&quot;&#x2F;&gt;</span><br><span class="line">        &lt;filter class&#x3D;&quot;solr.LowerCaseFilterFactory&quot;&#x2F;&gt;</span><br><span class="line">        &lt;filter class&#x3D;&quot;solr.TrimFilterFactory&quot;&#x2F;&gt;</span><br><span class="line">        &lt;filter class&#x3D;&quot;solr.PatternReplaceFilterFactory&quot; replace&#x3D;&quot;all&quot; replacement&#x3D;&quot;&quot; pattern&#x3D;&quot;([^a-z])&quot;&#x2F;&gt;</span><br><span class="line">      &lt;&#x2F;analyzer&gt;</span><br><span class="line">    &lt;&#x2F;fieldType&gt;</span><br><span class="line">...</span><br><span class="line">  &lt;copyField source&#x3D;&quot;url&quot; dest&#x3D;&quot;text&quot;&#x2F;&gt;</span><br><span class="line">  &lt;copyField source&#x3D;&quot;price&quot; dest&#x3D;&quot;price_c&quot;&#x2F;&gt;</span><br><span class="line">  &lt;copyField source&#x3D;&quot;author&quot; dest&#x3D;&quot;author_s&quot;&#x2F;&gt;</span><br><span class="line">&lt;&#x2F;schema&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>Get a list of all fields.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema&#x2F;fields</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;fields&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;indexed&quot;: true,</span><br><span class="line">            &quot;name&quot;: &quot;_version_&quot;,</span><br><span class="line">            &quot;stored&quot;: true,</span><br><span class="line">            &quot;type&quot;: &quot;long&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;indexed&quot;: true,</span><br><span class="line">            &quot;name&quot;: &quot;author&quot;,</span><br><span class="line">            &quot;stored&quot;: true,</span><br><span class="line">            &quot;type&quot;: &quot;text_general&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;indexed&quot;: true,</span><br><span class="line">            &quot;multiValued&quot;: true,</span><br><span class="line">            &quot;name&quot;: &quot;cat&quot;,</span><br><span class="line">            &quot;stored&quot;: true,</span><br><span class="line">            &quot;type&quot;: &quot;string&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">&quot;...&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;responseHeader&quot;: &#123;</span><br><span class="line">        &quot;QTime&quot;: 1,</span><br><span class="line">        &quot;status&quot;: 0</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Get a list of all dynamic field declarations:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema&#x2F;dynamicfields</span><br></pre></td></tr></table></figure>
<ul>
<li>Get a list of all field types.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema&#x2F;fieldtypes</span><br></pre></td></tr></table></figure>
<h3 id="引用参考">引用参考</h3>
<p><a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/search_validate_deploy_solr_rest_api.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/6.0/topics/search_validate_deploy_solr_rest_api.html</a></p>
<p><a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/search_configuration.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/6.0/topics/search_configuration.html</a></p>
<p><a href="https://lucene.apache.org/solr/guide/7_0/schema-api.html" target="_blank" rel="noopener">https://lucene.apache.org/solr/guide/7_0/schema-api.html</a></p>
<h3 id="管理页面">管理页面</h3>
<p><a href="http://bigdata-3.baofoo.cn:8983/solr/#/~collections" target="_blank" rel="noopener">http://bigdata-3.baofoo.cn:8983/solr/#/~collections</a></p>
<p><a href="http://bigdata-3.baofoo.cn:8889/hue/dashboard/browse/test_collection" target="_blank" rel="noopener">http://bigdata-3.baofoo.cn:8889/hue/dashboard/browse/test_collection</a></p>
]]></content>
  </entry>
  <entry>
    <title>Linux中文编码转换</title>
    <url>/2020/01/07/Linux%E4%B8%AD%E6%96%87%E7%BC%96%E7%A0%81%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[<p>linux shell 配置文件中默认的字符集编码为UTF－8</p>
<p>accii 文件显示中文乱码</p>
<p>用iconv进行转换就可以了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">iconv -f GBK -t UTF-8 1.csv &gt; 3.csv</span><br></pre></td></tr></table></figure>
<p>查了下iconv命令用法如下：</p>
<p>iconv [选项…] [文件…]</p>
<p>有如下选项可用:</p>
<p>输入/输出格式规范：<br>
-f, --from-code=名称 原始文本编码<br>
-t, --to-code=名称 输出编码</p>
<p>信息：<br>
-l, --list 列举所有已知的字符集</p>
<p>输出控制：<br>
-c 从输出中忽略无效的字符<br>
-o, --output=FILE 输出文件<br>
-s, --silent 关闭警告<br>
–verbose 打印进度信息</p>
<p>iconv -f utf-8 -t gb2312 /server_test/reports/software_.txt &gt; /server_test/reports/software_asserts.txt</p>
<p>中文字符集编码有 gb2312 , cp936 ,GBK，GB18030</p>
]]></content>
  </entry>
  <entry>
    <title>JanusGraph Server搭建 hbase+ solr</title>
    <url>/2020/01/07/JanusGraph%20Server%E6%90%AD%E5%BB%BA%20hbase+%20solr/</url>
    <content><![CDATA[<p>JanusGraph Server搭建 hbase+ solr</p>
<p><a href="https://blog.csdn.net/goandozhf/article/details/80105895#2068-1524796329245" target="_blank" rel="noopener">https://blog.csdn.net/goandozhf/article/details/80105895#2068-1524796329245</a></p>
<p>创建janusgraph-hbase-solr-server.properties文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gremlin.graph&#x3D;org.janusgraph.core.JanusGraphFactory</span><br><span class="line">storage.backend&#x3D;hbase</span><br><span class="line">storage.batch-loading&#x3D;true</span><br><span class="line">storage.hbase.table &#x3D; janusgraph-test</span><br><span class="line">storage.hostname&#x3D;172.20.85.111</span><br><span class="line">cache.db-cache &#x3D; true</span><br><span class="line">cache.db-cache-clean-wait &#x3D; 20</span><br><span class="line">cache.db-cache-time &#x3D; 180000</span><br><span class="line">cache.db-cache-size &#x3D; 0.5</span><br><span class="line">ids.block-size&#x3D;100000000</span><br><span class="line">storage.buffer-size&#x3D;102400</span><br><span class="line">storage.hbase.region-count &#x3D; 15</span><br><span class="line">index.search.backend&#x3D;solr</span><br><span class="line">index.search.solr.mode&#x3D;http</span><br><span class="line">index.search.solr.http-urls&#x3D;http:&#x2F;&#x2F;172.20.85.111:8983&#x2F;solr</span><br><span class="line">index.search.hostname&#x3D;172.20.85.111</span><br><span class="line">index.search.index-name&#x3D;janusgraph-test</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>HBase 工具和实用程序</title>
    <url>/2020/01/07/HBase%20%E5%B7%A5%E5%85%B7%E5%92%8C%E5%AE%9E%E7%94%A8%E7%A8%8B%E5%BA%8F/</url>
    <content><![CDATA[<h3 id="hbase-pe">hbase pe</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ bin&#x2F;hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows&#x3D;10 --nomapred increment 10</span><br></pre></td></tr></table></figure>
<h3 id="hbase-ltt">hbase ltt</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.util.LoadTestTool -compression NONE -write 8:8 -num_keys 1048576</span><br><span class="line"></span><br><span class="line">hbase ltt -compression NONE -write 8:8 -num_keys 1048576</span><br></pre></td></tr></table></figure>
<h3 id="hbase-canary">hbase canary</h3>
<p>Canary 工具可以帮助用户“测试”HBase 集群状态</p>
<p><strong>测试每个表的每个区域的每个列族</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase canary</span><br></pre></td></tr></table></figure>
<p><strong>对特定表格的每个区域的每个列族进行 Canary 测试</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase canary test-01 test-02</span><br></pre></td></tr></table></figure>
<h3 id="CompleteBulkLoad">CompleteBulkLoad</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles &lt;hdfs:&#x2F;&#x2F;storefileoutput&gt; &lt;tablename&gt;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>distcp 文件迁移</title>
    <url>/2020/01/07/distcp%20%E6%96%87%E4%BB%B6%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line">for DB in `cat db_name.txt`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"> hadoop distcp -D mapreduce.job.queuename=bf_yarn_pool.production -D ipc.client.fallback-<span class="keyword">to</span>-simple-auth-allowed=<span class="literal">true</span> -i -overwrite hdfs://<span class="number">192.168</span><span class="number">.81</span><span class="number">.30</span>:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/$DB.db hdfs://<span class="number">172.20</span><span class="number">.85</span><span class="number">.39</span>:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/$DB.db</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop distcp \</span><br><span class="line">-Dmapred.jobtracker.maxtasks.per.job&#x3D;1800000 \   #任务最大map数（数据分成多map任务）</span><br><span class="line">-Dmapred.job.max.map.running&#x3D;4000 \              #最大map并发</span><br><span class="line">-Ddistcp.bandwidth&#x3D;150000000 \                   #带宽</span><br><span class="line">-Ddfs.replication&#x3D;2 \                            #复制因子，两副本</span><br><span class="line">-Ddistcp.skip.dir&#x3D;$skipPath \                    #过滤的目录（不拷贝的目录）</span><br><span class="line">-Dmapred.map.max.attempts&#x3D;9 \                    #每个task最大尝试次数</span><br><span class="line">-Dmapred.fairscheduler.pool&#x3D;distcp \             #指定任务运行的pool</span><br><span class="line">-pugp \                                          #保留属性（用户，组，权限）</span><br><span class="line">-i \                                             #忽略失败的task</span><br><span class="line">-skipcrccheck \                                  #忽略CRC校验（防止源，目标集群hdfs版本不一致导致任务失败。）</span><br><span class="line">hdfs:&#x2F;&#x2F;clusterA:9000&#x2F;AAA&#x2F;data  \                 #源地址</span><br><span class="line">hdfs:&#x2F;&#x2F;clusterB:9000&#x2F;BBB&#x2F;data                    #目标地址</span><br></pre></td></tr></table></figure>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/xy_app_spark/tables/fi_gw_express_order_idcard1_encrypt/pk_year=2018/pk_month=2018-10 hdfs://172.20.85.39:8020/user/hive/warehouse/credit_mining.db/fi_gw_express_order_idcard1_encrypt/pk_year=2018/pk_month=2018-10</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/xy_app_spark/tables/fi_gw_express_order_idcard1_encrypt/pk_year=2018/pk_month=2018-11 hdfs://172.20.85.39:8020/user/hive/warehouse/credit_mining.db/fi_gw_express_order_idcard1_encrypt/pk_year=2018/pk_month=2018-11</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/xy_app_spark/tables/fo_payment_encrypt/pk_year=2018/pk_month=2018-10 hdfs://172.20.85.39:8020/user/hive/warehouse/credit_mining.db/fo_payment_encrypt/pk_year=2018/pk_month=2018-10</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/xy_app_spark/tables/fo_payment_encrypt/pk_year=2018/pk_month=2018-11 hdfs://172.20.85.39:8020/user/hive/warehouse/credit_mining.db/fo_payment_encrypt/pk_year=2018/pk_month=2018-11</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020//user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time_v2_encrypt hdfs://172.20.85.39:8020/user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time_v2_encrypt</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i hdfs://192.168.81.30:8020/user/hive/warehouse/xy_ods_db.db/credit_logprocessor_rocord hdfs://172.20.85.39:8020/user/hive/warehouse/xy_ods_db.db/credit_logprocessor_rocord</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/hive/warehouse/xy_ods_db.db/credit_logprocessor_rocord/pk_day=2018-11-11 hdfs://172.20.85.39:8020/user/hive/warehouse/xy_ods_db.db/credit_logprocessor_rocord/pk_day=2018-11-11</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/hive/warehouse/xy_ods.db/ods_verification_cardno_d_incr/pk_year=2017 hdfs://172.20.85.39:8020/user/hive/warehouse/xy_ods.db/ods_verification_cardno_d_incr/pk_year=2017</p>
<p>hadoop distcp -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -update -skipcrccheck hdfs://192.168.81.30:8020/user/hive/warehouse/xy_ods.db/ods_verification_cardno_d_incr/pk_year=2017/pk_month=2017-07/pk_day=2017-07-23/000011_0 hdfs://172.20.85.39:8020/user/hive/warehouse/xy_ods.db/ods_verification_cardno_d_incr/pk_year=2017/pk_month=2017-07/pk_day=2017-07-23/000011_0</p>
<p>sudo -u xy_app_spark hadoop distcp -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -update -skipcrccheck hdfs://172.20.85.29:8020/user/xy_app_spark/bulkload/NORMAL/xy_app_spark-image_current_report hdfs://172.20.85.59:8020/user/xy_app_spark/bulkload/NORMAL/xy_app_spark-image_current_report</p>
<p>sudo -u hdfs hdfs ec -getPolicy -path /user</p>
<p>sudo -u xy_app_spark hadoop distcp -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -update  hdfs://172.20.85.29:8020/user/xy_app_spark/bulkload/NORMAL/xy_app_spark-image_current_report hdfs://172.20.85.59:8020/user/xy_app_spark/bulkload/NORMAL/xy_app_spark-image_current_report</p>
<p>hadoop fs -get /user/hive/warehouse/baofoo_cutpayment.db/protocol_payment_order /home/yarn/protocol_payment_order<br>
hadoop fs -get /user/hive/warehouse/baofoo_cutpayment.db/protocol_payment_business_order /home/yarn/protocol_payment_business_order</p>
<p>baofoo_cutpayment.protocol_payment_order<br>
baofoo_cutpayment.protocol_payment_business_order</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true  hdfs://192.168.81.30:8020/user/hue/oozie/workspaces/ hdfs://172.20.85.39:8020/user/hue/oozie/workspaces/</p>
<p>Bad status for request TOpenSessionReq(username=‘hue’, password=None, client_protocol=6, configuration={‘idle_session_timeout’: ‘900’, ‘impala.doas.user’: u’hue’}): TOpenSessionResp(status=TStatus(errorCode=None, errorMessage=“User ‘yarn’ is not authorized to delegate to ‘hue’.\n”, sqlState=‘HY000’, infoMessages=None, statusCode=3), sessionHandle=TSessionHandle(sessionId=THandleIdentifier(secret=’\xdcs{\xf9A\x12N\xb1\x97\x18\xf4\xbb\xc8\x90#\xa7’, guid=’\x12\x13\x8b\xcd\xa1\xdfA\x07\x9c\xf9\x16i\x97\rU9’)), configuration=None, serverProtocolVersion=5)</p>
<p>/user/xy_app_spark/tables/fo_payment_encrypt<br>
/user/xy_app_spark/tables/t_serve_business_order_real_time_encrypt<br>
/user/xy_app_spark/tables/fi_gw_agrt_express_order_encrypt<br>
/user/xy_app_spark/tables/fi_gw_express_order_idcard1_encrypt</p>
<p>ifactive=<code>sudo -u hdfs hdfs haadmin -getServiceState namenode402</code><br>
echo $ifactive<br>
if [[ $ifactive =~ “active” ]]; then<br>
  nameservice=172.20.85.29<br>
else<br>
nameservice=172.20.85.39<br>
fi<br>
echo $nameservice</p>
<p>hdfs://ns1/user/hive/warehouse/credit_dfp.db/t_device_access_d_incr</p>
<p>hdfs://ns1/user/hive/warehouse/credit_dfp.db/t_device_query_d_incr</p>
<p>hdfs://ns1/user/hive/warehouse/credit_dfp.db/t_device_access_d_incr</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time/pk_year=2019 hdfs://172.20.85.29:8020/user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time/pk_year=2019</p>
<p>旧集群/user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time/pk_year=2018/pk_month=2018-12<br>
到新集群 旧集群/user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time/pk_year=2019/pk_month=2018-12</p>
<p>hadoop distcp /user/hive/warehouse/sample_08 /user/hive/warehouse/t3</p>
<p>/etc/init.d/mysql</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/hive/warehouse/xy_app_spark.db/snapshot/current/nono/washer_all/step4 hdfs://172.20.85.29:8020/user/hive/warehouse/xy_app_spark.db/snapshot/current/nono/washer_all/step4</p>
<p>#!/bin/bash<br>
for tb in <code>cat tb_name.txt</code><br>
do<br>
hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/hive/warehouse/xy_wulichuang.db/$tb hdfs://172.20.85.29:8020/user/hive/warehouse/xy_wulichuang.db/$tb<br>
done</p>
]]></content>
  </entry>
  <entry>
    <title>cdh85-42 内存分析 cache 占用过高 434G 改后 变为 384G</title>
    <url>/2020/01/07/cdh85-42%20%E5%86%85%E5%AD%98%E5%88%86%E6%9E%90%20cache%20%E5%8D%A0%E7%94%A8%E8%BF%87%E9%AB%98%20434G%20%E6%94%B9%E5%90%8E%20%E5%8F%98%E4%B8%BA%20384G/</url>
    <content><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[root@cdh85-42 ~]# free -h</span><br><span class="line">              total        used        free      shared  buff&#x2F;cache   available</span><br><span class="line">Mem:           502G         34G        1.5G        4.2G        467G        461G</span><br><span class="line">Swap:          4.0G          0B        4.0G</span><br><span class="line">[root@cdh85-42 ~]# </span><br><span class="line">[root@cdh85-42 ~]# </span><br><span class="line">[root@cdh85-42 ~]# cat &#x2F;proc&#x2F;meminfo </span><br><span class="line">MemTotal:       527318720 kB</span><br><span class="line">MemFree:         1513916 kB</span><br><span class="line">MemAvailable:   483579148 kB</span><br><span class="line">Buffers:              28 kB</span><br><span class="line">Cached:         472560904 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:         249755248 kB</span><br><span class="line">Inactive:       254111684 kB</span><br><span class="line">Active(anon):   25622464 kB</span><br><span class="line">Inactive(anon): 10038512 kB</span><br><span class="line">Active(file):   224132784 kB</span><br><span class="line">Inactive(file): 244073172 kB</span><br><span class="line">Unevictable:           0 kB</span><br><span class="line">Mlocked:               0 kB</span><br><span class="line">SwapTotal:       4194300 kB</span><br><span class="line">SwapFree:        4194300 kB</span><br><span class="line">Dirty:              2216 kB</span><br><span class="line">Writeback:            64 kB</span><br><span class="line">AnonPages:      31309320 kB</span><br><span class="line">Mapped:           113004 kB</span><br><span class="line">Shmem:           4352956 kB</span><br><span class="line">Slab:           17466532 kB</span><br><span class="line">SReclaimable:   15190848 kB</span><br><span class="line">SUnreclaim:      2275684 kB</span><br><span class="line">KernelStack:       38048 kB</span><br><span class="line">PageTables:       574580 kB</span><br><span class="line">NFS_Unstable:          0 kB</span><br><span class="line">Bounce:                0 kB</span><br><span class="line">WritebackTmp:          0 kB</span><br><span class="line">CommitLimit:    267853660 kB</span><br><span class="line">Committed_AS:   313602836 kB</span><br><span class="line">VmallocTotal:   34359738367 kB</span><br><span class="line">VmallocUsed:     1170752 kB</span><br><span class="line">VmallocChunk:   34089979900 kB</span><br><span class="line">HardwareCorrupted:     0 kB</span><br><span class="line">AnonHugePages:      6144 kB</span><br><span class="line">CmaTotal:              0 kB</span><br><span class="line">CmaFree:               0 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">DirectMap4k:      372288 kB</span><br><span class="line">DirectMap2M:    32319488 kB</span><br><span class="line">DirectMap1G:    505413632 kB</span><br><span class="line">[root@cdh85-42 ~]#</span><br></pre></td></tr></table></figure>
<p><a href="https://fivezh.github.io/2017/06/18/centos-7-memory-available/" target="_blank" rel="noopener">https://fivezh.github.io/2017/06/18/centos-7-memory-available/</a></p>
<p><a href="https://blog.csdn.net/starshine/article/details/7434942" target="_blank" rel="noopener">https://blog.csdn.net/starshine/article/details/7434942</a></p>
<h3 id="centos-6-7-linux-初始化脚本">centos 6 7 linux 初始化脚本</h3>
<p><a href="https://blog.51cto.com/12445535/2362407" target="_blank" rel="noopener">https://blog.51cto.com/12445535/2362407</a></p>
<h4 id="min-free-kbytes-调整-为-50G">min_free_kbytes 调整 为 50G</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes</span><br><span class="line">cat &#x2F;etc&#x2F;sysctl.conf # add vm.min_free_kbytes &#x3D; 52428800</span><br><span class="line">sysctl -p</span><br><span class="line">cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@cdh85-42 ~]# vim &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">## add</span><br><span class="line">vm.min_free_kbytes &#x3D; 52428800</span><br><span class="line">## add</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@cdh85-42 ~]# cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes</span><br><span class="line">90112</span><br><span class="line"></span><br><span class="line">[root@cdh85-42 ~]# sysctl -p</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_tw_reuse &#x3D; 1</span><br><span class="line">net.ipv4.tcp_tw_recycle &#x3D; 1</span><br><span class="line">net.ipv4.tcp_keepalive_time &#x3D; 1200</span><br><span class="line">net.ipv4.ip_local_port_range &#x3D; 10000 65000</span><br><span class="line">net.ipv4.tcp_max_syn_backlog &#x3D; 8192</span><br><span class="line">net.ipv4.tcp_max_tw_buckets &#x3D; 5000</span><br><span class="line">fs.file-max &#x3D; 655350</span><br><span class="line">net.ipv4.route.gc_timeout &#x3D; 100</span><br><span class="line">net.ipv4.tcp_syn_retries &#x3D; 1</span><br><span class="line">net.ipv4.tcp_synack_retries &#x3D; 1</span><br><span class="line">net.core.netdev_max_backlog &#x3D; 16384</span><br><span class="line">net.ipv4.tcp_max_orphans &#x3D; 16384</span><br><span class="line">net.ipv4.tcp_fin_timeout &#x3D; 2</span><br><span class="line">net.core.somaxconn &#x3D; 32768</span><br><span class="line">kernel.threads-max &#x3D; 196605</span><br><span class="line">kernel.pid_max &#x3D; 196605</span><br><span class="line">vm.max_map_count &#x3D; 393210</span><br><span class="line">vm.swappiness &#x3D; 0</span><br><span class="line">vm.min_free_kbytes &#x3D; 52428800</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@cdh85-42 ~]# </span><br><span class="line">[root@cdh85-42 ~]# </span><br><span class="line">[root@cdh85-42 ~]# free -m</span><br><span class="line">              total        used        free      shared  buff&#x2F;cache   available</span><br><span class="line">Mem:         514959       34901       82648        4258      397410      324618</span><br><span class="line">Swap:          4095           0        4095</span><br><span class="line">[root@cdh85-42 ~]# cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes</span><br><span class="line">52428800</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>anaconda3-5.2.0+python3.6.5+tensorflow1.11.0</title>
    <url>/2020/01/07/anaconda3-5.2.0+python3.6.5+tensorflow1.11.0/</url>
    <content><![CDATA[<h3 id="anaconda3-5-2-0">anaconda3-5.2.0</h3>
<p>官网<br>
<a href="https://www.anaconda.com/" target="_blank" rel="noopener">https://www.anaconda.com/</a></p>
<p>历史版本下载地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;repo.continuum.io&#x2F;archive&#x2F;</span><br><span class="line">https:&#x2F;&#x2F;repo.anaconda.com&#x2F;archive&#x2F;</span><br><span class="line">https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;archive&#x2F;?C&#x3D;N&amp;O&#x3D;D</span><br></pre></td></tr></table></figure>
<h3 id="win10安装">win10安装</h3>
<p><a href="https://blog.51cto.com/acevi/2103437" target="_blank" rel="noopener">https://blog.51cto.com/acevi/2103437</a></p>
<h3 id="linux-7-安装-anaconda-3-5-2-0-tensorflow1-11-0">linux 7 安装 anaconda 3-5.2.0  tensorflow1.11.0</h3>
<p><a href="https://blog.51cto.com/moerjinrong/2155178" target="_blank" rel="noopener">https://blog.51cto.com/moerjinrong/2155178</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">安装</span><br><span class="line">chmod +x  Anaconda3-5.2.0-Linux-x86_64.sh</span><br><span class="line">.&#x2F;Anaconda3-5.2.0-Linux-x86_64.sh</span><br><span class="line">安装过程中会需要不断回车来阅读并同意license。安装路径默认为用户目录(可以自己指定)，最后需要确认将路径加入用户的.bashrc中。</span><br><span class="line">In order to continue the installation process, please review the license</span><br><span class="line">agreement.</span><br><span class="line">Please, press ENTER to continue       </span><br><span class="line">&gt;&gt;&gt;  # 要继续安装过程，请查看许可证协议。请按ENTER继续</span><br><span class="line"></span><br><span class="line">然后按空格阅读许可协议，</span><br><span class="line"></span><br><span class="line">Do you accept the license terms? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes      # 是否接受协议，选yes</span><br><span class="line"></span><br><span class="line">Anaconda3 will now be installed into this location:</span><br><span class="line">&#x2F;root&#x2F;anaconda3</span><br><span class="line"></span><br><span class="line">  - Press ENTER to confirm the location</span><br><span class="line">  - Press CTRL-C to abort the installation</span><br><span class="line">  - Or specify a different location below</span><br><span class="line"></span><br><span class="line">[&#x2F;root&#x2F;anaconda3] &gt;&gt;&gt;  # 是否安装到当前家目录的anaconda3目录中，默认回车即可</span><br><span class="line"></span><br><span class="line">Do you wish the installer to prepend the Anaconda3 install location</span><br><span class="line">to PATH in your &#x2F;root&#x2F;.bashrc ? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes            # 是否添加环境变量到&#x2F;root&#x2F;.bashrc文件</span><br><span class="line">重新加载环境变量，执行：</span><br><span class="line"></span><br><span class="line">source ~&#x2F;.bashrc</span><br><span class="line">python -V</span><br><span class="line">pip list</span><br><span class="line">conda list</span><br></pre></td></tr></table></figure>
<p>silent install</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[xy_zhangpeng@bigdata-2 ~]$ bash Anaconda3-5.2.0-Linux-x86_64.sh -b -p $HOME&#x2F;anaconda3 -f</span><br></pre></td></tr></table></figure>
<p>To run the silent installation of Miniconda for macOS or Linux, specify the -b and -p arguments of the bash installer. The following arguments are supported:</p>
<ul>
<li>-b—Batch mode with no PATH modifications to ~/.bashrc. Assumes that you agree to the license agreement. Does not edit the .bashrc or .bash_profile files.</li>
<li>-p—Installation prefix/path.</li>
<li>-f—Force installation even if prefix -p already exists.</li>
</ul>
<h3 id="tensorflow-在-anaconda。。">tensorflow 在 anaconda。。</h3>
<p><a href="https://www.anaconda.com/tensorflow-in-anaconda/" target="_blank" rel="noopener">https://www.anaconda.com/tensorflow-in-anaconda/</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">安装Tensorflow</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如果GPU是NVIDIA的，就可以安装GPU版本的TensorFlow；如果不是，安装CPU版本的就好了。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1.因为要下载Tensorflow，所以我先在Anaconda的配置文件中添加清华镜像库，这样下载和更新的速度会快很多，命令：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line">#查看</span><br><span class="line">conda config --show</span><br><span class="line"></span><br><span class="line">#CPU版本</span><br><span class="line">pip install --upgrade tensorflow</span><br><span class="line"></span><br><span class="line">#GPU版本</span><br><span class="line">pip install --upgrade tensorflow-gpu</span><br><span class="line"></span><br><span class="line">#指定版本</span><br><span class="line">pip install tensorflow&#x3D;&#x3D;1.11.0</span><br></pre></td></tr></table></figure>
<h3 id="分布式TF">分布式TF</h3>
<p><a href="https://www.jianshu.com/p/fdb93e44a8cc" target="_blank" rel="noopener">https://www.jianshu.com/p/fdb93e44a8cc</a></p>
<h3 id="例子">例子</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 引入 tensorflow 模块</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># 创建一个整型常量，即 0 阶 Tensor</span><br><span class="line">t0 &#x3D; tf.constant(3, dtype&#x3D;tf.int32)</span><br><span class="line"></span><br><span class="line"># 创建一个浮点数的一维数组，即 1 阶 Tensor</span><br><span class="line">t1 &#x3D; tf.constant([3., 4.1, 5.2], dtype&#x3D;tf.float32)</span><br><span class="line"></span><br><span class="line"># 创建一个字符串的2x2数组，即 2 阶 Tensor</span><br><span class="line">t2 &#x3D; tf.constant([[&#39;Apple&#39;, &#39;Orange&#39;], [&#39;Potato&#39;, &#39;Tomato&#39;]], dtype&#x3D;tf.string)</span><br><span class="line"></span><br><span class="line"># 创建一个 2x3x1 数组，即 3 阶张量，数据类型默认为整型</span><br><span class="line">t3 &#x3D; tf.constant([[[5], [6], [7]], [[4], [3], [2]]])</span><br><span class="line"></span><br><span class="line"># 打印上面创建的几个 Tensor</span><br><span class="line">print(t0)</span><br><span class="line">print(t1)</span><br><span class="line">print(t2)</span><br><span class="line">print(t3)</span><br><span class="line">--------------------- </span><br><span class="line">作者：戈云飞 </span><br><span class="line">来源：CSDN </span><br><span class="line">原文：https:&#x2F;&#x2F;blog.csdn.net&#x2F;geyunfei_&#x2F;article&#x2F;details&#x2F;78782804 </span><br><span class="line">版权声明：本文为博主原创文章，转载请附上博文链接！</span><br></pre></td></tr></table></figure>
<h3 id="参考">参考</h3>
<p><a href="https://cloud.tencent.com/developer/article/1078485" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1078485</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1078028" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1078028</a></p>
<h3 id="parcel包安装">parcel包安装</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;repo.anaconda.com&#x2F;pkgs&#x2F;misc&#x2F;parcels&#x2F;</span><br><span class="line">替换为</span><br><span class="line">https:&#x2F;&#x2F;repo.continuum.io&#x2F;pkgs&#x2F;misc&#x2F;parcels&#x2F;</span><br></pre></td></tr></table></figure>
<h4 id="离线下载tensorlfow。-Keras">离线下载tensorlfow。 Keras</h4>
<p>方法：在有网络的机器下载好。zip到离线服务器。</p>
<ul>
<li>download</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">pip download msgpack-python&#x3D;&#x3D;0.5.6  -d pip_package</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mkdir pip_package</span><br><span class="line"> pip download  tensorflow&#x3D;&#x3D;1.11.0 -d pip_package</span><br><span class="line"> zip -r pip_package.zip pip_package</span><br></pre></td></tr></table></figure>
<ul>
<li>install</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>…</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F;tensorflow&#x2F;</span><br><span class="line"></span><br><span class="line">http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;packages&#x2F;2c&#x2F;0c&#x2F;74410a32bf753b280b28b685dc6620c65ccc3a09494398d47198af9f2bbb&#x2F;tensorflow-1.11.0rc2-cp36-cp36m-manylinux1_x86_64.whl#sha256&#x3D;b137211744ccbfec6fd5a5f62a47ce1a467fd760be8169a38c7a88121e8f6341</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;packages&#x2F;5e&#x2F;10&#x2F;aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab&#x2F;Keras-2.2.4-py2.py3-none-any.whl#sha256&#x3D;794d0c92c6c4122f1f0fcf3a7bc2f49054c6a54ddbef8d8ffafca62795d760b6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pip install xxx.whl</span><br></pre></td></tr></table></figure>
<h3 id="anaconda-cdh-parcels">anaconda  cdh  parcels</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.cloudera.com&#x2F;downloads&#x2F;partner&#x2F;anaconda.html</span><br><span class="line">https:&#x2F;&#x2F;docs.anaconda.com&#x2F;anaconda-scale&#x2F;cloudera-cdh&#x2F;</span><br><span class="line">http:&#x2F;&#x2F;docs.anaconda.com&#x2F;anaconda-repository&#x2F;user-guide&#x2F;tasks&#x2F;work-with-cloudera-parcels&#x2F;</span><br><span class="line">https:&#x2F;&#x2F;www.anaconda.com&#x2F;how-to-generate-custom-anaconda-parcels-for-cloudera-cdh-with-anaconda-enterprise-5&#x2F;</span><br></pre></td></tr></table></figure>
<h3 id="histroy">histroy</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1063  bash Anaconda3-5.2.0-Linux-x86_64.sh  -b -p &#x2F;opt&#x2F;anaconda3 -f</span><br><span class="line"> 1064  ll &#x2F;opt&#x2F;</span><br><span class="line"> 1065  python -V</span><br><span class="line"> 1066  ll</span><br><span class="line"> 1067  cd &#x2F;opt&#x2F;</span><br><span class="line"> 1068  ll</span><br><span class="line"> 1069  cd anaconda3&#x2F;</span><br><span class="line"> 1070  ll</span><br><span class="line"> 1071  .&#x2F;bin&#x2F;python -V</span><br><span class="line"> 1072  .&#x2F;bin&#x2F;conda -V</span><br><span class="line"> 1074  .&#x2F;bin&#x2F;conda config -h</span><br><span class="line"> 1075  .&#x2F;bin&#x2F;conda config --show</span><br><span class="line"> 1077  .&#x2F;bin&#x2F;conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line"> 1078  .&#x2F;bin&#x2F;conda config --show</span><br><span class="line"> 1080  .&#x2F;bin&#x2F;conda config --set show_channel_urls yes</span><br><span class="line"> 1081  .&#x2F;bin&#x2F;conda config --show</span><br><span class="line"> 1082  .&#x2F;pip -V</span><br><span class="line"> 1083  .&#x2F;bin&#x2F;pip -V</span><br><span class="line"> 1084  .&#x2F;bin&#x2F;pip install tensorflow&#x3D;&#x3D;1.11.0</span><br><span class="line"> 1085  .&#x2F;bin&#x2F;pip list all</span><br></pre></td></tr></table></figure>
<h3 id="Tensorflow报错-tf-estimator-package-not-installed">Tensorflow报错 tf.estimator package not installed</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、tf.estimator package 未安装</span><br><span class="line">tf.estimator package not installed</span><br><span class="line"></span><br><span class="line">我使用的环境是：</span><br><span class="line">anaconda 5.2.0， Python 3.6和TensorFlow 1.12</span><br><span class="line">然后在网上找了一下解决方案之后，发现需要更新一下numpy、pandas和matplotlib等package:</span><br><span class="line">pip install -U pandas</span><br><span class="line">pip install -U matplotlib</span><br><span class="line"></span><br><span class="line">将pandas的版本更新到了0.23.4，matplotlib更新到3.0.2就好了，然后import TensorFlow的时候就不会报错了</span><br><span class="line"></span><br><span class="line">2、import tensorflow的提醒</span><br><span class="line">FutureWarning: Conversion of the second argument of issubdtype from &#96;float&#96; to &#96;np.floating&#96; is deprecated. In future, it will be treated as &#96;np.float64 &#x3D;&#x3D; np.dtype(float).type&#96;.</span><br><span class="line">  from ._conv import register_converters as _register_converters</span><br><span class="line"></span><br><span class="line">这个问题还是环境问题，h5py出了问题，对h5py进行一个升级了就，后面再import tensorflow的时候，就不会出错了。</span><br><span class="line">pip install h5py&#x3D;&#x3D;2.8.0rc1</span><br><span class="line"></span><br><span class="line">3、</span><br><span class="line">TypeError: __init__() got an unexpected keyword argument &#39;serialized_options&#39;</span><br><span class="line"></span><br><span class="line">很有可能是，终端上的 protoc 版本 与python库内的protobuf版本不一样。</span><br><span class="line">pip install -U protobuf</span><br><span class="line"></span><br><span class="line">4、将Python程序不挂断的跑到服务器上面，</span><br><span class="line">注意CUDA_VISIBLE_DEVICES&#x3D;0需要放在nohup的前面。</span><br><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0 nohup python -u main.py</span><br><span class="line"></span><br><span class="line">作者：奔向算法的喵</span><br><span class="line">链接：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;2ea51363b080</span><br><span class="line">来源：简书</span><br><span class="line">简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>中文测试</title>
    <url>/2019/12/23/%E4%B8%AD%E6%96%87%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<p>中文测试</p>
<p>Hexo Admin 测试</p>
<p>免密登陆 测试</p>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/12/20/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start">Quick Start</h2>
<h3 id="Create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
