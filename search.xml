<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>oms升级</title>
    <url>/2025/05/16/oms%E5%8D%87%E7%BA%A7/</url>
    <content><![CDATA[<p>oms升级</p>
<p>备份数据库</p>
<p>sudo docker stop ${CONTAINER_NAME}</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker stop oms_4.2.8</span><br></pre></td></tr></table></figure>
<ol>
<li>
<p>登录配置文件中的 CM 心跳库，删除数据库中部分无用记录，节省备份时间。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 登录配置文件中的 CM 心跳库</span></span><br><span class="line">mysql -h172.20.85.200 -P3306 -uscm -p -D_cm_hb</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除无用的记录</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> heatbeat_sequence 用于汇报心跳，获取自增 ID</span></span><br><span class="line">delete from heatbeat_sequence where id &lt; (select max(id) from heatbeat_sequence);</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>执行下述命令，手工备份 rm、cm 和 cm_hb 库为 SQL 文件，并确认各文件的大小不为 0。</p>
<p>多地域场景下，各地域的 cm_hb 库均需要备份。例如有两个地域，则需要备份 4 个库：rm、cm、cm_hb1 和 cm_hb2。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mysqldump -h172.20.85.200 -P3306 -uscm -p --triggers=false _cm_hb &gt; /home/admin/_cm_hb.sql</span><br><span class="line">mysqldump -h172.20.85.200 -P3306 -uscm -p --triggers=false _cm &gt; /home/admin/_cm.sql</span><br><span class="line">mysqldump -h172.20.85.200 -P3306 -uscm -p --triggers=false _cm_hb_pbx &gt; /home/admin/_cm_hb_pbx.sql</span><br><span class="line">mysqldump -h172.20.85.200 -P3306 -uscm -p --triggers=false _rm &gt; /home/admin/_rm.sql</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>加载下载的 OMS 社区版安装包至 Docker 容器的本地镜像仓库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker load -i oms_4.2.9-ce.tar.gz</span><br></pre></td></tr></table></figure>
<p>启动 OMS 社区版 V4.2.9-CE 新容器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -dit --net host \</span><br><span class="line">-v &#x2F;opt&#x2F;oms&#x2F;config.yaml:&#x2F;home&#x2F;admin&#x2F;conf&#x2F;config.yaml \</span><br><span class="line">-v &#x2F;opt&#x2F;oms&#x2F;oms_logs:&#x2F;home&#x2F;admin&#x2F;logs \</span><br><span class="line">-v &#x2F;opt&#x2F;oms&#x2F;oms_store:&#x2F;home&#x2F;ds&#x2F;store \</span><br><span class="line">-v &#x2F;opt&#x2F;oms&#x2F;oms_run:&#x2F;home&#x2F;ds&#x2F;run \</span><br><span class="line">-e OMS_HOST_IP&#x3D;192.168.192.10 \</span><br><span class="line">-e IS_UPGRADE&#x3D;true \</span><br><span class="line">--privileged&#x3D;true \</span><br><span class="line">--pids-limit -1 \</span><br><span class="line">--ulimit nproc&#x3D;65535:65535 \</span><br><span class="line">--name oms_4.2.9 \</span><br><span class="line">068e4f2ed46e</span><br></pre></td></tr></table></figure>
<p>当同一地域下多个节点的配置文件（主要是 <code>cm_nodes</code>）相同时，一个地域只需要执行一次 <code>docker_init.sh</code></p>
<p>进入新容器。在 <code>root</code> 目录下，执行元数据初始化操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker exec -it oms_4.2.9 bash </span><br><span class="line"></span><br><span class="line">sleep 300</span><br><span class="line">bash &#x2F;root&#x2F;docker_init.sh</span><br></pre></td></tr></table></figure>
<p>系统参数要优化一下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sysctl -w  net.ipv4.tcp_tw_recycle&#x3D;0</span><br><span class="line">sysctl -w  net.ipv4.tcp_tw_reuse&#x3D;0</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>centos7.9 yum安装mysql </title>
    <url>/2025/03/14/centos7.9%20yum%E5%AE%89%E8%A3%85mysql%20/</url>
    <content><![CDATA[<p>wget -O /etc/yum.repos.d/CentOS-Base.repo <a href="https://mirrors.aliyun.com/repo/Centos-7.repo" target="_blank" rel="noopener">https://mirrors.aliyun.com/repo/Centos-7.repo</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;dev.mysql.com&#x2F;get&#x2F;mysql57-community-release-el7-11.noarch.rpm</span><br><span class="line">sudo rpm -ivh mysql57-community-release-el7-11.noarch.rpm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 停止服务并卸载</span><br><span class="line">sudo systemctl stop mysqld</span><br><span class="line">sudo yum remove mysql-community-server mysql-community-client</span><br><span class="line">sudo rm -rf &#x2F;var&#x2F;lib&#x2F;mysql &#x2F;etc&#x2F;my.cnf &#x2F;var&#x2F;log&#x2F;mysqld.log</span><br><span class="line"></span><br><span class="line"># 清理残留依赖</span><br><span class="line">sudo yum autoremove</span><br><span class="line"></span><br><span class="line"># 重新安装</span><br><span class="line">sudo yum install mysql-community-server mysql-community-client</span><br><span class="line">sudo systemctl start mysqld</span><br><span class="line"></span><br><span class="line"># 获取临时 root 密码</span><br><span class="line">sudo grep &#39;temporary password&#39; &#x2F;var&#x2F;log&#x2F;mysqld.log</span><br><span class="line"></span><br><span class="line">mysql -h localhost -u root -p</span><br><span class="line">mysql&gt;alter user root@localhost identified by &#39;****&#39;;</span><br><span class="line">mysql&gt;uninstall plugin validate_password;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget  https:&#x2F;&#x2F;ftpmirror.your.org&#x2F;pub&#x2F;percona&#x2F;percona&#x2F;yum&#x2F;release&#x2F;7&#x2F;os&#x2F;x86_64&#x2F;percona-xtrabackup-24-2.4.29-1.el7.x86_64.rpm</span><br><span class="line">yum install percona-xtrabackup-24-2.4.29-1.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">wget https:&#x2F;&#x2F;docs-tencentdb-1256569818.cos.ap-guangzhou.myqcloud.com&#x2F;qpress-11-linux-x64.tar</span><br><span class="line">tar xvf qpress-11-linux-x64.tar</span><br><span class="line">mv qpress &#x2F;usr&#x2F;bin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">登陆 10.0.18.90</span><br><span class="line"></span><br><span class="line">sudo su -</span><br><span class="line"></span><br><span class="line">scp -P2222 &#x2F;sour_backup&#x2F;backupSet&#x2F;mysql&#x2F;10.0.20.202&#x2F;2025-03-05_03-00-02&#x2F;2025-03-05_03-00-02&#x2F;stream_backup_2025-03-05_03-00-02 10.0.25.14:&#x2F;data&#x2F;mysql&#x2F;recovery</span><br><span class="line"></span><br><span class="line">xbstream --verbose --parallel&#x3D;4 --extract &lt; &#x2F;data&#x2F;mysql&#x2F;recovery&#x2F;stream_backup_2025-03-05_03-00-02 --directory &#x2F;data&#x2F;mysql&#x2F;recovery&#x2F; &gt; &#x2F;data&#x2F;recovery&#x2F;destream.log </span><br><span class="line"></span><br><span class="line"># xtrabackup --parallel&#x3D;4 --decompress --remove-original --target-dir&#x3D;&#x2F;data&#x2F;mysql&#x2F;recovery&#x2F; &gt; &#x2F;data&#x2F;recovery&#x2F;decompress.log</span><br><span class="line">xtrabackup --decompress --parallel&#x3D;4 --decrypt&#x3D;AES256 --encrypt-key&#x3D;&quot;lk92tBpEfn0nn88mBDt0%y8fPO0y%wuX&quot; --target-dir&#x3D;&#x2F;data&#x2F;mysql&#x2F;recovery&#x2F; --remove-original &gt; &#x2F;data&#x2F;recovery&#x2F;decompress.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xtrabackup --use-memory&#x3D;1024MB --prepare --target-dir&#x3D;&#x2F;data&#x2F;mysql&#x2F;recovery&#x2F; &gt; &#x2F;data&#x2F;recovery&#x2F;prepare_redo_only.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rm -rf &#x2F;data&#x2F;mysql&#x2F;mysqldata&#x2F;*</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;data&#x2F;mysql&#x2F;mysqldata&#x2F;&#123;binlog,innodb_log,innodb_ts,innodb_undo,log,mydata,tmpdir,sock,relaylog&#125;</span><br><span class="line"></span><br><span class="line"># xtrabackup --defaults-file&#x3D;&#x2F;etc&#x2F;my.cnf --move-back --target-dir&#x3D;&#x2F;data&#x2F;mysql&#x2F;recovery&#x2F;</span><br><span class="line">xtrabackup --defaults-file&#x3D;&#x2F;etc&#x2F;my.cnf --copy-back --target-dir&#x3D;&#x2F;data&#x2F;mysql&#x2F;recovery&#x2F;  </span><br><span class="line"></span><br><span class="line">touch &#x2F;data&#x2F;mysql&#x2F;mysqldata&#x2F;log&#x2F;error.log</span><br><span class="line">chown -R mysql.mysql &#x2F;data</span><br><span class="line">chmod -R 775 &#x2F;data</span><br><span class="line"></span><br><span class="line">systemctl start mysql</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>obdiag使用</title>
    <url>/2025/02/24/obdiag%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>官方文档：<a href="https://www.oceanbase.com/docs/common-obdiag-cn-1000000001491140" target="_blank" rel="noopener">https://www.oceanbase.com/docs/common-obdiag-cn-1000000001491140</a></p>
<p>obdiag安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y yum-utils</span><br><span class="line">sudo yum-config-manager --add-repo https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;oceanbase&#x2F;OceanBase.repo</span><br><span class="line">sudo yum install -y oceanbase-diagnostic-tool</span><br><span class="line">sh &#x2F;usr&#x2F;local&#x2F;oceanbase-diagnostic-tool&#x2F;init.sh</span><br></pre></td></tr></table></figure>
<h3 id="用户侧配置文件">用户侧配置文件</h3>
<p>用户侧配置文件可通过 <code>obdiag config &lt;option&gt;</code> 命令快速生成或者直接编辑配置文件，文件的默认路径是 <code>~/.obdiag/config.yml</code>，其样板文件位于 <code>~/.obdiag/example</code>。</p>
<p>obdiag config -h &lt;db_host&gt; -u &lt;sys_user&gt; [-p password] [-P port]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">obdiag config -h192.168.81.31 -uroot@sys -p&#39;***&#39; -P2881</span><br></pre></td></tr></table></figure>
<h2 id="check-命令组">check 命令组</h2>
<h3 id="总览">总览</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列出所有巡检套餐</span></span><br><span class="line">obdiag check list</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 全量巡检 (最常用)</span></span><br><span class="line">obdiag check run</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 列存POC检查</span></span><br><span class="line">obdiag check --cases=column_storage_poc</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> obproxy 版本检查</span></span><br><span class="line">obdiag check --obproxy_cases=proxy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 部署环境检查</span></span><br><span class="line">obdiag check --cases=build_before</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行 sysbench 时的巡检任务集合</span></span><br><span class="line">obdiag check --cases=sysbench_run</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行 sysbench 前的巡检任务集合</span></span><br><span class="line">obdiag check --cases=sysbench_free</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>部署 OceanBase 集群环境</title>
    <url>/2025/02/06/%E9%83%A8%E7%BD%B2%20OceanBase%20%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h2 id="部署-OceanBase-集群环境">部署 OceanBase 集群环境</h2>
<p><strong>磁盘初始化</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">umount &#x2F;dfs&#x2F;data&#123;1..8&#125;</span><br><span class="line"></span><br><span class="line">pvcreate &#x2F;dev&#x2F;nvme&#123;0..7&#125;n1 -y</span><br><span class="line"></span><br><span class="line">vgcreate &#x2F;dev&#x2F;obvg &#x2F;dev&#x2F;nvme0n1 &#x2F;dev&#x2F;nvme1n1 &#x2F;dev&#x2F;nvme2n1 &#x2F;dev&#x2F;nvme3n1 &#x2F;dev&#x2F;nvme4n1 &#x2F;dev&#x2F;nvme5n1 &#x2F;dev&#x2F;nvme6n1 &#x2F;dev&#x2F;nvme7n1 </span><br><span class="line"></span><br><span class="line"># 日志盘 ，内存的三倍</span><br><span class="line">lvcreate -n ob-log -L 3096G obvg --stripes&#x3D;6 --stripesize&#x3D;128</span><br><span class="line">lvcreate -n ob-data -l 100%Free obvg --stripes&#x3D;6 --stripesize&#x3D;1024</span><br><span class="line"></span><br><span class="line">mkfs.ext4 &#x2F;dev&#x2F;obvg&#x2F;ob-log -O 64bit   </span><br><span class="line">mkfs.xfs &#x2F;dev&#x2F;obvg&#x2F;ob-data </span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;data&#x2F;&#123;1,log1&#125;  &amp;&amp; chmod -R 755 &#x2F;data</span><br><span class="line">vi &#x2F;etc&#x2F;fstab</span><br><span class="line">&#x2F;*写入*&#x2F;</span><br><span class="line">&#x2F;dev&#x2F;obvg&#x2F;ob-data &#x2F;data&#x2F;1        xfs    defaults,noatime,nodiratime    0  0</span><br><span class="line">&#x2F;dev&#x2F;obvg&#x2F;ob-log  &#x2F;data&#x2F;log1      ext4    defaults,noatime,nodiratime,nodelalloc,barrier&#x3D;0     0  0</span><br><span class="line">mount -a</span><br></pre></td></tr></table></figure>
<h3 id="问题：-OBD-4300：x-x-x-x-failed-to-query-java-version-you-may-not-have-java-installed">问题：  OBD-4300：x.x.x.x: failed to query java version, you may not have java installed</h3>
<p>解决方法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ln -s &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-cloudera&#x2F;bin&#x2F;java  &#x2F;usr&#x2F;bin&#x2F;java</span><br></pre></td></tr></table></figure>
<h3 id="在线部署">在线部署</h3>
<p>当您选择在线部署时，可以参考本节命令在中控机上安装 OBD。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[admin@test001 ~]$ sudo yum install -y yum-utils</span><br><span class="line">[admin@test001 ~]$ sudo yum-config-manager --add-repo https://mirrors.aliyun.com/oceanbase/OceanBase.repo</span><br><span class="line">[admin@test001 ~]$ sudo yum install -y ob-deploy</span><br><span class="line">[admin@test001 ~]$ source /etc/profile.d/obd.sh</span><br></pre></td></tr></table></figure>
<h3 id="步骤二：使用白屏部署-OceanBase-数据库">步骤二：使用白屏部署 OceanBase 数据库</h3>
<ol>
<li>
<p>命令行执行 <code>obd web</code> 命令启动白屏界面，根据输出地址登录白屏界面并单击 <strong>开启体验之旅</strong> 开始部署。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[admin@test001 ~]$ obd web</span><br><span class="line">start OBD WEB in 0.0.0.0:8680</span><br><span class="line">please open http://172.xx.xxx.233:8680</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="OBD-一键测试">OBD 一键测试</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y yum-utils</span><br><span class="line">sudo yum-config-manager --add-repo https://mirrors.aliyun.com/oceanbase/OceanBase.repo</span><br><span class="line">sudo yum install obtpch</span><br><span class="line">sudo ln -s /usr/tpc-h-tools/tpc-h-tools/ /usr/local/</span><br><span class="line">obd test tpch myoceanbase --test-server 10.0.19.191 --tenant=ocp -s 100 --remote-tbl-dir=/tmp/tpch100</span><br><span class="line"></span><br><span class="line">obd test tpch myoceanbase --tenant=ocp  --test-only</span><br></pre></td></tr></table></figure>
<h3 id="注意">注意</h3>
<ul>
<li>OBD 运行 tpch，详细参数介绍请参考 <a href="https://www.oceanbase.com/docs/community-obd-cn-10000000001031935" target="_blank" rel="noopener">obd test tpch</a>。</li>
<li>在本例中，大幅参数使用的是默认参数，在用户场景下，可以根据自己的具体情况做一些参数上的调整。例如，在本例中使用的集群名为 <code>myoceanbase</code>，租户名是 <code>ocp</code>。</li>
<li>-s 100  自动生成测试数据的规模，单位：G。</li>
</ul>
<h2 id="使用-OBD">使用 OBD</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">obd cluster list</span><br><span class="line">obd cluster restart myoceanbase</span><br><span class="line">查看集群列表</span><br><span class="line">obd cluster list</span><br><span class="line"></span><br><span class="line"># 查看集群状态，以部署名为 myoceanbase 为例</span><br><span class="line">obd cluster display myoceanbase</span><br><span class="line"></span><br><span class="line"># 停止运行中的集群，以部署名为 myoceanbase 为例</span><br><span class="line">obd cluster stop myoceanbase</span><br><span class="line"></span><br><span class="line"># 销毁已部署的集群，以部署名为 myoceanbase 为例</span><br><span class="line">obd cluster destroy myoceanbase</span><br><span class="line"></span><br><span class="line">obd cluster autodeploy</span><br><span class="line"></span><br><span class="line">obd cluster deploy</span><br><span class="line">obd cluster start</span><br><span class="line">obd cluster stop</span><br><span class="line">obd cluster reload</span><br><span class="line">obd cluster upgrade</span><br><span class="line">obd cluster edit-config  myoceanbase</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">部署集群</span><br><span class="line">obd cluster deploy myoceanbase -c config.yaml</span><br><span class="line"># 启动集群</span><br><span class="line">obd cluster start myoceanbase</span><br><span class="line"></span><br><span class="line"># 重启集群组件</span><br><span class="line">obd cluster stop bx_ocp -c ocp-server-ce --wp</span><br><span class="line">obd cluster start bx_ocp -c ocp-server-ce --wp</span><br><span class="line">obd cluster restart bx_ocp -c oceanbase-ce --wp</span><br><span class="line"></span><br><span class="line">obd cluster restart bx_ocp --wp</span><br></pre></td></tr></table></figure>
<p><strong>导入数据</strong></p>
<p>并小于 2倍租户CPU就行</p>
<p>bin/obloader -hxx.xx.xx.xx -P端口 -u用户名 -t租户名 -D数据库名 -p --table ‘*’ --par --external-data --file-suffix ‘.tbl’ -f /data/0/xxx/tpch/100g_data/ --truncate-table --thread 32 --rpc-port 11208 --direct --parallel 32</p>
<p>t_history_trans_ethereum.ctrl</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lang&#x3D;java</span><br><span class="line">(</span><br><span class="line">address &quot;none&quot; map(1),</span><br><span class="line">hash &quot;none&quot; map(3),</span><br><span class="line">pk_hex &quot;none&quot; map(2),</span><br><span class="line">hash_code &quot;none&quot; map(4),</span><br><span class="line">reorg &quot;none&quot; map(5),</span><br><span class="line">chain &quot;none&quot; map(6),</span><br><span class="line">block_number &quot;none&quot; map(7),</span><br><span class="line">block_timestamp &quot;none&quot; map(8),</span><br><span class="line">transaction_type &quot;none&quot; map(9),</span><br><span class="line">transaction_index &quot;none&quot; map(10),</span><br><span class="line">nonce &quot;none&quot; map(11),</span><br><span class="line">receipt_status &quot;none&quot; map(12),</span><br><span class="line">from_address &quot;none&quot; map(13),</span><br><span class="line">to_address &quot;none&quot; map(14),</span><br><span class="line">value &quot;none&quot; map(15),</span><br><span class="line">price &quot;none&quot; map(16),</span><br><span class="line">amount &quot;none&quot; map(17),</span><br><span class="line">function_code &quot;none&quot; map(18),</span><br><span class="line">action &quot;none&quot; map(19),</span><br><span class="line">gas_used &quot;none&quot; map(20),</span><br><span class="line">gas_price &quot;none&quot; map(21),</span><br><span class="line">transaction_fee &quot;none&quot; map(22)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>– 先单个文件测试， 并指定控制文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;obloader -h172.20.192.146 -P2881 -uroot -tbf -Daddress -p --table &#39;t_history_trans_ethereum&#39; --par --external-data -f &#x2F;dfs&#x2F;data3&#x2F;data-source-dir&#x2F;part-04999-cb9f741d-fb71-4423-9890-9ed42da9270d-c000.snappy.parquet --ctl-path &#x2F;home&#x2F;hadoop&#x2F;controls</span><br></pre></td></tr></table></figure>
<p>– 开启旁路，从hdfs批量导入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;obloader -h172.20.192.146 -P2881 -uroot -tbf -Daddress -p --table &#39;t_history_trans_bsc&#39; --par --external-data  --file-regular-expression &#39;.*\.parquet&#39; --file-suffix &#39;.parquet&#39; -f &#x2F;dfs&#x2F;data3&#x2F;data-source-dir --storage-uri &#39;hdfs:&#x2F;&#x2F;ns1&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;history_bsc&#x2F;?hdfs-site-file&#x3D;&#x2F;dfs&#x2F;data3&#x2F;data-source-dir&#x2F;hadoop-conf&#x2F;hdfs-site.xml&amp;core-site-file&#x3D;&#x2F;dfs&#x2F;data3&#x2F;data-source-dir&#x2F;hadoop-conf&#x2F;core-site.xml&#39; --rpc-port 2882 --direct --thread 64 --parallel 16</span><br></pre></td></tr></table></figure>
<p><strong>obdiag</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source &#x2F;usr&#x2F;local&#x2F;oceanbase-diagnostic-tool&#x2F;init.sh</span><br><span class="line">obdiag check    -- 一键巡检</span><br><span class="line">obdiag analyze log --since 30m</span><br><span class="line">obdiag analyze flt_trace --flt_trace_id 00060d0f-aa35-8aec-3195-f6c9eb13b800</span><br></pre></td></tr></table></figure>
<h1><a href="https://ask.oceanbase.com/t/topic/35609528" target="_blank" rel="noopener">OCP 告警通知ocp_monagent 进程停止，怎么调大可用内存？</a></h1>
<p>可以到主机上黑屏调大ocp_monagent进程的内存限制，修改方法：</p>
<ul>
<li>
<p>针对 3.2.x ~ 4.2.0 版本的ocp-agent：</p>
<ul>
<li>修改ocp_agent/conf/agentd.yaml 中 ocp_monagent 下的 “memoryQuota” 参数，默认是2048MB，可以根据QPS和主机内存使用情况适当调大；</li>
<li>重启ocp_agent后生效；</li>
<li><strong>需要注意</strong> ：重装ocp_agent后，之前修改的内存限制会被重置，需要按照以上方式重新修改一次；</li>
</ul>
</li>
<li>
<p>针对 4.2.1 及其之后版本的ocp-agent：</p>
<ul>
<li>可以执行 /home/admin/ocp_agent/bin/ocp_agentctl config -u monagent.limit.memory.quota=2048MB 命令修改ocp_monagent进程内存限制的参数值，monagent.limit.memory.quota 默认值是2048MB；</li>
<li>重启ocp_agent后生效；</li>
<li>该方法重装ocp-agent后不需要再次修改；</li>
</ul>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>trino连接Starrocks</title>
    <url>/2024/12/23/trino%E8%BF%9E%E6%8E%A5Starrocks/</url>
    <content><![CDATA[<h1>trino连接Starrocks</h1>
<p>#不设置这个参数，trino 任何表都不看到。如果设置这个参数，全部表就都出来了</p>
<p><strong>set</strong> <strong>global</strong> enable_groupby_use_output_alias=<strong>true</strong>;</p>
<p>starrocks.propertes</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">connector.name&#x3D;mysql </span><br><span class="line">connection-url&#x3D;jdbc:mysql:&#x2F;&#x2F;172.20.15.101:9030 </span><br><span class="line">connection-user&#x3D;tx_hadoop_rw </span><br><span class="line">connection-password&#x3D;**** </span><br><span class="line">case-insensitive-name-matching&#x3D;true</span><br></pre></td></tr></table></figure>
<p>在starrocks客户端执行以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set global enable_groupby_use_output_alias&#x3D;true;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>oms使用</title>
    <url>/2024/12/19/oms%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>oms使用</p>
<p>官网文档： <a href="https://www.oceanbase.com/docs/community-oms-cn-1000000001456771" target="_blank" rel="noopener">https://www.oceanbase.com/docs/community-oms-cn-1000000001456771</a></p>
<p>开启时序数据库时，8088端口冲突了，需要改一下。</p>
<p>注意：跨机房网络commit问题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget  https:&#x2F;&#x2F;obbusiness-private.oss-cn-shanghai.aliyuncs.com&#x2F;download-center&#x2F;opensource&#x2F;oms&#x2F;4.2.6_CE&#x2F;oms_4.2.6-ce.tar.gz</span><br><span class="line"></span><br><span class="line">docker load -i oms_4.2.6-ce.tar.gz </span><br><span class="line"></span><br><span class="line">docker images</span><br><span class="line"></span><br><span class="line">sudo docker run -d --net host --name oms-config-tool 04b873b7c52f bash &amp;&amp; sudo docker cp oms-config-tool:&#x2F;root&#x2F;docker_remote_deploy.sh . &amp;&amp; sudo docker rm -f oms-config-tool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bash docker_remote_deploy.sh -o &lt;OMS 容器挂载目录&gt; -i &lt;本机 IP 地址&gt; -d &lt;OMS_IMAGE&gt;</span><br><span class="line"></span><br><span class="line">bash docker_remote_deploy.sh -o &#x2F;opt&#x2F;oms -i 192.168.81.79 -d 04b873b7c52f</span><br></pre></td></tr></table></figure>
<p>mysql的  log_slave_updates 开启， 同步前查看binlog是否有延迟</p>
<p>mysql2store.useGtid=false</p>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2024/11/image-20241122113639437.png" alt="image-20241122113639437"></p>
<p>jvm修改    -Xms20480m -Xmx20480m -Xmn10240m</p>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2024/11/image-20241122113753543.png" alt="image-20241122113753543"></p>
<p>多地域单节点  不需要vip</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">机房1</span><br><span class="line"> - 192.168.81.78</span><br><span class="line">机房2</span><br><span class="line"> - 172.20.85.99</span><br></pre></td></tr></table></figure>
<p>现在单地域多节点的缩容,比如 ABCD，变成ACD</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 修改ACD的 yaml的cm_node,去掉B节点，执行docker_init.sh</span><br><span class="line">2. 修改cm 库中的表 location_cm、host 和 resource_group， rm 数据库中的表 cluster_info</span><br></pre></td></tr></table></figure>
<h2 id="使用限制">使用限制</h2>
<ul>
<li>无论是单个地域还是多个地域，只能存在一套 InfluxDB，不支持配置多套不同的 InfluxDB。</li>
<li>OMS 社区版多节点场景下，所有节点均要配置同一个 InfluxDB。</li>
</ul>
<p>登陆docker容器查看服务错误日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;home&#x2F;admin&#x2F;logs&#x2F;supervisor</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>OMS 社区版支持源端和目标端的字段数量不一致。全量迁移或增量同步过程中，如果报错源端表字段在目标端中不存在，您可以更新 <strong>full-Import</strong> 或 <strong>Incr-Sync</strong> 组件的参数 <code>ignoreRedunantColumnsReplicate</code> 为 <code>true</code> 后，恢复数据迁移任务的运行。</p>
</li>
<li>
<p>source.ignoreDdl = false</p>
</li>
<li>
<p>Coordinator.allowRecordTypes</p>
</li>
<li></li>
</ul>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2024/11/image-20241129112510948.png" alt="image-20241129112510948"></p>
<p>“DELETE,INSERT,UPDATE,DDL”</p>
<h3 id="oms已经有2个地域了，还需要再加一个地域。请问这个如何操作？">oms已经有2个地域了，还需要再加一个地域。请问这个如何操作？</h3>
<p>官网帮助   <a href="https://www.oceanbase.com/docs/community-oms-cn-1000000001456767" target="_blank" rel="noopener">https://www.oceanbase.com/docs/community-oms-cn-1000000001456767</a></p>
<p>vi  /opt/oms/config.yaml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># OMS 元数据库信息</span><br><span class="line">oms_meta_host: 172.20.85.200</span><br><span class="line">oms_meta_port: 3306</span><br><span class="line">oms_meta_user: scm</span><br><span class="line">oms_meta_password: ****</span><br><span class="line"></span><br><span class="line"># 用户可以自定义以下三个数据库的名称，OMS 社区版部署时会在元信息库中创建出这三个数据库</span><br><span class="line">drc_rm_db: _rm</span><br><span class="line">drc_cm_db: _cm</span><br><span class="line">drc_cm_heartbeat_db: _cm_hb</span><br><span class="line"></span><br><span class="line"># OMS 社区版集群配置</span><br><span class="line"># 下述参数填写扩容地域的配置</span><br><span class="line">cm_url: http:&#x2F;&#x2F;192.168.192.11:8088</span><br><span class="line">cm_location: 3</span><br><span class="line">cm_region: nh</span><br><span class="line">cm_region_cn: 南汇</span><br><span class="line">cm_is_default: true</span><br><span class="line">cm_nodes:</span><br><span class="line"> - 192.168.192.11</span><br><span class="line"></span><br><span class="line"># 时序数据库配置,默认占用了8088端口，需要改一下端口</span><br><span class="line"># 默认值为 false。如果您需要开启指标汇报功能，请设置为 true</span><br><span class="line">tsdb_enabled: true </span><br><span class="line"># 当 tsdb_enabled 为 true 时，请取消下述参数的注释并根据实际情况填写</span><br><span class="line">tsdb_service: &#39;INFLUXDB&#39;</span><br><span class="line">tsdb_url: &#39;172.20.85.99:8086&#39;</span><br><span class="line">tsdb_username: root</span><br><span class="line">tsdb_password: &#39;***&#39;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -dit --net host \</span><br><span class="line">-v &#x2F;opt&#x2F;oms&#x2F;config.yaml:&#x2F;home&#x2F;admin&#x2F;conf&#x2F;config.yaml \</span><br><span class="line">-v &#x2F;opt&#x2F;oms&#x2F;oms_logs:&#x2F;home&#x2F;admin&#x2F;logs \</span><br><span class="line">-v &#x2F;opt&#x2F;oms&#x2F;oms_store:&#x2F;home&#x2F;ds&#x2F;store \</span><br><span class="line">-v &#x2F;opt&#x2F;oms&#x2F;oms_run:&#x2F;home&#x2F;ds&#x2F;run \</span><br><span class="line">-e OMS_HOST_IP&#x3D;192.168.192.11 \</span><br><span class="line">--privileged&#x3D;true \</span><br><span class="line">--pids-limit -1 \</span><br><span class="line">--ulimit nproc&#x3D;65535:65535 \</span><br><span class="line">--name oms_4.2.6 \</span><br><span class="line">04b873b7c52f</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">docker exec -it  oms_4.2.6 bash</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>cloudcanl 2.2.4 安装部署</title>
    <url>/2024/12/18/cloudcanl%202.2.4%20%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>cloudcanl 2.2.4 安装部署</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum update</span><br><span class="line">sudo yum -y install epel-release</span><br><span class="line"># base util</span><br><span class="line"></span><br><span class="line">sudo yum install -y yum-utils</span><br><span class="line">sudo yum install -y lsof</span><br><span class="line">sudo yum install -y bc</span><br><span class="line">sudo yum install -y p7zip p7zip-plugins</span><br><span class="line"></span><br><span class="line"># install docker</span><br><span class="line"></span><br><span class="line">sudo yum-config-manager --add-repo https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line">sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</span><br><span class="line"></span><br><span class="line">#修改存储路径</span><br><span class="line">vim &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line"></span><br><span class="line">sudo systemctl start docker</span><br><span class="line">systemctl enable docker</span><br><span class="line">sudo wget &quot;https:&#x2F;&#x2F;pc.clougence.com&#x2F;docker-compose-1.28.3&quot; -O &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">sudo chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line"></span><br><span class="line"># check</span><br><span class="line"></span><br><span class="line">docker -v</span><br><span class="line">docker-compose version</span><br></pre></td></tr></table></figure>
<h2 id="系统准备">系统准备</h2>
<p>clougence用户ulimit上限调整为102400</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi  &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;20-nproc.conf</span><br><span class="line">## 调节clougence用户的资源限制</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 解压*</span><br><span class="line">7z x cloudcanal-2.2.4.7z -o.&#x2F;cloudcanal_home</span><br><span class="line">cd cloudcanal_home</span><br></pre></td></tr></table></figure>
<h2 id="5-启动CloudCanal">5. 启动CloudCanal</h2>
<ul>
<li>执行启动命令: <code>sh startup.sh</code></li>
<li>当终端出现 cloudcanal start 时，即启动成功<br>
<a href="https://www.askcug.com/assets/uploads/files/1620979032983-7b00e562-cd45-4905-a626-1356503d8213-image.png" target="_blank" rel="noopener"><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2023/04/1620979032983-7b00e562-cd45-4905-a626-1356503d8213-image-resized.png" alt="7b00e562-cd45-4905-a626-1356503d8213-image.png"></a></li>
</ul>
<h2 id="6-确认启动成功">6. 确认启动成功</h2>
<ul>
<li>启动过程将耗时 1 分钟左右，访问控制台 <code>http://{您部署机器的ip}:8111</code> 正确登录并开始操作</li>
</ul>
<h1>使用教程(必读)</h1>
<ul>
<li>为了方便您一开始快速体验 CloudCanal ，我们已自动帮您做好了一些初始化的工作</li>
<li><strong>请务必使用提供的默认账号登入，方便您快速体验</strong></li>
<li>用户名：test@clougence.com</li>
<li>密码：clougence2021</li>
<li>默认添加的测试 MySQL 数据库
<ul>
<li>cloudcanal_test_a(源端)和cloudcanal_test_b(目标端)这两个库中已准备用于测试的表和数据</li>
</ul>
</li>
<li>默认已添加了一台运行机器，用于执行具体的数据同步任务</li>
<li>如遇到需要发送短信的场景，<strong>先点击获取验证码，然后输入短信验证码 777777 即可</strong></li>
</ul>
<p>)</p>
<h1>CloudCanal社区版高可用部署教程</h1>
<p><a href="https://www.askcug.com/topic/111/cloudcanal%E7%A4%BE%E5%8C%BA%E7%89%88%E9%AB%98%E5%8F%AF%E7%94%A8%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B" target="_blank" rel="noopener">https://www.askcug.com/topic/111/cloudcanal社区版高可用部署教程</a></p>
<p><strong>Sidecar服务器重启</strong></p>
<p>#找到 CONTAINER ID</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker ps -a </span><br><span class="line">dokcer start 19a6d7579c94</span><br><span class="line">sudo docker exec -it -u clougence 19a6d7579c94 sh</span><br><span class="line">## 启动sidecar</span><br><span class="line">sh &#x2F;home&#x2F;clougence&#x2F;cloudcanal&#x2F;sidecar&#x2F;bin&#x2F;startSidecar.sh</span><br><span class="line">## 查看日志，确认是否有异常。如果都为INFO或者WARN日志就是正常的</span><br><span class="line">tail -f &#x2F;home&#x2F;clougence&#x2F;logs&#x2F;cloudcanal&#x2F;sidecar&#x2F;sidecar.log</span><br></pre></td></tr></table></figure>
<h3 id="修改-console-容器中-springboot-配置文件中配置">修改 console 容器中 springboot 配置文件中配置</h3>
<ul>
<li>文件路径:/opt/cloudcanal_home/console_data/cloudcanal/console/conf/business-output.properties</li>
<li>修改配置项
<ul>
<li>spring.datasource.url</li>
<li>spring.datasource.username</li>
<li>spring.datasource.password</li>
</ul>
</li>
</ul>
<p>java.lang.OutOfMemoryError: Java heap space  错误处理</p>
<p>进入容器，临时修改任务的内存大小</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker exec -it cloudcanal-sidecar sh</span><br><span class="line">vi &#x2F;home&#x2F;clougence&#x2F;cloudcanal&#x2F;cloudcanal&#x2F;bin&#x2F;startTask.sh</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>hive数据库限制容量</title>
    <url>/2024/12/17/hive%E6%95%B0%E6%8D%AE%E5%BA%93%E9%99%90%E5%88%B6%E5%AE%B9%E9%87%8F/</url>
    <content><![CDATA[<p>hive数据库限制容量</p>
<p>操作前<br>
检查是否需要扩容配额<br>
sudo su - hdfs<br>
hdfs dfs -count -q -h /user/hive/warehouse/yz.db<br>
操作过程<br>
实施<br>
sudo -u hdfs hdfs dfsadmin -setSpaceQuota 40G /user/hive/warehouse/yz.db<br>
操作后<br>
验证<br>
hdfs dfs -count -q -h /user/hive/warehouse/yz.db</p>
]]></content>
  </entry>
  <entry>
    <title>obd 接管集群</title>
    <url>/2024/12/04/obd%20%E6%8E%A5%E7%AE%A1%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>obd 接管集群</p>
<p>用obd cluster takeover 接管集群</p>
<p>这个接管需要目标集群obshell服务（ocs）是否没开启</p>
<p>ps -ef|grep  obshell</p>
<p>开启方式  端口默认是2886<br>
./bin/obshell agent start --ip $ip --port $port</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su - obuser</span><br><span class="line"></span><br><span class="line">export OB_ROOT_PASSWORD&#x3D;&quot;pwd&quot;</span><br><span class="line"></span><br><span class="line">&#x2F;home&#x2F;obuser&#x2F;baofu&#x2F;oceanbase&#x2F;bin&#x2F;obshell agent start --ip 10.0.22.172 -P 2886</span><br><span class="line"></span><br><span class="line">obd cluster takeover baofu -h10.0.22.170 -P2881 -p*****  --ssh-user&#x3D;obuser --ssh-password&#x3D;obuser</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>解决Hive创建文件数过多的问题</title>
    <url>/2024/12/03/%E8%A7%A3%E5%86%B3Hive%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E6%95%B0%E8%BF%87%E5%A4%9A%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1>解决Hive创建文件数过多的问题</h1>
<p><strong>用  DISTRIBUTE BY ;</strong></p>
<p>参考帮助    <a href="https://www.cnblogs.com/wcwen1990/p/7600161.html" target="_blank" rel="noopener">https://www.cnblogs.com/wcwen1990/p/7600161.html</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> insert overwrite table test partition(dt)</span><br><span class="line">select * from iteblog_tmp</span><br><span class="line"> DISTRIBUTE BY dt;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>centos7安装influxdb</title>
    <url>/2024/11/29/centos7%E5%AE%89%E8%A3%85influxdb/</url>
    <content><![CDATA[<h1>centos7安装influxdb</h1>
<p>更改存储路径时，记得给路径权限， 不然数据写不进</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh85-99 influxdb]# chown influxdb:influxdb -R data&#x2F; </span><br><span class="line">[root@cdh85-99 influxdb]# chown influxdb:influxdb -R wal</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/tongzidane/article/details/131850118" target="_blank" rel="noopener">https://blog.csdn.net/tongzidane/article/details/131850118</a></p>
]]></content>
  </entry>
  <entry>
    <title>ocp升级</title>
    <url>/2024/11/13/ocp%E5%8D%87%E7%BA%A7/</url>
    <content><![CDATA[<h3 id="ocp升级">ocp升级</h3>
<p>要在obd服务器上操作：</p>
<ol>
<li>
<p>登录 OBD 宿主机。</p>
</li>
<li>
<p>执行如下命令，使环境变量生效。</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;obbusiness-private.oss-cn-shanghai.aliyuncs.com&#x2F;download-center&#x2F;opensource&#x2F;oceanbase-all-in-one&#x2F;7&#x2F;x86_64&#x2F;oceanbase-all-in-one-4.3.3.1_20241023.el7.x86_64.tar.gz</span><br><span class="line"></span><br><span class="line">tar -xzvf oceanbase-all-in-one-4.3.3.1_20241023.el7.x86_64.tar.gz</span><br><span class="line"></span><br><span class="line">cd oceanbase-all-in-one&#x2F;bin &amp;&amp; bash install.sh</span><br><span class="line"></span><br><span class="line">source ~&#x2F;.oceanbase-all-in-one&#x2F;bin&#x2F;env.sh</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>执行 <code>obd web upgrade</code> ，在返回信息中获取升级向导页面的 IP 地址。   （升级的地址不一定是ocp-server的地址）</li>
</ol>
<p>数据库升级软件包目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;home&#x2F;root&#x2F;software&#x2F;sys-package</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>爱可生数据管理平台测试</title>
    <url>/2024/10/18/%E7%88%B1%E5%8F%AF%E7%94%9F%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<p>yum install -y sudo vi less which wget net-tools fontconfig libaio hwloc libpciaccess lsof perl-Data-Dumper perl lz4 libunwind libev zstd</p>
<p>把包上传到10.0.19.151  上然后执行  tar -zxvf dmpother.tar.gz -C /data/umc/components/  就可以了</p>
<p>/opt/mysql/base/5.7.25/bin/mysql -uroot -p -P3306 -h10.0.19.151 #mysql-jfdfh6 #10.0.19.151:3306 #mysql-g001</p>
<p>mysql-g003<br>
caZgcDQG-85lJS=@</p>
<p>mysql-g002<br>
vjE$v-+h2PJq4OCe</p>
<p>mysql-g001<br>
_qx%Xc$$+X-934$*</p>
<p>mysql-g004</p>
<p>HiqxgHl0ho@#_=Xw</p>
<p><a href="https://www.cnblogs.com/f-ck-need-u/p/9279703.html" target="_blank" rel="noopener">https://www.cnblogs.com/f-ck-need-u/p/9279703.html</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sysbench --mysql-host&#x3D;10.0.19.47 \</span><br><span class="line">         --mysql-port&#x3D;3306 \</span><br><span class="line">         --mysql-user&#x3D;yangze \</span><br><span class="line">         --mysql-password&#x3D;yangze \</span><br><span class="line">         &#x2F;usr&#x2F;share&#x2F;sysbench&#x2F;oltp_common.lua \</span><br><span class="line">         --tables&#x3D;2 \</span><br><span class="line">         --table_size&#x3D;50000000 \</span><br><span class="line">         prepare</span><br><span class="line">         </span><br><span class="line">sysbench --mysql-host&#x3D;10.0.19.47 \</span><br><span class="line">         --mysql-port&#x3D;3306 \</span><br><span class="line">         --mysql-user&#x3D;yangze \</span><br><span class="line">         --mysql-password&#x3D;yangze \</span><br><span class="line">         &#x2F;usr&#x2F;share&#x2F;sysbench&#x2F;oltp_common.lua \</span><br><span class="line">         --tables&#x3D;2 \</span><br><span class="line">         cleanup</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Yarn ResourceManager HA 故障转移问题定位</title>
    <url>/2024/08/28/Yarn%20ResourceManager%20HA%20%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/</url>
    <content><![CDATA[<h1>Yarn ResourceManager HA 故障转移问题定位</h1>
<p>用户无法在yarn集群上提交任务</p>
<p><strong>解决方案:</strong></p>
<p>关闭所有的ResourceManager进程</p>
<p>通过zk客户端查看</p>
<p>sh /opt/cloudera/parcels/CDH/lib/zookeeper/bin/zkCli.sh</p>
<p>ls /rmstore/ZKRMStateRoot/RMAppRoot 目录下</p>
<p>不为空则使用该命令 rmr  /rmstore/ZKRMStateRoot/RMAppRoot 删除目录文件</p>
<p>删除完再启动ResourceManager恢复正常<br>
此种方式是将ZK中需要RM恢复的任务清空实现的，即RM不恢复正在运行的任务，会导致集群正在运行的任务停止</p>
<p>原文链接：<a href="https://blog.csdn.net/anguoan/article/details/128613672" target="_blank" rel="noopener">https://blog.csdn.net/anguoan/article/details/128613672</a></p>
]]></content>
  </entry>
  <entry>
    <title>用户无法在yarn集群上提交任务</title>
    <url>/2024/08/28/%E7%94%A8%E6%88%B7%E6%97%A0%E6%B3%95%E5%9C%A8yarn%E9%9B%86%E7%BE%A4%E4%B8%8A%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h1>Yarn ResourceManager HA 故障转移问题定位</h1>
<p>用户无法在yarn集群上提交任务</p>
<p><strong>解决方案:</strong></p>
<p>关闭所有的ResourceManager进程</p>
<p>通过zk客户端查看</p>
<p>sh /opt/cloudera/parcels/CDH/lib/zookeeper/bin/zkCli.sh</p>
<p>ls /rmstore/ZKRMStateRoot/RMAppRoot 目录下</p>
<p>不为空则使用该命令 rmr  /rmstore/ZKRMStateRoot/RMAppRoot 删除目录文件</p>
<p>删除完再启动ResourceManager恢复正常<br>
此种方式是将ZK中需要RM恢复的任务清空实现的，即RM不恢复正在运行的任务，会导致集群正在运行的任务停止</p>
<p>原文链接：<a href="https://blog.csdn.net/anguoan/article/details/128613672" target="_blank" rel="noopener">https://blog.csdn.net/anguoan/article/details/128613672</a></p>
]]></content>
  </entry>
  <entry>
    <title>SR日常运维</title>
    <url>/2024/08/19/SR%E6%97%A5%E5%B8%B8%E8%BF%90%E7%BB%B4/</url>
    <content><![CDATA[<h2 id="1-发生-“close-index-channel-failed”-和-“too-many-tablet-versions”-错误应该如何处理？">1. 发生 “close index channel failed” 和 “too many tablet versions” 错误应该如何处理？</h2>
<p>上述报错是因为导入频率太快，数据没能及时合并 (Compaction) ，从而导致版本数超过支持的最大未合并版本数。默认支持的最大未合并版本数为 1000。可以通过如下方法解决上述报错：</p>
<ul>
<li>
<p>增大单次导入的数据量，降低导入频率。</p>
</li>
<li>
<p>修改 BE 配置文件 <strong>be.conf</strong> 中相关参数的配置，以加快 Compaction：</p>
<ul>
<li>
<p>对于明细表、聚合表和更新表，可以适当调大 <code>cumulative_compaction_num_threads_per_disk</code>、<code>base_compaction_num_threads_per_disk</code> 和 <code>cumulative_compaction_check_interval_seconds</code> 的值。例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cumulative_compaction_num_threads_per_disk &#x3D; 4</span><br><span class="line">base_compaction_num_threads_per_disk &#x3D; 2</span><br><span class="line">cumulative_compaction_check_interval_seconds &#x3D; 2</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>对于主键表，可以适当调大 <code>update_compaction_num_threads_per_disk</code> 的值。适当调小 <code>update_compaction_per_tablet_min_interval_seconds</code> 的值。</p>
</li>
</ul>
<p>修改完成后，需要观察内存和 I/O，确保内存和 I/O 正常。</p>
</li>
</ul>
<p>curl -XPOST <a href="http://172.20.85.188:8040/api/update_config?tablet_max_versions=10000" target="_blank" rel="noopener">http://172.20.85.188:8040/api/update_config?tablet_max_versions=10000</a></p>
<p>动态修改分桶是生效的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE BAOFOO_CM_V2.cm_entry_asy_ma SET (&quot;dynamic_partition.buckets&quot; &#x3D; &quot;9&quot;)</span><br></pre></td></tr></table></figure>
<p>动态修改分区周期是生效的，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE test_db.&#96;yq_protocol_payment_order2&#96; SET (&quot;dynamic_partition.time_unit&quot; &#x3D; &quot;month&quot;);</span><br><span class="line"></span><br><span class="line">ALTER TABLE test_db.&#96;yq_protocol_payment_order2&#96; SET (&quot;dynamic_partition.enable&quot; &#x3D; &quot;false&quot;);</span><br><span class="line">ALTER TABLE test_db.&#96;yq_protocol_payment_order2&#96; ADD PARTITIONS START (&quot;2022-01-10&quot;) END (&quot;2025-01-01&quot;) EVERY (interval 1 month);</span><br><span class="line">ALTER TABLE test_db.&#96;yq_protocol_payment_order2&#96; SET (&quot;dynamic_partition.enable&quot; &#x3D; &quot;true&quot;);</span><br></pre></td></tr></table></figure>
<p>START (“2022-01-10”) END (“2025-01-01”) EVERY (interval 1 month)  这种方式添加分区要<strong>注意周期的结束时间</strong></p>
<p><strong>但是要注意结束的周期， 必须手动补全缺失的分区</strong></p>
<p>例如  按小时分区改成按天分区，就会丢失15:00 到24点的分区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PARTITION p2024081914 VALUES [(&quot;2024-08-19 14:00:00&quot;), (&quot;2024-08-19 15:00:00&quot;)),</span><br><span class="line">PARTITION p20240820 VALUES [(&quot;2024-08-20 00:00:00&quot;), (&quot;2024-08-21 00:00:00&quot;)))</span><br></pre></td></tr></table></figure>
<p>#小时到天是有问题的<br>
#天到周没有问题，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PARTITION p20240817 VALUES [(&#39;2024-08-17 00:00:00&#39;), (&#39;2024-08-18 00:00:00&#39;)),</span><br><span class="line">PARTITION p2024_34 VALUES [(&#39;2024-08-19 00:00:00&#39;), (&#39;2024-08-26 00:00:00&#39;)),</span><br><span class="line">PARTITION p2024_35 VALUES [(&#39;2024-08-26 00:00:00&#39;), (&#39;2024-09-02 00:00:00&#39;)))</span><br></pre></td></tr></table></figure>
<p>2.3版本不支持小时和年分区</p>
]]></content>
  </entry>
  <entry>
    <title>starRocks副本数修改</title>
    <url>/2024/08/19/starRocks%E5%89%AF%E6%9C%AC%E6%95%B0%E4%BF%AE%E6%94%B9/</url>
    <content><![CDATA[<p>前面提到过，我们可以为分区设置单独的存储策略，比如增加分区时使用新的副本数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE starrocks.table07</span><br><span class="line">ADD PARTITION p20210313 VALUES LESS THAN (&quot;2021-03-14&quot;)</span><br><span class="line">(&quot;replication_num&quot;&#x3D;&quot;1&quot;);</span><br></pre></td></tr></table></figure>
<p>修改分区副本数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE starrocks.table07</span><br><span class="line">MODIFY PARTITION p20210313 SET(&quot;replication_num&quot;&#x3D;&quot;2&quot;);</span><br></pre></td></tr></table></figure>
<p>修改表的默认副本数量，新建分区副本数量默认使用此值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE starrocks.table07</span><br><span class="line">SET (&quot;default.replication_num&quot; &#x3D; &quot;2&quot;);</span><br></pre></td></tr></table></figure>
<p>修改单分区表的实际副本数量（只限单分区表）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE starrocks.table06</span><br><span class="line">SET (&quot;replication_num&quot; &#x3D; &quot;1&quot;);</span><br></pre></td></tr></table></figure>
<p>修改表所有分区的副本数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE starrocks.table01</span><br><span class="line">MODIFY PARTITION(*)</span><br><span class="line">SET (&quot;replication_num&quot; &#x3D; &quot;3&quot;);</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>CDH 5.13升级到5.16</title>
    <url>/2024/08/08/CDH%205.13%E5%8D%87%E7%BA%A7%E5%88%B05.16/</url>
    <content><![CDATA[<h1>CDH 5.13升级到5.16</h1>
<p>本篇章节主要介绍如何从CDH 5.13升级到5.16版本，此步骤也可以借鉴用来作其他CDH版本升级的指导。</p>
<p>首先，准备CDH 5.16的安装包，包括CM 5.16的rpm包，及CDH 5.16的parcel包。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;parcel包</span><br><span class="line">-rw-r--r-- 1 root root 2132782197 Oct 14 16:01 CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel </span><br><span class="line">-rw-r--r-- 1 root root         41 Oct 14 16:01 CDH-5.16.2-1.cdh5.16.2.p0.8-el7.parcel.sha </span><br><span class="line">&#x2F;&#x2F;CM安装包</span><br><span class="line">cloudera-manager-agent-5.16.2-1.cm5162.p0.7.el7.x86_64.rpm     </span><br><span class="line">cloudera-manager-server-5.16.2-1.cm5162.p0.7.el7.x86_64.rpm       </span><br><span class="line">cloudera-manager-daemons-5.16.2-1.cm5162.p0.7.el7.x86_64.rpm   </span><br><span class="line">cloudera-manager-server-db-2-5.16.2-1.cm5162.p0.7.el7.x86_64.rpm  repodata</span><br></pre></td></tr></table></figure>
<p>配置临时http服务及本地yum源<br>
进入到cm rpm安装包所在的目录，执行命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y createrepo</span><br><span class="line">createrepo .</span><br><span class="line">python -m SimpleHTTPServer 8900 &amp;</span><br></pre></td></tr></table></figure>
<p>，启动http服务监听，端口为8900<br>
配置本地yum源，添加cm_local.repo文件到/etc/yum.repos.d/目录下，内容如下，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[cm_local] </span><br><span class="line">name&#x3D;cm_local </span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;192.168.81.50:8900&#x2F; </span><br><span class="line">enabled&#x3D;1 </span><br><span class="line">gpgcheck&#x3D;0</span><br></pre></td></tr></table></figure>
<p>将此文件复制到集群每个节点</p>
<p>停止所有节点的Hadoop服务及cm server及cm agent服务<br>
从7180网页中停止Hadoop集群服务<br>
使用以下命令停止cm server及agent服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service cloudera-scm-server stop</span><br><span class="line">service cloudera-scm-agent stop</span><br></pre></td></tr></table></figure>
<p>升级Cloudera Manager服务<br>
在CM server节点升级以下服务<br>
cloudera-manager-server<br>
cloudera-manager-daemons<br>
cloudera-manager-agent<br>
在CM agent节点升级以下服务<br>
cloudera-manager-daemons<br>
cloudera-manager-agent<br>
执行命令如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum upgrade cloudera-manager-server cloudera-manager-daemons cloudera-manager-agent</span><br></pre></td></tr></table></figure>
<p>注：如果CM后台元数据库使用独立的mysql，则不用考虑升级元数据。如果是使用自带的postgresql，则也需要升级，执行yum upgrade cloudera-manager-server-db-2</p>
<p>启动CM服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service cloudera-scm-server start</span><br><span class="line">service cloudera-scm-agent start</span><br></pre></td></tr></table></figure>
<p>升级CDH<br>
将5.16的parcel包复制到CM server所在的节点对应的parcel-repo目录下，并重启cm server</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp * &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo&#x2F; </span><br><span class="line">service cloudera-scm-server restart</span><br></pre></td></tr></table></figure>
<p>停止老版本服务，激活并启动新版本parcel<br>
通过7180网页，选择主机-&gt;Parcel，停止老版本服务，激活并启动新版本parcel</p>
<p>启动Hadoop服务<br>
通过以上步骤，CM及Hadoop应用都升级好了，此时可以启动Hadoop服务。</p>
<p>注意： spark2要用  SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012-el7.parcel</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r  &#x2F;etc&#x2F;cloudera-scm-server&#x2F; &#x2F;opt&#x2F;cm_bak&#x2F;cloudera-scm-server</span><br><span class="line">scp -r &#x2F;etc&#x2F;cloudera-scm-agent&#x2F; &#x2F;opt&#x2F;cm_bak&#x2F;cloudera-scm-agent </span><br><span class="line"></span><br><span class="line">yum list |grep  cloudera-manager-server cloudera-manager-daemons cloudera-manager-agent </span><br><span class="line"></span><br><span class="line">ansible cdh81 -m shell -a &quot;src&#x3D;&#x2F;etc&#x2F;yum.repos.d&#x2F;cm_local.repo  dest&#x3D;&#x2F;etc&#x2F;yum.repos.d&#x2F;cm_local.repo&quot;</span><br><span class="line"></span><br><span class="line">ansible newhost -m shell -a &quot;src&#x3D;&#x2F;etc&#x2F;krb5.conf  dest&#x3D;&#x2F;etc&#x2F;krb5.conf&quot;</span><br><span class="line"></span><br><span class="line">ansible cdh81 -m shell -a &quot;service cloudera-scm-agent stop&quot;</span><br><span class="line"></span><br><span class="line">ansible cdh81 -m shell -a &quot;yum list |grep cloudera&quot;</span><br><span class="line">ansible cdh81 -m shell -a &quot;yum -y upgrade cloudera-manager-server cloudera-manager-daemons cloudera-manager-agent &quot;</span><br><span class="line">ansible cdh81 -m shell -a &quot;service cloudera-scm-agent start&quot;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>starRocks部署</title>
    <url>/2024/07/29/starRocks%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>[clickhouse]<br>
172.20.85.[111:113]<br>
172.20.85.[138:142]</p>
<p>[fe]<br>
172.20.85.[111:113]</p>
<p>[be]<br>
172.20.85.[138:142]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible clickhouse -m copy -a &quot;src&#x3D;&#x2F;root&#x2F;.bashrc dest&#x3D;&#x2F;root&#x2F;.bashrc &quot;</span><br><span class="line">ansible clickhouse -m shell -a &quot;java -version  &quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ansible clickhouse -m shell -a <span class="string">"yum install -y rsync  "</span></span><br><span class="line">ansible clickhouse  -m synchronize  -a <span class="string">"src=/opt/module/StarRocks dest=/opt/module/"</span>  </span><br><span class="line">ansible clickhouse -m shell -a <span class="string">" ls /opt/module/StarRocks "</span></span><br></pre></td></tr></table></figure>
<h2 id="fe部署">fe部署</h2>
<p>第一次启动需指定–helper参数，后续再启动无需指定此参数<br>
sh /opt/module/StarRocks/fe/bin/start_fe.sh --helper 172.20.85.111:9010 --daemon</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible fe -m shell -a &quot; sh &#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;fe&#x2F;bin&#x2F;stop_fe.sh &quot;</span><br><span class="line">ansible fe -m shell -a &quot; sh &#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;fe&#x2F;bin&#x2F;start_fe.sh --daemon &quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql -h 127.0.0.1 -P9030 -uroot -p</span><br><span class="line">mysql&gt; SHOW PROC '/frontends'\G</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> FOLLOWER <span class="string">"172.20.85.112:9010"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> FOLLOWER <span class="string">"172.20.85.142:9010"</span>;</span><br><span class="line"><span class="comment"># ALTER SYSTEM DROP FOLLOWER "172.20.85.142:9010";</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> OBSERVER <span class="string">"172.20.85.113:9010"</span>;</span><br></pre></td></tr></table></figure>
<h2 id="be-部署">be 部署</h2>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; SHOW PROC '/backends'\G</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BACKEND <span class="string">"cdh85-138:9050"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BACKEND <span class="string">"cdh85-139:9050"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BACKEND <span class="string">"cdh85-140:9050"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BACKEND <span class="string">"cdh85-141:9050"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BACKEND <span class="string">"cdh85-142:9050"</span>;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible be -m shell -a &quot; &#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;be&#x2F;bin&#x2F;stop_be.sh &quot;</span><br><span class="line">ansible be -m shell -a &quot; &#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;be&#x2F;bin&#x2F;start_be.sh --daemon &quot;</span><br></pre></td></tr></table></figure>
<p>如添加过程出现错误，需要通过以下命令将该 BE 节点从集群移除。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; ALTER SYSTEM decommission BACKEND "host:port";</span><br></pre></td></tr></table></figure>
<h2 id="监控">监控</h2>
<p>wget <a href="https://github.com/prometheus/prometheus/releases/download/v2.29.1/prometheus-2.29.1.linux-amd64.tar.gz" target="_blank" rel="noopener">https://github.com/prometheus/prometheus/releases/download/v2.29.1/prometheus-2.29.1.linux-amd64.tar.gz</a><br>
tar -xf prometheus-2.29.1.linux-amd64.tar.gz -C /usr/local/</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">'StarRocks_Cluster01'</span> <span class="comment"># 每一个集群称之为一个job，可以自定义名字作为StarRocks集群名</span></span><br><span class="line">  <span class="attr">metrics_path:</span> <span class="string">'/metrics'</span>    <span class="comment"># 指定获取监控项目的Restful Api</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">static_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> <span class="string">['cdh85-111:8030','cdh85-112:8030','cdh85-113:8030']</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">group:</span> <span class="string">fe</span> <span class="comment"># 这里配置了 fe 的 group，该 group 中包含了 3 个 Frontends</span></span><br><span class="line"></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targets:</span> <span class="string">['cdh85-138:8040',</span> <span class="string">'cdh85-139:8040'</span><span class="string">,</span> <span class="string">'cdh85-140:8040'</span><span class="string">,'cdh85-141:8040','cdh85-142:8040']</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">group:</span> <span class="string">be</span> <span class="comment"># 这里配置了 be 的 group，该 group 中包含了 5 个 Backends</span></span><br></pre></td></tr></table></figure>
<h2 id="权限">权限</h2>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建用户</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> dba_starRocks@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'dba_starRocks'</span>;</span><br><span class="line"></span><br><span class="line">给权限</span><br><span class="line"></span><br><span class="line"><span class="keyword">GRANT</span> SELECT_PRIV,LOAD_PRIV,ALTER_PRIV,CREATE_PRIV,DROP_PRIV <span class="keyword">ON</span> *.*  <span class="keyword">TO</span> dba_starRocks;</span><br><span class="line"></span><br><span class="line">取消权限</span><br><span class="line"><span class="keyword">REVOKE</span> LOAD_PRIV,ALTER_PRIV,CREATE_PRIV,DROP_PRIV <span class="keyword">ON</span> *.*  <span class="keyword">FROM</span>  dba_starRocks;</span><br><span class="line"></span><br><span class="line">查看权限</span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">GRANTS</span> <span class="keyword">FOR</span> dba_starRocks</span><br></pre></td></tr></table></figure>
<h2 id="测试环境">测试环境</h2>
<p>[sr]<br>
10.0.19.[156:158]</p>
<h3 id="fe-重启">fe 重启</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible sr -m shell -a &quot; sh &#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;fe&#x2F;bin&#x2F;stop_fe.sh &quot;</span><br><span class="line">ansible sr -m shell -a &quot; sh &#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;fe&#x2F;bin&#x2F;start_fe.sh --daemon &quot;</span><br></pre></td></tr></table></figure>
<h3 id="be重启">be重启</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible sr -m shell -a &quot;sh  &#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;be&#x2F;bin&#x2F;stop_be.sh &quot;</span><br><span class="line">ansible sr -m shell -a &quot;sh  &#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;be&#x2F;bin&#x2F;start_be.sh --daemon &quot;</span><br></pre></td></tr></table></figure>
<h2 id="hive外部表">hive外部表</h2>
<p>需要把hdfs-site.xml文件 放到be和fe的conf目录里  并重启</p>
<p>遇到权限问题  修改 hadoop_env.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_USER_NAME&#x3D;hdfs</span><br></pre></td></tr></table></figure>
<h2 id="手动升级">手动升级</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\cp -r &#x2F;opt&#x2F;module&#x2F;StarRocks-2.3.0-rc01&#x2F;apache_hdfs_broker &#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;apache_hdfs_broker&#x2F;..&#x2F;</span><br></pre></td></tr></table></figure>
<h2 id="监控-2">监控</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible be -m shell -a &quot; netstat -nap |grep 9050 &quot;</span><br></pre></td></tr></table></figure>
<h2 id="supervisord守护进程">supervisord守护进程</h2>
<p>/etc/supervisord.d/StartRocks.ini</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[program:be]</span><br><span class="line">process_name&#x3D;%(program_name)s                        ;进程名称</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;be                   ;工作目录be所在路径</span><br><span class="line">command&#x3D;sh &#x2F;opt&#x2F;module&#x2F;StarRocks&#x2F;be&#x2F;bin&#x2F;start_be.sh  ;运行的命令be的启动sh命令</span><br><span class="line">autostart&#x3D;true                                       ;是否随supervisor自动开启</span><br><span class="line">autorestart&#x3D;true                                     ;是否在挂了之后重启，意外关闭后会重启，比如kill掉</span><br><span class="line">user&#x3D;root                                            ;用户</span><br><span class="line">numprocs&#x3D;1                                           ;进程数 如果需要同时启动多个进程，进程名称需要不一样</span><br><span class="line">startretries&#x3D;3                                       ;启动重试次数</span><br><span class="line">stopasgroup&#x3D;true                                     ;是否停止子进程</span><br><span class="line">killasgroup&#x3D;true                                     ;是否杀死子进程</span><br><span class="line">startsecs&#x3D;5                                          ;启动5秒后，如果还是运行状态才认为进程已经启动</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">supervisorctl status        &#x2F;&#x2F;查看所有进程的状态</span><br><span class="line">supervisorctl stop be       &#x2F;&#x2F;停止</span><br><span class="line">supervisorctl start be      &#x2F;&#x2F;启动</span><br><span class="line">supervisorctl restart be    &#x2F;&#x2F;重启</span><br><span class="line">supervisorctl update be     &#x2F;&#x2F;配置文件修改后使用该命令加载新的配置</span><br><span class="line">supervisorctl reload        &#x2F;&#x2F;重新启动配置中的所有程序</span><br></pre></td></tr></table></figure>
<h1>官网升级</h1>
<p><strong>升级 BE 前的准备</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#为了避免 BE 重启期间不必要的 Tablet 修复，进而影响升级后的集群性能，建议在升级前先在 FE Leader 上执行如下命令以禁用 Tablet 调度功能，</span></span><br><span class="line">&gt; mysql -h 127.0.0.1 -P9030 -uroot -p</span><br><span class="line"></span><br><span class="line">admin <span class="keyword">set</span> frontend config (<span class="string">"max_scheduling_tablets"</span>=<span class="string">"0"</span>);</span><br><span class="line">admin <span class="keyword">set</span> frontend config (<span class="string">"disable_balance"</span>=<span class="string">"true"</span>);</span><br><span class="line">admin <span class="keyword">set</span> frontend config (<span class="string">"disable_colocate_balance"</span>=<span class="string">"true"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在所有 BE 重启升级完成后，通过 show backends 命令确认所有 BE 的 Alive 状态为 true 后，启用 Tablet 调度功能，</span></span><br><span class="line"></span><br><span class="line">admin <span class="keyword">set</span> frontend config (<span class="string">"max_scheduling_tablets"</span>=<span class="string">"10000"</span>);</span><br><span class="line">admin <span class="keyword">set</span> frontend config (<span class="string">"disable_balance"</span>=<span class="string">"false"</span>);</span><br><span class="line">admin <span class="keyword">set</span> frontend config (<span class="string">"disable_colocate_balance"</span>=<span class="string">"false"</span>);</span><br></pre></td></tr></table></figure>
<p><strong>升级</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scp StarRocks-2.5.0-rc03.tar.gz usdp05:/opt/module/</span><br><span class="line"><span class="meta">#</span><span class="bash">ansible clickhouse  -m copy  -a <span class="string">"src=/opt/module/StarRocks-2.5.0-rc03.tar.gz dest=/opt/module/"</span></span></span><br><span class="line"></span><br><span class="line">cd /opt/module</span><br><span class="line">tar -zxvf StarRocks-2.3.18.tar.gz </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 升级 BE 节点</span></span><br><span class="line">supervisorctl stop starrocks_be</span><br><span class="line">cd /opt/module/StarRocks/be</span><br><span class="line">sh bin/stop_be.sh</span><br><span class="line">rm -rf lib.bak &amp;&amp; mv lib lib.bak </span><br><span class="line">rm -rf bin.bak &amp;&amp; mv bin bin.bak</span><br><span class="line">cp -r /opt/module/StarRocks-2.3.18/be/lib .</span><br><span class="line">cp -r /opt/module/StarRocks-2.3.18/be/bin .</span><br><span class="line">ps aux | grep starrocks_be</span><br><span class="line">supervisorctl start starrocks_be</span><br><span class="line">ps aux | grep starrocks_be</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 升级 FE 节点</span></span><br><span class="line">supervisorctl stop starrocks_fe</span><br><span class="line">cd /opt/module/StarRocks/fe</span><br><span class="line">rm -rf meta.bak/ &amp;&amp; cp -r meta meta.bak</span><br><span class="line">rm -rf lib.bak &amp;&amp; mv lib lib.bak </span><br><span class="line">rm -rf bin.bak &amp;&amp; mv bin bin.bak</span><br><span class="line">rm -rf spark-dpp.bak &amp;&amp; mv spark-dpp spark-dpp.bak</span><br><span class="line">cp -r /opt/module/StarRocks-2.3.18/fe/lib .</span><br><span class="line">cp -r /opt/module/StarRocks-2.3.18/fe/bin .</span><br><span class="line">cp -r /opt/module/StarRocks-2.3.18/fe/spark-dpp .</span><br><span class="line">supervisorctl start starrocks_fe</span><br><span class="line"><span class="meta">#</span><span class="bash"> sh bin/start_fe.sh --daemon</span></span><br><span class="line">ps aux | grep StarRocksFE</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 升级 Broker</span></span><br><span class="line">cd /opt/module/StarRocks/apache_hdfs_broker</span><br><span class="line">rm -rf lib.bak &amp;&amp; mv lib lib.bak </span><br><span class="line">rm -rf bin.bak &amp;&amp; mv bin bin.bak</span><br><span class="line">cp -r /opt/module/StarRocks-2.5.0-rc03/apache_hdfs_broker/lib  .   </span><br><span class="line">cp -r /opt/module/StarRocks-2.5.0-rc03/apache_hdfs_broker/bin  .</span><br><span class="line">sh bin.bak/stop_broker.sh</span><br><span class="line">sh bin/start_broker.sh --daemon</span><br></pre></td></tr></table></figure>
<h3 id="批量操作">批量操作</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ansible sr -m shell -a " sh /opt/module/StarRocks/be/bin/stop_be.sh "</span><br><span class="line">ansible sr -m shell -a " sh /opt/module/StarRocks/be/bin/start_be.sh --daemon "</span><br><span class="line"></span><br><span class="line">ansible sr -m shell -a "supervisorctl status "</span><br><span class="line">ansible sr -m shell -a "supervisorctl start starrocks_be"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ansible sr -m shell -a " sh /opt/module/StarRocks/fe/bin/stop_fe.sh "</span><br><span class="line">ansible sr -m shell -a " sh /opt/module/StarRocks/fe/bin/start_fe.sh --daemon "</span><br><span class="line"></span><br><span class="line">ansible sr -m shell -a "supervisorctl status "</span><br><span class="line">ansible sr -m shell -a "supervisorctl start starrocks_fe"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">supervisorctl start starrocks_broker</span><br></pre></td></tr></table></figure>
<h2 id="扩缩容-BE-集群">扩缩容 BE 集群</h2>
<h2 id="系统优化">系统优化</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot; #创建数据文件目录&quot;</span><br><span class="line"> for i in &#123;1..10&#125; ;do mkdir &#x2F;dfs&#x2F;data$i -p;done</span><br><span class="line"> for i in &#123;a..j&#125;;do &#x2F;usr&#x2F;sbin&#x2F;mkfs.xfs &#x2F;dev&#x2F;sd$i;done</span><br><span class="line"> echo -e &quot;&#x2F;dev&#x2F;sda	&#x2F;dfs&#x2F;data10     xfs	defaults	0 0</span><br><span class="line"> &#x2F;dev&#x2F;sdb	&#x2F;dfs&#x2F;data1     xfs	defaults	0 0</span><br><span class="line"> &#x2F;dev&#x2F;sdc	&#x2F;dfs&#x2F;data2     xfs	defaults	0 0</span><br><span class="line"> &#x2F;dev&#x2F;sdd	&#x2F;dfs&#x2F;data3     xfs	defaults	0 0</span><br><span class="line"> &#x2F;dev&#x2F;sde	&#x2F;dfs&#x2F;data4     xfs	defaults	0 0</span><br><span class="line"> &#x2F;dev&#x2F;sdf	&#x2F;dfs&#x2F;data5     xfs	defaults	0 0</span><br><span class="line"> &#x2F;dev&#x2F;sdg	&#x2F;dfs&#x2F;data6     xfs	defaults	0 0</span><br><span class="line"> &#x2F;dev&#x2F;sdh	&#x2F;dfs&#x2F;data7     xfs	defaults	0 0</span><br><span class="line"> &#x2F;dev&#x2F;sdi	&#x2F;dfs&#x2F;data8     xfs	defaults	0 0</span><br><span class="line"> &#x2F;dev&#x2F;sdj	&#x2F;dfs&#x2F;data9     xfs	defaults	0 0&quot;  &gt;&gt; &#x2F;etc&#x2F;fstab</span><br><span class="line"> mount -a</span><br><span class="line"> </span><br><span class="line">--修改主机名</span><br><span class="line">hostnamectl set-hostname cdh85-234</span><br><span class="line">-- 关闭交换区</span><br><span class="line">sudo vim &#x2F;etc&#x2F;sysctl.conf    </span><br><span class="line">vm.swappiness &#x3D; 0   </span><br><span class="line">-- 允许内存超配</span><br><span class="line">sudo vim &#x2F;etc&#x2F;sysctl.conf </span><br><span class="line">vm.overcommit_memory &#x3D; 1</span><br><span class="line">-- 关闭透明大页</span><br><span class="line">vim &#x2F;etc&#x2F;default&#x2F;grub   # 找到 GRUB_CMDLINE_LINUX 字段所在一行，在末尾添加 transparent_hugepage&#x3D;madvise    GRUB_CMDLINE_LINUX&#x3D;&quot;..其他参数... transparent_hugepage&#x3D;madvise&quot;  </span><br><span class="line">-- cpupower设置performance</span><br><span class="line">cpupower frequency-set -g performance</span><br><span class="line">--在溢出时中止 TCP</span><br><span class="line">sudo vim &#x2F;etc&#x2F;sysctl.conf   </span><br><span class="line">net.ipv4.tcp_abort_on_overflow &#x3D; 1 </span><br><span class="line">-- 关闭内核错误生成的文件</span><br><span class="line">sudo vim &#x2F;etc&#x2F;sysctl.conf   </span><br><span class="line">kernel.core_pattern &#x3D; |&#x2F;bin&#x2F;false</span><br><span class="line"></span><br><span class="line">-- 设置最大文件打开数</span><br><span class="line">ansible newhost  -m copy  -a &quot;src&#x3D;&#x2F;etc&#x2F;security&#x2F;limits.conf dest&#x3D;&#x2F;etc&#x2F;security&#x2F;limits.conf&quot;</span><br><span class="line">ansible newhost  -m copy  -a &quot;src&#x3D;&#x2F;etc&#x2F;systemd&#x2F;system.conf dest&#x3D;&#x2F;etc&#x2F;systemd&#x2F;system.conf&quot;</span><br><span class="line">ansible newhost  -m copy  -a &quot;src&#x3D;&#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;20-nproc.conf dest&#x3D;&#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;20-nproc.conf&quot;</span><br><span class="line"></span><br><span class="line">ansible newhost -m copy  -a &quot;src&#x3D;&#x2F;etc&#x2F;sysctl.conf dest&#x3D;&#x2F;etc&#x2F;sysctl.conf &quot;</span><br><span class="line">ansible newhost  -m copy  -a &quot;src&#x3D;&#x2F;etc&#x2F;hosts dest&#x3D;&#x2F;etc&#x2F;hosts&quot;</span><br><span class="line">ansible newhost  -m copy  -a &quot;src&#x3D;&#x2F;etc&#x2F;profile dest&#x3D;&#x2F;etc&#x2F;profile&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible newhost -m shell -a &quot;mkdir &#x2F;opt&#x2F;module&#x2F; &quot;</span><br><span class="line">ansible be  -m copy  -a &quot;src&#x3D;&#x2F;opt&#x2F;module&#x2F;StarRocks-2.3.18.tar.gz dest&#x3D;&#x2F;opt&#x2F;module&#x2F;StarRocks-2.3.18.tar.gz&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">ansible newhost -m copy  -a &quot;src&#x3D;&#x2F;root&#x2F;supervisor-4.2.5.tar.gz dest&#x3D;&#x2F;tmp&#x2F;&quot;</span><br><span class="line">ansible newhost -m shell -a &quot;tar -zxvf &#x2F;tmp&#x2F;supervisor-4.2.5.tar.gz  &quot;</span><br><span class="line">ansible newhost -m shell -a &quot;yum install python-setuptools -y &quot;</span><br><span class="line">ansible newhost -m shell -a &quot;cd supervisor-4.2.5 &amp;&amp;  python setup.py install&quot;</span><br><span class="line">ansible newhost -m shell -a &quot;mkdir &#x2F;etc&#x2F;supervisor &quot;</span><br><span class="line">ansible newhost -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;supervisor&#x2F;supervisord.conf dest&#x3D;&#x2F;etc&#x2F;supervisor&#x2F;&quot;</span><br><span class="line">ansible newhost -m copy -a &quot;src&#x3D;&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;supervisord-4.2.5.service dest&#x3D;&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;&quot;</span><br><span class="line">ansible newhost -m shell -a &quot;systemctl enable supervisord-4.2.5.service&quot;</span><br><span class="line">ansible newhost -m shell -a &quot;systemctl is-enabled supervisord-4.2.5.service&quot;</span><br><span class="line">ansible newhost -m shell -a &quot;mkdir &#x2F;etc&#x2F;supervisord.d&#x2F;&quot;</span><br><span class="line">scp cdh85-189:&#x2F;etc&#x2F;supervisord.d&#x2F;starrocks_be.ini &#x2F;tmp&#x2F;</span><br><span class="line">ansible be -m copy -a &quot;src&#x3D;&#x2F;tmp&#x2F;starrocks_be.ini dest&#x3D;&#x2F;etc&#x2F;supervisord.d&#x2F;&quot;</span><br><span class="line">ansible newhost -m shell -a &quot;systemctl start supervisord-4.2.5.service&quot;</span><br><span class="line">ansible newhost -m shell -a &quot;systemctl status supervisord-4.2.5.service&quot;</span><br><span class="line">ansible newhost -m shell -a &quot;supervisorctl status all&quot;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>在vim保存时获得sudo权限</title>
    <url>/2024/07/05/%E5%9C%A8vim%E4%BF%9D%E5%AD%98%E6%97%B6%E8%8E%B7%E5%BE%97sudo%E6%9D%83%E9%99%90/</url>
    <content><![CDATA[<h1>在vim保存时获得sudo权限</h1>
<p>在维护线上服务的时候，经常要编辑一些不属于操作用户的文件，就是只有r权限的那种，每次保存都会提示read only。只能先记下来改了什么，然后再退出，然后 sudo vim 再做保存。</p>
<p>下面的命令可以不退出vim进程，直接用<a href="https://so.csdn.net/so/search?q=vim%E5%91%BD%E4%BB%A4&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener">vim命令</a>获取sudo权限，然后直接保存文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">:w !  sudo  tee  %</span><br></pre></td></tr></table></figure>
<p>命令:w !{cmd}，让 vim 执行一个外部命令{cmd}，然后把当前缓冲区的内容从 stdin 传入。</p>
<p>tee 是一个把 stdin 保存到文件的小工具。</p>
<p>而 %，是vim当中一个只读寄存器的名字，总保存着当前编辑文件的文件路径。</p>
<p>所以执行这个命令，就相当于从vim外部修改了当前编辑的文件。</p>
<p>执行<code>:w !</code> <code>sudo</code> <code>tee</code> %后，会提示输入密码，之后再选择重新加载文件，即可看到保存后的文件</p>
]]></content>
  </entry>
  <entry>
    <title>tidb备份、还原、cdc实时同步</title>
    <url>/2024/05/21/tidb%E5%A4%87%E4%BB%BD%E3%80%81%E8%BF%98%E5%8E%9F%E3%80%81cdc%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/</url>
    <content><![CDATA[<p>tidb备份，还原，cdc实时同步</p>
<h2 id="全量备份">全量备份</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#指定时间备份</span><br><span class="line">tiup br backup full --pd &quot;172.20.192.115:2379&quot; \</span><br><span class="line">--storage &quot;s3:&#x2F;&#x2F;tidb&#x2F;all20240520?access-key&#x3D;minioadmin&amp;secret-access-key&#x3D;***&amp;endpoint&#x3D;http:&#x2F;&#x2F;172.20.192.151:6060&amp;force-path-style&#x3D;true&quot; \</span><br><span class="line">--backupts&#x3D;&#39;2024&#x2F;05&#x2F;18 00:00:00&#39;</span><br></pre></td></tr></table></figure>
<h2 id="增量备份">增量备份</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取上一次的备份时间戳</span><br><span class="line">tiup br validate decode --field&#x3D;&quot;end-version&quot; --storage &#39;s3:&#x2F;&#x2F;tidb&#x2F;all20240520?access-key&#x3D;minioadmin&amp;secret-access-key&#x3D;minioadmin&amp;endpoint&#x3D;http:&#x2F;&#x2F;172.20.192.151:6060&#39;| tail -n1</span><br><span class="line"></span><br><span class="line">tiup br backup full --pd &quot;172.20.192.115:2379&quot; \</span><br><span class="line">--storage &quot;s3:&#x2F;&#x2F;tidb&#x2F;add20240520?access-key&#x3D;minioadmin&amp;secret-access-key&#x3D;***&amp;endpoint&#x3D;http:&#x2F;&#x2F;172.20.192.151:6060&amp;force-path-style&#x3D;true&quot; \</span><br><span class="line">--lastbackupts&#x3D;&#39;449829037670400000&#39;</span><br></pre></td></tr></table></figure>
<h2 id="cdc实时同步">cdc实时同步</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取上一次的备份时间戳</span><br><span class="line">tiup br validate decode --field&#x3D;&quot;end-version&quot; --storage &#39;s3:&#x2F;&#x2F;tidb&#x2F;all20240520?access-key&#x3D;minioadmin&amp;secret-access-key&#x3D;***&amp;endpoint&#x3D;http:&#x2F;&#x2F;172.20.192.151:6060&#39;| tail -n1</span><br><span class="line"></span><br><span class="line">tiup cdc cli changefeed create --server&#x3D;http:&#x2F;&#x2F;172.20.192.108:8300 --sink-uri&#x3D;&quot;mysql:&#x2F;&#x2F;root:***@172.20.192.233:32570&quot; --changefeed-id&#x3D;&quot;prod-to-k8s&quot; --start-ts&#x3D;&quot;449887328375668790&quot;</span><br></pre></td></tr></table></figure>
<p>–  对于错误代码为 8138、8139 和 8140 的错误，可以通过设置 set @@tidb_enable_mutation_checker=0 跳过检查。<br>
–  对于错误代码为 8141 的错误，可以通过设置 set @@tidb_txn_assertion_level=OFF 跳过检查。</p>
<p>“error”: {<br>
“time”: “2024-05-20T18:08:00.547340271+08:00”,<br>
“addr”: “172.20.192.107:8300”,<br>
“code”: “CDC:ErrChangefeedUnretryable”,<br>
“message”: “[CDC:ErrMySQLTxnError]MySQL txn error: context deadline exceeded”<br>
}</p>
<p>tiup cdc cli changefeed create --server=http://172.20.192.107:8300 --sink-uri=“mysql://root:***@172.20.192.233:32570?max-txn-row=20” --changefeed-id=“prod-to-k8s” --start-ts=“449889998692417548”  --config /home/tidb/cdc-prod-to-k8s.toml</p>
<h3 id="开启日志备份">开启日志备份</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tiup br log start --task-name&#x3D;pitr --pd&#x3D;&quot;172.20.192.115:2379&quot; \</span><br><span class="line">--storage&#x3D;&#39;s3:&#x2F;&#x2F;tidb&#x2F;log-backup?access-key&#x3D;minioadmin&amp;secret-access-key&#x3D;***&amp;endpoint&#x3D;http:&#x2F;&#x2F;172.20.192.151:6060&amp;force-path-style&#x3D;true&quot;&#39;</span><br></pre></td></tr></table></figure>
<p>tiup br log status --task-name=pitr --pd=“172.20.192.115:2379”</p>
]]></content>
  </entry>
  <entry>
    <title>TIDB搭建双集群主从复制</title>
    <url>/2024/05/16/TIDB%E6%90%AD%E5%BB%BA%E5%8F%8C%E9%9B%86%E7%BE%A4%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/</url>
    <content><![CDATA[<h1>TIDB搭建双集群主从复制</h1>
<p>官网参考文档</p>
<p><a href="https://docs.pingcap.com/zh/tidb/v6.5/replicate-between-primary-and-secondary-clusters#%E6%90%AD%E5%BB%BA%E5%8F%8C%E9%9B%86%E7%BE%A4%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener">https://docs.pingcap.com/zh/tidb/v6.5/replicate-between-primary-and-secondary-clusters#搭建双集群主从复制</a></p>
<h1>TiCDC 安装部署与集群运维</h1>
<p><a href="https://docs.pingcap.com/zh/tidb/v6.5/deploy-ticdc" target="_blank" rel="noopener">https://docs.pingcap.com/zh/tidb/v6.5/deploy-ticdc</a></p>
<ol>
<li>
<p>备份数据。</p>
<p>在上游集群中执行 BACKUP 语句备份数据：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">MySQL [(none)]&gt; BACKUP DATABASE * TO '`s3://backup?access-key=minio&amp;secret-access-key=miniostorage&amp;endpoint=http://$&#123;HOST_IP&#125;:6060&amp;force-path-style=true`' RATE_LIMIT = 120 MB/SECOND;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+----------------------+----------+--------------------+---------------------+---------------------+</span><br><span class="line">| Destination          | Size     | BackupTS           | Queue Time          | Execution Time      |</span><br><span class="line">+----------------------+----------+--------------------+---------------------+---------------------+</span><br><span class="line">| local:&#x2F;&#x2F;&#x2F;tmp&#x2F;backup&#x2F; | 10315858 | 431434047157698561 | 2022-02-25 19:57:59 | 2022-02-25 19:57:59 |</span><br><span class="line">+----------------------+----------+--------------------+---------------------+---------------------+</span><br><span class="line">1 row in set (2.11 sec)</span><br></pre></td></tr></table></figure>
<p>备份语句提交成功后，TiDB 会返回关于备份数据的元信息，这里需要重点关注 BackupTS，它意味着该时间点之前数据会被备份，后边的教程中，将使用 BackupTS 作为<strong>数据校验截止时间</strong>和 <strong>TiCDC 增量扫描的开始时间</strong>。</p>
</li>
<li>
<p>恢复数据。</p>
<p>在下游集群中执行 RESTORE 语句恢复数据：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">mysql&gt; RESTORE DATABASE * FROM '`s3://backup?access-key=minio&amp;secret-access-key=miniostorage&amp;endpoint=http://$&#123;HOST_IP&#125;:6060&amp;force-path-style=true`';</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+----------------------+----------+--------------------+---------------------+---------------------+</span><br><span class="line">| Destination          | Size     | BackupTS           | Queue Time          | Execution Time      |</span><br><span class="line">+----------------------+----------+--------------------+---------------------+---------------------+</span><br><span class="line">| local:&#x2F;&#x2F;&#x2F;tmp&#x2F;backup&#x2F; | 10315858 | 431434141450371074 | 2022-02-25 20:03:59 | 2022-02-25 20:03:59 |</span><br><span class="line">+----------------------+----------+--------------------+---------------------+---------------------+</span><br><span class="line">1 row in set (41.85 sec)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>创建一个 TiCDC 同步任务，实时同步主集群数据到从集群</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tiup cdc cli changefeed create --server&#x3D;http:&#x2F;&#x2F;172.20.192.108:8300 --sink-uri&#x3D;&quot;mysql:&#x2F;&#x2F;root:Admi*@172.20.192.101:4000&quot; --changefeed-id&#x3D;&quot;upstream-to-downstream&quot; --start-ts&#x3D;&quot;445309452101091541&quot;</span><br></pre></td></tr></table></figure>
<p>以上命令中：</p>
<ul>
<li><code>--server</code>：TiCDC 集群任意一节点的地址</li>
<li><code>--sink-uri</code>：同步任务下游的地址</li>
<li><code>--start-ts</code>：TiCDC 同步的起点，需要设置为实际的备份时间点（也就是<a href="https://docs.pingcap.com/zh/tidb/v6.5/replicate-between-primary-and-secondary-clusters#%E7%AC%AC-2-%E6%AD%A5%E8%BF%81%E7%A7%BB%E5%85%A8%E9%87%8F%E6%95%B0%E6%8D%AE" target="_blank" rel="noopener">第 2 步：迁移全量数据</a>提到的 BackupTS）</li>
</ul>
<p>使用 TiCDC 命令行工具来查看集群状态</p>
<p>tiup ctl:v6.5.0 cdc capture list --server=http://172.20.192.107:8300</p>
<h2 id="查询同步任务列表-（管理-Changefeed）">查询同步任务列表  （管理 Changefeed）</h2>
<p>使用以下命令来查询同步任务列表：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">curl -X GET http://172.20.192.107:8300/api/v1/changefeeds</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"upstream_id"</span>: 7247017946125403024,</span><br><span class="line">        <span class="string">"namespace"</span>: <span class="string">"default"</span>,</span><br><span class="line">        <span class="string">"id"</span>: <span class="string">"upstream-to-downstream"</span>,</span><br><span class="line">        <span class="string">"state"</span>: <span class="string">"normal"</span>,</span><br><span class="line">        <span class="string">"checkpoint_tso"</span>: 448645177183305734,</span><br><span class="line">        <span class="string">"checkpoint_time"</span>: <span class="string">"2024-03-26 17:32:10.637"</span>,</span><br><span class="line">        <span class="string">"error"</span>: null</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>checkpoint</code> 即为 TiCDC 已经将该时间点前的数据同步到了下游。</li>
<li><code>state</code> 为该同步任务的状态：
<ul>
<li><code>normal</code>：正常同步</li>
<li><code>stopped</code>：停止同步（手动暂停）</li>
<li><code>error</code>：停止同步（出错）</li>
<li><code>removed</code>：已删除任务（只在指定 <code>--all</code> 选项时才会显示该状态的任务。未指定时，可通过 <code>query</code> 查询该状态的任务）</li>
<li><code>finished</code>：任务已经同步到指定 <code>target-ts</code>，处于已完成状态（只在指定 <code>--all</code> 选项时才会显示该状态的任务。未指定时，可通过 <code>query</code> 查询该状态的任务）。</li>
</ul>
</li>
</ul>
<h2 id="暂停同步任务等参考官网">暂停同步任务等参考官网</h2>
<p><a href="https://docs.pingcap.com/zh/tidb/stable/ticdc-open-api-v2" target="_blank" rel="noopener">https://docs.pingcap.com/zh/tidb/stable/ticdc-open-api-v2</a></p>
<p>暂停</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST http:&#x2F;&#x2F;172.20.192.107:8300&#x2F;api&#x2F;v2&#x2F;changefeeds&#x2F;upstream-to-downstream&#x2F;pause</span><br></pre></td></tr></table></figure>
<p>恢复</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST http:&#x2F;&#x2F;172.20.192.107:8300&#x2F;api&#x2F;v2&#x2F;changefeeds&#x2F;upstream-to-downstream&#x2F;resume -d &#39;&#123;&#125;&#39;</span><br></pre></td></tr></table></figure>
<p>删除</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X DELETE http:&#x2F;&#x2F;172.20.192.107:8300&#x2F;api&#x2F;v2&#x2F;changefeeds&#x2F;prod-to-k8s</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tiup cdc cli changefeed create --server&#x3D;http:&#x2F;&#x2F;172.20.192.108:8300 --sink-uri&#x3D;&quot;mysql:&#x2F;&#x2F;root:Ad*@172.20.192.233:32570&quot; --changefeed-id&#x3D;&quot;prod-to-k8s&quot; --start-ts&#x3D;&quot;449753049437044846&quot;  --disable-gc-check</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>漫道云环境安装Tidb</title>
    <url>/2024/05/16/%E6%BC%AB%E9%81%93%E4%BA%91%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85Tidb/</url>
    <content><![CDATA[<p>漫道云环境安装Tidb</p>
<h3 id="安装-TiDB-Operator-CRDs">安装 TiDB Operator CRDs</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;pingcap&#x2F;tidb-operator&#x2F;v1.6.0-beta.1&#x2F;manifests&#x2F;crd.yaml</span><br></pre></td></tr></table></figure>
<p>helm delete tidb-operator -n tidb-admin</p>
<p>kubectl create namespace tidb-cluster &amp;&amp; \</p>
<p>wget <a href="https://raw.githubusercontent.com/pingcap/tidb-operator/v1.6.0-beta.1/examples/advanced/tidb-cluster.yaml" target="_blank" rel="noopener">https://raw.githubusercontent.com/pingcap/tidb-operator/v1.6.0-beta.1/examples/advanced/tidb-cluster.yaml</a></p>
<p>kubectl -n tidb-cluster apply -f  tidb-cluster.yaml</p>
<p>kubectl -n tidb-cluster delete -f tidb-cluster.yaml</p>
<p>wget <a href="https://raw.githubusercontent.com/pingcap/tidb-operator/v1.6.0-beta.1/examples/basic/tidb-dashboard.yaml" target="_blank" rel="noopener">https://raw.githubusercontent.com/pingcap/tidb-operator/v1.6.0-beta.1/examples/basic/tidb-dashboard.yaml</a></p>
<p>kubectl -n tidb-cluster apply -f  tidb-dashboard.yaml</p>
<p>kubectl -n tidb-cluster delete -f  tidb-dashboard.yaml</p>
<p>wget  <a href="https://raw.githubusercontent.com/pingcap/tidb-operator/v1.6.0-beta.1/examples/advanced/tidb-monitor.yaml" target="_blank" rel="noopener">https://raw.githubusercontent.com/pingcap/tidb-operator/v1.6.0-beta.1/examples/advanced/tidb-monitor.yaml</a></p>
<p>kubectl -n tidb-cluster apply -f  tidb-monitor.yaml</p>
<p>kubectl -n tidb-cluster delete -f  tidb-monitor.yaml</p>
]]></content>
  </entry>
  <entry>
    <title>tidb 升级</title>
    <url>/2024/04/28/tidb%20%E5%8D%87%E7%BA%A7/</url>
    <content><![CDATA[<p>tidb 升级</p>
<ol>
<li>
<p>先升级 TiUP 版本（建议 <code>tiup</code> 版本不低于 <code>1.11.3</code>）：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">tiup update --self</span><br><span class="line">tiup --version</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>再升级 TiUP Cluster 版本（建议 <code>tiup cluster</code> 版本不低于 <code>1.11.3</code>）：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">tiup update cluster</span><br><span class="line">tiup cluster --version</span><br></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tiup cluster check tidb-prod-101 --cluster</span><br></pre></td></tr></table></figure>
<p>停止组件cdc</p>
<p>tiup cluster stop tidb-prod -R cdc</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tiup cluster upgrade tidb-prod-101 v7.5.1</span><br></pre></td></tr></table></figure>
<p>tiup cluster display tidb-prod-101</p>
]]></content>
  </entry>
  <entry>
    <title>Minio 模拟兼容 S3 的存储服务</title>
    <url>/2024/04/17/Minio%20%E6%A8%A1%E6%8B%9F%E5%85%BC%E5%AE%B9%20S3%20%E7%9A%84%E5%AD%98%E5%82%A8%E6%9C%8D%E5%8A%A1/</url>
    <content><![CDATA[<h3 id="搭建-MinIO-作为备份存储系统-，兼容S3">搭建 MinIO 作为备份存储系统 ，兼容S3</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;dl.min.io&#x2F;server&#x2F;minio&#x2F;release&#x2F;linux-amd64&#x2F;minio</span><br><span class="line">chmod +x minio</span><br><span class="line"></span><br><span class="line"># 配置访问 minio 的 access-key access-screct-id</span><br><span class="line"></span><br><span class="line">export HOST_IP&#x3D;&#39;172.20.192.151&#39; </span><br><span class="line">export MINIO_ROOT_USER&#x3D;&#39;minio&#39;</span><br><span class="line">export MINIO_ROOT_PASSWORD&#x3D;&#39;miniostorage&#39;</span><br><span class="line"></span><br><span class="line"># 创建数据目录,  其中 backup 为 bucket 的名称</span><br><span class="line"></span><br><span class="line">mkdir -p data&#x2F;backup</span><br><span class="line"></span><br><span class="line"># 启动 minio, 暴露端口在 6060</span><br><span class="line"></span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;minio server &#x2F;opt&#x2F;module&#x2F;data --address :6060 &amp;</span><br></pre></td></tr></table></figure>
<h2 id="单节点部署多磁盘">单节点部署多磁盘</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;opt&#x2F;module&#x2F;minio server --address :6060  &#x2F;dfs&#x2F;data1&#x2F;s3data &#x2F;dfs&#x2F;data2&#x2F;s3data &#x2F;dfs&#x2F;data3&#x2F;s3data &amp;</span><br></pre></td></tr></table></figure>
<h2 id="备份-还原-TIDB数据">备份 还原 TIDB数据</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mycli -h172.20.192.70 -P4000 -uroot</span><br><span class="line"></span><br><span class="line">BACKUP DATABASE test TO &#39;s3:&#x2F;&#x2F;backup?access-key&#x3D;minio&amp;secret-access-key&#x3D;miniostorage&amp;endpoint&#x3D;http:&#x2F;&#x2F;172.20.192.151:6060&amp;force-path-style&#x3D;true&#39; RATE_LIMIT &#x3D; 120 MB&#x2F;SECOND;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mycli -h172.20.192.115 -P4000 -uroot</span><br><span class="line"></span><br><span class="line">mysql&gt; RESTORE DATABASE test FROM &#39;s3:&#x2F;&#x2F;backup?access-key&#x3D;minio&amp;secret-access-key&#x3D;miniostorage&amp;endpoint&#x3D;http:&#x2F;&#x2F;172.20.192.151:6060&amp;force-path-style&#x3D;true&#39;;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Nginx 高可用集群解决方案 Nginx + Keepalived</title>
    <url>/2024/04/17/Nginx%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%20Nginx%20+%20Keepalived/</url>
    <content><![CDATA[<h2 id="Nginx-高可用集群解决方案-Nginx-Keepalived">Nginx 高可用集群解决方案 Nginx + Keepalived</h2>
<ol>
<li>
<p>修改 vim /opt/nginx/nginx.conf</p>
</li>
<li>
<p>重启容器</p>
</li>
</ol>
<p>docker restart 1f163b3a620a</p>
<p>172.20.192.151</p>
<p>172.20.192.152</p>
]]></content>
  </entry>
  <entry>
    <title>使用Yarn标签机制实现任务资源隔离</title>
    <url>/2024/04/09/%E4%BD%BF%E7%94%A8Yarn%E6%A0%87%E7%AD%BE%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0%E4%BB%BB%E5%8A%A1%E8%B5%84%E6%BA%90%E9%9A%94%E7%A6%BB/</url>
    <content><![CDATA[<h1>使用Yarn标签机制实现任务资源隔离</h1>
<p>官网参考地址：  <a href="https://docs.cloudera.com/runtime/7.2.0/yarn-allocate-resources/topics/yarn-configuring-node-labels.html" target="_blank" rel="noopener">https://docs.cloudera.com/runtime/7.2.0/yarn-allocate-resources/topics/yarn-configuring-node-labels.html</a></p>
<p>参考地址： <a href="https://zhuanlan.zhihu.com/p/674882366" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/674882366</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@bigdata-1 ~]# sudo -u hdfs hadoop fs -mkdir -p &#x2F;yarn&#x2F;node-labels</span><br><span class="line">[root@bigdata-1 ~]# sudo -u hdfs hadoop fs -chown -R yarn:yarn &#x2F;yarn</span><br><span class="line">[root@bigdata-1 ~]# sudo -u hdfs hadoop fs -chmod -R 700 &#x2F;yarn</span><br><span class="line">[root@bigdata-1 ~]# hadoop fs -ls &#x2F;yarn</span><br></pre></td></tr></table></figure>
<p>YARN Service Advanced Configuration Snippet (Safety Valve) for yarn-site.xml</p>
<p>add the following:</p>
<ul>
<li>
<p>Set the following property to enable Node Labels:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Name: yarn.node-labels.enabled</span><br><span class="line">Value: true</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Set the following property to reference the HDFS node label directory</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Name: yarn.node-labels.fs-store.root-dir</span><br><span class="line">Value: hdfs:&#x2F;&#x2F;:&#x2F;</span><br></pre></td></tr></table></figure>
<p>For example,</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Name: yarn.node-labels.fs-store.root-dir</span><br><span class="line">Value: hdfs:&#x2F;&#x2F;node-1.example.com:8020&#x2F;yarn&#x2F;node-labels&#x2F;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="使用命令方式给yarn集群添加标签"><strong>使用命令方式给yarn集群添加标签</strong></h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u yarn yarn rmadmin -addToClusterNodeLabels &quot;x(exclusive&#x3D;true),y(exclusive&#x3D;false)&quot;</span><br></pre></td></tr></table></figure>
<p><strong>使用命令方式节点关联标签：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u yarn  yarn rmadmin -replaceLabelsOnNode &quot;bigdata-2.baofoo.cn&#x3D;x bigdata-3.baofoo.cn&#x3D;y&quot;</span><br></pre></td></tr></table></figure>
<h3 id="提交MR任务指定标签：-测试下来不能用">提交MR任务指定标签：  测试下来不能用</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.0.1-tests.jar TestDFSIO -D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark -Dmapreduce.job.node-label-expression&#x3D;x -write -nrFiles 10 -fileSize 1000</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Apache Kyuubi on CDH 部署</title>
    <url>/2024/04/03/Apache%20Kyuubi%20on%20CDH%20%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h1>Apache Kyuubi on CDH 部署</h1>
<p><a href="https://shmily-qjj.top/ee1c2df4/" target="_blank" rel="noopener">https://shmily-qjj.top/ee1c2df4/</a></p>
<h1>Apache Kyuubi on Spark 在 CDH 上的深度实践</h1>
<p><a href="https://www.slidestalk.com/openLooKeng/22" target="_blank" rel="noopener">https://www.slidestalk.com/openLooKeng/22</a></p>
<p>vi  <a href="http://kyuubi-env.sh" target="_blank" rel="noopener">kyuubi-env.sh</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-cloudera</span><br><span class="line">export SPARK_HOME&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;spark3</span><br><span class="line">export FLINK_HOME&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;FLINK</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hive</span><br><span class="line">export HADOOP_CONF_DIR&#x3D;&#x2F;etc&#x2F;hadoop&#x2F;conf</span><br><span class="line">export YARN_CONF_DIR&#x3D;&#x2F;etc&#x2F;hadoop&#x2F;conf</span><br><span class="line">export KYUUBI_JAVA_OPTS&#x3D;&quot;-Xmx10g -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk&#x3D;4096 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction&#x3D;70 -XX:+UseCMSInitiatingOccupancyOnly -XX:+CMSClassUnloadingEnabled </span><br><span class="line">-XX:+CMSParallelRemarkEnabled -XX:+UseCondCardMark -XX:MaxDirectMemorySize&#x3D;1024m  -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath&#x3D;.&#x2F;logs -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -Xloggc:.&#x2F;logs&#x2F;kyuubi-server-gc-%t.log -XX:+Us</span><br><span class="line">eGCLogFileRotation -XX:NumberOfGCLogFiles&#x3D;10 -XX:GCLogFileSize&#x3D;5M -XX:NewRatio&#x3D;3 -XX:MetaspaceSize&#x3D;512m&quot;</span><br></pre></td></tr></table></figure>
<p>vi  conf/kyuubi-defaults.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kyuubi.frontend.bind.host                10.0.0.1</span><br><span class="line">kyuubi.frontend.protocols                THRIFT_BINARY</span><br><span class="line">kyuubi.frontend.thrift.binary.bind.port  10009</span><br><span class="line">#</span><br><span class="line"># kyuubi.engine.type                       SPARK_SQL</span><br><span class="line"># kyuubi.engine.share.level                USER</span><br><span class="line"># kyuubi.session.engine.initialize.timeout PT3M</span><br><span class="line">#</span><br><span class="line">kyuubi.ha.addresses                      zk1:2181,zk2:2181,zk3:2181</span><br><span class="line">kyuubi.ha.namespace                      kyuubi</span><br><span class="line"></span><br><span class="line"># 不指定 spark就是local模式运行了</span><br><span class="line">spark.master&#x3D;yarn</span><br><span class="line">spark.submit.deployMode&#x3D;cluster</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">kyuubi.authentication&#x3D;LDAP</span><br><span class="line">kyuubi.authentication.ldap.baseDN&#x3D;dc&#x3D;org</span><br><span class="line">kyuubi.authentication.ldap.domain&#x3D;apache.org</span><br><span class="line">kyuubi.authentication.ldap.binddn&#x3D;uid&#x3D;kyuubi,OU&#x3D;Users,DC&#x3D;apache,DC&#x3D;org</span><br><span class="line">kyuubi.authentication.ldap.bindpw&#x3D;kyuubi123123</span><br><span class="line">kyuubi.authentication.ldap.url&#x3D;ldap:&#x2F;&#x2F;hostname.com:389&#x2F;</span><br></pre></td></tr></table></figure>
<h3 id="重启">重启</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">注意 ：kyuubi 停止的时候   spark on yarn的  kyuubi_application  并没有停掉，   需要在yarn里kill，  这样重启  才能加载新加入的jar包和新的配置文件。</span><br><span class="line"></span><br><span class="line">sudo -u hdfs bin&#x2F;kyuubi start</span><br><span class="line">sudo -u hdfs bin&#x2F;kyuubi stop</span><br><span class="line">sudo -u hdfs bin&#x2F;kyuubi restart</span><br></pre></td></tr></table></figure>
<p>bin/beeline -u ‘jdbc:hive2://10.0.19.132:10009/’ -n hadoop</p>
<p>bin/beeline -u ‘jdbc:hive2://172.20.192.36:10009/’ -n hadoop</p>
<p>jdbc:hive2://172.20.192.36:10009/?spark.app.name=test_kyuubi_application;spark.sql.adaptive.enabled=true;spark.driver.memory=4G;spark.executor.instances=256;spark.executor.cores=4;spark.executor.memory=16g</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">?spark.app.name&#x3D;test_kyuubi_application;spark.dynamicAllocation.enabled&#x3D;true;spark.shuffle.service.enabled&#x3D;true;spark.shuffle.useOldFetchProtocol&#x3D;true;spark.dynamicAllocation.initialExecutors&#x3D;10;spark.dynamicAllocation.minExecutors&#x3D;5;spark.dynamicAllocation.maxExecutors&#x3D;200;spark.dynamicAllocation.executorIdleTimeout&#x3D;60s;spark.dynamicAllocation.cachedExecutorIdleTimeout&#x3D;10min;spark.executor.cores&#x3D;1;spark.executor.memory&#x3D;2G;spark.yarn.queue&#x3D;bf_yarn_pool.development</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>CDH6.3.2 升级 Spark3.3.0 版本</title>
    <url>/2024/04/03/CDH6.3.2%20%E5%8D%87%E7%BA%A7%20Spark3.3.0%20%E7%89%88%E6%9C%AC/</url>
    <content><![CDATA[<h1>CDH6.3.2 升级 Spark3.3.0 版本</h1>
<p><a href="https://juejin.cn/post/7140053569431928845" target="_blank" rel="noopener">https://juejin.cn/post/7140053569431928845</a></p>
<p>根据上面的文档进行部署  还有下列操作需要补充</p>
<p>上传到要部署 spark3 的客户端机器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf spark-3.3.0-bin-3.0.0-cdh6.3.2.tgz -C /opt/cloudera/parcels/CDH/lib</span><br><span class="line">cd /opt/cloudera/parcels/CDH/lib</span><br><span class="line">mv spark-3.3.0-bin-3.0.0-cdh6.3.2/ spark3</span><br></pre></td></tr></table></figure>
<h3 id="配置-conf">配置 conf</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">shell复制代码cd /opt/cloudera/parcels/CDH/lib/spark3/conf</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 开启日志</span></span></span><br><span class="line">mv log4j2.properties.template log4j2.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># spark-defaults.conf 配置</span></span></span><br><span class="line">cp /opt/cloudera/parcels/CDH/lib/spark/conf/spark-defaults.conf ./</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改 spark-defaults.conf</span></span><br><span class="line">vim /opt/cloudera/parcels/CDH/lib/spark3/conf/spark-defaults.conf</span><br><span class="line">删除 spark.extraListeners、spark.sql.queryExecutionListeners、spark.yarn.jars</span><br><span class="line">添加 spark.yarn.jars=hdfs://ns1/user/spark/3versionJars/*</span><br><span class="line"></span><br><span class="line">hadoop fs -mkdir -p /spark/3versionJars</span><br><span class="line">cd /opt/cloudera/parcels/CDH/lib/spark3/jars</span><br><span class="line">hadoop fs -put *.jar hdfs://ns1/user/spark/3versionJars</span><br></pre></td></tr></table></figure>
<h2 id="优化设置">优化设置</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.kryoserializer.buffer.max 512m</span><br><span class="line">spark.serializer org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.authenticate false   （关闭数据块传输服务SASL加密认证）</span><br><span class="line">spark.io.encryption.enabled false   （关闭I&#x2F;O加密）</span><br><span class="line">spark.network.crypto.enabled false  （关闭基于AES算法的RPC加密）</span><br><span class="line">spark.shuffle.service.enabled true  （启用外部ShuffleService提高Shuffle稳定性）</span><br><span class="line">spark.shuffle.service.port 7337  （这个外部ShuffleService由YarnNodeManager提供，默认端口7337）</span><br><span class="line">spark.shuffle.useOldFetchProtocol true  （兼容旧的Shuffle协议避免报错）</span><br><span class="line">spark.sql.cbo.enabled true  (启用CBO基于代价的优化-代替RBO基于规则的优化-Optimizer)</span><br><span class="line">spark.sql.cbo.starSchemaDetection true  （星型模型探测，判断列是否是表的主键）</span><br><span class="line">spark.sql.datetime.java8API.enabled false</span><br><span class="line">spark.sql.sources.partitionOverwriteMode dynamic </span><br><span class="line">spark.sql.orc.mergeSchema true  （ORC格式Schema加载时从所有数据文件收集）</span><br><span class="line">spark.sql.parquet.mergeSchema false (根据情况设置，我们集群大多数都是parquet，从所有文件收集Schema会影响性能，所以从随机一个Parquet文件收集Schema)</span><br><span class="line">spark.sql.parquet.writeLegacyFormat true  （兼容旧集群）</span><br><span class="line">spark.sql.autoBroadcastJoinThreshold 1048576  （当前仅支持运行了ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan的Hive Metastore表，以及直接在数据文件上计算统计信息的基于文件的数据源表）</span><br><span class="line">spark.sql.adaptive.enabled true   （Spark AQE[adaptive query execution]启用，AQE的优势：执行计划可动态调整、调整的依据是中间结果的精确统计信息）</span><br><span class="line">spark.sql.adaptive.forceApply false</span><br><span class="line">spark.sql.adaptive.logLevel info</span><br><span class="line">spark.sql.adaptive.advisoryPartitionSizeInBytes 256m  （倾斜数据分区拆分，小数据分区合并优化时，建议的分区大小，与spark.sql.adaptive.shuffle.targetPostShuffleInputSize含义相同）</span><br><span class="line">spark.sql.adaptive.coalescePartitions.enabled true  （是否开启合并小数据分区默认开启，调优策略之一）</span><br><span class="line">spark.sql.adaptive.coalescePartitions.minPartitionSize 1m  （合并后最小的分区大小）</span><br><span class="line">spark.sql.adaptive.coalescePartitions.initialPartitionNum 1024  （合并前的初始分区数）</span><br><span class="line">spark.sql.adaptive.fetchShuffleBlocksInBatch true  （是否批量拉取blocks,而不是一个个的去取，给同一个map任务一次性批量拉取blocks可以减少io 提高性能）</span><br><span class="line">spark.sql.adaptive.localShuffleReader.enabled true （不需要Shuffle操作时，使用LocalShuffleReader，例如将SortMergeJoin转为BrocastJoin）</span><br><span class="line">spark.sql.adaptive.skewJoin.enabled true   （Spark会通过拆分的方式自动处理Join过程中有数据倾斜的分区）</span><br><span class="line">spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes 128m</span><br><span class="line">spark.sql.adaptive.skewJoin.skewedPartitionFactor 5  （判断倾斜的条件：分区大小大于所有分区大小中位数的5倍，且大于spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes的值）</span><br></pre></td></tr></table></figure>
<p>将 CDH 集群的 <a href="http://spark-env.sh" target="_blank" rel="noopener">spark-env.sh</a> 复制到 /opt/cloudera/parcels/CDH/lib/spark3/conf 下:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp /etc/spark/conf/spark-env.sh  /opt/cloudera/parcels/CDH/lib/spark3/conf</span><br><span class="line">chmod +x /opt/cloudera/parcels/CDH/lib/spark3/conf/spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">修改 spark-env.sh</span></span><br><span class="line">vim /opt/cloudera/parcels/CDH/lib/spark3/conf/spark-env.sh</span><br><span class="line"></span><br><span class="line">export SPARK_HOME=/opt/cloudera/parcels/CDH/lib/spark3</span><br><span class="line">HADOOP_CONF_DIR=$&#123;HADOOP_CONF_DIR:-/etc/hadoop/conf&#125;</span><br></pre></td></tr></table></figure>
<p>将 gateway 节点的 hive-site.xml 复制到 spark3/conf 目录下，不需要做变动:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/hive/conf/hive-site.xml /opt/cloudera/parcels/CDH/lib/spark3/conf/</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp -r &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;*.xml &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;spark3&#x2F;conf&#x2F;</span><br><span class="line">cp &#x2F;etc&#x2F;hive&#x2F;conf&#x2F;hive-site.xml &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;spark3&#x2F;conf&#x2F;</span><br><span class="line"></span><br><span class="line"># 奇怪用快捷方式就不行</span><br><span class="line">cd &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;spark3&#x2F;conf</span><br><span class="line">ln -s &#x2F;etc&#x2F;hive&#x2F;conf&#x2F;hive-site.xml hive-site.xml</span><br><span class="line">ln -s &#x2F;etc&#x2F;hive&#x2F;conf&#x2F;hdfs-site.xml hdfs-site.xml</span><br><span class="line">ln -s &#x2F;etc&#x2F;hive&#x2F;conf&#x2F;core-site.xml core-site.xml</span><br><span class="line">ln -s &#x2F;etc&#x2F;hive&#x2F;conf&#x2F;mapred-site.xml mapred-site.xml</span><br><span class="line">ln -s &#x2F;etc&#x2F;hive&#x2F;conf&#x2F;yarn-site.xml yarn-site.xml</span><br><span class="line">ln -s &#x2F;etc&#x2F;spark&#x2F;conf&#x2F;spark-defaults.conf spark-defaults.conf</span><br><span class="line">ln -s &#x2F;etc&#x2F;spark&#x2F;conf&#x2F;spark-env.sh spark-env.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;local&#x2F;bin</span><br><span class="line">ln -s &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;spark3&#x2F;bin&#x2F;pyspark pyspark3</span><br></pre></td></tr></table></figure>
<p>新增一个spark3-shell的快捷方式</p>
<p>vim  /opt/cloudera/parcels/CDH/bin/spark3-shell</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Autodetect JAVA_HOME <span class="keyword">if</span> not defined</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Reference: http://stackoverflow.com/questions/59895/can<span class="_">-a</span>-bash-script-tell-what-directory-its-stored-in</span></span><br><span class="line">SOURCE="$&#123;BASH_SOURCE[0]&#125;"</span><br><span class="line">BIN_DIR="$( dirname "$SOURCE" )"</span><br><span class="line">while [ -h "$SOURCE" ]</span><br><span class="line">do</span><br><span class="line">SOURCE="$(readlink "$SOURCE")"</span><br><span class="line">[[ $SOURCE != /* ]] &amp;&amp; SOURCE="$DIR/$SOURCE"</span><br><span class="line">BIN_DIR="$( cd -P "$( dirname "$SOURCE"  )" &amp;&amp; pwd )"</span><br><span class="line">done</span><br><span class="line">BIN_DIR="$( cd -P "$( dirname "$SOURCE" )" &amp;&amp; pwd )"</span><br><span class="line">CDH_LIB_DIR=$BIN_DIR/../../CDH/lib</span><br><span class="line">LIB_DIR=$BIN_DIR/../lib</span><br><span class="line">export HADOOP_HOME=$CDH_LIB_DIR/hadoop</span><br><span class="line">. $CDH_LIB_DIR/bigtop-utils/bigtop-detect-javahome</span><br><span class="line">exec $LIB_DIR/spark3/bin/spark-shell "$@"</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;bin&#x2F;spark3-shell</span><br></pre></td></tr></table></figure>
<p>用法：alternatives --install &lt;链接&gt; &lt;名称&gt; &lt;路径&gt; &lt;优先度&gt;</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alternatives --install &#x2F;usr&#x2F;bin&#x2F;spark3-shell spark3-shell &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;bin&#x2F;spark3-shell 1</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>大数据平台常见异常处理汇总</title>
    <url>/2024/04/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E5%B8%B8%E8%A7%81%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p>HiveMetaStore状态不良导DDLSQL耗时200s以上</p>
<p>HMS进程报错：hive metastore server Failed to sync requested HMS notifications up to the event ID xxx</p>
<p>原因分析：查看sentry异常CounterWait源码发现传递的id比currentid大导致一直等待超时，超时时间默认为200s（<a href="http://sentry.notification.sync.timeout.ms" target="_blank" rel="noopener">sentry.notification.sync.timeout.ms</a>）。 开启了hdfs-sentry acl同步后，hdfs，sentry，HMS三者间权限同步的消息处理。当突然大批量的目录权限消息需要处理，后台线程处理不过来，消息积压滞后就会出现这个异常。这个异常不影响集群使用，只是会导致create，drop table慢需要等200s，这样等待也是为了追上最新的id。我们这次同时出现了HMS参与同步消息处理的线程被异常退出，导致sentry的sentry_hms_notification_id表数据一直没更新，需要重启HMS。如果积压了太多消息，让它慢慢消费处理需要的时间太长，可能一直追不上，这时可以选择丢掉这些消息。</p>
<p>解决：</p>
<p>①可以通过设置sentry.notification.sync.timeout.ms参数调小超时时间，减小等待时间，积压不多的话可以让它自行消费处理掉。</p>
<p>②丢掉未处理的消息，在sentry的sentry_hms_notification_id表中插入一条最大值(等于当前消息的id，从notification_sequence表中获取) ，重启sentry服务。（notification_log 表存储了消息日志信息）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * from  NOTIFICATION_SEQUENCE</span><br><span class="line">insert into SENTRY_HMS_NOTIFICATION_ID VALUES (14094172);</span><br></pre></td></tr></table></figure>
<p>Hive开启自动获取表上次访问时间(lastAccessTime)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.security.authorization.sqlstd.confwhitelist.append&lt;&#x2F;name&gt;</span><br><span class="line">   &lt;value&gt;hive\.exec\.pre\.hooks&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.exec.pre.hooks&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook$PreExec&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<h2 id="Spark">Spark</h2>
<p>Shuffle异常导致任务失败 报错：org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1</p>
<p>原因：</p>
<p>shuffle分为shuffle write和shuffle read两部分。 shuffle write的分区数由上一阶段的RDD分区数控制，shuffle read的分区数则是由Spark提供的一些参数控制。 shuffle write可以简单理解为类似于saveAsLocalDiskFile的操作，将计算的中间结果按某种规则临时放到各个executor所在的本地磁盘上。 shuffle read的时候数据的分区数则是由spark提供的一些参数控制。可以想到的是，如果这个参数值设置的很小，同时shuffle read的量很大，那么将会导致一个task需要处理的数据非常大。结果导致JVM crash，从而导致取shuffle数据失败，同时executor也丢失了，看到Failed to connect to host的错误，也就是executor lost的意思。有时候即使不会导致JVM crash也会造成长时间的gc。</p>
<p>解决思路：减少shuffle的数据量和增加处理shuffle数据的分区数</p>
<p>①spark.sql.shuffle.partitions控制分区数，默认为200，根据shuffle的量以及计算的复杂度提高这个值 shuffle并行度</p>
<p>②提高spark.executor.memory</p>
<p>③map side join或是broadcast join来规避shuffle的产生</p>
<p>④分析数据倾斜 解决数据倾斜</p>
<p>⑤增加失败的重试次数和重试的时间间隔 通过spark.shuffle.io.maxRetries控制重试次数，默认是3，可适当增加，例如10。 通过spark.shuffle.io.retryWait控制重试的时间间隔，默认是5s，可适当增加，例如10s。</p>
<p>⑥类似RemoteShuffleService的服务，解决Shuffle单台机器IO瓶颈，记录Shuffle状态，大批量提升Shuffle效率和稳定性。</p>
]]></content>
  </entry>
  <entry>
    <title>从 TiDB 集群迁移数据至另一 TiDB 集群</title>
    <url>/2024/03/26/%E4%BB%8E%20TiDB%20%E9%9B%86%E7%BE%A4%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E8%87%B3%E5%8F%A6%E4%B8%80%20TiDB%20%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h2 id="迁移全量数据">迁移全量数据</h2>
<p>关闭 GC。</p>
<p>为了保证增量迁移过程中新写入的数据不丢失，在开始备份之前，需要关闭上游集群的垃圾回收 (GC) 机制，以确保系统不再清理历史数据。</p>
<p>执行如下命令关闭 GC：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">MySQL [test]&gt; SET GLOBAL tidb_gc_enable=FALSE;</span><br></pre></td></tr></table></figure>
<p>备份数据。</p>
<p>在上游集群中执行 BACKUP 语句备份数据：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">MySQL [(none)]&gt; BACKUP DATABASE * TO 's3://backup?access-key=minio&amp;secret-access-key=miniostorage&amp;endpoint=http://$&#123;HOST_IP&#125;:6060&amp;force-path-style=true' RATE_LIMIT = 120 MB/SECOND;</span><br></pre></td></tr></table></figure>
<p>备份语句提交成功后，TiDB 会返回关于备份数据的元信息，这里需要重点关注 BackupTS，它意味着该时间点之前数据会被备份，后边的教程中，将使用 BackupTS 作为<strong>数据校验截止时间</strong>和 <strong>TiCDC 增量扫描的开始时间</strong>。</p>
<p>恢复数据。</p>
<p>在下游集群中执行 RESTORE 语句恢复数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">BACKUP</span> <span class="keyword">DATABASE</span> BASE <span class="keyword">TO</span> <span class="string">'s3://tidb/BASE_BAK20231031?access-key=minio&amp;secret-access-key=miniostorage&amp;endpoint=http://172.20.192.151:6060&amp;force-path-style=true'</span> RATE_LIMIT = <span class="number">120</span> MB/<span class="keyword">SECOND</span>;</span><br></pre></td></tr></table></figure>
<h2 id="使用-TiCDC-命令行工具来查看集群状态">使用 TiCDC 命令行工具来查看集群状态</h2>
<p>tiup ctl:v6.5.0 cdc capture list --server=http://172.20.192.107:8300</p>
<p>创建一个 TiCDC 同步任务，备份主集群数据到从集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tiup cdc cli changefeed create --server&#x3D;http:&#x2F;&#x2F;172.20.192.108:8300 --sink-uri&#x3D;&quot;mysql:&#x2F;&#x2F;root:Admi*@172.20.192.101:4000&quot; --changefeed-id&#x3D;&quot;upstream-to-downstream&quot; --start-ts&#x3D;&quot;445309452101091541&quot; -d</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>tidb常用操作</title>
    <url>/2024/03/26/tidb%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<p>tidb常用操作</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tiup cluster deploy tidb-test-v1 v6.5.2 ./topology.yaml --user root -p</span><br><span class="line">tiup cluster list</span><br><span class="line">tiup cluster clean tidb-test-v1 --all</span><br><span class="line"><span class="meta">#</span><span class="bash"> tiup cluster destroy tidb-test-v1</span></span><br><span class="line">tiup cluster start tidb-test-v1 --init</span><br><span class="line">tiup cluster display tidb-prod</span><br><span class="line">tiup cluster stop tidb-test-v1</span><br><span class="line">tiup cluster restart tidb-test-v1 </span><br><span class="line">tiup cluster edit-config tidb-test-v1 </span><br><span class="line">tiup cluster check tidb-test-v1 scale-out.yml --cluster --user root -p</span><br><span class="line">tiup cluster check tidb-test-v1 scale-out.yml --cluster --apply --user root -p</span><br><span class="line">tiup cluster scale-out tidb-test-v1 scale-out.yml --user root -p</span><br><span class="line">tiup ctl:v6.5.2 pd -u http://172.20.192.70:2379 store limit all engine tiflash 200 add-peer</span><br><span class="line">-- 如果你使用 TiUP 部署，可以用 tiup ctl:v&lt;CLUSTER_VERSION&gt; pd 代替 pd-ctl -u &lt;pd_ip:pd_port&gt; 命令。</span><br><span class="line">tiup ctl:v6.5.2 pd store</span><br><span class="line">tiup ctl:v6.5.2 pd -u http://172.20.192.70:2379 store limit store id 8487838  220 add-peer</span><br><span class="line">tiup ctl:v6.5.2 pd -i -u http://172.20.192.70:2379</span><br><span class="line">tiup cluster restart tidb-test-v1 -N 172.20.192.74:9090</span><br><span class="line">tiup cluster reload tidb-test-v1</span><br><span class="line">tiup install bench</span><br><span class="line">tiup diag config clinic.token eyJrIjoicFpJaDJ1a042ZjQ0a2E2NyIsInUiOjYwMywiaWQiOjU3Nn0=</span><br><span class="line">tiup diag collect tidb-test-v1</span><br></pre></td></tr></table></figure>
<h3 id="备份与恢复">备份与恢复</h3>
<p>如果是 BR 备份，还原之后是同名的；<br>
dumplings 备份的话，可以在还原的时候指定 route 来设置要恢复到的数据库。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mycli -h172.20.192.70 -P4000 -uroot</span><br><span class="line"></span><br><span class="line">BACKUP DATABASE test TO &#39;s3:&#x2F;&#x2F;backup?access-key&#x3D;minio&amp;secret-access-key&#x3D;miniostorage&amp;endpoint&#x3D;http:&#x2F;&#x2F;172.20.192.151:6060&amp;force-path-style&#x3D;true&#39; RATE_LIMIT &#x3D; 120 MB&#x2F;SECOND;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mycli -h172.20.192.115 -P4000 -uroot</span><br><span class="line"></span><br><span class="line">mysql&gt; RESTORE DATABASE test FROM &#39;s3:&#x2F;&#x2F;backup?access-key&#x3D;minio&amp;secret-access-key&#x3D;miniostorage&amp;endpoint&#x3D;http:&#x2F;&#x2F;172.20.192.151:6060&amp;force-path-style&#x3D;true&#39;;</span><br></pre></td></tr></table></figure>
<h4 id="不停机升级">不停机升级</h4>
<p>tiup cluster upgrade tidb-prod v6.5.3</p>
<p>重启grafana</p>
<p>tiup cluster restart tidb-prod -R grafana</p>
<h2 id="备份">备份</h2>
<p>tiup br log start --task-name=pitr --pd=&quot;${PD_IP}:2379&quot; \ --storage=‘s3://tidb-pitr-bucket/backup-data/log-backup’</p>
]]></content>
  </entry>
  <entry>
    <title>k8s-1.23.17离线安装</title>
    <url>/2024/03/20/k8s-1.23.17%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p><strong>k8s-1.23.17离线安装</strong></p>
<h1>环境设置centos-7.9</h1>
<p>init_env.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo modprobe br_netfilter</span><br><span class="line">lsmod | grep br_netfilter</span><br><span class="line"></span><br><span class="line">cat&lt;&lt;EOF&gt; &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables  &#x3D; 1</span><br><span class="line">net.ipv4.ip_forward                 &#x3D; 1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line"></span><br><span class="line">sed -i &#39;$acloudeon  ALL&#x3D;(ALL)  NOPASSWD: NOPASSWD: ALL&#39; &#x2F;etc&#x2F;sudoers</span><br><span class="line"></span><br><span class="line">swapoff -a</span><br><span class="line">sed -i &#39;s&#x2F;.*swap.*&#x2F;#&amp;&#x2F;&#39; &#x2F;etc&#x2F;fstab</span><br></pre></td></tr></table></figure>
<h1>安装docker</h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xvf rpm_docker_20.10.24.tar</span><br><span class="line">yum localinstall docker&#x2F;* -y</span><br><span class="line">systemctl start docker</span><br><span class="line">sudo cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">     &quot;https:&#x2F;&#x2F;5il73zj3.mirror.aliyuncs.com&quot;,</span><br><span class="line">     &quot;https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&quot;,</span><br><span class="line">     &quot;https:&#x2F;&#x2F;registry.docker-cn.com&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;dockerhub.baofu.com&quot;],</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br><span class="line">systemctl enable docker.service</span><br><span class="line">systemctl status docker</span><br><span class="line">docker -v</span><br><span class="line">docker info|grep &quot;Cgroup Driver&quot;</span><br><span class="line">docker load &lt; images_k8s_core_1.23.17.tar</span><br><span class="line">docker load &lt; images_k8s_cni_flannel_0.15.0.tar</span><br><span class="line">docker load &lt; images_k8s_dashboard_v2.5.1.tar</span><br></pre></td></tr></table></figure>
<h1>安装k8s</h1>
<h2 id="主节点">主节点</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xvf rpm_k8s_1.23.17.tar</span><br><span class="line">yum localinstall k8s&#x2F;* -y</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start kubelet</span><br><span class="line">systemctl enable kubelet</span><br><span class="line">kubeadm init \</span><br><span class="line">--image-repository&#x3D;registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers \</span><br><span class="line">--kubernetes-version&#x3D;v1.23.17 \</span><br><span class="line">--pod-network-cidr&#x3D;10.244.0.0&#x2F;16</span><br><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">kubectl apply -f kube-flannel.yml</span><br></pre></td></tr></table></figure>
<h2 id="工作节点">工作节点</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xvf rpm_k8s_1.23.17.tar</span><br><span class="line">yum localinstall k8s&#x2F;* -y</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start kubelet</span><br><span class="line">systemctl enable kubelet</span><br><span class="line">kubeadm join ...</span><br></pre></td></tr></table></figure>
<p>注意：  在master执行 kubeadm token create --print-join-command， 然后复制出 在work节点执行</p>
<h2 id="开起主节点调度">开起主节点调度</h2>
<p>kubectl edit node master</p>
<p>把master的不可调度的污点给去掉</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">taints:</span><br><span class="line">- effect: NoSchedule</span><br><span class="line">  key: node-role.kubernetes.io&#x2F;master</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>安装 K8s 集群</title>
    <url>/2024/03/20/%E5%AE%89%E8%A3%85%20K8s%20%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h1>安装 K8s 集群</h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#注意系统内核升级到5.4 及以上。</span><br><span class="line"></span><br><span class="line">rpm --import https:&#x2F;&#x2F;www.elrepo.org&#x2F;RPM-GPG-KEY-elrepo.org</span><br><span class="line">yum install https:&#x2F;&#x2F;www.elrepo.org&#x2F;elrepo-release-7.el7.elrepo.noarch.rpm</span><br><span class="line">yum --enablerepo&#x3D;elrepo-kernel install kernel-ml</span><br><span class="line">grub2-set-default 0</span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">VERSION&#x3D;&#96;curl -s https:&#x2F;&#x2F;api.github.com&#x2F;repos&#x2F;labring&#x2F;sealos&#x2F;releases&#x2F;latest | grep -oE &#39;&quot;tag_name&quot;: &quot;[^&quot;]+&quot;&#39; | head -n1 | cut -d&#39;&quot;&#39; -f4&#96;</span><br><span class="line"></span><br><span class="line">wget https:&#x2F;&#x2F;mirror.ghproxy.com&#x2F;https:&#x2F;&#x2F;github.com&#x2F;labring&#x2F;sealos&#x2F;releases&#x2F;download&#x2F;$&#123;VERSION&#125;&#x2F;sealos_$&#123;VERSION#v&#125;_linux_amd64.tar.gz \</span><br><span class="line">  &amp;&amp; tar zxvf sealos_$&#123;VERSION#v&#125;_linux_amd64.tar.gz sealos &amp;&amp; chmod +x sealos &amp;&amp; mv sealos &#x2F;usr&#x2F;bin</span><br><span class="line"></span><br><span class="line">#注意 master的cpu数量要大于1</span><br><span class="line">sealos run registry.cn-shanghai.aliyuncs.com&#x2F;labring&#x2F;kubernetes:v1.27.7 registry.cn-shanghai.aliyuncs.com&#x2F;labring&#x2F;helm:v3.9.4 registry.cn-shanghai.aliyuncs.com&#x2F;labring&#x2F;cilium:v1.13.4 \</span><br><span class="line">     --masters 10.6.59.131,10.6.59.132,10.6.59.133 \</span><br><span class="line">     --nodes 10.6.59.134,10.6.59.135 -p &quot;2haolou@64&quot;</span><br><span class="line">	 </span><br><span class="line">#要求hostname每个节点不一样	 </span><br><span class="line">hostnamectl set-hostname node1	 </span><br><span class="line"></span><br><span class="line">#重置集群</span><br><span class="line">sealos reset</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>k8s高可用集群搭建-详细版</title>
    <url>/2024/03/19/k8s%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA-%E8%AF%A6%E7%BB%86%E7%89%88/</url>
    <content><![CDATA[<p><strong>k8s高可用集群搭建-详细版</strong></p>
<h1>环境</h1>
<p>系统: CenterOS 7.9</p>
<p>docker: 20.10.24</p>
<p>k8s: 1.23.17</p>
<h1>目标</h1>
<p>3个master节点、LoadBalancer、一个work节点</p>
<p>172.23.80.241 master1</p>
<p>172.23.80.242 master2</p>
<p>172.23.80.243 master3</p>
<p>172.23.80.244 worker1</p>
<h1>三个阶段</h1>
<h3 id="1、搭建k8s-master集群">1、搭建k8s master集群</h3>
<p>​    每个k8s master节点都可以对外提供服务(apiserver)，但是集群本身不提供负载和故障转移的功能，这些高可用功能由Load Balancer提供，woker节点或client均通过Load Balancer加入或访问k8s master。k8s master 集群至少需要三个节点，因为k8s集群在组网时，etcd组件构成集群时至少需要三个节点，etcd为高可用存储数据库。</p>
<p>​    每个k8s master节点之所以都可以独立对外提供服务，是因为k8s master唯一对外提高服务的组件apiserver是无状态实例，apiserver之所以无状态，是因为apiserver的数据都被存储到etcd数据库中，从而使得apiserver从有状态服务变成了一个无状态服务，所以Load Balancer 负载对象也就是apiserver（端口6443）。</p>
<p>​    对于controller-manager、scheduler这两个组件来说，高可用的逻辑也是启用多个实例来实现的，不同与apiserver，这两个组件由于工作逻辑的独特性，一个k8s集群中有且只有一个controller-manager和scheduler在工作，所以启动多个实例它们必须工作在主备模式，即一个active，多个backup的模式；它们通过分布式锁的方式实现内部选举，决定谁来工作，最终抢到分布式锁（k8s集群endpoint）的controller-manager、scheduler成为active状态代表集群controller-manager、scheduler组件工作，抢到锁的controller-manager和scheduler会周期性的向apiserver通告自己的心跳信息，以维护自己active状态，避免其他controller-manager、scheduler进行抢占；其他controller-manager、scheduler收到活动的controller-manager、scheduler的心跳信息后自动切换为backup状态；一旦在规定时间备用controller-manager、scheduler没有收到活动的controller-manager、scheduler的心跳，此时就会触发选举，重复上述过程；</p>
<h3 id="2、搭建Load-Balancer（提供负载和故障转移功能）">2、搭建Load Balancer（提供负载和故障转移功能）</h3>
<h3 id="3、work节点（kubelet）通过Load-Balancer-join到k8s集群">3、work节点（kubelet）通过Load Balancer join到k8s集群</h3>
<h1>前期准备（所有节点）</h1>
<h3 id="1、修改主机名和配置-hosts">1、修改主机名和配置 hosts</h3>
<p>hostnamectl set-hostname  k8s-master-80-241</p>
<p>hostnamectl set-hostname  k8s-master-80-242</p>
<p>hostnamectl set-hostname  k8s-master-80-243</p>
<p>hostnamectl set-hostname  k8s-worker-80-244</p>
<p><strong>master节点hosts配置</strong></p>
<p>cat &gt;&gt; /etc/hosts&lt;&lt;EOF<br>
172.23.80.241 k8s-master-80-241 cluster-endpoint<br>
172.23.80.242 k8s-master-80-242<br>
172.23.80.243 k8s-master-80-243<br>
EOF</p>
<p><strong>woker节点hosts配置</strong></p>
<p>cat &gt;&gt; /etc/hosts&lt;&lt;EOF</p>
<p>172.23.80.244 k8s-worker-80-244</p>
<p>${LoadBalancerVIP} cluster-endpoint<br>
EOF</p>
<p>其中:</p>
<p>cluster-endpoint 是映射到该 IP 的自定义 DNS 名称，这里配置hosts映射：192.168.0.113   cluster-endpoint。 这将允许你将 --control-plane-endpoint=cluster-endpoint 传递给 kubeadm init，并将相同的 DNS 名称传递给 kubeadm join。 之后你可以修改 cluster-endpoint 以指向高可用性方案中的负载均衡器的地址。</p>
<p>${LoadBalancerVIP} 值为根据LoadBalancer设置的VIP地址。</p>
<h3 id="2、时间同步">2、时间同步</h3>
<p>yum install chrony -y<br>
systemctl start chronyd<br>
systemctl enable chronyd<br>
chronyc sources</p>
<h3 id="3、关闭防火墙">3、关闭防火墙</h3>
<p>systemctl stop firewalld<br>
systemctl disable firewalld</p>
<h3 id="4、关闭swap">4、关闭swap</h3>
<p># 临时关闭；关闭swap主要是为了性能考虑<br>
swapoff -a<br>
# 可以通过这个命令查看swap是否关闭了<br>
free<br>
# 永久关闭<br>
sed -ri ‘s/.<em>swap.</em>/#&amp;/’ /etc/fstab</p>
<h3 id="5、禁用SELinux">5、禁用SELinux</h3>
<p># 临时关闭<br>
setenforce 0<br>
# 永久禁用<br>
sed -i ‘s/^SELINUX=enforcing$/SELINUX=disabled/’ /etc/selinux/config</p>
<h3 id="6、允许-iptables-检查桥接流量（可选）">6、允许 iptables 检查桥接流量（可选）</h3>
<p>sudo modprobe br_netfilter<br>
lsmod | grep br_netfilter</p>
<p># 设置所需的 sysctl 参数，参数在重新启动后保持不变<br>
cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf<br>
net.bridge.bridge-nf-call-iptables  = 1<br>
net.bridge.bridge-nf-call-ip6tables = 1<br>
net.ipv4.ip_forward                 = 1<br>
EOF<br>
# 应用 sysctl 参数而不重新启动<br>
sudo sysctl --system</p>
<h1>第一阶段</h1>
<h2 id="安装Docker-所有">安装Docker(所有)</h2>
<p>参考: k8s-1.23.17离线安装</p>
<h2 id="安装master1">安装master1</h2>
<p>参考: k8s-1.23.17离线安装</p>
<p>#安装kubelet、kubeadm、kubectl</p>
<p>#设置为开机自启并现在立即启动</p>
<p>systemctl enable --now kubelet<br>
#初始化 高可用k8s集群 核心在于master 初始化参数–control-plane-endpoint --v=5</p>
<p>kubeadm init \</p>
<p>–image-repository=<a href="http://registry.cn-hangzhou.aliyuncs.com/google_containers" target="_blank" rel="noopener">registry.cn-hangzhou.aliyuncs.com/google_containers</a> \</p>
<p>–kubernetes-version=v1.23.17 \</p>
<p>–control-plane-endpoint=cluster-endpoint \</p>
<p>–service-cidr=10.1.0.0/16 \</p>
<p>–pod-network-cidr=10.244.0.0/16 \</p>
<p>–upload-certs \</p>
<p>–v=5</p>
<p>其中</p>
<p>–control-plane-endpoint 这里的值是配置的DNS名称，因为LoadBalancer是在k8s master集群安装好之后。如果值要配置IP地址，其值必须配成LoadBalancer的负载地址，所以需要先安装好LoadBalancer。端口是LoadBalancer的负载端口可以不配置，默认6443。</p>
<p>–upload-certs</p>
<p>#安装网络插件</p>
<h2 id="安装master2、master3，加入master1集群-构建高可用集群">安装master2、master3，加入master1集群,构建高可用集群</h2>
<p>参考: k8s-1.23.17离线安装</p>
<p>#安装kubelet、kubeadm、kubectl</p>
<p>#设置为开机自启并现在立即启动</p>
<p>systemctl enable --now kubelet</p>
<p>#加入集群并初始化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm join cluster-endpoint:6443 --token c1pg2r.bt0a83bv4c3io8o4 \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:7a3586d7cdf45e5d7c681e22ebb7004f2a3a20c9dee31622ca2e887c56f25900 \</span><br><span class="line">        --control-plane \</span><br><span class="line">        --certificate-key 93a07f3e181677a5a18485f1eef5d374c7c874a80402f292a9440446bcc8d42c</span><br><span class="line">        --v&#x3D;5</span><br><span class="line"></span><br><span class="line"># 证如果过期了，可以使用下面命令生成新证书上传，这里会打印出certificate key(在master节点执行)</span><br><span class="line">kubeadm init phase upload-certs --upload-certs</span><br><span class="line"># 你还可以在 【init】期间指定自定义的 --certificate-key，以后可以由 join 使用。 要生成这样的密钥，可以使用以下命令</span><br><span class="line"># kubeadm certs certificate-key</span><br><span class="line"># --control-plane 标志通知 kubeadm join 创建一个新的控制平面，加入master必须加这个标记</span><br><span class="line"># --certificate-key ... 将导致从集群中的 kubeadm-certs Secret 下载控制平面证书并使用给定的密钥进行解密</span><br></pre></td></tr></table></figure>
<p>组网成功后修改hosts配置</p>
<p>master2</p>
<p>172.23.80.241 k8s-master-80-241</p>
<p>172.23.80.242 k8s-master-80-242 cluster-endpoint</p>
<p>172.23.80.243 k8s-master-80-243</p>
<p>master3</p>
<p>172.23.80.241 k8s-master-80-241</p>
<p>172.23.80.242 k8s-master-80-242</p>
<p>172.23.80.243 k8s-master-80-243 cluster-endpoint</p>
<h2 id="安装Dashboard">安装Dashboard</h2>
<p>参考k8s-1.23.17离线安装</p>
<p><strong>至此k8s master高可用集群（三个master）已经搭建完成</strong></p>
<h1></h1>
]]></content>
  </entry>
  <entry>
    <title>挂载虚拟IP</title>
    <url>/2024/02/27/%E6%8C%82%E8%BD%BD%E8%99%9A%E6%8B%9FIP/</url>
    <content><![CDATA[<p>在200-223上关闭VIP：<br>
/sbin/ifconfig bond0:1 192.168.192.1 netmask 255.255.255.0 down</p>
<p>在200-223上启动VIP：<br>
/sbin/ifconfig bond0:1 192.168.192.1 netmask 255.255.255.0 up</p>
<p>/sbin/arping  -c 5 -U -I bond0 192.168.192.1 &gt;/dev/null 2&gt;&amp;1</p>
<p>bond0:1  这是个别名</p>
]]></content>
  </entry>
  <entry>
    <title>使用 Kubekey一键 离线 在线 部署 kubernetes 集群</title>
    <url>/2024/02/01/%E4%BD%BF%E7%94%A8%20Kubekey%E4%B8%80%E9%94%AE%20%E7%A6%BB%E7%BA%BF%20%E5%9C%A8%E7%BA%BF%20%E9%83%A8%E7%BD%B2%20kubernetes%20%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h1>使用 Kubekey 一键 离线/在线 部署 kubernetes 集群</h1>
<p>在线部署：</p>
<p>创建配置文件</p>
<p>./kk create config --with-kubesphere</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./kk  init os -f ./config-sample.yaml  <span class="comment"># 初始化集群机器，此操作会安装系统依赖，开启lpvs模块等。前提是确保对应node的网络通畅。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择中文区下载(访问 GitHub 受限时使用)</span></span><br><span class="line"><span class="built_in">export</span> KKZONE=cn</span><br><span class="line"></span><br><span class="line">./kk  create cluster -f ./config-sample.yaml  <span class="comment"># 创建集群</span></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>oblogproxy 部署</title>
    <url>/2024/01/29/oblogproxy%20%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>oblogproxy 部署</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> wget https:&#x2F;&#x2F;obbusiness-private.oss-cn-shanghai.aliyuncs.com&#x2F;download-center&#x2F;opensource&#x2F;oblogproxy&#x2F;v2.0.0_BP1&#x2F;oblogproxy-2.0.0-101000012023121819.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">  rpm -i oblogproxy-2.0.0-101000012023121819.el7.x86_64.rpm </span><br><span class="line">  </span><br><span class="line"> cd &#x2F;usr&#x2F;local&#x2F;oblogproxy&#x2F;</span><br><span class="line"></span><br><span class="line">配置文件添加ob集群系统的用户名和密码</span><br><span class="line">  vim conf&#x2F;conf.json  </span><br><span class="line">  &quot;ob_sys_username&quot;: &quot;&quot;,</span><br><span class="line">  &quot;ob_sys_password&quot;: &quot;&quot;,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  srun.sh start</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>使用图形化界面部署 OCP</title>
    <url>/2024/01/26/%E4%BD%BF%E7%94%A8%E5%9B%BE%E5%BD%A2%E5%8C%96%E7%95%8C%E9%9D%A2%E9%83%A8%E7%BD%B2%20OCP/</url>
    <content><![CDATA[<p>ocp部署</p>
<h1>使用图形化界面部署 OCP</h1>
<p><a href="https://www.oceanbase.com/docs/common-ocp-1000000000368849" target="_blank" rel="noopener">https://www.oceanbase.com/docs/common-ocp-1000000000368849</a></p>
<h2 id="注意事项">注意事项</h2>
<h3 id="1-注意要更改一下datafile-size大小，-不然会占满磁盘">1.<strong>注意要更改一下datafile_size大小， 不然会占满磁盘</strong></h3>
<ul>
<li>
<p><code>datafile_size</code> 与 <code>datafile_disk_percentage</code> 均可以用来设置数据文件占用的磁盘空间，您可以选择其中任意一项进行配置。其他情况下：</p>
<ul>
<li>如果两个配置项均已配置，即 <code>datafile_size</code> 与 <code>datafile_disk_percentage</code> 同时配置为非 0 的值，则以 <code>datafile_size</code> 设置的值为准。</li>
<li>如果两个配置项均未配置，即 <code>datafile_size</code> 的值为 <code>OM</code> 且 <code>datafile_disk_percentage</code> 的值为 <code>0</code>，则系统会根据日志和数据是否共用同一磁盘来自动计算数据文件占用其所在磁盘总空间的百分比，即：
<ul>
<li>如果日志和数据共用同一磁盘，则数据文件占用其所在磁盘总空间的百分比为 60%。</li>
<li>如果日志和数据独占磁盘时，则数据文件占用其所在磁盘总空间的百分比为 90%。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>配置 <code>datafile_maxsize</code> 时，其值需要大于当前数据文件占用的磁盘空间大小 <code>datafile_size</code>（或 <code>datafile_disk_percentage</code>），如果设置的值小于当前数据文件占用的磁盘空间大小，则不会触发自动扩容。</p>
</li>
<li>
<p>如果 <code>datafile_maxsize</code> 的值超过了当前磁盘的最大可用空间，则以实际磁盘的可用大小作为最大值。</p>
</li>
<li>
<p>对于 <code>datafile_next</code>：</p>
<ul>
<li>
<p>当 <code>datafile_next</code> 设置为小于或等于 <code>1G</code> 的值时，<code>datafile_next</code> 的取值为 <code>min (1G, datafile_maxsize * 10%)</code>。</p>
</li>
<li>
<p>当 <code>datafile_next</code> 设置为大于 <code>1G</code> 的值时，<code>datafile_next</code> 的取值为 <code>min ( datafile_next, 磁盘剩余空间)</code>。</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">datafile_size  100G</span><br><span class="line">datafile_maxsize 100G</span><br><span class="line">log_disk_size 200G</span><br></pre></td></tr></table></figure>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2024/01/image-20240125171731578.png" alt="image-20240125171731578"></p>
<p>2.<strong>如果安装的不对，用命令删除集群，重新安装</strong></p>
<p>obd cluster destroy myoceanbase</p>
<p>3.<strong>要记住 元信息租户配置，以后用得上。</strong></p>
<p>4.<strong>如果不能上网，打不开图形化安装界面。</strong></p>
<h1>用户规划</h1>
<ol>
<li>
<p>执行以下命令，创建用户。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">useradd -U oceanbase -d /home/oceanbase -s /bin/bash</span><br><span class="line">mkdir -p /home/oceanbase</span><br><span class="line">sudo chown -R oceanbase:oceanbase /home/oceanbase</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>执行以下命令，为用户 oceanbase 设置密码。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">passwd oceanbase</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>为用户 oceanbase 设置 sudo 权限。</p>
<p>执行以下命令，打开 <code>/etc/sudoers</code> 文件。在 <code>/etc/sudoers</code> 文件末尾添加以下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Same thing without a password</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> %wheel        ALL=(ALL)       NOPASSWD: ALL</span></span><br><span class="line">oceanbase       ALL=(ALL)       NOPASSWD: ALL</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>在执行以下命令，查看目录权限。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ls -ld /data</span><br><span class="line">ls -ld /home/oceanbase</span><br></pre></td></tr></table></figure>
<p>返回以下结果：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">drwxr-xr-x 2 oceanbase oceanbase 4096 2 月 9 18:43</span><br><span class="line">drwxr-xr-x 2 oceanbase oceanbase 4096 2 月 9 18:43 /data</span><br><span class="line">drwxr-xr-x 2 oceanbase oceanbase 4096 2 月 9 18:43 /home/oceanbase</span><br></pre></td></tr></table></figure>
<p>若 oceanbase 用户无权限，则以 root 用户执行以下命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chown -R oceanbase:oceanbase /data</span><br><span class="line">chown -R oceanbase:oceanbase /home/oceanbase</span><br></pre></td></tr></table></figure>
<p>此处填写您真实的挂载目录。此处 <code>/data</code>、<code>/home/oceanbase</code> 为示例挂载目录。</p>
</li>
</ol>
<p><strong>修改ocp系统参数，重启生效</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">obd cluster stop myocp -c ocp-server-ce --wp</span><br><span class="line">obd cluster start myocp -c ocp-server-ce --wp</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>tidb部署</title>
    <url>/2023/11/02/tidb%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>新建用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd tidb</span><br><span class="line">su - tidb</span><br></pre></td></tr></table></figure>
<h2 id="在中控机上部署-TiUP-组件">在中控机上部署 TiUP 组件</h2>
<p>在中控机上部署 TiUP 组件有两种方式：在线部署和离线部署。</p>
<h3 id="在线部署">在线部署</h3>
<p>以普通用户身份登录中控机。以 <code>tidb</code> 用户为例，后续安装 TiUP 及集群管理操作均通过该用户完成：</p>
<ol>
<li>
<p>执行如下命令安装 TiUP 工具：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">curl --proto <span class="string">'=https'</span> --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>按如下步骤设置 TiUP 环境变量：</p>
<ol>
<li>
<p>重新声明全局环境变量：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> .bash_profile</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>确认 TiUP 工具是否安装：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">which</span> tiup</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li>
<p>安装 TiUP cluster 组件：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">tiup cluster</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>如果已经安装，则更新 TiUP cluster 组件至最新版本：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">tiup update --self &amp;&amp; tiup update cluster</span><br></pre></td></tr></table></figure>
<p>预期输出 <code>“Update successfully!”</code> 字样。</p>
</li>
<li>
<p>验证当前 TiUP cluster 版本信息。执行如下命令查看 TiUP cluster 组件版本：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">tiup --binary cluster</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="优化系统配置">优化系统配置</h2>
<p>vim /etc/sysctl.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">net.ipv4.tcp_tw_recycle &#x3D; 0</span><br><span class="line">net.ipv4.tcp_syncookies &#x3D; 0</span><br><span class="line">net.core.somaxconn &#x3D; 32768</span><br><span class="line">vm.swappiness &#x3D; 0</span><br><span class="line">vm.overcommit_memory &#x3D; 0</span><br><span class="line">fs.file-max &#x3D; 1000000</span><br></pre></td></tr></table></figure>
<p>/etc/security/limits.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* soft nofile 1048576</span><br><span class="line">* hard nofile 1048576</span><br><span class="line">* soft nproc unlimited</span><br><span class="line">* hard nproc unlimited</span><br><span class="line">tidb    soft    nofile    1000000</span><br><span class="line">tidb    hard    nofile    1000000</span><br><span class="line">tidb    soft    stack    10240</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible tidb101 -m shell -a &quot; yum -y install numactl.x86_64 &quot;</span><br><span class="line"></span><br><span class="line">ansible tidb101 -m copy -a &quot; src&#x3D;&#x2F;etc&#x2F;security&#x2F;limits.conf  dest&#x3D;&#x2F;etc&#x2F;security&#x2F;limits.conf &quot;</span><br><span class="line">ansible tidb101 -m copy -a &quot; src&#x3D;&#x2F;etc&#x2F;sysctl.conf  dest&#x3D;&#x2F;etc&#x2F;sysctl.conf&quot;</span><br><span class="line"></span><br><span class="line">ansible tidb101 -m shell -a &quot;sysctl -p&quot;</span><br><span class="line"></span><br><span class="line">ansible tidb101 -m shell -a &quot;cpupower frequency-set -g performance&quot;</span><br><span class="line"></span><br><span class="line"># 重新挂载一下磁盘，让noatime参数生效</span><br><span class="line">mount -o remount &#x2F;dfs&#x2F;data1</span><br><span class="line">mount -o remount &#x2F;dfs&#x2F;data2</span><br><span class="line">mount -o remount &#x2F;dfs&#x2F;data3</span><br><span class="line">mount -o remount &#x2F;dfs&#x2F;data4</span><br><span class="line">mount -o remount &#x2F;dfs&#x2F;data5</span><br><span class="line">mount -o remount &#x2F;dfs&#x2F;data6</span><br><span class="line">mount -o remount &#x2F;dfs&#x2F;data7</span><br><span class="line">mount -o remount &#x2F;dfs&#x2F;data8</span><br></pre></td></tr></table></figure>
<h2 id="执行部署命令">执行部署命令</h2>
<ol>
<li>
<p>检查集群存在的潜在风险：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">tiup cluster check ./blockin-topology-101.yaml --user root [-p] [-i /home/root/.ssh/gcp_rsa]</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>自动修复集群存在的潜在风险：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">tiup cluster check ./blockin-topology-101.yaml --apply --user root [-p] [-i /home/root/.ssh/gcp_rsa]</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>部署 TiDB 集群：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">tiup cluster deploy tidb-prod-101 v6.5.5 ./blockin-topology-101.yaml --user root [-p] [-i /home/root/.ssh/gcp_rsa]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>tiup cluster list</p>
<p>tiup cluster display  tidb-prod-101</p>
<p>tiup cluster scale-in <cluster-name> --node 10.0.1.4:8300</p>
]]></content>
  </entry>
  <entry>
    <title>SR运维</title>
    <url>/2023/09/13/SR%E8%BF%90%E7%BB%B4/</url>
    <content><![CDATA[<h2 id="修复tablet">修复tablet</h2>
<p><strong>注意： 要连接fe的leader节点</strong></p>
<p>找到错误的tablet</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SHOW PROC &#39;&#x2F;statistic&#x2F;12026&#39;;</span><br></pre></td></tr></table></figure>
<p>找到tablet对应错误的 IsErrorState 的节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">show tablet 341603</span><br><span class="line">SHOW PROC &#39;&#x2F;dbs&#x2F;12026&#x2F;335109&#x2F;partitions&#x2F;335078&#x2F;335110&#x2F;341603&#39;;</span><br></pre></td></tr></table></figure>
<p>setBad</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ADMIN SET REPLICA STATUS PROPERTIES(&quot;tablet_id&quot; &#x3D; &quot;341603&quot;, &quot;backend_id&quot; &#x3D; &quot;10006&quot; , &quot;status&quot; &#x3D; &quot;bad&quot;);</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>用 arthas 来分析异常进程</title>
    <url>/2023/08/03/%E7%94%A8%20arthas%20%E6%9D%A5%E5%88%86%E6%9E%90%E5%BC%82%E5%B8%B8%E8%BF%9B%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="用-arthas-来分析异常进程">用 arthas 来分析异常进程</h2>
<p>arthas使用参考：  <a href="https://arthas.aliyun.com/doc/quick-start.html" target="_blank" rel="noopener">https://arthas.aliyun.com/doc/quick-start.html</a></p>
<p>使用和目标进程一致的用户启动，否则可能 attach 失败</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u hbase env PATH&#x3D;$PATH java -jar arthas-boot.jar</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dashboard</span><br></pre></td></tr></table></figure>
<p>运行 dashboard 命令回车，就可以查看该进程占用资源的总体情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">thread -n 3</span><br></pre></td></tr></table></figure>
<p>输出资源占用前三名的线程</p>
<h2 id="通过-thread-命令来获取到进程的-Main-Class">通过 thread 命令来获取到进程的 Main Class</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ thread 1 | grep &#39;main(&#39;</span><br><span class="line">    at demo.MathGame.main(MathGame.java:17)</span><br></pre></td></tr></table></figure>
<h2 id="通过-jad-来反编译-Main-Class">通过 jad 来反编译 Main Class</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">$ jad demo.MathGame</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>EFAK - Kafka可视化管理工具</title>
    <url>/2023/07/26/EFAK%20-%20Kafka%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<h1>EFAK - Kafka可视化管理工具</h1>
<p><strong>提取 EFAK</strong></p>
<p>这里我们解压到<code>/data/soft/new</code>目录并解压：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">tar -zxvf efak-xxx-bin.tar.gz</span><br></pre></td></tr></table></figure>
<p>如果之前安装过版本，删除修改后的版本，重命名当前版本，如下图：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rm -rf efakmv efak-xxx efak</span><br></pre></td></tr></table></figure>
<p>然后，配置 EFAK 配置文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/profileexport KE_HOME=/data/soft/new/efakexport PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<p>最后，我们使用<code>. /etc/profile</code>使配置立即生效。</p>
<p>配置EFAK系统文件 根据自身Kafka集群的实际情况配置EFAK，例如zookeeper地址、Kafka集群的版本类型（zk为低版本，kafka为高版本）、开启安全认证的Kafka集群等。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;KE_HOME&#125;</span>/conf</span><br><span class="line">vi system-config.properties</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># multi zookeeper &amp; kafka cluster list</span></span><br><span class="line"><span class="comment"># Settings prefixed with 'kafka.eagle.' will be deprecated, use 'efak.' instead</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">efak.zk.cluster.alias=cluster1,cluster2</span><br><span class="line">cluster1.zk.list=cdh192-35.hadoop.xy:2181,cdh192-36.hadoop.xy:2181,cdh192-159.hadoop.xy:2181</span><br><span class="line">cluster2.zk.list=cdh192-150.hadoop.xy:2181,cdh192-150.hadoop.xy:2181,cdh192-150.hadoop.xy:2181</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># zookeeper enable acl</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">cluster1.zk.acl.enable=<span class="literal">false</span></span><br><span class="line">cluster1.zk.acl.schema=digest</span><br><span class="line">cluster1.zk.acl.username=<span class="built_in">test</span></span><br><span class="line">cluster1.zk.acl.password=test123</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># broker size online list</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">cluster1.efak.broker.size=20</span><br><span class="line">cluster2.efak.broker.size=20</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># zk client thread limit</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">kafka.zk.limit.size=16</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># EFAK webui port</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">efak.webui.port=8048</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># EFAK enable distributed</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">efak.distributed.enable=<span class="literal">false</span></span><br><span class="line">efak.cluster.mode.status=master</span><br><span class="line">efak.worknode.master.host=localhost</span><br><span class="line">efak.worknode.port=8085</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka jmx acl and ssl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">cluster1.efak.jmx.acl=<span class="literal">false</span></span><br><span class="line">cluster1.efak.jmx.user=keadmin</span><br><span class="line">cluster1.efak.jmx.password=keadmin123</span><br><span class="line">cluster1.efak.jmx.ssl=<span class="literal">false</span></span><br><span class="line">cluster1.efak.jmx.truststore.location=/data/ssl/certificates/kafka.truststore</span><br><span class="line">cluster1.efak.jmx.truststore.password=ke123456</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka offset storage</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">cluster1.efak.offset.storage=kafka</span><br><span class="line">cluster2.efak.offset.storage=kafka</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka jmx uri</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">cluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://172.20.192.147:9393/jmxrmi</span><br><span class="line">cluster2.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://172.20.192.102:9393/jmxrmi</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka metrics, 15 days by default</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">efak.metrics.charts=<span class="literal">true</span></span><br><span class="line">efak.metrics.retain=15</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sql topic records max</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">efak.sql.topic.records.max=5000</span><br><span class="line">efak.sql.topic.preview.records.max=10</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># delete kafka topic token</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">efak.topic.token=keadmin</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sasl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">cluster1.efak.sasl.enable=<span class="literal">false</span></span><br><span class="line">cluster1.efak.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">cluster1.efak.sasl.mechanism=SCRAM-SHA-256</span><br><span class="line">cluster1.efak.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=<span class="string">"kafka"</span> password=<span class="string">"kafka-eagle"</span>;</span><br><span class="line">cluster1.efak.sasl.client.id=</span><br><span class="line">cluster1.efak.blacklist.topics=</span><br><span class="line">cluster1.efak.sasl.cgroup.enable=<span class="literal">false</span></span><br><span class="line">cluster1.efak.sasl.cgroup.topics=</span><br><span class="line">cluster2.efak.sasl.enable=<span class="literal">false</span></span><br><span class="line">cluster2.efak.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">cluster2.efak.sasl.mechanism=PLAIN</span><br><span class="line">cluster2.efak.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=<span class="string">"kafka"</span> password=<span class="string">"kafka-eagle"</span>;</span><br><span class="line">cluster2.efak.sasl.client.id=</span><br><span class="line">cluster2.efak.blacklist.topics=</span><br><span class="line">cluster2.efak.sasl.cgroup.enable=<span class="literal">false</span></span><br><span class="line">cluster2.efak.sasl.cgroup.topics=</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka ssl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">cluster3.efak.ssl.enable=<span class="literal">false</span></span><br><span class="line">cluster3.efak.ssl.protocol=SSL</span><br><span class="line">cluster3.efak.ssl.truststore.location=</span><br><span class="line">cluster3.efak.ssl.truststore.password=</span><br><span class="line">cluster3.efak.ssl.keystore.location=</span><br><span class="line">cluster3.efak.ssl.keystore.password=</span><br><span class="line">cluster3.efak.ssl.key.password=</span><br><span class="line">cluster3.efak.ssl.endpoint.identification.algorithm=https</span><br><span class="line">cluster3.efak.blacklist.topics=</span><br><span class="line">cluster3.efak.ssl.cgroup.enable=<span class="literal">false</span></span><br><span class="line">cluster3.efak.ssl.cgroup.topics=</span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sqlite jdbc driver address</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment">#efak.driver=org.sqlite.JDBC</span></span><br><span class="line"><span class="comment">#efak.url=jdbc:sqlite:/hadoop/kafka-eagle/db/ke.db</span></span><br><span class="line"><span class="comment">#efak.username=root</span></span><br><span class="line"><span class="comment">#efak.password=www.kafka-eagle.org</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka mysql jdbc driver address</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line">efak.driver=com.mysql.cj.jdbc.Driver</span><br><span class="line">efak.url=jdbc:mysql://172.20.192.240:3306/ke?useUnicode=<span class="literal">true</span>&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span><br><span class="line">efak.username=scm</span><br><span class="line">efak.password=*****</span><br></pre></td></tr></table></figure>
<p><strong>启动EFAK服务器（独立）</strong> 在$KE_HOME/bin目录中，有一个ke.sh脚本文件。执行启动命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;KE_HOME&#125;</span>/binchmod +x ke.sh ke.sh start</span><br></pre></td></tr></table></figure>
<p>之后，当 EFAK 服务器重新启动或停止时，执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ke.sh restartke.sh stop</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>用 Sysbench 测试 TiDB</title>
    <url>/2023/07/21/%E7%94%A8%20Sysbench%20%E6%B5%8B%E8%AF%95%20TiDB/</url>
    <content><![CDATA[<h1>用 Sysbench 测试 TiDB</h1>
<h3 id="Sysbench-配置">Sysbench 配置</h3>
<p>以下为 Sysbench 配置文件样例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql-host&#x3D;&#123;TIDB_HOST&#125;</span><br><span class="line">mysql-port&#x3D;4000</span><br><span class="line">mysql-user&#x3D;root</span><br><span class="line">mysql-password&#x3D;password</span><br><span class="line">mysql-db&#x3D;sbtest</span><br><span class="line">time&#x3D;600</span><br><span class="line">threads&#x3D;&#123;8, 16, 32, 64, 128, 256&#125;</span><br><span class="line">report-interval&#x3D;10</span><br><span class="line">db-driver&#x3D;mysql</span><br></pre></td></tr></table></figure>
<p>可根据实际需求调整其参数，其中 <code>TIDB_HOST</code> 为 TiDB server 的 IP 地址（配置文件中不能写多个地址），<code>threads</code> 为测试中的并发连接数，可在 “8, 16, 32, 64, 128, 256” 中调整，导入数据时，建议设置 threads = 8 或者 16。调整后，将该文件保存为名为 <strong>config</strong> 的文件。</p>
<p><strong>配置文件</strong>参考示例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql-host&#x3D;172.16.30.33</span><br><span class="line">mysql-port&#x3D;4000</span><br><span class="line">mysql-user&#x3D;root</span><br><span class="line">mysql-password&#x3D;password</span><br><span class="line">mysql-db&#x3D;sbtest</span><br><span class="line">time&#x3D;600</span><br><span class="line">threads&#x3D;16</span><br><span class="line">report-interval&#x3D;10</span><br><span class="line">db-driver&#x3D;mysql</span><br></pre></td></tr></table></figure>
<h3 id="数据导入">数据导入</h3>
<p>注意</p>
<p>如果 TiDB 启用了乐观事务模型（默认为悲观锁模式），当发现并发冲突时，会回滚事务。将 <code>tidb_disable_txn_auto_retry</code> 设置为 <code>off</code> 会开启事务冲突后的自动重试机制，可以尽可能避免事务冲突报错导致 Sysbench 程序退出的问题。</p>
<p>在数据导入前，需要对 TiDB 进行简单设置。在 MySQL 客户端中执行如下命令：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> tidb_disable_txn_auto_retry = <span class="keyword">off</span>;</span><br></pre></td></tr></table></figure>
<p>然后退出客户端。</p>
<p>重新启动 MySQL 客户端执行以下 SQL 语句，创建数据库 <code>sbtest</code>：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> sbtest;</span><br></pre></td></tr></table></figure>
<p>调整 Sysbench 脚本创建索引的顺序。Sysbench 按照“建表-&gt;插入数据-&gt;创建索引”的顺序导入数据。对于 TiDB 而言，该方式会花费更多的导入时间。你可以通过调整顺序来加速数据的导入。</p>
<p>假设使用的 Sysbench 版本为 <a href="https://github.com/akopytov/sysbench/tree/1.0.20" target="_blank" rel="noopener">1.0.20</a>，可以通过以下两种方式来修改：</p>
<ol>
<li>直接下载为 TiDB 修改好的 <a href="https://raw.githubusercontent.com/pingcap/tidb-bench/master/sysbench/sysbench-patch/oltp_common.lua" target="_blank" rel="noopener">oltp_common.lua</a> 文件，覆盖 <code>/usr/share/sysbench/oltp_common.lua</code> 文件。</li>
<li>将 <code>/usr/share/sysbench/oltp_common.lua</code> 的第 <a href="https://github.com/akopytov/sysbench/blob/1.0.20/src/lua/oltp_common.lua#L235-L240" target="_blank" rel="noopener">235-240</a> 行移动到第 198 行以后。</li>
</ol>
<p>注意</p>
<p>此操作为可选操作，仅节约了数据导入时间。</p>
<p>命令行输入以下命令，开始导入数据，config 文件为上一步中配置的文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sysbench --config-file=config oltp_point_select --tables=32 --table-size=10000000 prepare</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>tidb 批量导入历史数据</title>
    <url>/2023/07/21/tidb%20%E6%89%B9%E9%87%8F%E5%AF%BC%E5%85%A5%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<p>tidb 批量导入历史数据</p>
<p>cdh192-150:/opt/script/tidb-import-data.bash</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">for i in &#123;4..9&#125;;</span><br><span class="line">do</span><br><span class="line">if [ ! -d "/dfs/data3/data-source-dir" ]; then</span><br><span class="line">mkdir /dfs/data3/data-source-dir</span><br><span class="line">chmod 777 /dfs/data3/data-source-dir</span><br><span class="line">sudo -u hdfs hadoop fs -get hdfs://172.20.192.36:8020/user/hive/warehouse/test.db/history_ethereum/part-$i* /dfs/data3/data-source-dir/</span><br><span class="line">fi</span><br><span class="line">source /home/tidb/.bash_profile</span><br><span class="line">tiup tidb-lightning -config /home/tidb/tidb-lightning.toml</span><br><span class="line">rm -rf /dfs/data3/data-source-dir</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>vim tidb-lightning.toml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">[lightning]</span></span><br><span class="line"><span class="comment"># 日志</span></span><br><span class="line"><span class="string">level</span> <span class="string">=</span> <span class="string">"info"</span></span><br><span class="line"><span class="string">file</span> <span class="string">=</span> <span class="string">"tidb-lightning.log"</span></span><br><span class="line"><span class="string">max-error</span> <span class="string">=</span> <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="string">[tikv-importer]</span></span><br><span class="line"><span class="string">incremental-import</span> <span class="string">=</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># "local"：默认使用该模式，适用于 TiB 级以上大数据量，但导入期间下游 TiDB 无法对外提供服务。</span></span><br><span class="line"><span class="string">backend</span> <span class="string">=</span> <span class="string">"local"</span></span><br><span class="line"><span class="comment"># # "tidb"：TiB 级以下数据量也可以采用 `tidb` 后端模式，下游 TiDB 可正常提供服务。关于导入模式更多信息请参阅：https://docs.pingcap.com/zh/tidb/stable/tidb-lightning-overview#tidb-lightning-整体架构</span></span><br><span class="line"><span class="comment"># 设置排序的键值对的临时存放地址，目标路径必须是一个空目录，目录空间须大于待导入数据集的大小。建议设为与 `data-source-dir` 不同的磁盘目录并使用闪存介质，独占 I/O 会获得更好的导入性能。</span></span><br><span class="line"><span class="string">sorted-kv-dir</span> <span class="string">=</span> <span class="string">"/dfs/data2/sorted-kv-dir"</span></span><br><span class="line"></span><br><span class="line"><span class="string">[mydumper]</span></span><br><span class="line"><span class="comment"># 源数据目录</span></span><br><span class="line"><span class="string">data-source-dir</span> <span class="string">=</span> <span class="string">"/dfs/data3/data-source-dir"</span> <span class="comment"># 本地或 S3 路径，例如：'s3://my-bucket/sql-backup'</span></span><br><span class="line"></span><br><span class="line"><span class="string">[[mydumper.files]]</span></span><br><span class="line"><span class="comment"># 解析 AWS Aurora parquet 文件所需的表达式   part-00000-34e0bdac-df83-46a1-a198-b5c51a745cc2-c000.snappy.parquet</span></span><br><span class="line"><span class="string">pattern</span> <span class="string">=</span> <span class="string">'.*.snappy.parquet'</span></span><br><span class="line"><span class="string">schema</span> <span class="string">=</span> <span class="string">'ADDRESS'</span></span><br><span class="line"><span class="string">table</span> <span class="string">=</span> <span class="string">'T_HISTORY_TRANS_ETHEREUM'</span></span><br><span class="line"><span class="string">type</span> <span class="string">=</span> <span class="string">'parquet'</span></span><br><span class="line"></span><br><span class="line"><span class="string">[tidb]</span></span><br><span class="line"><span class="comment"># 目标集群的信息</span></span><br><span class="line"><span class="string">host</span> <span class="string">=</span> <span class="string">"172.20.192.115"</span>                <span class="comment"># 例如：172.16.32.1</span></span><br><span class="line"><span class="string">port</span> <span class="string">=</span> <span class="number">4000</span>                <span class="comment"># 例如：4000</span></span><br><span class="line"><span class="string">user</span> <span class="string">=</span> <span class="string">"root"</span>         <span class="comment"># 例如："root"</span></span><br><span class="line"><span class="string">password</span> <span class="string">=</span> <span class="string">""</span>      <span class="comment"># 例如："rootroot"</span></span><br><span class="line"><span class="string">status-port</span> <span class="string">=</span> <span class="number">10080</span>  <span class="comment"># 导入过程 Lightning 需要在从 TiDB 的“状态端口”获取表结构信息，例如：10080</span></span><br><span class="line"><span class="string">pd-addr</span> <span class="string">=</span> <span class="string">"172.20.192.115:2379"</span>     <span class="comment"># 集群 PD 的地址，Lightning 通过 PD 获取部分信息，例如 172.16.31.3:2379。当 backend = "local" 时 status-port 和 pd-addr 必须正确填写，否则导入将出现异常。</span></span><br><span class="line"></span><br><span class="line"><span class="string">[checkpoint]</span></span><br><span class="line"><span class="string">enable</span> <span class="string">=</span> <span class="literal">true</span></span><br><span class="line"><span class="string">driver</span> <span class="string">=</span> <span class="string">"file"</span></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>用broker导入数据 ，全部be节点cpu 100%</title>
    <url>/2023/07/13/%E7%94%A8broker%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%20%EF%BC%8C%E5%85%A8%E9%83%A8be%E8%8A%82%E7%82%B9cpu%20100%25/</url>
    <content><![CDATA[<p>用broker导80亿数据 ，全部be节点cpu 100%</p>
<p>【详述】全部be节点cpu 100%<br>
【做过哪些操作】<br>
【业务影响】已经不能查询<br>
【StarRocks版本】2.5.5<br>
【集群规模】4fe（3 follower+1observer）4be（fe与be混部）<br>
【机器信息】256C/1024G/万兆</p>
<h3 id="方法1：">方法1：</h3>
<p>vim conf/be.conf   开启下列2个参数，然后重启，在be.info日志里找到TabletId ，然后根据TabletId 找到table_id</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># sys_log_verbose_modules &#x3D; *</span><br><span class="line"># log_buffer_level &#x3D; -1</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep &quot;apply_rowset_commit start&quot; log&#x2F;be.INFO | less</span><br></pre></td></tr></table></figure>
<p>– 多个磁盘循环删掉元数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for i in &#123;1..8&#125;;do bin&#x2F;meta_tool.sh --operation&#x3D;delete_meta --root_path&#x3D;&#x2F;dfs&#x2F;data$&#123;i&#125;&#x2F;storage --table_id&#x3D;20642901;done</span><br></pre></td></tr></table></figure>
<h3 id="方法2：">方法2：</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RECOVER TABLE [db_name.]table_name;  -- 恢复 table</span><br><span class="line">show tablet from ADDRESS.T_HISTORY_TRANS_BSC   -- 找到TabletId </span><br><span class="line">show tablet 20882137    -- 根据TabletId找到table_id</span><br><span class="line">-- 多个磁盘循环删掉元数据</span><br><span class="line">for i in &#123;1..8&#125;;do bin&#x2F;meta_tool.sh --operation&#x3D;delete_meta --root_path&#x3D;&#x2F;dfs&#x2F;data$&#123;i&#125;&#x2F;storage --table_id&#x3D;20642901;done</span><br></pre></td></tr></table></figure>
<h3 id="方法3：">方法3：</h3>
<p>强制删除应该也是可以的（没有验证）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DROP TABLE my_table FORCE;  -- 强制删除表，并清理磁盘文件。</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>cloudcanal监控配置</title>
    <url>/2023/07/05/cloudcanal%E7%9B%91%E6%8E%A7%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>cloudcanal监控配置</p>
<p>mysql -uclougence -h127.0.0.1 -P25000 -p123456</p>
]]></content>
  </entry>
  <entry>
    <title>mycli安装</title>
    <url>/2023/06/25/mycli%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>Linux Centos7安装python3.7  <a href="https://blog.csdn.net/fen_fen/article/details/123520752" target="_blank" rel="noopener">https://blog.csdn.net/fen_fen/article/details/123520752</a></p>
<p>pip3 install mycli</p>
<p>设置环境变量</p>
<p>vim  ~/.bash_profile</p>
<p>export PATH=/usr/local/python3/bin:$PATH</p>
]]></content>
  </entry>
  <entry>
    <title>trino集群优化</title>
    <url>/2023/05/08/trino%E9%9B%86%E7%BE%A4%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<p>trino集群优化</p>
<p><a href="https://www.cnblogs.com/erlou96/p/16878170.html" target="_blank" rel="noopener">https://www.cnblogs.com/erlou96/p/16878170.html</a></p>
<h2 id="config-properties">config.properties</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">coordinator&#x3D;true</span><br><span class="line">node-scheduler.include-coordinator&#x3D;true</span><br><span class="line">http-server.http.port&#x3D;8089</span><br><span class="line">#query.max-total-memory-per-node&#x3D;2GB</span><br><span class="line">discovery-server.enabled&#x3D;true</span><br><span class="line">discovery.uri&#x3D;http:&#x2F;&#x2F;cdh192-147.hadoop.xy:8089</span><br><span class="line"></span><br><span class="line">#禁用保留池  (新版本已经删除了)</span><br><span class="line">#experimental.reserved-pool-enabled&#x3D;false</span><br><span class="line">#在单个 worker 上面可以使用的最大的user memory值 （默认JVM max memory * 0.1）调整到 JVM max memory * 0.25</span><br><span class="line">#query.max-memory-per-node&#x3D;100GB</span><br><span class="line">#单个Query在单个Worker上允许的最大user memory + system memory 调整到 JVM max memory * 0.4</span><br><span class="line">#query.max-total-memory-per-node&#x3D;16GB</span><br><span class="line">#这个内存主要是第三方库的内存分配，无法统计跟踪。 （默认JVM max memory * 0.3）调整到 JVM max memory * 0.2</span><br><span class="line">#memory.heap-headroom-per-node&#x3D;8GB</span><br><span class="line">#单个查询在所有任务调度的节点上瞬间最大能用的内存（单个查询的峰值内存） 调整到 &lt;&#x3D; query.max-total-memory-per-node * workers * 0.8</span><br><span class="line">query.max-memory&#x3D;300GB</span><br><span class="line">#当presto集群发生OOM时的内存保护策略，配置 total-reservation 是kill 掉占用内存最大的任务</span><br><span class="line">query.low-memory-killer.policy&#x3D;total-reservation</span><br><span class="line"></span><br><span class="line">#延长等待时间</span><br><span class="line">exchange.http-client.request-timeout&#x3D;10s</span><br><span class="line">#从其他Presto节点获取数据的线程数。较高的值可以提高大型群集或具有很高并发性的群集的性能(默认值：25)</span><br><span class="line">exchange.client-threads&#x3D;128</span><br><span class="line">exchange.http-client.idle-timeout &#x3D; 10s</span><br><span class="line">#并行运算符（例如联接和聚合）的默认本地并发性。较低的值对于同时运行许多查询的集群更好（默认值：16，必须是2的幂次）</span><br><span class="line">task.concurrency&#x3D;2</span><br><span class="line">task.max-worker-threads&#x3D;256</span><br><span class="line">#可以创建用于处理HTTP响应的最大线程数。在具有大量并发查询的群集上或在具有数百或数千个工作程序的群集上，可以调高（默认值:100）</span><br><span class="line">task.http-response-threads&#x3D;2000</span><br><span class="line">join-distribution-type&#x3D;AUTOMATIC</span><br><span class="line">node-scheduler.max-splits-per-node&#x3D;2000</span><br><span class="line">query.max-stage-count&#x3D;4000</span><br><span class="line"></span><br><span class="line">#自动kill运行时长超过20分钟的sql（代替脚本kill任务，让presto 自动管理）：</span><br><span class="line">query.max-run-time&#x3D;1200s</span><br><span class="line">#解决新版本 remote too large 报错 </span><br><span class="line">exchange.http-client.max-content-length&#x3D;128MB</span><br><span class="line">node-manager.http-client.max-content-length&#x3D;64MB</span><br></pre></td></tr></table></figure>
<h2 id="jvm-config">jvm.config</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-Xmx1024G</span><br><span class="line">-Xms1024G</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:+UseGCOverheadLimit</span><br><span class="line">-XX:OnOutOfMemoryError&#x3D;kill -9 %p</span><br><span class="line">-DHADOOP_USER_NAME&#x3D;hdfs</span><br><span class="line">-Duser.timezone&#x3D;Asia&#x2F;Shanghai</span><br><span class="line">-Djdk.attach.allowAttachSelf&#x3D;true</span><br><span class="line">-XX:G1ReservePercent&#x3D;15</span><br><span class="line">-XX:InitiatingHeapOccupancyPercent&#x3D;40</span><br><span class="line">-XX:ConcGCThreads&#x3D;8</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>集成Flink HistoryServer至CDH</title>
    <url>/2023/05/08/%E9%9B%86%E6%88%90Flink%20HistoryServer%E8%87%B3CDH/</url>
    <content><![CDATA[<h1>集成Flink HistoryServer至CDH</h1>
<p>参考 <a href="https://blog.csdn.net/guiyifei/article/details/109325980" target="_blank" rel="noopener">https://blog.csdn.net/guiyifei/article/details/109325980</a></p>
<h1>制作Flink的parcel包和<a href="https://so.csdn.net/so/search?q=csd&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener">csd</a>文件</h1>
<h2 id="1-下载制作脚本">1. 下载制作脚本</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;EvenGui&#x2F;flink-parcel-master</span><br><span class="line"></span><br><span class="line">cd flink-parcel-master</span><br></pre></td></tr></table></figure>
<h2 id="修改参数vim-flink-parcel-properties">修改参数vim flink-parcel.properties</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#FLINk 下载地址</span><br><span class="line">FLINK_URL&#x3D;  &#x2F;opt&#x2F;soft&#x2F;flink-1.14.6-bin-scala_2.12.tgz</span><br><span class="line"></span><br><span class="line">#flink版本号</span><br><span class="line">FLINK_VERSION&#x3D;1.14.6</span><br><span class="line"></span><br><span class="line">#扩展版本号</span><br><span class="line">EXTENS_VERSION&#x3D;BIN-SCALA_2.12</span><br><span class="line"></span><br><span class="line">#操作系统版本，以centos为例</span><br><span class="line">OS_VERSION&#x3D;7</span><br><span class="line"></span><br><span class="line">#CDH 小版本</span><br><span class="line">CDH_MIN_FULL&#x3D;5.13.1</span><br><span class="line">CDH_MAX_FULL&#x3D;6.3.2</span><br><span class="line"></span><br><span class="line">#CDH大版本</span><br><span class="line">CDH_MIN&#x3D;5</span><br><span class="line">CDH_MAX&#x3D;6</span><br></pre></td></tr></table></figure>
<h2 id="编译parcel">编译parcel</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sh build.sh parcel</span><br></pre></td></tr></table></figure>
<p>./FLINK-1.14.6-BIN-SCALA_2.12/lib/flink/bin/start-scala-shell.sh does not exist.</p>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2023/03/image-20230310160047820.png" alt="image-20230310160047820"></p>
<p>编译会报错，我是创建一个空文件</p>
<p>编译完会在flink-parcel项目根目录下生成_build文件夹</p>
<h2 id="编译csd">编译csd</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sh build.sh csd</span><br></pre></td></tr></table></figure>
<h2 id="配置参考">配置参考</h2>
<ol>
<li>
<p>修改flink-conf.yaml文件<br>
# 是否清理不存在的作业(即已经过期的)<br>
historyserver.archive.clean-expired-jobs：false</p>
<p># 每一个归档目录下可以保留的最大job数，设置为-1即不限制<br>
historyserver.archive.retained-jobs：-1</p>
<p># HistoryServer 地址<br>
historyserver.web.address：0.0.0.0</p>
<p># HistoryServer web地址<br>
historyserver.web.port：9999</p>
<p># web端刷新间隔<br>
historyserver.web.refresh-interval: 10000</p>
<p># 配置已归档jm路径<br>
jobmanager.archive.fs.dir: hdfs:///project/flink/history-server/</p>
<p>#historyserver监控归档路径，该路径要和jm配置的一样<br>
historyserver.archive.fs.dir：hdfs:///project/flink/history-server/</p>
<p># 每10s扫描一次归档路径<br>
historyserver.archive.fs.refresh-interval: 10000</p>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>MongoDB6.0集群搭建(Centos7)</title>
    <url>/2023/04/28/MongoDB6.0%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA(Centos7)/</url>
    <content><![CDATA[<h1>MongoDB6.0集群搭建(Centos7)</h1>
<p>wget  <a href="https://downloads.mongodb.com/compass/mongosh-1.8.1-linux-x64.tgz" target="_blank" rel="noopener">https://downloads.mongodb.com/compass/mongosh-1.8.1-linux-x64.tgz</a></p>
<p>wget <a href="https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-6.0.5.tgz" target="_blank" rel="noopener">https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-6.0.5.tgz</a></p>
<p>参考   <a href="https://www.jianshu.com/p/21d33d6cb56c" target="_blank" rel="noopener">https://www.jianshu.com/p/21d33d6cb56c</a></p>
<p><a href="https://blog.csdn.net/networken/article/details/115409367" target="_blank" rel="noopener">https://blog.csdn.net/networken/article/details/115409367</a></p>
]]></content>
  </entry>
  <entry>
    <title>HBase常用的Filter</title>
    <url>/2023/04/23/HBase%E5%B8%B8%E7%94%A8%E7%9A%84Filter/</url>
    <content><![CDATA[<h1>HBase常用的Filter</h1>
<p>简介：</p>
<p>​    根据技术调研的过程可以明显的体会到hbase的存储方式和数据库的存储有着明显的区别，查询的方式也有着很大不同，HBase主要是通过这种filter来对数据进行筛选。同时对于数据的体量较大（10亿级别以上的数据数据量），检索和修改的场景较多时是比较适合使用hbase。</p>
<p>HBase过滤器可以根据分为：列簇与列类型过滤器，行键过滤器，其他过滤器<br>
HBase Filter 概览<br>
查询hbase支持的filter 列表</p>
<p>[<img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码">](javascript:void(0);)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">base(main):001:0&gt; show_filters</span><br><span class="line">DependentColumnFilter                                                                                                                                                  </span><br><span class="line">KeyOnlyFilter                                                                                                                                                          </span><br><span class="line">ColumnCountGetFilter                                                                                                                                                   </span><br><span class="line">SingleColumnValueFilter                                                                                                                                                </span><br><span class="line">PrefixFilter                                                                                                                                                           </span><br><span class="line">SingleColumnValueExcludeFilter                                                                                                                                         </span><br><span class="line">FirstKeyOnlyFilter                                                                                                                                                     </span><br><span class="line">ColumnRangeFilter                                                                                                                                                      </span><br><span class="line">ColumnValueFilter                                                                                                                                                      </span><br><span class="line">TimestampsFilter                                                                                                                                                       </span><br><span class="line">FamilyFilter                                                                                                                                                           </span><br><span class="line">QualifierFilter                                                                                                                                                        </span><br><span class="line">ColumnPrefixFilter                                                                                                                                                     </span><br><span class="line">RowFilter                                                                                                                                                              </span><br><span class="line">MultipleColumnPrefixFilter                                                                                                                                             </span><br><span class="line">InclusiveStopFilter                                                                                                                                                    </span><br><span class="line">PageFilter                                                                                                                                                             </span><br><span class="line">ValueFilter                                                                                                                                                            </span><br><span class="line">ColumnPaginationFilter                                                                                                                                                 </span><br><span class="line">Took 0.0033 seconds</span><br></pre></td></tr></table></figure>
<p>[<img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码">](javascript:void(0);)</p>
<p>HBase Filter 使用简单例子<br>
行键过滤器<br>
RowFilter:针对行键进行过滤</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;,FILTER&#x3D;&gt;&quot;RowFilter(&#x3D;,&#39;binaryprefix:row&#39;)&quot;</span><br></pre></td></tr></table></figure>
<p>说明:把test表中rowkey 以row开头的数据查出来</p>
<p>支持的比较运算符：= != &gt; &gt;= &lt; &lt;= 。</p>
<p>PrefixFilter:行键前缀过滤器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;,FILTER&#x3D;&gt;&quot;PrefixFilter(&#39;row0&#39;)&quot;</span><br></pre></td></tr></table></figure>
<p>说明：把test表中rowkey 以row0开头的数据查出来</p>
<p>FirstKeyOnlyFilter:扫描全表，显示每个逻辑行的第一个键值对</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;,FILTER&#x3D;&gt;&quot;FirstKeyOnlyFilter()&quot;</span><br></pre></td></tr></table></figure>
<p>说明：一个rowkey可以有多个version,同一个rowkey的同一个column也会有多个的值,只拿出key中的第一个column的第一个version</p>
<p>列簇与列过滤器</p>
<p>FamilyFilter:针对列族进行比较和过滤</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;,FILTER&#x3D;&gt;&quot;FamilyFilter(&#x3D;,&#39;substring:f1&#39;)&quot;</span><br></pre></td></tr></table></figure>
<p>说明：查询列簇前缀以“f1”开头的数据</p>
<p>QualifierFilter:列标识过滤器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;,FILTER&#x3D;&gt;&quot;QualifierFilter(&#x3D;,&#39;substring:name&#39;)&quot;</span><br></pre></td></tr></table></figure>
<p>说明：查询所有列簇中列名是name的列数据</p>
<p>ColumnPrefixFilter:对列名前缀进行过滤</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;,FILTER&#x3D;&gt;&quot;ColumnPrefixFilter(&#39;n&#39;)&quot;</span><br></pre></td></tr></table></figure>
<p>说明：查询所有列簇中列以“n”开头的数据</p>
<p>MultipleColumnPrefixFilter:可以指定多个前缀</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;,FILTER&#x3D;&gt;&quot;MultipleColumnPrefixFilter(&#39;n&#39;,&#39;m&#39;)&quot;</span><br></pre></td></tr></table></figure>
<p>说明：查询test表，所有列簇中列名以“n“或者”m&quot;开头的数据</p>
<p>ColumnRangeFilter:设置范围按字典序对列名进行过滤</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;,FILTER&#x3D;&gt;&quot;ColumnRangeFilter(&#39;name&#39;,true,&#39;music&#39;,false)&quot;</span><br></pre></td></tr></table></figure>
<p>说明：ColumnRangeFilter过滤器则可以扫描出符合过滤条件的列范围，起始和终止列名用单引号引用，true 和 false 参数可指明结果中包含的起始或终止列。</p>
<p>值过滤器</p>
<p>ValueFilter：值过滤器，找到符合值条件的键值对</p>
<p>[<img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码">](javascript:void(0);)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">模糊查找：scan &#39;test&#39;, FILTER &#x3D;&gt; &quot;ValueFilter(&#x3D;,&#39;substring:张&#39;)&quot;</span><br><span class="line"></span><br><span class="line">说明：查询test表中，所有列值为以“张”开头的数据</span><br><span class="line"></span><br><span class="line">精确查询：scan &#39;test&#39;, FILTER &#x3D;&gt; &quot;ValueFilter(&#x3D;,&#39;binary:张三&#39;)&quot;</span><br><span class="line"></span><br><span class="line">说明：查询test表中，所有列值为“张三”的数据</span><br><span class="line"></span><br><span class="line">注意1：ValueFilter 过滤器可以利用 get 和 scan 方法对单元格进行过滤，但是使用 get 方法时，需要指定行键</span><br></pre></td></tr></table></figure>
<p>[<img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码">](javascript:void(0);)</p>
<p>SingleColumnValueFilter：在指定的列族和列中进行比较的值过滤器</p>
<p>[<img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码">](javascript:void(0);)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;, &#123;FILTER &#x3D;&gt; &quot;SingleColumnValueFilter(&#39;liecuA&#39;, &#39;name&#39;, &#x3D;, &#39;substring:张&#39;)&quot;,FORMATTER &#x3D;&gt; &#39;toString&#39;&#125;</span><br><span class="line"></span><br><span class="line">   说明：查询test表，列簇：liecuA 下 name列以“张”开头的数据</span><br><span class="line"></span><br><span class="line">   注意：如果查询的数据存在没有liecuA：name 对应值得数据也会展示出啦</span><br><span class="line"></span><br><span class="line">   解决方案：HBase入门：shell模糊查询_u011236069的博客-CSDN博客   注意事项2</span><br></pre></td></tr></table></figure>
<p>[<img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码">](javascript:void(0);)</p>
<p>SingleColumnValueExcludeFilter：排除匹配成功的值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;, &#123;FILTER &#x3D;&gt; &quot;SingleColumnValueExcludeFilter(&#39;liecuA&#39;, &#39;name&#39;, &#x3D;, &#39;substring:张&#39;)&quot;,FORMATTER &#x3D;&gt; &#39;toString&#39;&#125;</span><br></pre></td></tr></table></figure>
<p>其他过滤器</p>
<p>​    还有一些其他的过滤器，比如与分页，时间等</p>
<p>ColumnCountGetFilter：限制每个逻辑行返回键值对的个数，在 get 方法中使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">get &#39;test&#39;, &#39;row1&#39;, FILTER &#x3D;&gt; &quot;ColumnCountGetFilter(3)&quot;</span><br><span class="line"></span><br><span class="line">  说明：查询test表行键为“row1”的数据，只允许显示3列</span><br></pre></td></tr></table></figure>
<p>TimestampsFilter：时间戳过滤，支持等值，可以设置多个时间戳</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;, Filter &#x3D;&gt; &quot;TimestampsFilter(1636561062,1636993106)&quot;</span><br><span class="line"></span><br><span class="line">说明： 注意Filter需要再使用前进行导入</span><br></pre></td></tr></table></figure>
<p>PageFilter：对查询结果按行进行分页显示</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;, &#123; STARTROW &#x3D;&gt; &#39;row1&#39;, ENDROW &#x3D;&gt; &#39;row5&#39;, FILTER &#x3D;&gt; &quot;PageFilter(3)&quot; &#125;</span><br><span class="line"></span><br><span class="line">  说明：查询test表根据行键从“row1”到“row5” 查询3行数据</span><br></pre></td></tr></table></figure>
<p>ColumnPaginationFilter：对一行的所有列分页，只返回 [offset,offset+limit] 范围内的列</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scan &#39;test&#39;, &#123; STARTROW &#x3D;&gt; &#39;row1&#39;, ENDROW &#x3D;&gt; &#39;row5&#39;, FILTER &#x3D;&gt; &quot;ColumnPaginationFilter(2,1)&quot; &#125;</span><br><span class="line"></span><br><span class="line">  说明：ColumnPaginationFilter过滤器对一个逻辑行的所有列进行分页显示。</span><br></pre></td></tr></table></figure>
<p>总结</p>
<p>​    一次查询可以使用多个Filter进行多维度筛选 ，各Filter之间使用关系运算符：AND 或 OR 进行连接，请使用大些字母避免使用shell 方式查找无效！！！</p>
]]></content>
  </entry>
  <entry>
    <title>centos绑定虚拟IP</title>
    <url>/2023/04/19/centos%E7%BB%91%E5%AE%9A%E8%99%9A%E6%8B%9FIP/</url>
    <content><![CDATA[<p>配置虚拟IP</p>
<p>/sbin/ifconfig bond0:2 172.20.192.241 netmask 255.255.255.0 up</p>
<p>广播通知</p>
<p>/sbin/arping -c 5 -U -I bond0 172.20.192.241 &gt;/dev/null 2&gt;&amp;1</p>
<p>关闭虚拟IP</p>
<p>/sbin/ifconfig bond0:2 172.20.192.241 netmask 255.255.255.0 down</p>
]]></content>
  </entry>
  <entry>
    <title>Trino 开启用户名密码校验</title>
    <url>/2023/04/19/Trino%20%E5%BC%80%E5%90%AF%E7%94%A8%E6%88%B7%E5%90%8D%E5%AF%86%E7%A0%81%E6%A0%A1%E9%AA%8C/</url>
    <content><![CDATA[<p><a href="https://sslover.me/2022/03/15/trino-deploy-tutorials/" target="_blank" rel="noopener">https://sslover.me/2022/03/15/trino-deploy-tutorials/</a></p>
<h4 id="开启-TLS-支持">开启 TLS 支持</h4>
<p>当采用用户名/密码的方式进行认证时，Trino 强制要求我们使用 TLS，所以我们需要对其进行配置。</p>
<p>首先我们需要创建 HTTPS 证书，由于是本地测试，所以我们直接自己签发一个。Trino 支持 PEM 文件和 JKS 文件，我们使用标准的 PEM 文件进行配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成私钥</span><br><span class="line">[root@cdh192-150 ssl]# openssl genrsa -out server.key 2048</span><br><span class="line"># 生成 CSR </span><br><span class="line">[root@cdh192-150 ssl]# openssl req -new -key server.key -out server.csr</span><br><span class="line"># 自己派发证书</span><br><span class="line">[root@cdh192-150 ssl]# openssl x509 -req -days 36500 -in server.csr -signkey server.key -out server.crt</span><br><span class="line"># 生成 PEM 格式文件</span><br><span class="line">[root@cdh192-150 ssl]# cat server.key server.crt &gt; server.pem</span><br><span class="line"># 校验 PEM 文件</span><br><span class="line">[root@cdh192-150 ssl]# openssl rsa -in server.pem -check -noout</span><br><span class="line">[root@cdh192-150 ssl]# openssl x509 -in server.pem -text -noout</span><br></pre></td></tr></table></figure>
<p>生成完证书后，在 <code>config.properties</code> 文件中添加如下配置即可开启 TLS 支持：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 开启 https</span><br><span class="line">http-server.https.enabled&#x3D;true</span><br><span class="line"># 指定 https 端口</span><br><span class="line">http-server.https.port&#x3D;8443</span><br><span class="line"># 指定证书文件</span><br><span class="line">http-server.https.keystore.path&#x3D;&#x2F;opt&#x2F;module&#x2F;trino-server-405&#x2F;etc&#x2F;ssl&#x2F;server.pem</span><br><span class="line"># 开启web-ui访问 </span><br><span class="line">http-server.authentication.allow-insecure-over-http&#x3D;true</span><br></pre></td></tr></table></figure>
<p>最后修改 JDBC 连接地址，由于是自签证书，所以我们需要关闭证书校验：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jdbc:trino:&#x2F;&#x2F;127.0.0.1:8443?SSL&#x3D;true&amp;SSLVerification&#x3D;NONE</span><br></pre></td></tr></table></figure>
<p>同理，CLI 命令修改为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">trino --server https:&#x2F;&#x2F;localhost:8443 --user user --password --execute &#39;SELECT * FROM mysql.test.user&#39; --insecure</span><br></pre></td></tr></table></figure>
<h2 id="开启-Trino-内部通讯-Internal-Communication">开启 Trino 内部通讯 (Internal Communication)</h2>
<p>使用 TLS 以及一个 configured shared secret 是使用基于文件的用户名密码认证的必要条件。 本节阐述如何开启 Internal Communication 并配置一个 configured shared secret。</p>
<p>在本例中我们使用 <code>lhoOHWfehuKgkXnu4E6Gq5L5K4i8iMPlHFwUzrj25r5lUKUZSn</code> 作为内部通信密码进行示范。</p>
<h3 id="配置-shared-secert">配置 shared secert</h3>
<p>编辑配置文件，将内部通讯的密文配置在所有 Trino 节点的配置文件中。注意，该密文必须一致，否则 worker 节点无法连接至 coordinator。</p>
<p>打开 Trino 的配置文件: <code>vim $TRINO_HOME/etc/config.properties</code></p>
<p>在配置文件中配好 <code>internal-communication.shared-secret</code> 配置项:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Set the shared secret to the same value in config.properties on all nodes of the cluster </span><br><span class="line">internal-communication.shared-secret&#x3D;lhoOHWfehuKgkXnu4E6Gq5L5K4i8iMPlHFwUzrj25r5lUKUZSn</span><br></pre></td></tr></table></figure>
<h2 id="创建用户名密码文件">创建用户名密码文件</h2>
<p>至此，TLS 和内部通讯都已经配置成功。接下来，我们要为 Trino 添加用户名密码校验功能。</p>
<h3 id="修改-Trino-配置文件">修改 Trino 配置文件</h3>
<p>打开 Trino 的配置文件: <code>vim $TRINO_HOME/etc/config.properties</code></p>
<p>在配置文件中配好 <code>http-server.authentication.type</code> 配置项:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Password file authentication http-server.authentication.type&#x3D;PASSWORD</span><br></pre></td></tr></table></figure>
<p>根据文档，一旦上述配置项配置完毕，我们就需要创建一个新文件 <code>$TRINO_HOME/etc/password-authenticator.properties</code>， 并在该文件中添加部分配置。</p>
<p>创建该配置文件: <code>vim $TRINO_HOME/etc/password-authenticator.properties</code></p>
<p>在该配置文件中添加如下配置:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">password-authenticator.name&#x3D;file </span><br><span class="line">file.password-file&#x3D;&#x2F;opt&#x2F;trino-server-392&#x2F;etc&#x2F;password.db</span><br></pre></td></tr></table></figure>
<h3 id="添加用户与密码至-password-db">添加用户与密码至 password.db</h3>
<p>根据上述配置文件内 <code>password.db</code> 的位置，创建一个空的 password 文件:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">touch password.db</span><br></pre></td></tr></table></figure>
<p>使用如下命令添加或更新名为 <code>flinkrt</code> 用户的密码。备注: 在本示例中，<code>flinkrt</code> 用户的密码同样为 <code>flinkrt</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">htpasswd -B -C 10 password.db flinkrt</span><br></pre></td></tr></table></figure>
<p>执行结果如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@p0-tkldmp-rc01 etc]# htpasswd -B -C 10 password.db flinkrt </span><br><span class="line">New password:  </span><br><span class="line">Re-type new password:  Adding password for user flinkrt</span><br></pre></td></tr></table></figure>
<h4 id="权限控制">权限控制</h4>
<p>默认在不开启权限控制的情况下，用户都拥有最大权限，这显示是很不安全的，所以需要配置权限控制。</p>
<p>创建 <code>access-control.properties</code> 文件，指定权限控制文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">access-control.name&#x3D;file</span><br><span class="line"># 指定控制文件位置</span><br><span class="line">security.config-file&#x3D;&#x2F;opt&#x2F;module&#x2F;trino-server-405&#x2F;etc&#x2F;rules.json</span><br><span class="line"># 自动刷新间隔，不再需要重启 Trino 即可自动生效</span><br><span class="line">security.refresh-period&#x3D;1s</span><br></pre></td></tr></table></figure>
<p>创建 <code>group-provider.properties</code> 文件，指定群组配置文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">group-provider.name&#x3D;file</span><br><span class="line">file.group-file&#x3D;&#x2F;opt&#x2F;module&#x2F;trino-server-405&#x2F;etc&#x2F;group.txt</span><br></pre></td></tr></table></figure>
<p>然后编辑群组映射关系 <code>group.txt</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">admin:admin-user</span><br><span class="line">user:user1,user2</span><br></pre></td></tr></table></figure>
<p>然后创建 <code>rules.json</code> 文件编写规则，具体的权限控制比较复杂，可以参考 <a href="https://trino.io/docs/current/security/file-system-access-control.html" target="_blank" rel="noopener">File-based access control</a>，这里我们只写一个基础的权限控制规则，实现 <code>admin</code> 组的用户可以进行所有操作，任意用户可以访问 <code>mysql</code>，user 可以读取 <code>postgresql</code> 的数据，<code>system</code> 没有用户可以访问。权限匹配是从上到下的，如果没有匹配到，则默认拒绝权限。具体配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;catalogs&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;group&quot;:&quot;admin&quot;,</span><br><span class="line">            &quot;allow&quot;:&quot;all&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;catalog&quot;:&quot;mysql&quot;,</span><br><span class="line">            &quot;allow&quot;:&quot;all&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;:&quot;user1&quot;,</span><br><span class="line">            &quot;catalog&quot;:&quot;hive&quot;,</span><br><span class="line">            &quot;allow&quot;:&quot;read-only&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;catalog&quot;:&quot;system&quot;,</span><br><span class="line">            &quot;allow&quot;:&quot;none&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>再创建用户 user2 及 admin-user，来测试一下效果</p>
]]></content>
  </entry>
  <entry>
    <title>win10多用户同时远程连接</title>
    <url>/2023/04/18/win10%E5%A4%9A%E7%94%A8%E6%88%B7%E5%90%8C%E6%97%B6%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/</url>
    <content><![CDATA[<h2 id="win10-解决多用户同时远程连接">win10 解决多用户同时远程连接</h2>
<p>下载破解软件，用管理员身份运行</p>
<p><a href="https://github.com/stascorp/rdpwrap" target="_blank" rel="noopener">https://github.com/stascorp/rdpwrap</a></p>
<p>修改配置文件 ，从 <a href="https://github.com/sebaxakerhtc/rdpwrap.ini" target="_blank" rel="noopener">https://github.com/sebaxakerhtc/rdpwrap.ini</a> 下载</p>
<p>C:\Program Files\RDP Wrapper\rdpwrap.ini</p>
<p>重启服务器就可以了。</p>
]]></content>
  </entry>
  <entry>
    <title>Alluxio常用操作</title>
    <url>/2023/04/17/Alluxio%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="Alluxio常用操作">Alluxio常用操作</h2>
<h2 id="启动一个Alluxio集群">启动一个Alluxio集群</h2>
<h3 id="格式化Alluxio">格式化Alluxio</h3>
<p>在首次启动Alluxio之前，必须先格式化日志。</p>
<blockquote>
<p>格式化日记将删除Alluxio中的所有元数据。 但是，格式化不会涉及底层存储的数据。</p>
</blockquote>
<p>在master节点上，使用以下命令格式化Alluxio：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio formatMaster</span><br></pre></td></tr></table></figure>
<h3 id="启动Alluxio">启动Alluxio</h3>
<p>启动Alluxio集群，在master点上确保<code>conf/workers</code>文件中所有worker的主机名都是正确的。</p>
<p>在master点上，运行以下命令启动Alluxio集群：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-start.sh all SudoMount</span><br></pre></td></tr></table></figure>
<p>这将在此节点上启动master，并在<code>conf/workers</code>文件中指定的所有节点上启动所有workers。 <code>SudoMount</code>参数使workers可以尝试使用<code>sudo</code>特权（如果尚未挂载）来挂载RamFS。</p>
<h3 id="验证Alluxio集群是否在运行">验证Alluxio集群是否在运行</h3>
<p>要验证Alluxio是否正在运行，请访问<code>http://&lt;alluxio_master_hostname&gt;:19999</code>以查看Alluxio master的状态页面。</p>
<p>Alluxio带有一个简单的程序可以在Alluxio中读写示例文件。 使用以下命令运行示例程序：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio runTests</span><br></pre></td></tr></table></figure>
<p>以下是在Alluxio集群上执行的常见操作。</p>
<h3 id="停止Alluxio">停止Alluxio</h3>
<p>停止一个Alluxio服务，运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-stop.sh all</span><br></pre></td></tr></table></figure>
<p>这将停止<code>conf/workers</code>和<code>conf/masters</code>中列出的所有节点上的所有进程。</p>
<p>可以使用以下命令仅停止master和workers：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-stop.sh masters # 停止所有conf&#x2F;masters的masters</span><br><span class="line">$ .&#x2F;bin&#x2F;alluxio-stop.sh workers # 停止所有conf&#x2F;workers的workers</span><br></pre></td></tr></table></figure>
<p>如果不想使用ssh登录所有节点来停止所有进程，可以在每个节点上运行命令以停止每个组件。 对于任何节点，可以使用以下命令停止master或worker：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-stop.sh master # 停止本地master</span><br><span class="line">$ .&#x2F;bin&#x2F;alluxio-stop.sh worker # 停止本地worker</span><br></pre></td></tr></table></figure>
<h3 id="重新启动Alluxio">重新启动Alluxio</h3>
<p>与启动Alluxio类似。 如果已经配置了<code>conf/workers</code>和<code>conf/masters</code>，可以使用以下命令启动集群：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-start.sh all</span><br></pre></td></tr></table></figure>
<p>可以使用以下命令仅启动masters或workers：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-start.sh masters # 启动conf&#x2F;masters中全部的master</span><br><span class="line">$ .&#x2F;bin&#x2F;alluxio-start.sh workers # 启动conf&#x2F;workers中全部的worker</span><br></pre></td></tr></table></figure>
<p>如果不想使用<code>ssh</code>登录所有节点来启动所有进程，可以在每个节点上运行命令以启动每个组件。 对于任何节点，可以使用以下命令启动master或worker：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-start.sh master # 启动本地master</span><br><span class="line">$ .&#x2F;bin&#x2F;alluxio-start.sh worker # 启动本地worker</span><br></pre></td></tr></table></figure>
<h3 id="格式化日志">格式化日志</h3>
<p>在任何master节点上，运行以下命令格式化Alluxio日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio formatMaster</span><br></pre></td></tr></table></figure>
<p>格式化日记将删除Alluxio中的所有元数据。 但是，将不会触及底层存储的数据。</p>
<h3 id="动态添加-减少worker">动态添加/减少worker</h3>
<p>动态添加worker到Alluxio集群就像通过适当配置启动新Alluxio worker进程一样简单。 在大多数情况下，新worker配置应与所有其他worker配置相同。 在新worker上运行以下命令，以将其添加到集群。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-start.sh worker SudoMount # 启动本地 worker</span><br></pre></td></tr></table></figure>
<p>一旦worker启动，它将在Alluxio master上注册，并成为Alluxio集群的一部分。</p>
<p>减少worker只需要简单停止一个worker进程。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-stop.sh worker # 停止本地 worker</span><br></pre></td></tr></table></figure>
<p>一旦worker被停止，master将在预定的超时值（通过master参数<code>alluxio.master.worker.timeout</code>配置）后将此worker标记为缺失。 主机视worker为”丢失”，并且不再将其包括在集群中。</p>
<h3 id="更新master配置">更新master配置</h3>
<p>为了更新master配置，必须首先停止服务，更新master节点上的<code>conf/alluxio-site.properties</code>文件，并将文件复制到所有节点（例如，使用<code>bin/alluxio copyDir conf/</code>），<a href="https://docs.alluxio.io/os/user/stable/cn/deploy/Running-Alluxio-on-a-Cluster.html?q=workers#restart-alluxio" target="_blank" rel="noopener">然后重新启动服务</a>。</p>
<h3 id="更新worker配置">更新worker配置</h3>
<p>如果只需要为worker节点更新某些本地配置（例如，更改分配给该worker的存储容量或更新存储路径），则无需停止并重新启动master节点。 可以只停止本地worker，更新此worker上的配置（例如<code>conf/alluxio-site.properties</code>）文件，然后重新启动此worker</p>
]]></content>
  </entry>
  <entry>
    <title>部署Alluxio集群</title>
    <url>/2023/04/17/%E9%83%A8%E7%BD%B2Alluxio%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>准备<br>
下载文件：wget <a href="https://downloads.alluxio.io/downloads/files/2.9.2/alluxio-2.9.2-bin.tar.gz" target="_blank" rel="noopener">https://downloads.alluxio.io/downloads/files/2.9.2/alluxio-2.9.2-bin.tar.gz</a></p>
<p>解压目录：/opt/servers/alluxio</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;downloads.alluxio.io&#x2F;downloads&#x2F;files&#x2F;2.9.2&#x2F;alluxio-2.9.2-bin.tar.gz</span><br><span class="line">tar -xzf alluxio-2.9.2-bin.tar.gz -C &#x2F;opt&#x2F;servers&#x2F;alluxio</span><br></pre></td></tr></table></figure>
<p>创建目录：mkdir -p /mnt/ramdisk</p>
<p>机器：10.0.21.190（master，worker）10.0.21.191（master，worker）10.0.21.192（worker）</p>
<h2 id="HA-配置">HA 配置</h2>
<p>搭建高可用集群前的准备:</p>
<ul>
<li>
<p>①确保Zookeeper服务已经运行</p>
</li>
<li>
<p>②一个单独安装的可靠的共享日志存储系统(可用HDFS或S3等系统)</p>
</li>
<li>
<p>③这个配置针对Alluxio 2.x版本，不适用于1.x版本</p>
</li>
<li>
<p>④需要事先创建好ramdisk挂载目录</p>
</li>
</ul>
<p>配置hive alluxio文件系统：core-site.xml</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;fs.alluxio.impl&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;value&gt;alluxio.hadoop.FileSystem&lt;&#x2F;value&gt;</span><br><span class="line">      &lt;description&gt;The Alluxio FileSystem (Hadoop 1.x and 2.x)&lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;fs.AbstractFileSystem.alluxio.impl&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;value&gt;alluxio.hadoop.AlluxioFileSystem&lt;&#x2F;value&gt;</span><br><span class="line">      &lt;description&gt;The Alluxio AbstractFileSystem (Hadoop 2.x)&lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
<p><a href="http://xn--alluxio-env-zp8qf48f1kcf30e98p2xax394at74c.sh" target="_blank" rel="noopener">在所有机器上配置alluxio-env.sh</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALLUXIO_HOME&#x3D;&#x2F;opt&#x2F;servers&#x2F;alluxio</span><br><span class="line">ALLUXIO_LOGS_DIR&#x3D;&#x2F;opt&#x2F;servers&#x2F;alluxio&#x2F;logs</span><br><span class="line">ALLUXIO_RAM_FOLDER&#x3D;&#x2F;mnt&#x2F;ramdisk</span><br><span class="line">JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_141-cloudera</span><br><span class="line">ALLUXIO_WORKER_JAVA_OPTS&#x3D;&quot; -XX:+PrintGCDetails -XX:+PrintTenuringDistribution -XX:+PrintGCTimeStamps&quot;</span><br><span class="line">ALLUXIO_MASTER_JAVA_OPTS&#x3D;&quot; -Xms2048M -Xmx4096M&quot;</span><br><span class="line">ALLUXIO_JAVA_OPTS+&#x3D;&quot; -Djava.library.path&#x3D;&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop&quot;</span><br></pre></td></tr></table></figure>
<h2 id="在10-0-21-190机器上配置Master和Worker：alluxio-site-properties">在10.0.21.190机器上配置Master和Worker：alluxio-site.properties</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 10.0.21.190    Master Worker</span><br><span class="line"></span><br><span class="line"># Common properties</span><br><span class="line"></span><br><span class="line">alluxio.master.hostname&#x3D;10.0.21.190</span><br><span class="line">alluxio.underfs.hdfs.configuration&#x3D;&#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;core-site.xml:&#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;hdfs-site.xml</span><br><span class="line">alluxio.master.mount.table.root.ufs&#x3D;hdfs:&#x2F;&#x2F;ns1&#x2F;user&#x2F;alluxio&#x2F;ha&#x2F;</span><br><span class="line"></span><br><span class="line"># Worker properties</span><br><span class="line"></span><br><span class="line">alluxio.worker.ramdisk.size&#x3D;512MB</span><br><span class="line">alluxio.worker.tieredstore.levels&#x3D;1</span><br><span class="line">alluxio.worker.tieredstore.level0.alias&#x3D;MEM</span><br><span class="line">alluxio.worker.tieredstore.level0.dirs.path&#x3D;&#x2F;mnt&#x2F;ramdisk</span><br><span class="line"></span><br><span class="line"># HA properties</span><br><span class="line"></span><br><span class="line">alluxio.zookeeper.enabled&#x3D;true</span><br><span class="line">alluxio.zookeeper.address&#x3D;10.0.21.190:2181,10.0.21.191:2181,10.0.21.192:2181</span><br><span class="line">alluxio.master.journal.type&#x3D;UFS</span><br><span class="line">alluxio.master.journal.folder&#x3D;hdfs:&#x2F;&#x2F;ns1&#x2F;user&#x2F;alluxio&#x2F;journal&#x2F;</span><br><span class="line">alluxio.worker.block.heartbeat.timeout.ms&#x3D;300000</span><br><span class="line">alluxio.zookeeper.session.timeout&#x3D;120s</span><br><span class="line"></span><br><span class="line"># User properties</span><br><span class="line"></span><br><span class="line">alluxio.user.file.readtype.default&#x3D;CACHE_PROMOTE</span><br><span class="line">alluxio.user.file.writetype.default&#x3D;ASYNC_THROUGH</span><br><span class="line">alluxio.user.metrics.collection.enable&#x3D;true</span><br><span class="line">alluxio.master.metrics.time.series.interval&#x3D;1000</span><br><span class="line"></span><br><span class="line"># Security properties</span><br><span class="line"></span><br><span class="line">alluxio.security.authorization.permission.enabled&#x3D;true</span><br><span class="line">alluxio.security.authentication.type&#x3D;SIMPLE</span><br><span class="line">alluxio.master.security.impersonation.hive.users&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.hive.groups&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.yarn.users&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.yarn.groups&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.hdfs.users&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.hdfs.groups&#x3D;*</span><br></pre></td></tr></table></figure>
<h2 id="在10-0-21-191机器上配置Master和Worker：alluxio-site-properties">在10.0.21.191机器上配置Master和Worker：alluxio-site.properties</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 10.0.21.191      Master Worker</span><br><span class="line"></span><br><span class="line"># Common properties</span><br><span class="line"></span><br><span class="line">alluxio.master.hostname&#x3D;10.0.21.191</span><br><span class="line">alluxio.underfs.hdfs.configuration&#x3D;&#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;core-site.xml:&#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;hdfs-site.xml</span><br><span class="line">alluxio.master.mount.table.root.ufs&#x3D;hdfs:&#x2F;&#x2F;ns1&#x2F;user&#x2F;alluxio&#x2F;ha&#x2F;</span><br><span class="line"></span><br><span class="line"># Worker properties</span><br><span class="line"></span><br><span class="line">alluxio.worker.ramdisk.size&#x3D;512MB</span><br><span class="line">alluxio.worker.tieredstore.levels&#x3D;1</span><br><span class="line">alluxio.worker.tieredstore.level0.alias&#x3D;MEM</span><br><span class="line">alluxio.worker.tieredstore.level0.dirs.path&#x3D;&#x2F;mnt&#x2F;ramdisk</span><br><span class="line"></span><br><span class="line"># HA properties</span><br><span class="line"></span><br><span class="line">alluxio.zookeeper.enabled&#x3D;true</span><br><span class="line">alluxio.zookeeper.address&#x3D;10.0.21.190:2181,10.0.21.191:2181,10.0.21.192:2181</span><br><span class="line">alluxio.master.journal.type&#x3D;UFS</span><br><span class="line">alluxio.master.journal.folder&#x3D;hdfs:&#x2F;&#x2F;ns1&#x2F;user&#x2F;alluxio&#x2F;journal&#x2F;</span><br><span class="line">alluxio.worker.block.heartbeat.timeout.ms&#x3D;300000</span><br><span class="line">alluxio.zookeeper.session.timeout&#x3D;120s</span><br><span class="line"></span><br><span class="line"># User properties</span><br><span class="line"></span><br><span class="line">alluxio.user.file.readtype.default&#x3D;CACHE_PROMOTE</span><br><span class="line">alluxio.user.file.writetype.default&#x3D;ASYNC_THROUGH</span><br><span class="line">alluxio.user.metrics.collection.enable&#x3D;true</span><br><span class="line">alluxio.master.metrics.time.series.interval&#x3D;1000</span><br><span class="line"></span><br><span class="line"># Security properties</span><br><span class="line"></span><br><span class="line">alluxio.security.authorization.permission.enabled&#x3D;true</span><br><span class="line">alluxio.security.authentication.type&#x3D;SIMPLE</span><br><span class="line">alluxio.master.security.impersonation.hive.users&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.hive.groups&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.yarn.users&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.yarn.groups&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.hdfs.users&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.hdfs.groups&#x3D;*</span><br></pre></td></tr></table></figure>
<h2 id="在10-0-21-192机器上配置Worker：alluxio-site-properties">在10.0.21.192机器上配置Worker：alluxio-site.properties</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 10.0.21.192      Worker</span><br><span class="line"></span><br><span class="line"># Common properties</span><br><span class="line"></span><br><span class="line"># Worker不需要写alluxio.master.hostname参数和alluxio.master.journal.folder参数</span><br><span class="line"></span><br><span class="line">alluxio.underfs.hdfs.configuration&#x3D;&#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;core-site.xml:&#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;hdfs-site.xml</span><br><span class="line">alluxio.master.mount.table.root.ufs&#x3D;hdfs:&#x2F;&#x2F;ns1&#x2F;user&#x2F;alluxio&#x2F;ha&#x2F;</span><br><span class="line"></span><br><span class="line"># Worker properties</span><br><span class="line"></span><br><span class="line">alluxio.worker.ramdisk.size&#x3D;512MB</span><br><span class="line">alluxio.worker.tieredstore.levels&#x3D;1</span><br><span class="line">alluxio.worker.tieredstore.level0.alias&#x3D;MEM</span><br><span class="line">alluxio.worker.tieredstore.level0.dirs.path&#x3D;&#x2F;mnt&#x2F;ramdisk</span><br><span class="line"></span><br><span class="line"># HA properties</span><br><span class="line"></span><br><span class="line">alluxio.zookeeper.enabled&#x3D;true</span><br><span class="line">alluxio.zookeeper.address&#x3D;10.0.21.190:2181,10.0.21.191:2181,10.0.21.192:2181</span><br><span class="line">alluxio.worker.block.heartbeat.timeout.ms&#x3D;300000</span><br><span class="line">alluxio.zookeeper.session.timeout&#x3D;120s</span><br><span class="line"></span><br><span class="line"># User properties</span><br><span class="line"></span><br><span class="line">alluxio.user.file.readtype.default&#x3D;CACHE_PROMOTE</span><br><span class="line">alluxio.user.file.writetype.default&#x3D;ASYNC_THROUGH</span><br><span class="line">alluxio.user.metrics.collection.enable&#x3D;true</span><br><span class="line">alluxio.master.metrics.time.series.interval&#x3D;1000</span><br><span class="line"></span><br><span class="line"># Security properties</span><br><span class="line"></span><br><span class="line">alluxio.security.authorization.permission.enabled&#x3D;true</span><br><span class="line">alluxio.security.authentication.type&#x3D;SIMPLE</span><br><span class="line">alluxio.master.security.impersonation.hive.users&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.hive.groups&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.yarn.users&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.yarn.groups&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.hdfs.users&#x3D;*</span><br><span class="line">alluxio.master.security.impersonation.hdfs.groups&#x3D;*</span><br></pre></td></tr></table></figure>
<p>在所有机器上指定Master和Worker节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#vim masters</span><br><span class="line">10.0.21.190</span><br><span class="line">10.0.21.191</span><br><span class="line"></span><br><span class="line">#vim workers</span><br><span class="line">10.0.21.190</span><br><span class="line">10.0.21.191</span><br><span class="line">10.0.21.192</span><br></pre></td></tr></table></figure>
<p>测试部署是否成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;alluxio format</span><br><span class="line">.&#x2F;alluxio-start.sh all SudoMount</span><br><span class="line">.&#x2F;alluxio fsadmin report</span><br><span class="line">.&#x2F;alluxio runTests    # 如果出现Passed the test则说明部署成功</span><br></pre></td></tr></table></figure>
<p>测试高可用模式的自动故障处理: (假设此时hadoop101位primary master)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh hadoop101</span><br><span class="line">jps | grep AlluxioMaster</span><br><span class="line">kill -9 &lt;AlluxioMaster PID&gt;</span><br><span class="line">alluxio fs leader  # 显示新的primary Master(可能需要等待一小段时间选举)</span><br></pre></td></tr></table></figure>
<h3 id="补充：添加新的worker节点">补充：添加新的worker节点</h3>
<p>动态添加worker到Alluxio集群就像通过适当配置启动新Alluxio worker进程一样简单。 在大多数情况下，新worker配置应与所有其他worker配置相同。 在新worker上运行以下命令，以将其添加到集群。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-start.sh worker SudoMount # 启动本地 worker</span><br></pre></td></tr></table></figure>
<p>一旦worker启动，它将在Alluxio master上注册，并成为Alluxio集群的一部分。</p>
<p>减少worker只需要简单停止一个worker进程。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;bin&#x2F;alluxio-stop.sh worker # 停止本地 worker</span><br></pre></td></tr></table></figure>
<p>一旦worker被停止，master将在预定的超时值（通过master参数alluxio.master.worker.timeout配置）后将此worker标记为缺失。 主机视worker为“丢失”，并且不再将其包括在集群中。</p>
<p>实际步骤：</p>
<ul>
<li>1、修改workers文件，添加新服务器</li>
<li>2、scp alluxio目录 到新的服务器节点</li>
<li>3、在新服务节点手动挂载 sudo ./bin/alluxio-mount.sh SudoMount</li>
<li>4、master节点统一启动</li>
</ul>
<h3 id="补充：配置多级存储">补充：配置多级存储</h3>
<p>1、创建目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;mnt&#x2F;alluxio_data</span><br></pre></td></tr></table></figure>
<p>2、对于与启动Alluxio服务的用户组同组用户应给予770权限。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod 770 alluxio_data</span><br></pre></td></tr></table></figure>
<p>3、修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alluxio.worker.tieredstore.levels&#x3D;2</span><br><span class="line">alluxio.worker.tieredstore.level0.alias&#x3D;MEM</span><br><span class="line">alluxio.worker.tieredstore.level0.dirs.path&#x3D;&#x2F;mnt&#x2F;ramdisk</span><br><span class="line">alluxio.worker.tieredstore.level0.dirs.mediumtype&#x3D;MEM</span><br><span class="line">alluxio.worker.tieredstore.level0.dirs.quota&#x3D;1GB</span><br><span class="line"></span><br><span class="line">alluxio.worker.tieredstore.level1.alias&#x3D;HDD</span><br><span class="line">alluxio.worker.tieredstore.level1.dirs.path&#x3D;&#x2F;mnt&#x2F;alluxio_data</span><br><span class="line">alluxio.worker.tieredstore.level1.dirs.mediumtype&#x3D;HDD</span><br><span class="line">alluxio.worker.tieredstore.level1.dirs.quota&#x3D;50GB</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>hbase的master和regionserver的jvm调优</title>
    <url>/2023/03/29/hbase%E7%9A%84master%E5%92%8Cregionserver%E7%9A%84jvm%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[<h1>hbase的master和regionserver的jvm调优</h1>
<p><a href="https://blog.csdn.net/Robots_/article/details/112475758" target="_blank" rel="noopener">https://blog.csdn.net/Robots_/article/details/112475758</a></p>
]]></content>
  </entry>
  <entry>
    <title>Hbase Call queue is full问题分析及解决</title>
    <url>/2023/03/21/Hbase%20Call%20queue%20is%20full%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%E5%8F%8A%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<h1>Hbase Call queue is full问题分析及解决</h1>
<p><a href="https://hijiazz.gitee.io/hbase-callqueue-isfull/" target="_blank" rel="noopener">https://hijiazz.gitee.io/hbase-callqueue-isfull/</a></p>
]]></content>
  </entry>
  <entry>
    <title>CDH 6.3.2 集成 flink 1.14.6</title>
    <url>/2023/03/10/CDH%206.3.2%20%E9%9B%86%E6%88%90%20flink%201.14.6/</url>
    <content><![CDATA[<h1>CDH 6.3.2 集成 flink 1.14.6</h1>
<h1>制作Flink的parcel包和<a href="https://so.csdn.net/so/search?q=csd&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener">csd</a>文件</h1>
<h2 id="1-下载制作脚本">1. 下载制作脚本</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"># 克隆源码</span><br><span class="line">git clone https:<span class="comment">//github.com/pkeropen/flink-parcel.git</span></span><br><span class="line">cd flink-parcel</span><br></pre></td></tr></table></figure>
<h2 id="2-修改参数">2 修改参数</h2>
<p>vim flink-parcel.properties</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#FLINK 下载地址</span></span><br><span class="line"><span class="attr">FLINK_URL</span>=<span class="string">https://archive.apache.org/dist/flink/flink-1.14.6/flink-1.14.6-bin-scala_2.12.tgz</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#flink版本号</span></span><br><span class="line"><span class="attr">FLINK_VERSION</span>=<span class="string">1.14.6</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#扩展版本号</span></span><br><span class="line"><span class="attr">EXTENS_VERSION</span>=<span class="string">BIN-SCALA_2.12</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#操作系统版本，以centos为例</span></span><br><span class="line"><span class="attr">OS_VERSION</span>=<span class="string">7</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#CDH 小版本</span></span><br><span class="line"><span class="attr">CDH_MIN_FULL</span>=<span class="string">5.13.1</span></span><br><span class="line"><span class="attr">CDH_MAX_FULL</span>=<span class="string">6.3.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#CDH大版本</span></span><br><span class="line"><span class="attr">CDH_MIN</span>=<span class="string">5</span></span><br><span class="line"><span class="attr">CDH_MAX</span>=<span class="string">6</span></span><br></pre></td></tr></table></figure>
<h2 id="4-编译parcel">4 编译parcel</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"># 赋予执行权限</span><br><span class="line">chmod +x ./build.sh</span><br><span class="line"># 执行编译脚本</span><br><span class="line">./build.sh parcel</span><br></pre></td></tr></table></figure>
<p>编译完会在flink-parcel项目根目录下生成_build文件夹</p>
<h2 id="5-编译csd">5 编译csd</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"># 编译flink on yarn版本</span><br><span class="line">./build.sh  csd_on_yarn</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>内网穿透</title>
    <url>/2023/02/22/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/</url>
    <content><![CDATA[<h1>内网穿透</h1>
<p>花生壳的http不能免费使用了，网上找了资料实测下面这2款可以用</p>
<h2 id="localhost-run">localhost.run</h2>
<p><a href="http://localhost.run/" target="_blank" rel="noopener">http://localhost.run/</a></p>
<p>1、这个使用最简单，不需要下载任何软件，简单一行ssh命令就行。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh -R 80:localhost:8080 nokey@localhost.run</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-R命令表示反向端口转发</span><br><span class="line">80是被指定的转发端口</span><br><span class="line">localhost是你局域网ip地址（对应127.0.0.1）</span><br><span class="line">8080是你的局域网端口号</span><br><span class="line">nokey@localhost.run是一台处在公网的服务器</span><br></pre></td></tr></table></figure>
<h2 id="FastTunnel">FastTunnel</h2>
<p><a href="https://suidao.io/" target="_blank" rel="noopener">https://suidao.io/</a></p>
<ol>
<li>在个人中心获取用于客户端登录的<code>AccessKey</code></li>
<li>在创建隧道页面，创建需要穿透的隧道信息。</li>
<li>下载并启动客户端软件, 根据提示录入<code>AccessKey</code>进行登录, 登录成功后根据映射信息提示进行访问你的服务。</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>doris部署</title>
    <url>/2023/02/21/doris%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>doris部署</p>
<h2 id="fe部署">fe部署</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一次启动需指定--helper参数，后续再启动无需指定此参数</span><br><span class="line">sh &#x2F;opt&#x2F;module&#x2F;apache-doris-fe&#x2F;bin&#x2F;start_fe.sh --helper cdh192-150:19010 --daemon</span><br><span class="line"></span><br><span class="line">ansible fe -m shell -a &quot; sh &#x2F;opt&#x2F;module&#x2F;apache-doris-fe&#x2F;bin&#x2F;stop_fe.sh &quot;</span><br><span class="line">ansible fe -m shell -a &quot; sh &#x2F;opt&#x2F;module&#x2F;apache-doris-fe&#x2F;bin&#x2F;start_fe.sh --daemon &quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql -h 127.0.0.1 -P9030 -uroot -p</span><br><span class="line">mysql&gt; SHOW PROC '/frontends'\G</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> FOLLOWER <span class="string">"cdh192-151:19010"</span>;</span><br><span class="line"><span class="comment"># ALTER SYSTEM DROP FOLLOWER "cdh192-151:19010";</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> OBSERVER <span class="string">"cdh192-152:19010"</span>;</span><br><span class="line"><span class="comment"># ALTER SYSTEM drop OBSERVER "cdh192-152:19010";</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> FOLLOWER <span class="string">"cdh192-153:19010"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> FOLLOWER <span class="string">"cdh192-146.hadoop.xy:19010"</span>;</span><br><span class="line"></span><br><span class="line">netstat -nap |grep 19010</span><br></pre></td></tr></table></figure>
<h2 id="be-部署">be 部署</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible be -m shell -a &quot; sh &#x2F;opt&#x2F;module&#x2F;apache-doris-be&#x2F;bin&#x2F;stop_be.sh &quot;</span><br><span class="line">ansible be -m shell -a &quot; sh &#x2F;opt&#x2F;module&#x2F;apache-doris-be&#x2F;bin&#x2F;start_be.sh --daemon &quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql&gt; SHOW PROC '/backends'\G</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BACKEND <span class="string">"cdh192-150:9050"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BACKEND <span class="string">"cdh192-151:9050"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BACKEND <span class="string">"cdh192-152:9050"</span>;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">SYSTEM</span> <span class="keyword">ADD</span> BACKEND <span class="string">"cdh192-146:9050"</span>;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible be -m shell -a &quot;sh &#x2F;opt&#x2F;module&#x2F;apache-doris-be&#x2F;bin&#x2F;stop_be.sh &quot;</span><br><span class="line">ansible be -m shell -a &quot;sh &#x2F;opt&#x2F;module&#x2F;apache-doris-be&#x2F;bin&#x2F;start_be.sh --daemon &quot;</span><br></pre></td></tr></table></figure>
<h2 id="doris升级">doris升级</h2>
<p><strong>重要！！在升级之前需要备份元数据（整个目录都需要备份）！！</strong></p>
<p><strong>前置工作</strong></p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 关闭副本均衡逻辑。关闭后，不会再触发普通表副本的均衡操作。</span></span><br><span class="line">$ mysql-client &gt; admin set frontend config("disable_balance" = "true");</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭 colocation 表的副本均衡逻辑。关闭后，不会再触发 colocation 表的副本重分布操作。</span></span><br><span class="line">$ mysql-client &gt; admin set frontend config("disable_colocate_balance" = "true");</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭副本调度逻辑。关闭后，所有已产生的副本修复和均衡任务不会再被调度。</span></span><br><span class="line">$ mysql-client &gt; admin set frontend config("disable_tablet_scheduler" = "true");</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /opt/module/apache-doris-fe</span><br><span class="line">cp -r doris-meta/ doris-meta_bak</span><br></pre></td></tr></table></figure>
<p><strong>升级</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scp apache-doris-fe-1.2.0-bin-x86_64.tar.xz cdh192-146:/opt/module/</span><br><span class="line">scp apache-doris-be-1.2.0-bin-x86_64.tar.xz cdh192-146:/opt/module/</span><br><span class="line">scp apache-doris-java-udf-jar-with-dependencies-1.2.0-bin-x86_64.tar.xz cdh192-146:/opt/module/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ansible cdh-online  -m copy  -a <span class="string">"src=/opt/module/apache-doris-be-1.2.0-bin-x86_64.tar.xz dest=/opt/module/"</span></span></span><br><span class="line">ssh cdh192-150 </span><br><span class="line">cd /opt/module</span><br><span class="line">tar -xvf apache-doris-be-1.2.1-bin-x86_64.tar.xz</span><br><span class="line">tar -xvf apache-doris-fe-1.2.1-bin-x86_64.tar.xz</span><br><span class="line">tar -xvf apache-doris-dependencies-1.2.1-bin-x86_64.tar.xz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 升级 BE 节点</span></span><br><span class="line">cd /opt/module/apache-doris-be</span><br><span class="line">rm -rf lib.bak &amp;&amp; mv lib lib.bak </span><br><span class="line">rm -rf bin.bak &amp;&amp; mv bin bin.bak</span><br><span class="line">cp -r /opt/module/apache-doris-be-1.2.1-bin-x86_64/lib .</span><br><span class="line">cp -r /opt/module/apache-doris-be-1.2.1-bin-x86_64/bin .</span><br><span class="line">cp /opt/module/apache-doris-dependencies-1.2.1-bin-x86_64/java-udf-jar-with-dependencies.jar lib/</span><br><span class="line">sh bin.bak/stop_be.sh</span><br><span class="line">sh bin/start_be.sh --daemon</span><br><span class="line">ps aux | grep doris_be</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 升级 FE 节点</span></span><br><span class="line">cd /opt/module/apache-doris-fe</span><br><span class="line">rm -rf lib.bak &amp;&amp; mv lib lib.bak </span><br><span class="line">rm -rf bin.bak &amp;&amp; mv bin bin.bak</span><br><span class="line">cp -r /opt/module/apache-doris-1.2.1-hive_211-fe-x86_64/lib .</span><br><span class="line">cp -r /opt/module/apache-doris-fe-1.2.1-bin-x86_64/bin .</span><br><span class="line">sh bin.bak/stop_fe.sh</span><br><span class="line">sh bin/start_fe.sh --daemon</span><br><span class="line">ps aux | grep apache-doris-fe</span><br></pre></td></tr></table></figure>
<p>新加节点</p>
<p>sh /opt/module/apache-doris-fe/bin/start_fe.sh --helper cdh192-151.hadoop.xy:19010 --daemon</p>
<p><strong>正确的顺序</strong>：</p>
<ol>
<li>在系统注册：ALTER SYSTEM ADD FOLLOWER “follower_host:edit_log_port”;</li>
<li>启动新节点：./bin/start_fe.sh --helper leader_fe_host:edit_log_port --daemon</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>hk-blockin系统优化</title>
    <url>/2023/02/10/hk-blockin%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<p>群用到了那些组件？ 版本有什么要求？</p>
<p>trino386，这个是jdk11了，安装两个目录就行了， phoenix5， hive3.1.2, hadoop.3.1.3,spark2.4.8,spark3.2 flink1.12.7, flink和spark都想升级。 iceberg.0.13.2,</p>
<p>trino ui地址  <a href="http://cdh192-57:8089/ui/" target="_blank" rel="noopener">http://cdh192-57:8089/ui/</a>  用户 presto          jdbc 172.20.192.57  用户presto</p>
<p>软件都安装在/home/hadoop/bigdata/这个下面</p>
<p>[root@hadoop102 ~]$ xsync/home/hadoop/bigdata/hadoop/etc/hadoop/capacity-scheduler.xml</p>
<p>刷新yarn集群的队列</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;HADOOP_HOME&#125;&#x2F;bin&#x2F;yarn rmadmin -refreshQueues&#96; 或&#96;yarn rmadmin -refreshQueues</span><br></pre></td></tr></table></figure>
<p>hbase</p>
<p>cdh192-[234-236]</p>
<p>/opt/local/</p>
<p>tar -zxvf hbase-2.4.11-bin.tar.gz -C /opt/module/</p>
<p>mv /opt/module/hbase/lib/client-facing-thirdparty/slf4j-reload4j-1.7.33.jar /opt/module/hbase/lib/client-facing-thirdparty/slf4j-reload4j-1.7.33.jar.bak</p>
<p>echo usdp02 &gt; conf/backup-masters</p>
<p>phenix 部署</p>
<p><a href="https://app.yinxiang.com/fx/73f7b1c8-9c10-4b48-92a8-50afa5d3e9c8" target="_blank" rel="noopener">https://app.yinxiang.com/fx/73f7b1c8-9c10-4b48-92a8-50afa5d3e9c8</a></p>
<p><a href="http://zkCli.sh" target="_blank" rel="noopener">zkCli.sh</a> -server 127.0.0.1:2181</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">        资源计算方法：</span><br><span class="line">        DefaultResourseCalculator只会计算内存</span><br><span class="line">        DominantResourceCalculator 考虑内存和CPU</span><br><span class="line">  &lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<h2 id="phoenix-client">phoenix client</h2>
<p>172.20.192.151:/opt/module/phoenix-hbase-2.1-5.1.2/</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">phoenix-sqlline cdh192-150,cdh192-151,cdh192-152,cdh192-153,cdh192-154:2181</span><br><span class="line"></span><br><span class="line">&#x2F;opt&#x2F;module&#x2F;phoenix-hbase-2.1-5.1.2&#x2F;bin&#x2F;sqlline.py cdh192-150,cdh192-151,cdh192-152,cdh192-153,cdh192-154:2181</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>anaconda部署文档</title>
    <url>/2022/12/30/anaconda%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/</url>
    <content><![CDATA[<p>anaconda部署文档</p>
<p><a href="https://docs.anaconda.com/anaconda/install/linux/" target="_blank" rel="noopener">https://docs.anaconda.com/anaconda/install/linux/</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash .&#x2F;Anaconda3-2022.10-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<p>退出默认环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda config --set auto_activate_base False</span><br></pre></td></tr></table></figure>
<p>conda activate</p>
<p>conda deactivate #退出环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -czvf anaconda3.tar.gz anaconda3&#x2F;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u hdfs hadoop fs -put &#x2F;opt&#x2F;module&#x2F;anaconda3.tar.gz &#x2F;user&#x2F;yarn&#x2F;mapreduce&#x2F;</span><br></pre></td></tr></table></figure>
<p>anaconda已安装 在/opt/module/anaconda3/</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.pyspark.python&#x3D;python&#x2F;bin&#x2F;python3</span><br><span class="line">spark.pyspark.driver.python&#x3D;&#x2F;opt&#x2F;module&#x2F;anaconda3&#x2F;bin&#x2F;python3</span><br><span class="line">spark.yarn.dist.archives&#x3D;hdfs:&#x2F;&#x2F;ns1&#x2F;user&#x2F;yarn&#x2F;mapreduce&#x2F;anaconda3.zip#python</span><br></pre></td></tr></table></figure>
<p>ln -s /opt/module/anaconda3/bin/python3 python3</p>
]]></content>
  </entry>
  <entry>
    <title>解决freeipa证书过期</title>
    <url>/2022/12/09/%E8%A7%A3%E5%86%B3freeipa%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F/</url>
    <content><![CDATA[<h1>FreeIPA 服务器在重启后不会启动</h1>
<p>参考   <a href="https://redhatlinux.guru/2020/10/09/freeipa-server-will-not-start-after-reboot/" target="_blank" rel="noopener">https://redhatlinux.guru/2020/10/09/freeipa-server-will-not-start-after-reboot/</a></p>
<p><strong>原因 证书过期：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@freeipa ~]# ipa-getcert list </span><br><span class="line">Number of certificates and requests being tracked: 9.</span><br><span class="line">Request ID &#39;20190830074301&#39;:</span><br><span class="line">	status: CA_UNREACHABLE</span><br><span class="line">	ca-error: Server at https:&#x2F;&#x2F;freeipa.baofoo.cn&#x2F;ipa&#x2F;xml failed request, will retry: -504 (libcurl failed to execute the HTTP POST transaction, explaining:  Failed connect to freeipa.baofoo.cn:443; Connection refused).</span><br><span class="line">	stuck: no</span><br><span class="line">	key pair storage: type&#x3D;NSSDB,location&#x3D;&#39;&#x2F;etc&#x2F;dirsrv&#x2F;slapd-BAOFOO-CN&#39;,nickname&#x3D;&#39;Server-Cert&#39;,token&#x3D;&#39;NSS Certificate DB&#39;,pinfile&#x3D;&#39;&#x2F;etc&#x2F;dirsrv&#x2F;slapd-BAOFOO-CN&#x2F;pwdfile.txt&#39;</span><br><span class="line">	certificate: type&#x3D;NSSDB,location&#x3D;&#39;&#x2F;etc&#x2F;dirsrv&#x2F;slapd-BAOFOO-CN&#39;,nickname&#x3D;&#39;Server-Cert&#39;,token&#x3D;&#39;NSS Certificate DB&#39;</span><br><span class="line">	CA: IPA</span><br><span class="line">	issuer: CN&#x3D;Certificate Authority,O&#x3D;BAOFOO.CN</span><br><span class="line">	subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">	expires: 2021-08-30 07:43:01 UTC</span><br><span class="line">	dns: freeipa.baofoo.cn</span><br><span class="line">	principal name: ldap&#x2F;freeipa.baofoo.cn@BAOFOO.CN</span><br><span class="line">	key usage: digitalSignature,nonRepudiation,keyEncipherment,dataEncipherment</span><br><span class="line">	eku: id-kp-serverAuth,id-kp-clientAuth</span><br><span class="line">	pre-save command: </span><br><span class="line">	post-save command: &#x2F;usr&#x2F;libexec&#x2F;ipa&#x2F;certmonger&#x2F;restart_dirsrv BAOFOO-CN</span><br><span class="line">	track: yes</span><br><span class="line">	auto-renew: yes</span><br></pre></td></tr></table></figure>
<p>问题<br>
这更多是利基问题。但可能对其他人有用。我在家里有一个用于 DNS 的 FreeIPA 服务器设置。在简单的重新启动以向 VM 添加一些 RAM 后，服务器将无法启动。我收到如下错误。</p>
<p>IPA 服务器错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl status ipa</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">● ipa.service - Identity, Policy, Audit</span><br><span class="line">Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;ipa.service; enabled; vendor preset: disabled)</span><br><span class="line">Active: failed (Result: exit-code) since Fri 2020-10-09 14:57:15 EDT; 1s ago</span><br><span class="line">Process: 1110 ExecStart&#x3D;&#x2F;usr&#x2F;sbin&#x2F;ipactl start (code&#x3D;exited, status&#x3D;1&#x2F;FAILURE)</span><br><span class="line">Main PID: 1110 (code&#x3D;exited, status&#x3D;1&#x2F;FAILURE)</span><br><span class="line"></span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Aborting ipactl</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Starting Directory Service</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Starting krb5kdc Service</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Starting kadmin Service</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Starting named Service</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local ipactl[1110]: Starting httpd Service</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local systemd[1]: ipa.service: main process exited, code&#x3D;exited, status&#x3D;1&#x2F;FAILURE</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local systemd[1]: Failed to start Identity, Policy, Audit.</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local systemd[1]: Unit ipa.service entered failed state.</span><br><span class="line">Oct 09 14:57:15 ipasrv.home.local systemd[1]: ipa.service failed.</span><br></pre></td></tr></table></figure>
<h5 id="Apache-服务错误">Apache 服务错误</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl status httpd -l</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">● httpd.service - The Apache HTTP Server</span><br><span class="line">Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;httpd.service; disabled; vendor preset: disabled)</span><br><span class="line">Drop-In: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;httpd.service.d</span><br><span class="line">└─ipa.conf</span><br><span class="line">Active: failed (Result: exit-code) since Fri 2020-10-09 14:57:44 EDT; 9s ago</span><br><span class="line">Docs: man:httpd(8)</span><br><span class="line">man:apachectl(8)</span><br><span class="line">Process: 1532 ExecStart&#x3D;&#x2F;usr&#x2F;sbin&#x2F;httpd $OPTIONS -DFOREGROUND (code&#x3D;exited, status&#x3D;1&#x2F;FAILURE)</span><br><span class="line">Process: 1529 ExecStartPre&#x3D;&#x2F;usr&#x2F;libexec&#x2F;ipa&#x2F;ipa-httpd-kdcproxy (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line">Main PID: 1532 (code&#x3D;exited, status&#x3D;1&#x2F;FAILURE)</span><br><span class="line"></span><br><span class="line">Oct 09 14:57:42 ipasrv.home.local systemd[1]: Starting The Apache HTTP Server...</span><br><span class="line">Oct 09 14:57:43 ipasrv.home.local ipa-httpd-kdcproxy[1529]: ipa: WARNING: Unable to connect to dirsrv: cannot connect to &amp;#039;ldapi:&#x2F;&#x2F;%2fvar%2frun%2fslapd-HOME-LOCAL.socket&amp;#039;:</span><br><span class="line">Oct 09 14:57:43 ipasrv.home.local ipa-httpd-kdcproxy[1529]: ipa-httpd-kdcproxy: WARNING  Unable to connect to dirsrv: cannot connect to &amp;#039;ldapi:&#x2F;&#x2F;%2fvar%2frun%2fslapd-HOME-LOCAL.socket&amp;#039;:</span><br><span class="line">Oct 09 14:57:43 ipasrv.home.local ipa-httpd-kdcproxy[1529]: ipa: WARNING: Disabling KDC proxy</span><br><span class="line">Oct 09 14:57:43 ipasrv.home.local ipa-httpd-kdcproxy[1529]: ipa-httpd-kdcproxy: WARNING  Disabling KDC proxy</span><br><span class="line">Oct 09 14:57:44 ipasrv.home.local systemd[1]: httpd.service: main process exited, code&#x3D;exited, status&#x3D;1&#x2F;FAILURE</span><br><span class="line">Oct 09 14:57:44 ipasrv.home.local systemd[1]: Failed to start The Apache HTTP Server.</span><br><span class="line">Oct 09 14:57:44 ipasrv.home.local systemd[1]: Unit httpd.service entered failed state.</span><br><span class="line">Oct 09 14:57:44 ipasrv.home.local systemd[1]: httpd.service failed.</span><br></pre></td></tr></table></figure>
<p>解析度<br>
不会用这个绕过灌木丛。底线是我的证书已过期。下面是解决它的步骤。</p>
<h3 id="1-–-使用忽略失败服务的选项启动-IPA-服务器。">1 – 使用忽略失败服务的选项启动 IPA 服务器。</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipactl start --ignore-service-failure</span><br></pre></td></tr></table></figure>
<p>示例输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Existing service file detected!</span><br><span class="line">Assuming stale, cleaning and proceeding</span><br><span class="line">Starting Directory Service</span><br><span class="line">Starting krb5kdc Service</span><br><span class="line">Starting kadmin Service</span><br><span class="line">Starting named Service</span><br><span class="line">Starting httpd Service</span><br><span class="line">Failed to start httpd Service</span><br><span class="line">Forced start, ignoring httpd Service, continuing normal operation</span><br><span class="line">Starting ipa-custodia Service</span><br><span class="line">Starting ntpd Service</span><br><span class="line">Starting pki-tomcatd Service</span><br></pre></td></tr></table></figure>
<h2 id="2-–-接下来运行ipa-cert-fix命令来更新过期的证书。">2 – 接下来运行ipa-cert-fix命令来更新过期的证书。</h2>
<p>ipa-cert-fix</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">                          WARNING</span><br><span class="line"></span><br><span class="line">ipa-cert-fix is intended for recovery when expired certificates</span><br><span class="line">prevent the normal operation of IPA.  It should ONLY be used</span><br><span class="line">in such scenarios, and backup of the system, especially certificates</span><br><span class="line">and keys, is STRONGLY RECOMMENDED.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The following certificates will be renewed: </span><br><span class="line"></span><br><span class="line">Dogtag sslserver certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  3</span><br><span class="line">  Expires: 2021-08-19 07:42:09</span><br><span class="line"></span><br><span class="line">Dogtag subsystem certificate:</span><br><span class="line">  Subject: CN&#x3D;CA Subsystem,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  4</span><br><span class="line">  Expires: 2021-08-19 07:42:09</span><br><span class="line"></span><br><span class="line">Dogtag ca_ocsp_signing certificate:</span><br><span class="line">  Subject: CN&#x3D;OCSP Subsystem,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  2</span><br><span class="line">  Expires: 2021-08-19 07:42:09</span><br><span class="line"></span><br><span class="line">Dogtag ca_audit_signing certificate:</span><br><span class="line">  Subject: CN&#x3D;CA Audit,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  5</span><br><span class="line">  Expires: 2021-08-19 07:42:09</span><br><span class="line"></span><br><span class="line">IPA IPA RA certificate:</span><br><span class="line">  Subject: CN&#x3D;IPA RA,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  7</span><br><span class="line">  Expires: 2021-08-19 07:42:30</span><br><span class="line"></span><br><span class="line">IPA Apache HTTPS certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  9</span><br><span class="line">  Expires: 2021-08-30 07:43:31</span><br><span class="line"></span><br><span class="line">IPA LDAP certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  8</span><br><span class="line">  Expires: 2021-08-30 07:43:01</span><br><span class="line"></span><br><span class="line">IPA KDC certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  10</span><br><span class="line">  Expires: 2021-08-30 07:43:38</span><br><span class="line"></span><br><span class="line">Enter &quot;yes&quot; to proceed: yes</span><br><span class="line">Proceeding.</span><br><span class="line">Renewed Dogtag sslserver certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  11</span><br><span class="line">  Expires: 2023-11-30 01:22:29</span><br><span class="line"></span><br><span class="line">Renewed Dogtag subsystem certificate:</span><br><span class="line">  Subject: CN&#x3D;CA Subsystem,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  12</span><br><span class="line">  Expires: 2023-11-30 01:22:30</span><br><span class="line"></span><br><span class="line">Renewed Dogtag ca_ocsp_signing certificate:</span><br><span class="line">  Subject: CN&#x3D;OCSP Subsystem,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  13</span><br><span class="line">  Expires: 2023-11-30 01:22:31</span><br><span class="line"></span><br><span class="line">Renewed Dogtag ca_audit_signing certificate:</span><br><span class="line">  Subject: CN&#x3D;CA Audit,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  14</span><br><span class="line">  Expires: 2023-11-30 01:22:31</span><br><span class="line"></span><br><span class="line">Renewed IPA IPA RA certificate:</span><br><span class="line">  Subject: CN&#x3D;IPA RA,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  15</span><br><span class="line">  Expires: 2023-11-30 01:22:32</span><br><span class="line"></span><br><span class="line">Renewed IPA Apache HTTPS certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  16</span><br><span class="line">  Expires: 2023-12-11 01:22:32</span><br><span class="line"></span><br><span class="line">Renewed IPA LDAP certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  17</span><br><span class="line">  Expires: 2023-12-11 01:22:32</span><br><span class="line"></span><br><span class="line">Renewed IPA KDC certificate:</span><br><span class="line">  Subject: CN&#x3D;freeipa.baofoo.cn,O&#x3D;BAOFOO.CN</span><br><span class="line">  Serial:  18</span><br><span class="line">  Expires: 2023-12-11 01:22:33</span><br><span class="line"></span><br><span class="line">Becoming renewal master.</span><br><span class="line">The ipa-cert-fix command was successful</span><br><span class="line">[root@freeipa ~]# ipactl restart</span><br></pre></td></tr></table></figure>
<h3 id="3-–-更新证书后，重新启动-IPA-服务器，">3 – 更新证书后，重新启动 IPA 服务器，</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipactl restart</span><br></pre></td></tr></table></figure>
<p>示例输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Restarting Directory Service</span><br><span class="line">Restarting krb5kdc Service</span><br><span class="line">Restarting kadmin Service</span><br><span class="line">Restarting named Service</span><br><span class="line">Restarting httpd Service</span><br><span class="line">Restarting ipa-custodia Service</span><br><span class="line">Restarting ntpd Service</span><br><span class="line">Restarting pki-tomcatd Service</span><br><span class="line">Restarting ipa-otpd Service</span><br><span class="line">Restarting ipa-dnskeysyncd Service</span><br><span class="line">ipa: INFO: The ipactl command was successful</span><br></pre></td></tr></table></figure>
<h3 id="4-–-最后使用ipactl-status命令验证IPA服务器启动。">4 – 最后使用ipactl status命令验证IPA服务器启动。</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipactl status</span><br></pre></td></tr></table></figure>
<p>示例输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Directory Service: RUNNING</span><br><span class="line">krb5kdc Service: RUNNING</span><br><span class="line">kadmin Service: RUNNING</span><br><span class="line">named Service: RUNNING</span><br><span class="line">httpd Service: RUNNING</span><br><span class="line">ipa-custodia Service: RUNNING</span><br><span class="line">ntpd Service: RUNNING</span><br><span class="line">pki-tomcatd Service: RUNNING</span><br><span class="line">ipa-otpd Service: RUNNING</span><br><span class="line">ipa-dnskeysyncd Service: RUNNING</span><br><span class="line">ipa: INFO: The ipactl command was successful</span><br></pre></td></tr></table></figure>
<h1>Upgrade   版本升级</h1>
<p>[root@freeipa ipa]# rpm -aq ipa-server<br>
ipa-server-4.6.5-11.el7.centos.x86_64</p>
<p>ipa-server-upgrade</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipa-getcert list  #查看全部证书</span><br><span class="line"></span><br><span class="line">ipa-cacert-manage renew  #更新证书     会在到期日期前 28 天自动更新以下证书</span><br><span class="line"></span><br><span class="line">手工更新证书</span><br><span class="line">ipa-getcert resubmit -i REQUEST_ID</span><br><span class="line"></span><br><span class="line">ipa-getcert resubmit -i cert_nickname   #cert_nickname自行替换，可由ipa-getcert list 命令获取到，结合自带的certmonger服务可以更新证书过期时间</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>cdh 安装Pheonix5.12</title>
    <url>/2022/11/14/cdh%20%E5%AE%89%E8%A3%85Pheonix5.12/</url>
    <content><![CDATA[<h1>cdh 安装Pheonix5.12</h1>
<p>Phoenix5.12-之安装部署（Hbase-2.1.1）<br>
Phoenix与Hbase的版本兼容是很严格的，我们需要使用Phoenix去适配Hbase，有很严格的版本标准，具体的版本匹配如下图。</p>
<p>具体版本的选择请查阅官网下载页面：<a href="https://phoenix.apache.org/download.html" target="_blank" rel="noopener">https://phoenix.apache.org/download.html</a></p>
<h1>1 解压</h1>
<p>tar -zxvf phoenix-hbase-2.1-5.1.2-bin.tar.gz -C /opt/module/</p>
<h1>2 重新命名</h1>
<p>mv phoenix-hbase-2.1-5.1.2-bin/ phoenix-hbase-2.1-5.1.2</p>
<h1>3 将phoenix-server-hbase-2.1-5.1.2.jar拷贝到Hbase的Master和Slave节点的lib目录下</h1>
<p>cp phoenix-server-hbase-2.1-5.1.2.jar …/hbase-2.1.1/lib/</p>
<h1>4 配置环境变量 hbase和phoenix 在/etc/profile</h1>
<p>export HBASE_HOME=/opt/module/hbase-2.1.1<br>
export PATH=$PATH:$HBASE_HOME/bin</p>
<h1>PHOENIX_HOME</h1>
<p>export PHOENIX_HOME=/opt/module/phoenix-hbase-2.1-5.1.2<br>
export PATH=$PATH:$PHOENIX_HOME/bin<br>
export PHOENIX_CLASSPATH=$PHOENIX_HOME</p>
<h1>source</h1>
<p>source /etc/profile</p>
]]></content>
  </entry>
  <entry>
    <title>Dbeaver连接phoenix</title>
    <url>/2022/11/12/Dbeaver%E8%BF%9E%E6%8E%A5phoenix/</url>
    <content><![CDATA[<h1>Dbeaver连接phoenix</h1>
<p>1.切换到/opt/cloudera/parcels/PHOENIX-5.0.0-cdh6.2.0.p0.1308267/lib/phoenix，下载</p>
<p>phoenix-5.0.0-cdh6.2.0-client.jar这个jar包</p>
<p>2.下载客户端配置</p>
<p>里面有个phoenix-5.0.0-cdh6.2.0-client.jar下载下来</p>
<p>3.执行下面命令:</p>
<p>jar -uf phoenix-5.0.0-cdh6.2.0-client.jar hbase-site.xml 将hbase-site.xml添加到phoenix-5.0.0-cdh6.2.0-client.jar里</p>
<p>4.打开Dbeaver，创建新链接-&gt;编辑驱动-&gt;添加文件(刚才的jar包)-&gt;找到类-&gt;确定</p>
<p>5.填写连接信息，点击测试连接，成功！</p>
]]></content>
  </entry>
  <entry>
    <title>Docker设置容器开机自启</title>
    <url>/2022/11/07/Docker%E8%AE%BE%E7%BD%AE%E5%AE%B9%E5%99%A8%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF/</url>
    <content><![CDATA[<p><strong>Docker设置容器开机自启</strong></p>
<p>设置docker开机自启</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable docker</span><br></pre></td></tr></table></figure>
<p>设置docker容器开机自启<br>
创建docker容器时设置开机自启<br>
#在使用docker run启动容器时，使用–restart参数来设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run --restart&#x3D;always --name imagesName</span><br></pre></td></tr></table></figure>
<p>修改已创建的docker容器开机自启<br>
#如果创建时未指定 --restart=always ,可通过update 命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker update --restart&#x3D;always imagesName</span><br></pre></td></tr></table></figure>
<p>参数说明<br>
  --restart具体参数值详细信息：</p>
<p>no - 容器退出时，不重启容器；<br>
on-failure - 只有在非0状态退出时才从新启动容器；<br>
always - 无论退出状态是如何，都重启容器；</p>
<p>还可以在使用on - failure策略时，指定Docker将尝试重新启动容器的最大次数。默认情况下，Docker将尝试永远重新启动容器。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run --restart&#x3D;on-failure:10 imagesName</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>jpsall xsync命令脚本</title>
    <url>/2022/10/28/jpsall%20xsync%E5%91%BD%E4%BB%A4%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<p>vim  /usr/bin/xsync</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1. 判断参数个数</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line"> echo Not Enough Arguement!</span><br><span class="line"> exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2. 遍历集群所有机器</span></span><br><span class="line">for host in usdp01 usdp02 usdp03</span><br><span class="line">do</span><br><span class="line"> echo ==================== $host ====================</span><br><span class="line"><span class="meta"> #</span><span class="bash">3. 遍历所有目录，挨个发送</span></span><br><span class="line"> for file in $@</span><br><span class="line"> do</span><br><span class="line"><span class="meta"> #</span><span class="bash">4. 判断文件是否存在</span></span><br><span class="line"> if [ -e $file ]</span><br><span class="line"> then</span><br><span class="line"><span class="meta"> #</span><span class="bash">5. 获取父目录</span></span><br><span class="line"> pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line"><span class="meta"> #</span><span class="bash">6. 获取当前文件的名称</span></span><br><span class="line"> fname=$(basename $file)</span><br><span class="line"> ssh $host "mkdir -p $pdir"</span><br><span class="line"> rsync -av $pdir/$fname $host:$pdir</span><br><span class="line"> else</span><br><span class="line"> echo $file does not exists!</span><br><span class="line"> fi</span><br><span class="line"> done</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p><strong><a href="http://jpsall.sh" target="_blank" rel="noopener">jpsall.sh</a> 脚本：</strong></p>
<p>vim /usr/bin/jpsall</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行jps命令查询每台服务器上的节点状态</span></span><br><span class="line">echo ======================集群节点状态====================</span><br><span class="line"></span><br><span class="line">for i in usdp01 usdp02 usdp03</span><br><span class="line">do</span><br><span class="line">        echo ====================== $i ====================</span><br><span class="line">        ssh $i "/opt/module/jdk1.8.0_202/bin/jps"</span><br><span class="line">done</span><br><span class="line">echo ======================执行完毕====================</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>StartRocks补数据</title>
    <url>/2022/10/10/StartRocks%E8%A1%A5%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h1>StartRocks补数据</h1>
<h2 id="创建catalog">创建catalog</h2>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">CATALOG</span> hive_catalog0 </span><br><span class="line">PROPERTIES(</span><br><span class="line">  <span class="string">"type"</span>=<span class="string">"hive"</span>, </span><br><span class="line">  <span class="string">"hive.metastore.uris"</span>=<span class="string">"thrift://172.20.85.29:9083,thrift://172.20.85.29:9083"</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SHOW</span> RESOURCES;</span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span> <span class="keyword">from</span> hive_catalog0</span><br></pre></td></tr></table></figure>
<h2 id="创建parquet文件格式的临时表">创建parquet文件格式的临时表</h2>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> PAY_TRADECENTER.T_TC_BASE_P </span><br><span class="line"> <span class="keyword">STORED</span> <span class="keyword">AS</span> parquet  </span><br><span class="line"> <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">SELECT</span>  *  <span class="keyword">from</span>  PAY_TRADECENTER.T_TC_BASE <span class="keyword">where</span> pk_day &gt;= <span class="string">'2022-07-16'</span> <span class="keyword">and</span>  pk_day &lt; <span class="string">'2022-08-17'</span></span><br></pre></td></tr></table></figure>
<h2 id="导入数据">导入数据</h2>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> query_timeout = <span class="number">259200</span>;</span><br><span class="line"><span class="keyword">set</span> exec_mem_limit=<span class="number">21474836480</span>;</span><br><span class="line"><span class="keyword">SET</span> enable_insert_strict = <span class="literal">false</span>; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span>  <span class="keyword">into</span> PAY_TRADECENTER.<span class="string">`T_TC_BASE`</span> (CREATED_AT ,<span class="keyword">ID</span>, TRADE_NO, OUT_TRADE_NO, PRODUCT_TYPE, SUB_PRODUCT_TYPE, PRODUCT_CHANNEL, TRADE_CHANNEL, REQUEST_DATE, REQUEST_SYSTEM, TRADE_AMT, SERVICE_FEE, CCY, MERCHANT_NO, MERCHANT_TRADE_NO, ORIG_TRADE_NO, TRADE_FINISH_DATE, MEMO, EXTEND, SUBJECT, TRADE_STATUS, TRADE_FAIL_STATUS, TRADE_TYPE, REFUND_AMT, RESULT_CODE, RESULT_DESC, TRACE_LOG_ID, SETTLE_ACCOUNT_NO, SETTLE_ACCOUNT_TYPE, FEE_ACCOUNT_NO, FEE_ACCOUNT_TYPE, SUB_MERCHANT_ID, AGENT_NO, STORE_ID, EXTEND1, EXTEND2, EXTEND3, CREATED_BY, UPDATED_AT, UPDATED_BY, USER_ID, EXTEND4, EXTEND5, EXTEND6, TOTAL_ORDER_AMT, UNION_INFO)</span><br><span class="line"><span class="keyword">SELECT</span> CREATED_AT ,<span class="keyword">ID</span>, TRADE_NO, OUT_TRADE_NO, PRODUCT_TYPE, SUB_PRODUCT_TYPE, PRODUCT_CHANNEL, TRADE_CHANNEL, REQUEST_DATE, REQUEST_SYSTEM, TRADE_AMT, SERVICE_FEE, CCY, MERCHANT_NO, MERCHANT_TRADE_NO, ORIG_TRADE_NO, TRADE_FINISH_DATE, MEMO, EXTEND, SUBJECT, TRADE_STATUS, TRADE_FAIL_STATUS, TRADE_TYPE, REFUND_AMT, RESULT_CODE, RESULT_DESC, TRACE_LOG_ID, SETTLE_ACCOUNT_NO, SETTLE_ACCOUNT_TYPE, FEE_ACCOUNT_NO, FEE_ACCOUNT_TYPE, SUB_MERCHANT_ID, AGENT_NO, STORE_ID, EXTEND1, EXTEND2, EXTEND3, CREATED_BY, UPDATED_AT, UPDATED_BY, USER_ID, EXTEND4, EXTEND5, EXTEND6, TOTAL_ORDER_AMT, UNION_INFO</span><br><span class="line"><span class="keyword">FROM</span>  hive_catalog0.PAY_TRADECENTER.T_TC_BASE_P</span><br></pre></td></tr></table></figure>
<h2 id="删除临时表">删除临时表</h2>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> PAY_TRADECENTER.T_TC_BASE_P</span><br></pre></td></tr></table></figure>
<p>PAY_AGGREGATE        T_ORDER_EXTERNAL        CREATED_AT<br>
PAY_BENEFIT        T_BIZ_CMD        CREATED_AT<br>
PAY_BENEFIT        T_ORDER_BASE        CREATED_AT<br>
PAY_BENEFIT        T_ORDER_BASE_BENEFIT        CREATED_AT<br>
<s>PAY_GATEWAY        T_ACQ_AGGREGATE        CREATED_AT</s><br>
<s>PAY_GATEWAY        T_ACQ_AGGREGATE_DYNAMIC        CREATED_AT</s><br>
<s>PAY_GATEWAY        T_ACQ_NOTIFY        CREATED_AT</s><br>
<s>PAY_GATEWAY        T_ACQ_REFUND        CREATED_AT</s><br>
<s>PAY_GATEWAY        T_RISK_INFO        CREATED_AT</s><br>
PAY_SETTLE        T_SETTLE_ORDER_ACQUIRING        TRADE_FINISH_DATE<br>
PAY_SETTLE        T_SETTLE_ORDER_RE·FUND        TRADE_FINISH_DATE<br>
PAY_TRADECENTER        T_BIZ_CMD        CREATED_AT<br>
PAY_TRADECENTER        T_TC_ACCOUNT_TRANS        CREATED_AT<br>
<s>PAY_TRADECENTER        T_TC_AGGREGATE_TRANS        CREATED_AT</s><br>
<s>PAY_TRADECENTER        T_TC_BASE        CREATED_AT</s><br>
<s>PAY_TRADECENTER        T_TC_CHANNEL_MSG        CREATED_AT</s><br>
<s>PAY_TRADECENTER        T_TC_CHANNEL_TRANS        CREATED_AT</s><br>
PAY_VERIFY        T_VERIFY_BANK_TRANS        VERIFY_DATE<br>
PAY_VERIFY        T_VERIFY_CHANNEL_COST        VERIFY_DATE<br>
PAY_VERIFY        T_VERIFY_CHANNEL_TRANS        VERIFY_DATE<br>
PAY_VERIFY        T_VERIFY_RESULT        VERIFY_DATE</p>
<p>– PAY_GATEWAY数据库<br>
PAY_GATEWAY.T_ACQ_AGGREGATE_DYNAMIC      保留一个月<br>
PAY_GATEWAY.T_ACQ_AGGREGATE          保留一个月<br>
PAY_GATEWAY.T_ACQ_NOTIFY            保留一个月<br>
PAY_GATEWAY.T_ACQ_REFUND            保留一个月<br>
PAY_GATEWAY.T_RISK_INFO            保留一个月<br>
PAY_GATEWAY.T_BIZ_CMD             暂不归档</p>
<p>– PAY_TRADECENTER数据库<br>
PAY_TRADECENTER.T_BIZ_CMD           保留一个月<br>
PAY_TRADECENTER.T_TC_ACCOUNT_TRANS       保留一个月<br>
PAY_TRADECENTER.T_TC_AGGREGATE_TRANS      保留一个月<br>
PAY_TRADECENTER.T_TC_BASE           保留一个月<br>
PAY_TRADECENTER.T_TC_CHANNEL_MSG        保留一个月<br>
PAY_TRADECENTER.T_TC_CHANNEL_TRANS       保留一个月</p>
<p>– PAY_AGGREGATE<br>
PAY_AGGREGATE.T_ORDER_EXTERNAL         保留四个月</p>
<p>– PAY_BENEFIT<br>
PAY_BENEFIT.T_ORDER_BASE            保留四个月<br>
PAY_BENEFIT.T_BIZ_CMD             保留四个月<br>
PAY_BENEFIT.T_ORDER_BASE_BENEFIT        保留四个月</p>
<p>---- PAY_VERIFY<br>
PAY_VERIFY.T_VERIFY_RESULT           保留四个月<br>
PAY_VERIFY.T_VERIFY_CHANNEL_TRANS       保留四个月<br>
PAY_VERIFY.T_VERIFY_CHANNEL_COST        保留四个月<br>
PAY_VERIFY.T_VERIFY_BANK_TRANS         保留四个月</p>
<p>– PAY_SETTLE<br>
PAY_SETTLE.T_SETTLE_ORDER_ACQUIRING      保留四个月<br>
PAY_SETTLE.T_SETTLE_ORDER_WITHDRAW       保留四个月<br>
PAY_SETTLE.T_SETTLE_ORDER_REFUND        保留四个月<br>
PAY_SETTLE.T_SETTLE_ORDER_CANCEL        保留四个月</p>
]]></content>
  </entry>
  <entry>
    <title>取消掉远程桌面mstsc顶部（侧面）连接栏</title>
    <url>/2022/08/18/%E5%8F%96%E6%B6%88%E6%8E%89%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2mstsc%E9%A1%B6%E9%83%A8%EF%BC%88%E4%BE%A7%E9%9D%A2%EF%BC%89%E8%BF%9E%E6%8E%A5%E6%A0%8F/</url>
    <content><![CDATA[<p>取消掉远程桌面mstsc顶部（侧面）连接栏</p>
<p>在进行mstsc远程桌面连接电脑或者虚拟机的时候，总是会出现一个连接栏。虽然点左边的图钉可以自动隐藏，但是每次鼠标滑到上面的时候，还是会冒出来，这个就有点闹心了。</p>
<p>查了下相关资料，解决了，特写下相关教程；</p>
<p>关闭步骤：在进行远程连接的时候，取消选择 显示-&gt;全屏显示时显示连接栏(B)。</p>
<p>如果要退出远程连接的话，Ctrl+ Alt+ Home,在全屏模式下，激活连接栏，然后叉掉就好了。</p>
<p>笔记本可能没有HOME键</p>
<p>HOME键和左方向键共用，打开HOME的方法是FN + HOME<br>
所有笔记本唤醒连接栏的方法就是<br>
Ctrl+ Alt+ FN + Home</p>
]]></content>
  </entry>
  <entry>
    <title>解决root用户对HDFS文件系统操作权限不够问题</title>
    <url>/2022/07/12/%E8%A7%A3%E5%86%B3root%E7%94%A8%E6%88%B7%E5%AF%B9HDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%93%8D%E4%BD%9C%E6%9D%83%E9%99%90%E4%B8%8D%E5%A4%9F%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1>解决root用户对HDFS文件系统操作权限不够问题</h1>
<p>HDFS文件系统的目录基本都属于supergroup用户组，所以就把用户添加到该用户组，即可解决很多权限问题。</p>
<p>1、<strong>在namenode节点</strong>的Linux执行如下命令增加supergroup</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">groupadd supergroup</span><br></pre></td></tr></table></figure>
<p>2、如将用户root增加到supergroup中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">usermod -a -G supergroup root</span><br></pre></td></tr></table></figure>
<p>3、同步系统的权限信息到HDFS文件系统</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u hdfs hdfs dfsadmin -refreshUserToGroupsMappings</span><br></pre></td></tr></table></figure>
<p>4、查看属于supergroup用户组的用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep &#39;supergroup:&#39; &#x2F;etc&#x2F;group</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Docker配置代理服务器</title>
    <url>/2022/05/30/Docker%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    <content><![CDATA[<h1>Docker配置代理服务器</h1>
<h2 id="背景环境说明">背景环境说明</h2>
<p>Docker环境安装在内网，只有一台公共的代理服务器能够上网，<br>
一般的docker pull会无法下载镜像。</p>
<p>新配置了代理服务器squid。（搭建squid代理服务器这里不讲，参考另一篇文章。不建议使用nginx坐正向代理，处理不了https。）<br>
操作系统： Redhat 7.5 X86<br>
内核: 3.10.0-862.el7.x86_64/docker 18.09<br>
代理服务器： 10.1.1.1:10000</p>
<h2 id="修改DockerService文件">修改DockerService文件</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service</span><br></pre></td></tr></table></figure>
<p>在[Service]下面添加：</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">[<span class="meta">Service</span>]</span><br><span class="line">Environment=<span class="string">"HTTP_PROXY=http://10.1.1.1:10000"</span></span><br><span class="line">Environment=<span class="string">"HTTPS_PROXY=https://proxy.example.com:443"</span></span><br></pre></td></tr></table></figure>
<h2 id="重启Docker服务">重启Docker服务</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl rstart docker </span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure>
<h2 id="检查">检查</h2>
<p>docker info,可以看到http proxy设置成功。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Debug</span> Mode (client): <span class="literal">false</span></span><br><span class="line"><span class="built_in">Debug</span> Mode (server): <span class="literal">false</span></span><br><span class="line">HTTP Proxy: http:<span class="comment">//10.1.1.1:10000</span></span><br><span class="line">Registry: https:<span class="comment">//index.docker.io/v1/</span></span><br><span class="line">Labels:</span><br><span class="line">Experimental: <span class="literal">false</span></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Linux 防止rm误删文件，将rm配置成mv到.trash</title>
    <url>/2022/05/17/Linux%20%E9%98%B2%E6%AD%A2rm%E8%AF%AF%E5%88%A0%E6%96%87%E4%BB%B6%EF%BC%8C%E5%B0%86rm%E9%85%8D%E7%BD%AE%E6%88%90mv%E5%88%B0.trash/</url>
    <content><![CDATA[<h1>Linux 防止rm误删文件，将rm配置成mv到.trash</h1>
<p>今天在服务器上操作想删除文件来着，结果一直提示我mv: 正在访问&quot;/data0/.trash/&quot;: 没有那个文件或目录，原来是为了防止误删把rm命令改写了，这样删错了后果就不会很严重</p>
<p>操作方法就是在~下的.bashrc或者.bash_profile文件加入如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p ~&#x2F;.trash</span><br><span class="line">alias rm&#x3D;trash  </span><br><span class="line">alias r&#x3D;trash  </span><br><span class="line">alias rl&#x3D;&#39;ls ~&#x2F;.trash&#39;</span><br><span class="line">alias ur&#x3D;undelfile</span><br><span class="line">undelfile()</span><br><span class="line">&#123;</span><br><span class="line">  mv -i ~&#x2F;.trash&#x2F;$@ .&#x2F;</span><br><span class="line">&#125;</span><br><span class="line">trash()</span><br><span class="line">&#123;</span><br><span class="line">  mv $@ ~&#x2F;.trash&#x2F;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样删除的文件会出现在家目录下的.trash文件中。</p>
<p>那么问题来了，如何删除.trash文件中的文件呢</p>
<p>继续在之前的.bashrc中添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cleartrash()</span><br><span class="line">&#123;</span><br><span class="line">    read -p &quot;clear sure?[n]&quot; confirm</span><br><span class="line">    [ $confirm &#x3D;&#x3D; &#39;y&#39; ] || [ $confirm &#x3D;&#x3D; &#39;Y&#39; ]  &amp;&amp; &#x2F;usr&#x2F;bin&#x2F;rm -rf ~&#x2F;.trash&#x2F;*</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>清理回收站直接使用$cleartrash即可</p>
]]></content>
  </entry>
  <entry>
    <title>centos7 简单开启代理上网</title>
    <url>/2022/05/10/centos7%20%E7%AE%80%E5%8D%95%E5%BC%80%E5%90%AF%E4%BB%A3%E7%90%86%E4%B8%8A%E7%BD%91/</url>
    <content><![CDATA[<p>centos7 简单开启代理上网</p>
<p><strong>服务端安装</strong>： 在可以上网的机器安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install squid -y</span><br><span class="line">systemctl restart squid</span><br></pre></td></tr></table></figure>
<p><strong>客户端安装</strong>：</p>
<p>yum  配置代理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;proxy&#x3D;http:&#x2F;&#x2F;IP:3128&#x2F;&quot; &gt;&gt; &#x2F;etc&#x2F;yum.conf</span><br></pre></td></tr></table></figure>
<p>系统全局代理（我测试下来，yum  不能用 ）</p>
<p>vi /etc/profile</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># export all_proxy&#x3D;http:&#x2F;&#x2F;IP:3128  只导入这个 ，测试下来wget不能用</span><br><span class="line">export http_proxy&#x3D;http:&#x2F;&#x2F;IP:3128</span><br><span class="line">export https_proxy&#x3D;http:&#x2F;&#x2F;IP:3128</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">测试代理是否生效</span><br><span class="line">curl http:&#x2F;&#x2F;myip.ipip.net</span><br><span class="line">curl https:&#x2F;&#x2F;myip.ipip.net&#x2F;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Python一秒钟搭建文件分享网页</title>
    <url>/2022/04/19/Python%E4%B8%80%E7%A7%92%E9%92%9F%E6%90%AD%E5%BB%BA%E6%96%87%E4%BB%B6%E5%88%86%E4%BA%AB%E7%BD%91%E9%A1%B5/</url>
    <content><![CDATA[<h1>Python一秒钟搭建文件分享网页</h1>
<p>Python的库是十分丰富的。在局域网内分享文件时，Python的http.server可以创建一个非常基本的Web服务器，相对于当前目录提供文件。http.server模块定义用于实现 HTTP 服务器（Web 服务器）的类。</p>
<h4 id="一、运行简单网页">一、运行简单网页</h4>
<p>先来试验一下，在cmd运行：python -m http.server，如果时linux平台的化，注意python版本3 。在浏览器输入IP地址:8000，即可获得的当前目录下的所有文件列表，以提供下载。需要注意的是防火墙要关闭或添加规则。</p>
<p>在局域网内暂时分享文件的话是十分方便的，比U盘拷贝省心很多。</p>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/image-20220419170008535.png" alt="image-20220419170008535"></p>
<p>查看官方文档：http请求映射到目录，检查目录中是否有index.html或index.htm文件（按顺序）。如果有，文件的内容将返回：否则，目录列表将通过调用list_directory()方法生成，此方法使用os.listdir()扫描目录。</p>
<h4 id="二、http-server具体使用方法">二、http.server具体使用方法</h4>
<p>使用 python -m http.server --help 再来看一下具体的使用方法。</p>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2022/04/image-20220419173412287.png" alt="image-20220419173412287"></p>
<p>port指定端口号，而不是默认的8000。</p>
<p>–bind ADDRESS, -b ADDRESS此方式是针对多网卡设备，绑定特定的网卡可以访问，而不是全部网卡。</p>
<p>–directory DIRECTORY, -d DIRECTORY指定特定的目录，而不是当前目录。</p>
<p>下面的命令指定E:\download目录并通过端口8888运行http.server ，所有网卡组成的局域网IP地址都可以访问。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m http.server --directory E:\download 8888</span><br></pre></td></tr></table></figure>
<p>下面的命令指定E:\download目录并通过端口8888运行http.server ，特定网卡的局域网IP地址可以访问。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m http.server --directory E:\download --bind 192.168.15.219 8888</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>typora + hexo博客中插入图片</title>
    <url>/2022/04/19/typora%20+%20hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<p>在使用了hexo搭建了博客后，最大的问题便是如何使用一款markdown工具来编辑博客了，我采取的就是Typora，这工具免费简单易用没广告，而且把图片保存到本地还是很方便的，因此大家只要稍微了解点markdown语法就可以上手使用了。</p>
<p>关于图片和图片路径的设置，有以下教程。</p>
<h2 id="typora-自动上传图片的教程">typora 自动上传图片的教程</h2>
<p><a href="https://connor-sun.github.io/posts/38835.html" target="_blank" rel="noopener">https://connor-sun.github.io/posts/38835.html</a></p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"picBed"</span>: &#123;</span><br><span class="line">    <span class="attr">"uploader"</span>: <span class="string">"aliyun"</span>,</span><br><span class="line">    <span class="attr">"aliyun"</span>: &#123;</span><br><span class="line">      <span class="attr">"accessKeyId"</span>: <span class="string">"LvX8msYp6t1UQv3h"</span>,</span><br><span class="line">      <span class="attr">"accessKeySecret"</span>: <span class="string">"BOmXyUnZ2ky4DJAIfRJOfrySHGwohV"</span>,</span><br><span class="line">      <span class="attr">"bucket"</span>: <span class="string">"ask3"</span>,</span><br><span class="line">      <span class="attr">"area"</span>: <span class="string">"oss-cn-hangzhou"</span>,</span><br><span class="line">      <span class="attr">"path"</span>: <span class="string">"img/md/"</span>,</span><br><span class="line">      <span class="attr">"customUrl"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="attr">"options"</span>: <span class="string">""</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="comment">// 插件设置</span></span><br><span class="line">  <span class="attr">"picgoPlugins"</span>: &#123;</span><br><span class="line">    <span class="attr">"picgo-plugin-super-prefix"</span>: <span class="literal">true</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"picgo-plugin-super-prefix"</span>: &#123;</span><br><span class="line">    <span class="attr">"prefixFormat"</span>: <span class="string">"YYYY/MM/"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意 ：   存储路径比如<code>img/</code>的话，上传的图片会默认放在OSS的<code>img</code>文件夹下。注意存储路径一定要以<code>/</code>结尾！存储路径是可选的，如果不需要请留空。</p>
<h3 id=""></h3>
<h1>让Typora的图片居左的两种方法</h1>
<ul>
<li>方法1：空格键</li>
<li>方法2：tab键</li>
</ul>
<p><a href="https://blog.csdn.net/sky0816/article/details/111321812" target="_blank" rel="noopener">https://blog.csdn.net/sky0816/article/details/111321812</a></p>
<h1>下面是老办法  （淘汰了）</h1>
<p>事先声明，所有博客文件均保存在 hexo/_posts/文件夹下</p>
<p>首先在 hexo &gt; source目录下建一个文件夹叫images，用来保存博客中的图片。</p>
<p>然后打开Typora的 文件 &gt; 偏好设置，进行如下设置。</p>
<p><img src="/images/image-20200116142728587.png" alt="image-20200116142728587"></p>
<p>这样的话所有的博客中的图片都将会保存到 /source/images/该博客md文件名/图片名称</p>
<p>但是仅仅这样设置还不够，这样设置在typora中倒是能看图片了，但是使用的却是相对于当前md文件的相对路径，可是如果启动hexo，是要用服务器访问的，而服务器显然无法根据这个相对路径正确访问到图片，因此还需要在typora中进行进一步设置。</p>
<p>在typora菜单栏点击 格式-&gt;图像-&gt;设置图片根目录，将hexo/source作为其根目录即可。</p>
<p><strong>一定要先设置了图片根目录后再插入图片，否则图片路径会不正确喔！</strong></p>
<p><strong>根目录设置：</strong></p>
<p>在文章头部插入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">typora-root-url: ..</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>然后图片地址换成绝对路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![image-20200116142728587](&#x2F;images&#x2F;image-20200116142728587.png)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>scala：函数至简原则、函数简化过程</title>
    <url>/2022/04/19/scala%EF%BC%9A%E5%87%BD%E6%95%B0%E8%87%B3%E7%AE%80%E5%8E%9F%E5%88%99%E3%80%81%E5%87%BD%E6%95%B0%E7%AE%80%E5%8C%96%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>scala：函数至简原则、函数简化过程</p>
<p>1.return可以省略，Scala会使用函数体的最后一行代码作为返回值<br>
2.如果函数体只有一行代码，可以省略花括号<br>
3.返回值类型如果能够推断出来，那么可以省略（:和返回值类型一起省略）<br>
4.如果有return，则不能省略返回值类型，必须指定<br>
5.如果函数明确声明unit，那么即使函数体中使用return关键字也不起作用<br>
6.Scala如果期望是无返回值类型，可以省略等号 这种形式称为过程<br>
7.如果函数无参，但是声明了参数列表，那么调用时，小括号，可加可不加<br>
8.如果函数没有参数列表，那么小括号可以省略，调用时小括号必须省略<br>
9.如果不关心名称，只关心逻辑处理，那么函数名（def）可以省略</p>
<p>代码示例</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Scala05_TestFun_review</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    //正常定义函数</span></span><br><span class="line"><span class="comment">    def f0(name:String): String =&#123;</span></span><br><span class="line"><span class="comment">      return name</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    println(f0("atguigu"))</span></span><br><span class="line"><span class="comment">    //（1）return可以省略，Scala会使用函数体的最后一行代码作为返回值</span></span><br><span class="line"><span class="comment">    def f1(name:String): String =&#123;</span></span><br><span class="line"><span class="comment">      name</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    println(f1("atguigu"))</span></span><br><span class="line"><span class="comment">    //（2）如果函数体只有一行代码，可以省略花括号</span></span><br><span class="line"><span class="comment">def f2(name:String): String = name</span></span><br><span class="line"><span class="comment">println(f2("atguigu"))</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">//（3）返回值类型如果能够推断出来，那么可以省略（:和返回值类型一起省略）</span></span><br><span class="line"><span class="comment">def f3(name:String)= name</span></span><br><span class="line"><span class="comment">println(f3("atguigu"))</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">//（4）如果有return，则不能省略返回值类型，必须指定</span></span><br><span class="line"><span class="comment">def f4(name:String) =&#123;</span></span><br><span class="line"><span class="comment">  return name</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">//（5）如果函数明确声明unit，那么即使函数体中使用return关键字也不起作用</span></span><br><span class="line"><span class="comment">def f5(name:String): Unit =&#123;</span></span><br><span class="line"><span class="comment">  return name</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">println(f5("atguigu"))</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">//（6）Scala如果期望是无返回值类型，可以省略等号   这种形式称为过程</span></span><br><span class="line"><span class="comment">def f6(name:String)&#123;</span></span><br><span class="line"><span class="comment">  println(name)</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">f6("banzhang")</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">//（7）如果函数无参，但是声明了参数列表，那么调用时，小括号，可加可不加</span></span><br><span class="line"><span class="comment">def f7(): Unit =&#123;</span></span><br><span class="line"><span class="comment">  println("jingjing")</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">f7</span></span><br><span class="line"><span class="comment">f7()</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">//（8）如果函数没有参数列表，那么小括号可以省略，调用时小括号必须省略</span></span><br><span class="line"><span class="comment">def f8: Unit =&#123;</span></span><br><span class="line"><span class="comment">  println("jingjing")</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">//f8()</span></span><br><span class="line"><span class="comment">f8</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//（9）如果不关心名称，只关心逻辑处理，那么函数名（def）可以省略</span></span><br><span class="line"><span class="comment">// 1.扩展f9的功能   2.具体扩展的功能是通过fun函数传递，很灵活</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f9</span></span>(fun:(<span class="type">String</span>)=&gt;<span class="type">Unit</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">  fun(<span class="string">"jingjing"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//def f10(s:String): Unit =&#123;</span></span><br><span class="line"><span class="comment">//  println(s)</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//f9(f10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//匿名函数 ：没有名字的函数，通过lambda表达式实现     (参数)=&gt;&#123;函数体&#125;</span></span><br><span class="line"><span class="comment">//f9((s:String)=&gt;&#123;println(s)&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//（1）参数的类型可以省略，会根据形参进行自动的推导</span></span><br><span class="line"><span class="comment">//f9((s)=&gt;&#123;println(s)&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//（2）类型省略之后，发现只有一个参数，则圆括号可以省略；其他情况：没有参数和参数超过1的永远不能省略圆括号。</span></span><br><span class="line"><span class="comment">//f9(s =&gt;&#123;println(s)&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//（3）匿名函数如果只有一行，则大括号也可以省略</span></span><br><span class="line"><span class="comment">//f9(s =&gt;println(s))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//（4）如果参数只出现一次，则参数省略且后面参数可以用_代替</span></span><br><span class="line"><span class="comment">//f9(println(_))</span></span><br><span class="line">  <span class="comment">//  （5） 如果可以推断出，println是一个函数体，而不是调用语句，那么(_)可以省</span></span><br><span class="line">  <span class="comment">//  f9(println)</span></span><br><span class="line">  <span class="comment">//反推导     f9((s:String)=&gt;println(s))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//加深大家理解</span></span><br><span class="line"><span class="comment">//定义一个函数，接受一个函数类型的参数，该函数类型有两个参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f11</span></span>(fun:(<span class="type">Int</span>,<span class="type">Int</span>)=&gt;<span class="type">Int</span>):<span class="type">Int</span>=&#123;</span><br><span class="line">  fun(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//println(f11((x:Int,y:Int)=&gt;&#123;x + y&#125;))</span></span><br><span class="line"><span class="comment">//println(f11((x,y)=&gt;x + y))</span></span><br><span class="line">println(f11(_ + _))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>hexo 常用命令</title>
    <url>/2022/04/19/hexo%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>hexo 常用命令 ：</p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line">$ hexo generate (hexo g) 生成静态文件</span><br><span class="line">$ hexo server (hexo s) 启动本地服务</span><br><span class="line">$ hexo deploy (hexo d) 提交到远程仓库</span><br><span class="line">$ hexo new page <span class="string">"xx"</span>(hexo n page) 创建页面 </span><br><span class="line">$ hexo new <span class="string">"xx"</span> (hexo n <span class="string">""</span>) 创建文章</span><br><span class="line">$ hexo d -g 生成静态并提交到远程仓库</span><br><span class="line">$ hexo s -g 生成静态文件并启动本地预览</span><br><span class="line">$ hexo clean (hexo cl)清除本地 public 文件</span><br></pre></td></tr></table></figure>
<p>其他参考：</p>
<p>5分钟搞定个人博客-hexo     <a href="https://www.jianshu.com/p/390f202c5b0e" target="_blank" rel="noopener">https://www.jianshu.com/p/390f202c5b0e</a></p>
<p>换终端更新hexo博客  <a href="https://www.jianshu.com/p/6a29f5243ab4" target="_blank" rel="noopener">https://www.jianshu.com/p/6a29f5243ab4</a></p>
<p>win10子系统安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt upgrade</span><br><span class="line">sudo apt-get install npm</span><br><span class="line">npm install -g hexo-cli@3.1.0</span><br></pre></td></tr></table></figure>
<h1>ubuntu18.04安装nodejs最新版、指定版 12.x 14.x</h1>
<p>今天准备在 ubuntu 服务器里面安装 nodejs 版本，ubuntu 18.04 仓库 nodejs 默认是 8.x 版本。</p>
<h2 id="1-通过-apt-安装-nodejs">1. 通过 apt 安装 nodejs</h2>
<p>在 Ubuntu 18.04 的默认仓库包含了一个 Node.js 的版本，截至当前，该仓库的 node.js 版本是 8.10.0 。要安装此版本，你可以使用 apt 包管理器。先刷新你的本地包索引，通过如下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br></pre></td></tr></table></figure>
<p>然后运行安装命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install nodejs</span><br></pre></td></tr></table></figure>
<h2 id="2-通过-PPA-安装指定或最新版本的-nodejs">2. 通过 PPA 安装指定或最新版本的 nodejs</h2>
<p>那么就需要使用 <a href="https://nodesource.com/" target="_blank" rel="noopener">nodesource</a> 来安装指定版本的 nodejs 了。其需要下载一个脚本，运行此脚本会在 ubuntu 里添加一个 nodejs 源，然后用 apt 就可以下载指定的 nodejs 了。</p>
<p>PPA 的全称为 personal package archive 。要安装 nodejs 12.x 版本，可以运行如下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~</span><br><span class="line">curl -sL https:&#x2F;&#x2F;deb.nodesource.com&#x2F;setup_12.x | sudo bash -</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install nodejs</span><br></pre></td></tr></table></figure>
<p>要安装 nodejs 最新版本，可以运行如下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~</span><br><span class="line">curl -sL https:&#x2F;&#x2F;deb.nodesource.com&#x2F;setup | sudo bash -</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install nodejs</span><br></pre></td></tr></table></figure>
<h2 id="3-如何卸载-nodejs">3. 如何卸载 nodejs</h2>
<p>执行如下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt remove nodejs</span><br></pre></td></tr></table></figure>
<p>此命令会卸载 nodejs，但是会保留配置文件，方便你以后再次安装 nodejs。</p>
<p>如果不想保留配置文件，继续执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt purge nodejs</span><br></pre></td></tr></table></figure>
<p>这将会卸载 nodejs 和其相关的配置文件。</p>
<p>最后，你还可以移除和 nodejs 一起安装但是现在没有被使用的包：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt autoremove</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>采用hive自带的方法生成Hfile，并将上亿大数据量导入HBASE</title>
    <url>/2022/04/13/%E9%87%87%E7%94%A8hive%E8%87%AA%E5%B8%A6%E7%9A%84%E6%96%B9%E6%B3%95%E7%94%9F%E6%88%90Hfile%EF%BC%8C%E5%B9%B6%E5%B0%86%E4%B8%8A%E4%BA%BF%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%8F%E5%AF%BC%E5%85%A5HBASE/</url>
    <content><![CDATA[<h1>采用hive自带的方法生成Hfile，并将上亿大数据量导入HBASE</h1>
<h2 id="1-引入HBASE自带的jar">1.引入HBASE自带的jar</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh85-100 ~]# sudo -u hdfs hive</span><br><span class="line">hive&gt; </span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-annotations-2.0.0-cdh6.0.1.jar                      ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-client-2.0.0-cdh6.0.1.jar                           ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-common-2.0.0-cdh6.0.1.jar                           ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-endpoint-2.0.0-cdh6.0.1.jar                         ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-examples-2.0.0-cdh6.0.1.jar                         ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-external-blockcache-2.0.0-cdh6.0.1.jar              ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-hadoop2-compat-2.0.0-cdh6.0.1.jar                   ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-hadoop-compat-2.0.0-cdh6.0.1.jar                    ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-http-2.0.0-cdh6.0.1.jar                             ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-it-2.0.0-cdh6.0.1.jar                               ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-mapreduce-2.0.0-cdh6.0.1.jar                        ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-metrics-2.0.0-cdh6.0.1.jar                          ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-metrics-api-2.0.0-cdh6.0.1.jar                      ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-procedure-2.0.0-cdh6.0.1.jar                        ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-protocol-2.0.0-cdh6.0.1.jar                         ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-protocol-shaded-2.0.0-cdh6.0.1.jar                  ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-replication-2.0.0-cdh6.0.1.jar                      ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-resource-bundle-2.0.0-cdh6.0.1.jar                  ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-rest-2.0.0-cdh6.0.1.jar                             ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-rsgroup-2.0.0-cdh6.0.1.jar                          ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-server-2.0.0-cdh6.0.1.jar                           ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-shaded-miscellaneous-2.1.0.jar                      ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-shaded-netty-2.1.0.jar                              ;</span><br><span class="line">add jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hbase&#x2F;lib&#x2F;hbase-shaded-protobuf-2.1.0.jar                           ;</span><br></pre></td></tr></table></figure>
<h2 id="2-0建Hfile表">2.0建Hfile表</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table tmp.fi_pt_card_library(</span><br><span class="line"> KEY STRING COMMENT &#39;HBASE_ROWKEY&#39; ,</span><br><span class="line">  sign_id STRING COMMENT &#39;id自增长&#39;,</span><br><span class="line">  ....</span><br><span class="line">)</span><br><span class="line">STORED AS</span><br><span class="line">INPUTFORMAT &#39;org.apache.hadoop.mapred.TextInputFormat&#39;</span><br><span class="line">OUTPUTFORMAT &#39;org.apache.hadoop.hive.hbase.HiveHFileOutputFormat&#39;</span><br><span class="line">TBLPROPERTIES (&#39;hfile.family.path&#39; &#x3D; &#39;&#x2F;tmp&#x2F;fi_pt_card_library&#x2F;info&#39;);</span><br></pre></td></tr></table></figure>
<p><strong>注意：此处的info应该与HBase中的family相同。</strong></p>
<h2 id="生成Hfile文件">生成Hfile文件</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; set mapreduce.job.queuename&#x3D;bf_yarn_pool.production;</span><br><span class="line"></span><br><span class="line">insert overwrite  table tmp.fi_pt_card_library</span><br><span class="line">SELECT * from tmp.fi_pt_card_library_0330 order by key</span><br></pre></td></tr></table></figure>
<p><strong>注意： fi_pt_card_library_0330表 不能有重复值， 而且要排序。</strong></p>
<p>这里的第一个字段会默认为Hbase的KEY，但是这里的key的名称可以不为key，其他的也可以。还有数据中不允许存在同样的KEY，如果出现同样的KEY会报错。同时KEY的值应当具有顺序，如果顺序不同也会出错。</p>
<h2 id="3-将数据导入Hbase中">3. 将数据导入Hbase中</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u hdfs hbase  org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles  -Dhbase.mapreduce.bulkload.max.hfiles.perRegion.perFamily&#x3D;2048 &#x2F;tmp&#x2F;fi_pt_card_library BAOFOO_FI:fi_pt_card_library</span><br></pre></td></tr></table></figure>
<p>亲测有效。<br>
最终花费了14915.376 seconds 将5亿条HIVE数据迁移到了Hbase中。</p>
]]></content>
  </entry>
  <entry>
    <title>clickhouse运维</title>
    <url>/2022/03/03/clickhouse%E8%BF%90%E7%BB%B4/</url>
    <content><![CDATA[<p>查看后台进程并杀死</p>
<p>– 这个命令和mysql是一样的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">show processlist</span><br></pre></td></tr></table></figure>
<p>– 如果进程太多，也可用通过查询系统表 processes，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from system.processes</span><br></pre></td></tr></table></figure>
<p>– 指定主要关心字段</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select </span><br><span class="line">  user,query_id,query,elapsed,memory_usage</span><br><span class="line">from system.processes</span><br></pre></td></tr></table></figure>
<p>杀死后台进程<br>
–  通过上面指令获取到进程相关信息后，可以用query_id条件kill进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KILL QUERY WHERE query_id&#x3D;&#39;2-857d-4a57-9ee0-327da5d60a90&#39;</span><br></pre></td></tr></table></figure>
<p>– 杀死default用户下的所有进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KILL QUERY WHERE user&#x3D;&#39;default&#39;</span><br></pre></td></tr></table></figure>
<p>clickhouse启动停止服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service clickhouse-server start</span><br><span class="line">service clickhouse-server stop</span><br><span class="line">service clickhouse-server restart</span><br></pre></td></tr></table></figure>
<h1>Clickhouse删除表某一天分区</h1>
<p>方法一：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE kuming.tableName DELETE WHERE toDate(insert_at_timestamp)&#x3D;&#39;2020-07-21&#39;;</span><br></pre></td></tr></table></figure>
<p>方法二：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE kuming.tableName DELETE WHERE insert_at_timestamp&lt;&#x3D;1596470399;</span><br></pre></td></tr></table></figure>
<p>方法三：（当前两种方法分区数据没有删除掉的时候可以用方法三）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE kuming.tableName DROP PARTITION &#39;2020-08-03&#39;;</span><br></pre></td></tr></table></figure>
<p>alter table 表名 drop partition 分区名</p>
<p>分区名可以用下语句查询</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select * from system.parts p where table &#x3D; &#39;表名&#39;</span><br></pre></td></tr></table></figure>
<h1>查看日志</h1>
<p>可以在clickhouse结点上查看/var/log/clickhouse-server/clickhouse-server.log，注意ERROR级别日志</p>
<p>– 特别主要：</p>
<p>可以在clickhouse结点上查看/var/log/clickhouse-server/clickhouse-server.log，注意ERROR级别日志</p>
<p>/var/log/clickhouse-server/clickhouse-server.err.log</p>
<h1>clickhouse 用户权限设置</h1>
<p><a href="https://www.jianshu.com/p/3e08a7150fb1" target="_blank" rel="noopener">https://www.jianshu.com/p/3e08a7150fb1</a></p>
<h3 id="创建角色和普通用户">创建角色和普通用户</h3>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">ROLE</span> accountant;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">ON</span> dbtest.* <span class="keyword">TO</span> accountant;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> mira HOST IP <span class="string">'127.0.0.1'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">WITH</span> sha256_password <span class="keyword">BY</span> <span class="string">'qwerty'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">GRANT</span> accountant <span class="keyword">TO</span> mira;</span><br></pre></td></tr></table></figure>
<h1>合并分区</h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">optimize table xxoxx final;</span><br><span class="line"></span><br><span class="line">optimize table xxoxx  partition ‘20211206’ final;</span><br></pre></td></tr></table></figure>
<h1>在客户端打印日志  --send_logs_level=trace</h1>
<p>[atguigu@hadoop102 lib]$ clickhouse-client --send_logs_level=trace &lt;&lt;&lt; 'select*from t_order_mt2 where total_amount &gt; toDecimal32(900.,2)&quot;;</p>
<h1>ReplacingMergeTree</h1>
<p>通过测试得到结论<br>
实际上是使用order by字段作为唯一键<br>
去重不能跨分区<br>
只有同—批插入(新版本)或合并分区时才会进行去重<br>
认定重复的数据保留，版本字段值最大的<br>
如果版本字段相同则按插入J顺序保留最后一笔<br>
I</p>
<h1>ClickHouse 分片双副本集群部署</h1>
<p>参考   <a href="https://www.jianshu.com/p/5bcaad0a02b1" target="_blank" rel="noopener">https://www.jianshu.com/p/5bcaad0a02b1</a></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建分布式数据库</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> PAY_TRADECENTER <span class="keyword">on</span> cluster ck_cluster;</span><br><span class="line"><span class="comment">--drop database PAY_TRADECENTER on cluster ck_cluster ;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在集群的每个机器上面创建mysql镜像库</span></span><br><span class="line"><span class="comment">-- drop database mysql_pay_tradecenter on cluster ck_cluster ;</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> mysql_pay_tradecenter <span class="keyword">on</span> cluster ck_cluster <span class="keyword">ENGINE</span> = MySQL(<span class="string">'****:3306'</span>, <span class="string">'PAY_TRADECENTER'</span>, <span class="string">'cs_hadoop'</span>, <span class="string">'***'</span>) ;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在集群的每个机器上面建立本地表 (设置分片)</span></span><br><span class="line"><span class="comment">-- drop table PAY_TRADECENTER.T_TC_BASE_LOCAL on cluster ck_cluster</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> PAY_TRADECENTER.T_TC_BASE_LOCAL <span class="keyword">on</span> cluster ck_cluster <span class="keyword">as</span> mysql_pay_tradecenter.T_TC_BASE  </span><br><span class="line"><span class="keyword">ENGINE</span> = ReplicatedMergeTree(<span class="string">'/clickhouse/tables/&#123;shard&#125;/T_TC_BASE_LOCAL'</span>, <span class="string">'&#123;replica&#125;'</span>) </span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYYYYMM(CREATED_AT) </span><br><span class="line">primary <span class="keyword">key</span> (<span class="keyword">ID</span>)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> (<span class="keyword">ID</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在集群的每个机器上面建立分布式表</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> PAY_TRADECENTER.T_TC_BASE <span class="keyword">on</span> cluster ck_cluster </span><br><span class="line"><span class="keyword">as</span> PAY_TRADECENTER.T_TC_BASE_LOCAL</span><br><span class="line"><span class="keyword">ENGINE</span> = <span class="keyword">Distributed</span>(ck_cluster, PAY_TRADECENTER, T_TC_BASE_LOCAL, <span class="keyword">rand</span>());</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> PAY_TRADECENTER.T_TC_BASE <span class="keyword">SELECT</span> * <span class="keyword">from</span> mysql_pay_tradecenter.T_TC_BASE</span><br></pre></td></tr></table></figure>
<h2 id="clickhouse用presto查询">clickhouse用presto查询</h2>
<p>测试下来clickhouse用presto查询  效率低</p>
<h2 id="卸载及删除安装文件">卸载及删除安装文件</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum list installed | grep clickhouse</span><br><span class="line">yum remove -y clickhouse-common-static</span><br><span class="line">yum remove -y clickhouse-server-common</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;clickhouse</span><br><span class="line">rm -rf &#x2F;etc&#x2F;clickhouse-*</span><br><span class="line">rm -rf &#x2F;var&#x2F;log&#x2F;clickhouse-server</span><br></pre></td></tr></table></figure>
<h2 id="备份与恢复">备份与恢复</h2>
<p><a href="https://github.com/AlexAkulov/clickhouse-backup" target="_blank" rel="noopener">https://github.com/AlexAkulov/clickhouse-backup</a></p>
<p>修改备份工具配置文件的端口和密码</p>
<p>cp  /etc/clickhouse-backup/config.yml.example  /etc/clickhouse-backup/config.yml</p>
<p><strong>创建备份</strong></p>
<p>• 查看可用命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ clickhouse-backup help</span><br></pre></td></tr></table></figure>
<p>• 显示要备份的表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ clickhouse-backup tables</span><br></pre></td></tr></table></figure>
<p>• 创建备份</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo clickhouse-backup create</span><br></pre></td></tr></table></figure>
<p>• 查看现有的本地备份</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo clickhouse-backup list</span><br></pre></td></tr></table></figure>
<p>- 从备份还原</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clickhouse-backup restore 2022-01-28T06-13-35</span><br></pre></td></tr></table></figure>
<p>– 恢复指定表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clickhouse-backup restore 2022-01-28T06-13-35 --table dbtest.abcd</span><br></pre></td></tr></table></figure>
<h2 id="Suggested-Syntax">Suggested Syntax</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE DATABASE test ENGINE&#x3D;MaterializeMySQL(&#39;127.0.0.1:3306&#39;, &#39;test&#39;, &#39;root&#39;, &#39;clickhouse&#39;)</span><br><span class="line">    TABLE OVERRIDE t1 (</span><br><span class="line">        COLUMNS (</span><br><span class="line">            -- sparse column list, replacing existing ones or adding new ones</span><br><span class="line">            timestamp DateTime CODEC(DoubleDelta, Default)</span><br><span class="line">            PROJECTION ...,</span><br><span class="line">            CONSTRAINT ...,</span><br><span class="line">            INDEX ...</span><br><span class="line">        )</span><br><span class="line">        -- storage parameters:</span><br><span class="line">        ORDER BY expr</span><br><span class="line">        PRIMARY KEY expr</span><br><span class="line">        PARTITION BY expr</span><br><span class="line">        SAMPLE BY expr</span><br><span class="line">        TTL ...</span><br><span class="line">    ),</span><br><span class="line">    -- multiple tables can be overridden</span><br><span class="line">    TABLE OVERRIDE t2 (</span><br><span class="line">        PARTITION BY tuple(id % 10, toYYYY(created))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>Detailed description / Documentation draft:<br>
The <code>EXPLAIN TABLE OVERRIDE</code> query can be used for pre-validating overrides. Example:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">EXPLAIN TABLE OVERRIDE mysql(&#39;127.0.0.1:3306&#39;, &#39;db&#39;, &#39;table&#39;, &#39;user&#39;, &#39;pw&#39;)</span><br><span class="line">    PARTITION BY tuple(toYYYYMM(created), id % 8)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>查看rcfile、orc、text和paquet等格式的文件</title>
    <url>/2022/02/22/%E6%9F%A5%E7%9C%8Brcfile%E3%80%81orc%E3%80%81text%E5%92%8Cpaquet%E7%AD%89%E6%A0%BC%E5%BC%8F%E7%9A%84%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<h3 id="hive查看rcfile和orc">hive查看rcfile和orc</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive --rcfilecat hdfs:&#x2F;&#x2F;cluster1&#x2F;tmp&#x2F;youni_user_profile_ft&#x2F;000000_0</span><br><span class="line">hive --rcfilecat hdfs:&#x2F;&#x2F;cluster1&#x2F;tmp&#x2F;000000-000009&#x2F;000000_0</span><br></pre></td></tr></table></figure>
<h3 id="hive查看orc">hive查看orc</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive --orcfiledump &#x2F;data&#x2F;datawarehouse&#x2F;xx&#x2F;xxxx&#x2F;xxxx&#x2F;part-00199</span><br></pre></td></tr></table></figure>
<h3 id="查看文本格式">查看文本格式</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -text &#x2F;tmp&#x2F;part-m-00000.snappy</span><br></pre></td></tr></table></figure>
<h3 id="查看parquet文件格式内容">查看parquet文件格式内容</h3>
<ol>
<li>下载对应的parquet-tools jar:<a href="http://logservice-resource.oss-cn-shanghai.aliyuncs.com/tools/parquet-tools-1.6.0rc3-SNAPSHOT.jar?spm=5176.doc52798.2.7.H3s2kL&amp;file=parquet-tools-1.6.0rc3-SNAPSHOT.jar" target="_blank" rel="noopener">http://logservice-resource.oss-cn-shanghai.aliyuncs.com/tools/parquet-tools-1.6.0rc3-SNAPSHOT.jar?spm=5176.doc52798.2.7.H3s2kL&amp;file=parquet-tools-1.6.0rc3-SNAPSHOT.jar</a></li>
</ol>
<p>git：<a href="https://github.com/apache/parquet-mr/tree/master/parquet-tools?spm=5176.doc52798.2.6.H3s2kL" target="_blank" rel="noopener">https://github.com/apache/parquet-mr/tree/master/parquet-tools?spm=5176.doc52798.2.6.H3s2kL</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看schema:</span><br><span class="line">java -jar  parquet-tools-1.6.0rc3-SNAPSHOT.jar  schema -d myparquet.parquet | head -n 10</span><br><span class="line">查看内容：</span><br><span class="line">java -jar  parquet-tools-1.6.0rc3-SNAPSHOT.jar  head -n 10 myparquet.parquet</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>tar不覆盖已存在文件</title>
    <url>/2022/02/14/tar%E4%B8%8D%E8%A6%86%E7%9B%96%E5%B7%B2%E5%AD%98%E5%9C%A8%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<p>tar在解压或创建的时候一不小心就盖已存在文件</p>
<p>tar 工具的 -k 参数就提供干这事<br>
-k,–keep-old-files 不覆盖已存在文件<br>
keep existing files; don’t overwrite them from archive</p>
<p>如:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar zxfk filename.tar.gz</span><br></pre></td></tr></table></figure>
<p>tar 【参数】 【文件或目录】</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#压缩比如把x文件夹打包并用gzip压缩。</span><br><span class="line">tar -zcvf x.tar.gz x</span><br><span class="line"></span><br><span class="line">#解压到当前目录</span><br><span class="line">tar -xzvf x.tar.gz</span><br><span class="line"></span><br><span class="line">#解压到父目录</span><br><span class="line">tar -xzvf x.tar.gz -C ..</span><br></pre></td></tr></table></figure>
<h4 id="常用参数：">常用参数：</h4>
<table>
<thead>
<tr>
<th>指定归档</th>
<th><strong>-f</strong> <strong>后面必须直接跟归档名</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>打包（create）</td>
<td><strong>-c</strong></td>
</tr>
<tr>
<td>解包</td>
<td><strong>-x</strong></td>
</tr>
<tr>
<td>详细地列出处理的文件</td>
<td><strong>-v</strong></td>
</tr>
<tr>
<td>查看打包的文件</td>
<td><strong>-t</strong></td>
</tr>
<tr>
<td>压缩</td>
<td><strong>-z</strong> <strong>调用<strong><strong>gzip</strong></strong>压缩</strong>  <strong>-j<strong><strong>调用</strong></strong>bzip2****压缩</strong></td>
</tr>
<tr>
<td>时间限制参数</td>
<td><strong>-N</strong> **比后面接的日期(yyyy/mm/dd)**<strong>还要新的才会被打包进新建的文件中</strong></td>
</tr>
<tr>
<td>排除指定文件</td>
<td><strong>–exclude FILE</strong>**：**<strong>不将</strong> <strong>FILE</strong> <strong>打包！</strong></td>
</tr>
<tr>
<td>保留原本文件的属性</td>
<td><strong>-p</strong></td>
</tr>
</tbody>
</table>
]]></content>
  </entry>
  <entry>
    <title>centos7删除日志文件后磁盘不释放空间处理脚本</title>
    <url>/2022/02/14/centos7%E5%88%A0%E9%99%A4%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E5%90%8E%E7%A3%81%E7%9B%98%E4%B8%8D%E9%87%8A%E6%94%BE%E7%A9%BA%E9%97%B4%E5%A4%84%E7%90%86%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<blockquote>
<p>背景：日志文件被人为删除，但日志对应的程序仍然在运行（进程在，IO未关闭），df看磁盘未释放<br>
但du看磁盘已释放空间。<br>
操作系统：<a href="https://so.csdn.net/so/search?q=CentOS7&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener">CentOS7</a>.5</p>
</blockquote>
<p>以下脚本可通过找出被删文件彻底释放磁盘空间</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">for p in &#96;ls &#x2F;proc&#x2F; |egrep &#39;^[0-9]&#123;1,5&#125;&#39;&#96;</span><br><span class="line">do</span><br><span class="line">cd &#x2F;proc&#x2F;$&#123;p&#125;&#x2F;fd;ls -l|grep deleted &amp;&amp; &gt;1 &amp;&amp; &gt;2</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>CDH集群部署</title>
    <url>/2022/02/07/CDH%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h2 id="一、集群规划">一、集群规划</h2>
<p>如果你正准备从0开始搭建一套CDH集群应用于生产环境，那么此时需要做的事情应该是 <strong>结合当前的数据、业务、硬件、节点、服务等对集群做合理的规划</strong>，而不是马上动手去安装软件。</p>
<p>合理的集群规划应该做到以下几点：</p>
<ul>
<li>充分了解当前的数据现状</li>
<li>与业务方深入沟通，了解将会在集群上运行的业务，集群将会为业务提供什么服务</li>
<li>结合数据现状与业务，合理预估未来的数据量增长</li>
<li>盘点当前可用的硬件资源，包括机柜机架、服务器、交换机等</li>
<li>当前硬件资源不充足的情况下，根据数据评估情况作出采购建议</li>
<li>根据业务属性与组成，合理规划集群的部署架构</li>
<li>根据可用硬件资源，对集群节点的服务角色进行合理划分</li>
</ul>
<p>以上步骤完成之后才是动手进行安装与部署。</p>
<p>你将会对集群的架构模式、应用方向与业务场景了然于胸，并确保这个集群（或者是集群组）能够<strong>提供稳定、高效、高性能的服务</strong>，为业务保驾护航。</p>
<p>并有能力能够提供 <strong>集群建设目标</strong>：</p>
<ul>
<li>
<p>性能需求</p>
</li>
<li>
<ul>
<li>简单查询100G数据量时，耗时上限</li>
<li>复杂查询（join）时，耗时上限</li>
<li>历史数据导入时，耗时上限</li>
<li>增量数据导入时，耗时上限</li>
</ul>
</li>
<li>
<p>可靠性需求：每月宕机次数（&lt;1），每月宕机时间（&lt;10min）</p>
</li>
<li>
<p>可用性：每台机器每月的宕机时间</p>
</li>
<li>
<p>容错性：机器故障，数据不丢失</p>
</li>
</ul>
<h3 id="1-1-硬件规划">1.1 硬件规划</h3>
<p>硬件规划决定集群将使用多少硬件资源，以及什么配置的硬件资源。</p>
<p>可以从以下几个维度进行评估：</p>
<ul>
<li>
<p>数据现状</p>
</li>
<li>
<ul>
<li>盘点所有数据情况，包括数据源、数据量、数据大小、数据维度等信息</li>
</ul>
</li>
<li>
<p>工作负载</p>
</li>
<li>
<ul>
<li>评估在集群与数据之上将执行的任务类型</li>
<li>如实时计算、离线计算、图像处理、关系网络等应用场景以及是否提供OLTP服务等</li>
</ul>
</li>
<li>
<p>未来数据量预估</p>
</li>
<li>
<ul>
<li>根据数据源与业务应用场景可以对未来衍生的数据总量与数据增量做大致评估</li>
<li>评估的时间范围视业务场景而定，建议做不少于一年的规划</li>
</ul>
</li>
<li>
<p>硬件资源现状</p>
</li>
<li>
<ul>
<li>盘点目前可用的硬件资源，确认是否满足所评估的规模及要求</li>
<li>机房机柜空间、电源（双）等是否充足（需考虑后续扩容问题）</li>
<li>网络交换机性能是否满足要求（建议万兆网卡（双））</li>
<li>查看服务器磁盘、内存、CPU等资源是否需要补充</li>
</ul>
</li>
<li>
<p>硬件选择</p>
</li>
<li>
<ul>
<li>现有硬件资源不满足的需求的情况下，结合运维建议提出需要增加或者新采购的硬件型号、配置等</li>
<li>确认所需服务器数量</li>
</ul>
</li>
</ul>
<p>示例主机列表：</p>
<ul>
<li>cdh2-1</li>
<li>cdh2-2</li>
<li>cdh2-3</li>
<li>cdh2-4</li>
<li>cdh2-5</li>
<li>cdh2-6</li>
<li>cdh2-7</li>
<li>cdh2-8</li>
</ul>
<p>服务器硬件情况如下：</p>
<ul>
<li>数量：8</li>
<li>CPU：10</li>
<li>内存：64G</li>
<li>硬盘：3.3T</li>
</ul>
<h3 id="1-2-集群架构">1.2 集群架构</h3>
<h3 id="混合型集群">混合型集群</h3>
<p>指由一个统一的大集群提供所有大数据服务，所有组件集中安装在同一个集群中，有部署简单、运维方便、易于使用等优点。</p>
<p>但是由于混合型集群集群承载了所有功能，职能繁多，网络带宽、磁盘IO等为集群共享，会因大型离线任务占用大量网络或磁盘IO峰值，对线上业务会造成短暂延迟。</p>
<p>且集群环境较为复杂，有较多对线上业务造成影响的风险。</p>
<h3 id="专用型集群">专用型集群</h3>
<p>专用型集群指根据不同的需求与功能职责对集群进行划分，由多个职责不同、硬件隔离的集群组成集群组环境提供服务。</p>
<p>子集群各司其职，根据自身业务最大化利用硬件资源，互相独立互不影响。部署较为复杂，运维难度增加。</p>
<p>专用型集群根据业务与应用场景可以划分如下：</p>
<ul>
<li>离线计算集群</li>
<li>实时计算集群</li>
<li>数据服务集群</li>
<li>GPU深度学习集群</li>
<li>图数据库集群</li>
</ul>
<p>等等。</p>
<p>HBase提供实时读写服务的生产环境下<strong>建议将HBase集群独立部署为数据服务集群</strong>，参考：<a href="https://zhuanlan.zhihu.com/p/72150364" target="_blank" rel="noopener">HBase最佳实践</a> - 「集群部署」小节。</p>
<h3 id="1-3-节点规划">1.3 节点规划</h3>
<p>进行节点角色划分时尽可能遵守以下原则：</p>
<ul>
<li>
<p>CM监控服务在小集群下可以部署在同一主机，大集群下需要独立部署</p>
</li>
<li>
<p>集群主节点与子节点独立部署（HDFS/HBase/Yarn），且各自子节点部署在相同主机上</p>
</li>
<li>
<ul>
<li>独立部署可以避免子节点大量读写、计算引起IO、CPU、网络等资源阻塞而导致主节点异常甚至宕机</li>
<li>HDFS/HBase/Yarn部署在相同主机上可以最大化利用数据本地化特性</li>
<li>如果数据量巨大，而集群存储空间不足的时候忽视以上两点，满足业务需求放在第一位</li>
</ul>
</li>
<li>
<p>Hive、Hue、Impala、Sentry等服务/元数据服务需要部署在同一主机</p>
</li>
<li>
<ul>
<li>Hue+Sentry对Hive与Impala进行权限控制的时候需要读取Linux主机的用户与用户组进行判别，如果部署在不同主机上则需要在每个主机上创建相同的用户与用户组</li>
<li>或者使用LDAP进行账号管理</li>
<li>条件允许情况下，独立主机或者压力小的主机</li>
</ul>
</li>
<li>
<p>Zookeeper尽量使用5个节点，且条件允许下最好在不同的物理主机上</p>
</li>
<li>
<ul>
<li>5个节点的zk可以保证leader的快速选举</li>
<li>在不同的物理主机上可以最大限度保证安全</li>
</ul>
</li>
</ul>
<p>示例集群主要进程分布如下：</p>
<ul>
<li>CM服务(monitors)：cdh2-4</li>
<li>HDFS主节点（NameNode）：cdh2-1,cdh2-2</li>
<li>HDFS容灾节点(JournalNode)：cdh2-[1:3]</li>
<li>HDFS数据节点（DataNode）：cdh2-[3:8]</li>
<li>HBase主节点(HMaster)：cdh2-1,cdh2-2</li>
<li>HBase数据节点(RegionServer)：cdh2-[3:8]</li>
<li>Yarn主节点(ResourceManager)：cdh2-1,cdh2-2</li>
<li>Yarn子节点(NodeManager)：cdh2-[3:8]</li>
<li>Hive元数据服务（Metastore、HiveServer2）：cdh2-3</li>
<li>Hue服务(HueServer)：cdh2-3</li>
<li>Impala服务(CatalogServer、StateStore)：cdh2-3</li>
<li>Sentry权限验证服务（SentryServer）：cdh2-3</li>
<li>Oozie服务(OozieServer)：cdh2-3</li>
<li>Zookeeper服务(Server)：cdh2-[1:5]</li>
<li>Solr服务(SolrServer)：cdh2-4</li>
<li>Spark服务(HistoryServer)：cdh2-4</li>
<li>Kakfa服务(KafkaBroker)：cdh2-[6:8]</li>
<li>Flume(Agent)：cdh2-[6-8]</li>
</ul>
<h2 id="二、集群安装与部署">二、集群安装与部署</h2>
<h3 id="2-1-打开系统网络">2.1 打开系统网络</h3>
<p>操作系统安装初始，如果无法ping通内部服务，则检查 <strong>/etc/sysconfig/network-scripts/ifcfg-ens33</strong> 文件，确认 <strong>ONBOOT</strong> 的值如果为no需要修改为yes（Centos7.5虚拟机安装初始默认为no），否则网络无法连通。</p>
<p>手动检查各个主机上的网络设置，如果有问题则修改配置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ONBOOT&#x3D;no 改成 ONBOOT&#x3D;yes</span><br><span class="line">vim &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33</span><br><span class="line">reboot</span><br></pre></td></tr></table></figure>
<h3 id="2-2-硬盘挂载">2.2 硬盘挂载</h3>
<p>如果服务器硬盘已插入还未挂载则需要先载入硬盘：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 格式化硬盘为xfs</span><br><span class="line">mkfs -t xfs &#x2F;dev&#x2F;sdb</span><br><span class="line"></span><br><span class="line"># 硬盘挂载</span><br><span class="line"># 主节点</span><br><span class="line">mount  &#x2F;dev&#x2F;sdb  &#x2F;opt</span><br><span class="line"># 计算节点</span><br><span class="line">mount  &#x2F;dev&#x2F;sdb  &#x2F;opt&#x2F;data1</span><br><span class="line">mount  &#x2F;dev&#x2F;sdc  &#x2F;opt&#x2F;data2</span><br><span class="line"></span><br><span class="line"># 查看挂载结果</span><br><span class="line">df -HT</span><br></pre></td></tr></table></figure>
<h3 id="2-3-自动化安装">2.3 自动化安装</h3>
<p>使用自动化脚本工具进行安装操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取安装脚本，上传相关安装软件包至服务器（JDK、MySQL、CM、CDH等）</span><br><span class="line">yum install -y git</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;chubbyjiang&#x2F;cdh-deploy-robot.git</span><br><span class="line">cd cdh-deploy-robot</span><br><span class="line"></span><br><span class="line"># 编辑节点主机名</span><br><span class="line">vi hosts</span><br><span class="line"># 修改安装配置项</span><br><span class="line">vi deploy_robot.config</span><br><span class="line"># 配置ssh免密登录</span><br><span class="line">sh deploy_robot.sh init_ssh</span><br><span class="line"># 执行安装</span><br><span class="line">sh deploy_robot.sh install_all</span><br></pre></td></tr></table></figure>
<p>安装脚本将会执行 配置SSH免密登录、安装软件、操作系统优化、Java等开发环境初始化、MySQL安装、CM服务安装、操作系统性能测试等过程。</p>
<p>脚本操作说明见：<a href="https://link.zhihu.com/?target=https%3A//github.com/chubbyjiang/cdh-deploy-robot">CDH集群自动化部署工具</a> 。</p>
<p>等待cloudera-scm-server进程起来后，在浏览器输入 ip:7180 进入CM管理界面部署CDH组件。</p>
<p>第三部分将详细描述集群手动安装过程，<strong>与自动安装达成的效果一致</strong>，如已通过自动脚本完成CM服务安装可直接前往第四部分CDH部署。</p>
<h2 id="三、手动安装">三、手动安装</h2>
<p>以下操作均为Centos7.5操作系统上进行。</p>
<h3 id="3-1-系统设置">3.1 系统设置</h3>
<h3 id="主机名配置">主机名配置</h3>
<p>设置集群机器主机名，并加入各自hosts文件中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 设置各主机主机名</span><br><span class="line">hostnamectl set-hostname cdh2-1</span><br><span class="line"></span><br><span class="line"># 更新hosts文件</span><br><span class="line">echo &quot;192.168.2.1 cdh2-1&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.2 cdh2-2&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.3 cdh2-3&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.4 cdh2-4&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.5 cdh2-5&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.6 cdh2-6&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.7 cdh2-7&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line">echo &quot;192.168.2.8 cdh2-8&quot; &gt;&gt; &#x2F;etc&#x2F;hosts</span><br><span class="line"></span><br><span class="line"># 拷贝</span><br><span class="line">scp &#x2F;etc&#x2F;hosts root@cdh2-2:&#x2F;etc</span><br></pre></td></tr></table></figure>
<h3 id="SSH免密登录">SSH免密登录</h3>
<p>需要有root权限的用户（root或者sudo权限）设置免密登录。</p>
<p>在各个主机上操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成密钥</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line">cat ~&#x2F;.ssh&#x2F;id_rsa.pub &gt;&gt; ~&#x2F;.ssh&#x2F;authorized_keys</span><br><span class="line"># 拷贝密钥到登录主机</span><br><span class="line">ssh-copy-id -i ~&#x2F;.ssh&#x2F;id_rsa.pub root@cdh2-1</span><br></pre></td></tr></table></figure>
<p>从登录主机上复制到其他主机：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 同步密钥</span><br><span class="line">scp ~&#x2F;.ssh&#x2F;authorized_keys root@cdh2-2:~&#x2F;.ssh&#x2F;</span><br><span class="line"># 测试</span><br><span class="line">ssh cdh2-2 date</span><br></pre></td></tr></table></figure>
<h3 id="安装Ansible">安装Ansible</h3>
<p>安装ansible批量管理主机：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 控制机器上安装ansible</span><br><span class="line">yum install -y ansible</span><br></pre></td></tr></table></figure>
<p>安装完毕后配置修改 <strong>/etc/ansible/hosts</strong> 对需要管理的主机进行配置，默认配置需要修改编辑 <strong>/etc/ansible/ansible.cfg</strong>。</p>
<p>ansible使用配置参考 <a href="https://link.zhihu.com/?target=https%3A//docs.ansible.com/ansible/latest/installation_guide/intro_installation.html%23managed-node-requirements">Ansible官网</a>。</p>
<p>host示例配置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[except1]</span><br><span class="line">cdh2-[2:8]</span><br><span class="line"></span><br><span class="line">[all]</span><br><span class="line">cdh2-[1:8]</span><br></pre></td></tr></table></figure>
<h3 id="关闭selinux">关闭selinux</h3>
<p>查看selinux状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Enforcing 开启状态</span><br><span class="line"># Disabled 关闭状态</span><br><span class="line">ansible all -a &quot;getenforce&quot;</span><br></pre></td></tr></table></figure>
<p>修改为关闭状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在控制机器上修改内容</span><br><span class="line">vim &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">SELINUX&#x3D;disable </span><br><span class="line"># 同步到其他机器</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;selinux&#x2F;config dest&#x3D;&#x2F;etc&#x2F;selinux&#x2F;config&quot;</span><br><span class="line"># 需要重启时候启用</span><br></pre></td></tr></table></figure>
<h3 id="禁用IPv6">禁用IPv6</h3>
<p>Centos7.5默认开启IPv6，CM组件明确说明不支持系统的IPv6功能，IPv6开启状态下可能会<strong>出现不可预料的错误</strong>，需要提前关闭。</p>
<p>查看IPv6启用状态可以通过以下几种方式：</p>
<ul>
<li>ifconfig：查看是否有IPv6的地址（inet6）</li>
<li>lsmod：查看是否有ipv6关键字</li>
<li>disable_ipv6：查看/proc/sys/net/ipv6/conf/all/disable_ipv6文件内容，0为开启，1为关闭</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看IPv6当前状态，有值则为打开，空则为关闭</span><br><span class="line">ansible all -a &quot;lsmod | grep ipv6&quot;</span><br></pre></td></tr></table></figure>
<p>IPv6打开的情况下如何关闭：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 第六行添加</span><br><span class="line">vim &#x2F;etc&#x2F;default&#x2F;grub</span><br><span class="line">GRUB_CMDLINE_LUNUX&#x3D;&quot;ipv6.disable&#x3D;1 ....&quot;</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;default&#x2F;grub dest&#x3D;&#x2F;etc&#x2F;default&#x2F;grub&quot;</span><br><span class="line"># 重启</span><br><span class="line">ansible all -a &quot;reboot&quot;</span><br><span class="line"># 验证</span><br><span class="line">ansible all -a &quot;lsmod | grep ipv6&quot;</span><br></pre></td></tr></table></figure>
<h3 id="防火墙设置">防火墙设置</h3>
<p>局域网内部安全情况下最好关闭防火墙，因为CM管理组件和CDH组件有大量的端口进行通讯，需要配置很多防火墙策略。</p>
<p>需要开放的端口可参考 <a href="https://link.zhihu.com/?target=https%3A//www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_ig_ports.html">官网说明</a>，如果不能确保开放所有所需端口，则需要关闭防火墙。</p>
<p>关闭防火墙：</p>
<p><strong>firewall</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;systemctl status firewalld&quot;</span><br><span class="line">ansible all -a &quot;systemctl stop firewalld&quot;</span><br><span class="line">ansible all -a &quot;systemctl disable firewalld&quot;</span><br></pre></td></tr></table></figure>
<p><strong>iptables</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;chkconfig iptables off&quot;</span><br><span class="line">ansible all -a &quot;service iptables status&quot;</span><br><span class="line">ansible all -a &quot;service iptables stop&quot;</span><br></pre></td></tr></table></figure>
<h3 id="DNS服务器">DNS服务器</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加dns，有则略过</span><br><span class="line">echo &quot;nameserver 114.114.114.114&quot; &gt;&gt; &#x2F;etc&#x2F;resolv.conf</span><br><span class="line">echo &quot;nameserver 8.8.8.8&quot; &gt;&gt; &#x2F;etc&#x2F;resolv.conf</span><br><span class="line"># 文件同步</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;resolv.conf dest&#x3D;&#x2F;etc&#x2F;resolv.conf&quot;</span><br></pre></td></tr></table></figure>
<h3 id="NTP配置">NTP配置</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 调整时区</span><br><span class="line">ansible all -a &quot;ln -sf &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime&quot;</span><br><span class="line"># 安装ntp</span><br><span class="line">ansible all -a &quot;yum install ntp -y&quot;</span><br><span class="line"># 手动同步时间（ntpd服务关闭的情况下），避免时间差距过大导致同步失败</span><br><span class="line">ansible all -a &quot;ntpdate -u 0.cn.pool.ntp.org&quot;</span><br><span class="line"></span><br><span class="line"># 配置ntp服务器地址</span><br><span class="line">vim &#x2F;etc&#x2F;ntp.conf</span><br><span class="line"># 有外网的情况下可直接配置外部ntp服务器</span><br><span class="line">echo &quot;server ntp1.aliyun.com&quot; &gt;&gt; &#x2F;etc&#x2F;ntp.conf</span><br><span class="line"># 其他备用ntp服务器</span><br><span class="line"># server 0.pool.ntp.org</span><br><span class="line"># server 1.pool.ntp.org</span><br><span class="line"># server 2.pool.ntp.org</span><br><span class="line"># server 0.pool.ntp.org  # 有域名负载均衡</span><br><span class="line"># server 0.cn.pool.ntp.org  # 有域名负载均衡</span><br><span class="line"># server ntp.tuna.tsinghua.edu.cn # 清华大学</span><br><span class="line"></span><br><span class="line"># 启动ntp</span><br><span class="line">ansible all -a &quot;systemctl start ntpd&quot;</span><br><span class="line"># 开机启动</span><br><span class="line">ansible all -a &quot;systemctl enable ntpd&quot;</span><br><span class="line"></span><br><span class="line"># 查看系统硬件时间</span><br><span class="line">hwclock --systohc</span><br></pre></td></tr></table></figure>
<p><strong>附：NTP内网服务器搭建</strong></p>
<p>配置内网NTP-Server(管理节点)。</p>
<p>ntp.conf配置文件内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#新增：日志文件</span><br><span class="line">logfile &#x2F;var&#x2F;log&#x2F;ntpd.log</span><br><span class="line">restrict default kod nomodify notrap nopeer noquery</span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict ::1</span><br><span class="line"></span><br><span class="line">#授权192.168.1.0网段上所有机器可以从这台机器上查询和时间同步</span><br><span class="line">restrict 192.168.0.0 mask 255.255.0.0 nomodify notrap</span><br><span class="line"></span><br><span class="line">#新增：时间服务器列表</span><br><span class="line">#server 0.cn.pool.ntp.org iburst</span><br><span class="line">#server 1.cn.pool.ntp.org iburst</span><br><span class="line">#server 2.cn.pool.ntp.org iburst</span><br><span class="line">#server 3.cn.pool.ntp.org iburst</span><br><span class="line"></span><br><span class="line">#新增：当外部时间不可用时，使用本地时间</span><br><span class="line">server 127.127.1.0 biurst</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br><span class="line"></span><br><span class="line">#新增：允许上层时间服务器主动修改本机时间</span><br><span class="line">#restrict 0.cn.pool.ntp.org nomodify notrap noquery</span><br><span class="line">#restrict 1.cn.pool.ntp.org nomodify notrap noquery</span><br><span class="line">#restrict 2.cn.pool.ntp.org nomodify notrap noquery</span><br><span class="line"></span><br><span class="line">includefile &#x2F;etc&#x2F;ntp&#x2F;crypto&#x2F;pw</span><br><span class="line">keys &#x2F;etc&#x2F;ntp&#x2F;keys</span><br><span class="line">disable monitor</span><br></pre></td></tr></table></figure>
<h3 id="3-2-软件包安装">3.2 软件包安装</h3>
<h3 id="修改yum源">修改yum源</h3>
<p>默认国外的yum源下载速度缓慢，替换为国内阿里云的yum源。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 备份</span><br><span class="line">ansible all -a &quot;mv &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo.backup&quot;</span><br><span class="line"># 下载</span><br><span class="line">ansible all -a &quot;wget -O &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo&quot;</span><br><span class="line"># 更新</span><br><span class="line">ansible all -m shell -a &quot;yum clean all &amp;&amp; yum makecache&quot;</span><br><span class="line">ansible all -a &quot;yum -y update&quot;</span><br></pre></td></tr></table></figure>
<h3 id="系统软件安装">系统软件安装</h3>
<p>安装后续需要用到的系统软件，以备日后服务器无外网无法下载的情况。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 包括不限于</span><br><span class="line">ansible all -a &quot;yum install -y expect bc net-tools iotop zip unzip telnet wget iperf3 fio ntfs-3g lzo iftop vim&quot;</span><br></pre></td></tr></table></figure>
<h3 id="JDK与Scala">JDK与Scala</h3>
<p>Java安装</p>
<p>JDK下载地址：<a href="https://link.zhihu.com/?target=https%3A//archive.cloudera.com/cm6/">Cloudera Archive CM</a>， 根据对应的cm版本选择下载。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以jdk1.8示例</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;tmp&#x2F;oracle-j2sdk1.8 dest&#x3D;&#x2F;tmp&#x2F;oracle-j2sdk1.8&quot;</span><br><span class="line">ansible all -m shell -a &quot;yum localinstall -y oracle-j2sdk1.8 &amp;&amp; ln -s &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_141-cloudera &#x2F;usr&#x2F;java&#x2F;default&quot;</span><br></pre></td></tr></table></figure>
<p>Scala安装</p>
<p>下载地址：<a href="https://link.zhihu.com/?target=https%3A//www.scala-lang.org/download/all.html">Scala</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 以2.11.8示例</span><br><span class="line">ansible all -m shell -a &quot;rm -rf &#x2F;usr&#x2F;scala &amp;&amp; mkdir -p &#x2F;usr&#x2F;scala&quot;</span><br><span class="line">cp &#x2F;tmp&#x2F;scala-2.11.8.tgz &#x2F;usr&#x2F;scala</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;usr&#x2F;scala&#x2F;scala-2.11.8.tgz dest&#x3D;&#x2F;usr&#x2F;scala&#x2F;scala-2.11.8.tgz&quot;</span><br><span class="line">ansible all -m shell -a &quot;cd &#x2F;usr&#x2F;scala &amp;&amp; tar -zxvf scala-2.11.8.tgz &amp;&amp; rm -rf scala-2.11.8.tgz&quot;</span><br></pre></td></tr></table></figure>
<p>配置环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;echo JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_141-cloudera &gt;&gt; &#x2F;etc&#x2F;profile&quot;</span><br><span class="line">ansible all -a &quot;echo SCALA_HOME&#x3D;&#x2F;usr&#x2F;scala&#x2F;scala-2.11.8 &gt;&gt; &#x2F;etc&#x2F;profile&quot;</span><br><span class="line">ansible all -a &quot;echo CLASSPATH&#x3D;$JAVA_HOME&#x2F;bin:$SCALA_HOME&#x2F;bin &gt;&gt; &#x2F;etc&#x2F;profile&quot;</span><br><span class="line">ansible all -a &quot;echo export PATH&#x3D;$JAVA_HOME:$SCALA_HOME:$CLASSPATH:$PATH &gt;&gt; &#x2F;etc&#x2F;profile&quot;</span><br><span class="line">ansible all -a &quot;source &#x2F;etc&#x2F;profile&quot;</span><br></pre></td></tr></table></figure>
<h3 id="Python与Python包">Python与Python包</h3>
<p>Centos7自带python2.7，Centos6自带python2.6需要升级。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -V</span><br><span class="line"># Python 2.7.5</span><br></pre></td></tr></table></figure>
<p>安装python3.6</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 依赖包安装</span><br><span class="line">ansible all -a &quot;yum install -y openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel gcc-c++ python36-devel cyrus-sasl-lib.x86_64 cyrus-sasl-devel.x86_64 libgsasl-devel.x86_64 epel-release&quot;</span><br><span class="line"># yum源下载</span><br><span class="line">ansible all -a &quot;yum install https:&#x2F;&#x2F;centos7.iuscommunity.org&#x2F;ius-release.rpm -y&quot;</span><br><span class="line"># 安装python3.6</span><br><span class="line">ansible all -a &quot;yum install python36 -y&quot;</span><br><span class="line"># 安装setuptools</span><br><span class="line">ansible all -a &quot;wget -P &#x2F;tmp --no-check-certificate https:&#x2F;&#x2F;pypi.python.org&#x2F;packages&#x2F;source&#x2F;s&#x2F;setuptools&#x2F;setuptools-19.6.tar.gz#md5&#x3D;c607dd118eae682c44ed146367a17e26&quot;</span><br><span class="line">ansible all -a &quot;tar -zxvf &#x2F;tmp&#x2F;setuptools-19.6.tar.gz&quot;</span><br><span class="line">ansible all -m shell -a &quot;cd &#x2F;tmp&#x2F;setuptools-19.6 &amp;&amp; python3.6 setup.py build &amp;&amp; python3.6 setup.py install&quot;</span><br><span class="line"># 安装pip3.6</span><br><span class="line">ansible all -a &quot;wget -P &#x2F;tmp --no-check-certificate https:&#x2F;&#x2F;pypi.python.org&#x2F;packages&#x2F;source&#x2F;p&#x2F;pip&#x2F;pip-8.0.2.tar.gz#md5&#x3D;3a73c4188f8dbad6a1e6f6d44d117eeb&quot;</span><br><span class="line">ansible all -a &quot;tar -zxvf &#x2F;tmp&#x2F;pip-8.0.2.tar.gz&quot;</span><br><span class="line">ansible all -m shell -a &quot;cd &#x2F;tmp&#x2F;pip-8.0.2 &amp;&amp; python3.6 setup.py build &amp;&amp; python3.6 setup.py install&quot;</span><br></pre></td></tr></table></figure>
<p>修改pip源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;mkdir ~&#x2F;.pip&quot;</span><br><span class="line">vim ~&#x2F;.pip&#x2F;pip.conf</span><br><span class="line"></span><br><span class="line"># 内容修改如下</span><br><span class="line">[global]</span><br><span class="line">index-url &#x3D; https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br><span class="line"></span><br><span class="line"># 文件同步</span><br><span class="line">ansibe all -m copy -a &quot;src&#x3D;&#x2F;root&#x2F;.pip&#x2F;pip.conf dest&#x3D;&#x2F;root&#x2F;.pip&#x2F;pip.conf&quot;</span><br><span class="line"></span><br><span class="line"># 更新</span><br><span class="line">ansible all -m shell -a &quot;pip3.6 install --upgrade setuptools &amp;&amp; easy_install-3.6 -U setuptools &amp;&amp; pip3.6 install --upgrade pip&quot;</span><br></pre></td></tr></table></figure>
<p>安装python所需依赖包：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansibe all -m copy -a &quot;src&#x3D;&#x2F;tmp&#x2F;requirements.txt dest&#x3D;&#x2F;tmp&#x2F;requirements.txt&quot;</span><br><span class="line">ansible all -a &quot;pip3.6 install -r &#x2F;tmp&#x2F;requirements.txt&quot;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-其他系统设置">3.3 其他系统设置</h3>
<h3 id="中文乱码">中文乱码</h3>
<p>安装操作系统时选择了中文语言，使用时发现部分中文会有乱码的情况，解决方案如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;locale.conf</span><br><span class="line"># 内容</span><br><span class="line">LANG&#x3D;zh_CN.UTF-8</span><br><span class="line">LC_CTYPE&#x3D;zh_CN.UTF-8</span><br><span class="line">LC_NUMERIC&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_TIME&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_COLLATE&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_MONETARY&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_MESSAGES&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_PAPER&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_NAME&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_ADDRESS&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_TELEPHONE&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_MEASUREMENT&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_IDENTIFICATION&#x3D;&quot;zh_CN.UTF-8&quot;</span><br><span class="line">LC_ALL&#x3D;</span><br><span class="line"></span><br><span class="line"># 更新</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;locale.conf dest&#x3D;&#x2F;etc&#x2F;locale.conf&quot;</span><br><span class="line">ansible all -a &quot;source &#x2F;etc&#x2F;locale.conf&quot;</span><br></pre></td></tr></table></figure>
<h3 id="tuned">tuned</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;systemctl start tuned&quot;</span><br><span class="line">ansible all -a &quot;systemctl status tuned&quot;</span><br><span class="line"># 显示No current active profile</span><br><span class="line">ansible all -a &quot;tuned-adm off&quot;</span><br><span class="line">ansible all -a &quot;tuned-adm list&quot;</span><br><span class="line"># 关闭tuned服务</span><br><span class="line">ansible all -a &quot;systemctl stop tuned&quot;</span><br><span class="line">ansible all -a &quot;systemctl disable tuned&quot;</span><br></pre></td></tr></table></figure>
<h3 id="大页面关闭">大页面关闭</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 输出[always] never意味着THP已启用，always [never]意味着THP未启用</span><br><span class="line">ansible all -a &quot;cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled&quot;</span><br><span class="line">ansible all -a &quot;cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag&quot;</span><br><span class="line"></span><br><span class="line"># 关闭</span><br><span class="line">ansible all -m shell -a &quot;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled&quot;</span><br><span class="line">ansible all -m shell -a &quot;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag&quot;</span><br><span class="line"># 设置开机关闭</span><br><span class="line">echo &quot;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag&quot; &gt;&gt; &#x2F;etc&#x2F;rc.local</span><br><span class="line">echo &quot;echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled&quot; &gt;&gt; &#x2F;etc&#x2F;rc.local</span><br><span class="line">chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br><span class="line"></span><br><span class="line"># 在GRUB_CMDLINE_LINUX项目后面添加一个参数：transparent_hugepage&#x3D;never</span><br><span class="line">vim &#x2F;etc&#x2F;default&#x2F;grub</span><br><span class="line">ansile all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;default&#x2F;grub dest&#x3D;&#x2F;etc&#x2F;default&#x2F;grub&quot;</span><br><span class="line"># 重新生成gurb.cfg文件</span><br><span class="line">ansible all -a &quot;grub2-mkconfig -o &#x2F;boot&#x2F;grub2&#x2F;grub.cfg&quot;</span><br></pre></td></tr></table></figure>
<h3 id="swappiness">swappiness</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;swappiness&quot;</span><br><span class="line">ansible all -a &quot;sysctl -w vm.swappiness&#x3D;1&quot;</span><br><span class="line">echo &quot;vm.swappiness&#x3D;1&quot; &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;sysctl.conf dest&#x3D;&#x2F;etc&#x2F;sysctl.conf&quot;</span><br></pre></td></tr></table></figure>
<h3 id="会话超时">会话超时</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;TMOUT&#x3D;900&quot;&gt;&gt;&#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>
<h3 id="内核优化">内核优化</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo -e &quot;\nnet.ipv4.tcp_tw_reuse &#x3D; 1</span><br><span class="line">\nnet.ipv4.tcp_tw_recycle &#x3D; 1</span><br><span class="line">\nnet.ipv4.tcp_keepalive_time &#x3D; 1200</span><br><span class="line">\nnet.ipv4.ip_local_port_range &#x3D; 10000 65000</span><br><span class="line">\nnet.ipv4.tcp_max_syn_backlog &#x3D; 8192</span><br><span class="line">\nnet.ipv4.tcp_max_tw_buckets &#x3D; 5000</span><br><span class="line">\nfs.file-max &#x3D; 655350</span><br><span class="line">\nnet.ipv4.route.gc_timeout &#x3D; 100</span><br><span class="line">\nnet.ipv4.tcp_syn_retries &#x3D; 1</span><br><span class="line">\nnet.ipv4.tcp_synack_retries &#x3D; 1</span><br><span class="line">\nnet.core.netdev_max_backlog &#x3D; 16384</span><br><span class="line">\nnet.ipv4.tcp_max_orphans &#x3D; 16384</span><br><span class="line">\nnet.ipv4.tcp_fin_timeout &#x3D; 2</span><br><span class="line">\net.core.somaxconn&#x3D;32768</span><br><span class="line">\kernel.threads-max&#x3D;196605</span><br><span class="line">\kernel.pid_max&#x3D;196605</span><br><span class="line">\vm.max_map_count&#x3D;393210&quot;  &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br></pre></td></tr></table></figure>
<h3 id="最大打开文件数">最大打开文件数</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ulimit -a</span><br><span class="line">sed -i &#39;$ a\* soft nofile 196605&#39; &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br><span class="line">sed -i &#39;$ a\* hard nofile 196605&#39; &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br><span class="line">echo &quot;* soft nproc 196605&quot; &gt;&gt; &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br><span class="line">echo &quot;* hard nproc 196605&quot; &gt;&gt; &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br></pre></td></tr></table></figure>
<h3 id="3-4-系统性能测试">3.4 系统性能测试</h3>
<h3 id="网络测试">网络测试</h3>
<p>使用iperf测试主机之间的网络传输效率。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在一个主机上启动iperf 服务端</span><br><span class="line">iperf3 -s -p 12345 -i 1</span><br><span class="line"># 另外一个主机启动iperf 客户端连接服务端</span><br><span class="line">iperf3 -c cdh85-19 -p 12345 -i 1 -t 10 -w 100K</span><br></pre></td></tr></table></figure>
<h3 id="磁盘IO测试">磁盘IO测试</h3>
<p>使用fio工具对io进行各个场景的读写性能测试。</p>
<p><strong>随机读</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fio -filename&#x3D;&#x2F;dev&#x2F;sda -direct&#x3D;1 -iodepth 1 -thread -rw&#x3D;randread -ioengine&#x3D;psync -bs&#x3D;4k -size&#x3D;60G -numjobs&#x3D;64 -runtime&#x3D;10 -group_reporting -name&#x3D;file -allow_mounted_write&#x3D;1</span><br></pre></td></tr></table></figure>
<p><strong>顺序读</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fio -filename&#x3D;&#x2F;dev&#x2F;sda -direct&#x3D;1 -iodepth 1 -thread -rw&#x3D;read -ioengine&#x3D;psync -bs&#x3D;4k -size&#x3D;60G -numjobs&#x3D;64 -runtime&#x3D;10 -group_reporting -name&#x3D;file -allow_mounted_write&#x3D;1</span><br></pre></td></tr></table></figure>
<p><strong>随机写</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fio -filename&#x3D;&#x2F;dev&#x2F;sda -direct&#x3D;1 -iodepth 1 -thread -rw&#x3D;randwrite -ioengine&#x3D;psync -bs&#x3D;4k -size&#x3D;60G -numjobs&#x3D;64 -runtime&#x3D;10 -group_reporting -name&#x3D;file -allow_mounted_write&#x3D;1</span><br></pre></td></tr></table></figure>
<p><strong>顺序写</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fio -filename&#x3D;&#x2F;dev&#x2F;sda -direct&#x3D;1 -iodepth 1 -thread -rw&#x3D;write -ioengine&#x3D;psync -bs&#x3D;4k -size&#x3D;60G -numjobs&#x3D;64 -runtime&#x3D;10 -group_reporting -name&#x3D;file -allow_mounted_write&#x3D;1</span><br></pre></td></tr></table></figure>
<p><strong>混合随机读写</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fio -filename&#x3D;&#x2F;dev&#x2F;sda -direct&#x3D;1 -iodepth 1 -thread -rw&#x3D;randrw -rwmixread&#x3D;30 -ioengine&#x3D;psync -bs&#x3D;4k -size&#x3D;60G -numjobs&#x3D;64 -runtime&#x3D;10 -group_reporting -name&#x3D;file -ioscheduler&#x3D;noop -allow_mounted_write&#x3D;1</span><br></pre></td></tr></table></figure>
<p>todo:</p>
<ul>
<li>内存性能测试</li>
<li>操作系统性能测试</li>
</ul>
<h3 id="3-5-MySQL数据库">3.5 MySQL数据库</h3>
<p>卸载已有mariadb数据库。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -qa|grep -i mariadb</span><br><span class="line">rpm -e mariadb-libs-5.5.60-1.el7_5.x86_64 --nodeps</span><br></pre></td></tr></table></figure>
<p>下载mysql安装包。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mysql官网下载bundle安装包并上传至服务器</span><br><span class="line"></span><br><span class="line"># 安装</span><br><span class="line">rpm -ivh &#x2F;tmp&#x2F;mysql-community-common-5.7.20-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh &#x2F;tmp&#x2F;mysql-community-libs-5.7.20-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh &#x2F;tmp&#x2F;mysql-community-client-5.7.20-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh &#x2F;tmp&#x2F;mysql-community-server-5.7.20-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>创建MySQL数据目录（非默认盘）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;opt&#x2F;mysql&#x2F;data</span><br></pre></td></tr></table></figure>
<p>修改mysql配置文件内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;my.conf</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"># datadir修改为&#x2F;opt&#x2F;mysql&#x2F;data</span><br><span class="line">datadir&#x3D;&#x2F;opt&#x2F;mysql&#x2F;data</span><br><span class="line">socket&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql.sock</span><br><span class="line"></span><br><span class="line"># set the isolation level to READ-COMMITTED</span><br><span class="line">transaction-isolation &#x3D; READ-COMMITTED</span><br><span class="line"></span><br><span class="line"># Disabling symbolic-links is recommended to prevent assorted security risks;</span><br><span class="line"># to do so, uncomment this line:</span><br><span class="line">symbolic-links &#x3D; 0</span><br><span class="line"></span><br><span class="line">key_buffer_size &#x3D; 32M</span><br><span class="line">max_allowed_packet &#x3D; 32M</span><br><span class="line">thread_stack &#x3D; 256K</span><br><span class="line">thread_cache_size &#x3D; 64</span><br><span class="line">query_cache_limit &#x3D; 8M</span><br><span class="line">query_cache_size &#x3D; 64M</span><br><span class="line">query_cache_type &#x3D; 1</span><br><span class="line"></span><br><span class="line"># max_connections - Allow 100 maximum connections for each database and then add 50 extra connections. </span><br><span class="line">max_connections &#x3D; 550</span><br><span class="line">#expire_logs_days &#x3D; 10</span><br><span class="line">#max_binlog_size &#x3D; 100M</span><br><span class="line"></span><br><span class="line">#log_bin should be on a disk with enough free space.</span><br><span class="line">#Replace &#39;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql_binary_log&#39; with an appropriate path for your</span><br><span class="line">#system and chown the specified folder to the mysql user.</span><br><span class="line"># 修改binlog存储路径</span><br><span class="line">log_bin&#x3D;&#x2F;opt&#x2F;mysql&#x2F;binlog&#x2F;mysql_binary_log</span><br><span class="line"></span><br><span class="line">#In later versions of MySQL, if you enable the binary log and do not set</span><br><span class="line">#a server_id, MySQL will not start. The server_id must be unique within</span><br><span class="line">#the replicating group.</span><br><span class="line">server_id&#x3D;1</span><br><span class="line"></span><br><span class="line">binlog_format &#x3D; mixed</span><br><span class="line"></span><br><span class="line">read_buffer_size &#x3D; 2M</span><br><span class="line">read_rnd_buffer_size &#x3D; 16M</span><br><span class="line">sort_buffer_size &#x3D; 8M</span><br><span class="line">join_buffer_size &#x3D; 8M</span><br><span class="line"></span><br><span class="line"># InnoDB settings</span><br><span class="line">innodb_file_per_table &#x3D; 1</span><br><span class="line">innodb_flush_log_at_trx_commit  &#x3D; 2</span><br><span class="line">innodb_log_buffer_size &#x3D; 64M</span><br><span class="line">innodb_buffer_pool_size &#x3D; 4G</span><br><span class="line">innodb_thread_concurrency &#x3D; 8</span><br><span class="line"># set the innodb_flush_method property to O_DIRECT.</span><br><span class="line">innodb_flush_method &#x3D; O_DIRECT</span><br><span class="line">innodb_log_file_size &#x3D; 512M</span><br><span class="line"></span><br><span class="line">[mysqld_safe]</span><br><span class="line">log-error&#x3D;&#x2F;var&#x2F;log&#x2F;mysqld.log</span><br><span class="line">pid-file&#x3D;&#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.pid</span><br><span class="line"></span><br><span class="line">sql_mode&#x3D;STRICT_ALL_TABLES</span><br></pre></td></tr></table></figure>
<p>备份文件logfile文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;ib_logfile0 &#x2F;tmp&#x2F;hadoop&#x2F;ib_logfile0.ba</span><br><span class="line">cp &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;ib_logfile1 &#x2F;tmp&#x2F;hadoop&#x2F;ib_logfile1.ba</span><br></pre></td></tr></table></figure>
<p>安装mysql驱动</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 官网下载mysql驱动并上传至服务器</span><br><span class="line"># Installing the MySQL JDBC Driver</span><br><span class="line">tar zxvf &#x2F;tmp&#x2F;mysql-connector-java-5.1.46.tar.gz</span><br><span class="line"></span><br><span class="line">ansible all -a &quot;mkdir -p &#x2F;usr&#x2F;share&#x2F;java&#x2F;&quot;</span><br><span class="line"># 注意去掉版本号，否则cm无法使用</span><br><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;tmp&#x2F;mysql-connector-java-5.1.46&#x2F;mysql-connector-java-5.1.46-bin.jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;usr&#x2F;share&#x2F;java&#x2F;mysql-connector-java.jar&quot;</span><br><span class="line">rm -rf &#x2F;tmp&#x2F;mysql-connector-java-5.1.46*</span><br></pre></td></tr></table></figure>
<p>启动mysql服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start mysqld</span><br><span class="line">systemctl status mysqld</span><br><span class="line">systemctl enable mysqld</span><br></pre></td></tr></table></figure>
<p>设置mysql账号密码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 初始密码</span><br><span class="line">grep &quot;temporary password&quot; &#x2F;var&#x2F;log&#x2F;mysqld.log</span><br><span class="line"># 重置</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;mysql_secure_installation</span><br></pre></td></tr></table></figure>
<p>mysql5.7以上强制密码策略不满足可以通过以下方式修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 默认1，中等强度策略</span><br><span class="line">mysql -u root -p&quot;$init_passwd&quot; --connect-expired-password -e &quot;set global validate_password_policy&#x3D;0&quot;</span><br><span class="line"># 默认8长度</span><br><span class="line">mysql -u root -p&quot;$init_passwd&quot; --connect-expired-password -e &quot;set global validate_password_length&#x3D;1&quot;</span><br></pre></td></tr></table></figure>
<p>进入mysql并创建数据库：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"></span><br><span class="line"># Configure the Cloudera Manager Server, Activity Monitor, Reports Manager, Cloudera Navigator Audit Server, and Cloudera Navigator Metadata Server databases to support the utf8mb4 character set encoding.</span><br><span class="line"># Configure all other databases to use the utf8 character set.</span><br><span class="line">CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON scm.* TO &#39;scm&#39;@&#39;%&#39; IDENTIFIED BY &#39;scm@DW&#39;;</span><br><span class="line">CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON amon.* TO &#39;amon&#39;@&#39;%&#39; IDENTIFIED BY &#39;amon@DW&#39;;</span><br><span class="line">CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON rman.* TO &#39;rman&#39;@&#39;%&#39; IDENTIFIED BY &#39;rman@DW&#39;;</span><br><span class="line">CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON hue.* TO &#39;hue&#39;@&#39;%&#39; IDENTIFIED BY &#39;hue@DW&#39;;</span><br><span class="line">CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON metastore.* TO &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;hive@DW&#39;;</span><br><span class="line">CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON sentry.* TO &#39;sentry&#39;@&#39;%&#39; IDENTIFIED BY &#39;sentry@DW&#39;;</span><br><span class="line">CREATE DATABASE nav DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON nav.* TO &#39;nav&#39;@&#39;%&#39; IDENTIFIED BY &#39;nav@DW&#39;;</span><br><span class="line">CREATE DATABASE navms DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON navms.* TO &#39;navms&#39;@&#39;%&#39; IDENTIFIED BY &#39;navms@DW&#39;;</span><br><span class="line">CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON oozie.* TO &#39;oozie&#39;@&#39;%&#39; IDENTIFIED BY &#39;oozie@DW&#39;;</span><br><span class="line">SHOW DATABASES;</span><br></pre></td></tr></table></figure>
<p><strong>注意事项：</strong></p>
<p>在大型集群中Activity Monitor 与 Service Monitor 使用的数据库应该分配不同的磁盘卷来进行读写。</p>
<p>在超过50个节点的集群中，不要将所有服务的数据库装在一个节点中，否则该节点的数据库压力会很大。最好能为每个服务配置不同位于不同节点上的数据库。</p>
<p>不需要使用专门的数据库服务器，但是每个服务的数据库应该分散在不同的节点上。</p>
<p>如果集群节点超过1000个，将mysql的max_allowed_packet值设置为16M。</p>
<p>For MySQL 5.6 and 5.7, you must install the MySQL-shared-compat or MySQL-shared package. This is required for the Cloudera Manager Agent package installation.</p>
<h3 id="3-6-安装Cloudera-Manager服务">3.6 安装Cloudera Manager服务</h3>
<p>本次过程<strong>不启用auto-ssl。</strong></p>
<h3 id="安装CM软件包">安装CM软件包</h3>
<p>创建免密root权限用户</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;etc&#x2F;sudoers</span><br><span class="line">cloudera-scm    ALL&#x3D;(ALL)    NOPASSWD:ALL </span><br><span class="line"></span><br><span class="line"># 同步</span><br><span class="line">ansible except1 -m copy -a &quot;src&#x3D;&#x2F;etc&#x2F;sudoers dest&#x3D;&#x2F;etc&#x2F;sudoers&quot;</span><br></pre></td></tr></table></figure>
<p>从 <a href="https://link.zhihu.com/?target=https%3A//archive.cloudera.com/cm6/">这里</a> 下载rpm离线安装包，所需文件及软件列表如下（以6.1版本为例）：</p>
<ul>
<li>allkeys.asc</li>
<li>cloudera-manager.repo</li>
<li>oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm</li>
<li>cloudera-manager-server-db-2-6.1.0-853290.el7.x86_64.rpm</li>
<li>cloudera-manager-server-6.1.0-853290.el7.x86_64.rpm</li>
<li>cloudera-manager-daemons-6.1.0-853290.el7.x86_64.rpm</li>
<li>cloudera-manager-agent-6.1.0-853290.el7.x86_64.rpm</li>
</ul>
<p>上传并安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 主节点安装所有服务</span><br><span class="line">rpm -ivh --force --nodeps &#x2F;tmp&#x2F;cm&#x2F;*.rpm</span><br><span class="line"># 子节点安装daemons和agent</span><br><span class="line">rpm -ivh --force --nodeps &#x2F;tmp&#x2F;cm&#x2F;daemons*.rpm</span><br><span class="line">rpm -ivh --force --nodeps &#x2F;tmp&#x2F;cm&#x2F;agent*.rpm</span><br></pre></td></tr></table></figure>
<p><strong>附：yum方式安装</strong></p>
<p>安装yum源</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -m copy -a &quot;src&#x3D;&#x2F;tmp&#x2F;cm&#x2F;cloudera-manager.repo dest&#x3D;&#x2F;etc&#x2F;yum.repos.d&quot;</span><br><span class="line">ansible all -m shell -a &quot;yum clean all &amp;&amp; yum makecache&quot;</span><br><span class="line"></span><br><span class="line"># 将会下载很多依赖包，需要联网</span><br><span class="line">ansible all -m shell -a &quot;yum localinstall cloudera-manager-daemons-6.1.0-769885.el7.x86_64.rpm cloudera-manager-agent-6.1.0-769885.el7.x86_64.rpm cloudera-manager-server-6.1.0-769885.el7.x86_64.rpm&quot;</span><br></pre></td></tr></table></figure>
<h3 id="CDH-parcel包">CDH parcel包</h3>
<p>在线安装耗时太长且不稳定，本次只考虑离线安装方式。</p>
<p>从 <a href="https://link.zhihu.com/?target=https%3A//archive.cloudera.com/cdh6/">这里</a> 下载以下文件及软件：</p>
<ul>
<li>manifest.json</li>
<li>CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel</li>
</ul>
<p>创建安装路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo</span><br><span class="line">mv &#x2F;tmp&#x2F;cm&#x2F;CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo</span><br><span class="line">mv &#x2F;tmp&#x2F;cm&#x2F;manifest.json &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo</span><br><span class="line"># 创建签名文件</span><br><span class="line">cd &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo &amp;&amp; sha1sum CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel | awk &#39;&#123; print $1 &#125;&#39; &gt; CDH-6.1.0-1.cdh6.1.0.p0.770702-el7.parcel.sha</span><br></pre></td></tr></table></figure>
<h3 id="初始化Cloudera-Manager">初始化Cloudera Manager</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cm数据库配置</span><br><span class="line">cat &#x2F;etc&#x2F;cloudera-scm-server&#x2F;db.properties</span><br><span class="line"># 初始化数据库</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;schema&#x2F;scm_prepare_database.sh -h cdh2-3 mysql scm scm</span><br><span class="line"># 启动server</span><br><span class="line">systemctl start cloudera-scm-server</span><br><span class="line"># 查看日志</span><br><span class="line">tail -f &#x2F;var&#x2F;log&#x2F;cloudera-scm-server&#x2F;cloudera-scm-server.log</span><br><span class="line"></span><br><span class="line"># 各个节点上启动agent</span><br><span class="line">ansible all -a &quot;systemctl start cloudera-scm-agent&quot;</span><br></pre></td></tr></table></figure>
<h3 id="3-7-附：卸载Cloudera-Manager服务">3.7 附：卸载Cloudera Manager服务</h3>
<p>删除mysql数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">drop database scm;</span><br><span class="line">drop database amon;</span><br><span class="line">drop database rman;</span><br><span class="line">drop database hue;</span><br><span class="line">drop database metastore;</span><br><span class="line">drop database sentry;</span><br><span class="line">drop database nav;</span><br><span class="line">drop database navms;</span><br><span class="line">drop database oozie;</span><br></pre></td></tr></table></figure>
<p>卸载软件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -a &quot;yum -y remove cloudera-manager-*&quot;</span><br><span class="line">ansible all -a &quot;umount &#x2F;var&#x2F;run&#x2F;cloudera-scm-agent&#x2F;process&quot;</span><br><span class="line"># 删除数据目录</span><br><span class="line">ansible all -a &quot;rm -Rf &#x2F;var&#x2F;lib&#x2F;cloudera* &#x2F;var&#x2F;log&#x2F;cloudera* &#x2F;var&#x2F;run&#x2F;cloudera* &#x2F;etc&#x2F;cloudera* &#x2F;tmp&#x2F;.scm_prepare_node.lock&quot;</span><br></pre></td></tr></table></figure>
<h2 id="四、CDH组件部署">四、CDH组件部署</h2>
<h3 id="4-1-界面安装">4.1 界面安装</h3>
<p>进入Web界面，根据节点角色划分安装CDH组件，并启动集群。</p>
<p>正常启动集群后需要根据业务与应用场景对各个组件的配置属性进行调优工作。</p>
<h3 id="4-2-组件配置调优">4.2 组件配置调优</h3>
<p>如果对组件配置调优无从下手，可以参考以下配置的调整。</p>
<h3 id="HBase">HBase</h3>
<p>根据使用的业务场景不同，HBase配置优化选项有很大的差异。</p>
<p>有关HBase的性能、配置优化与配置项计算说明可以参考：<a href="https://zhuanlan.zhihu.com/p/72150364" target="_blank" rel="noopener">HBase最佳实践</a> - 「性能优化」小节。</p>
<h3 id="Yarn">Yarn</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 开启压缩</span><br><span class="line">mapreduce.output.fileoutputformat.compress&#x3D;已启用</span><br><span class="line">mapreduce.output.fileoutputformat.compress.type&#x3D;BLOCK</span><br><span class="line">mapreduce.output.fileoutputformat.compress.codec&#x3D;org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">mapreduce.map.output.compress.codec&#x3D;org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">mapreduce.map.output.compress&#x3D;已启用</span><br><span class="line">zlib.compress.level&#x3D;DEFAULT_COMPRESSION</span><br><span class="line"></span><br><span class="line"># 设置资源队列</span><br><span class="line">yarn.nodemanager.resource.memory-mb：每台主机上能够被Yarn使用的内存大小</span><br><span class="line">yarn.app.mapreduce.am.resource.cpu-vcores：每台主机上能够被Yarn使用的CPU核心数</span><br><span class="line">yarn.scheduler.minimum-allocation-mb：Container最小申请的内存大小</span><br><span class="line">yarn.scheduler.maximum-allocation-mb：Container最大可申请的内存大小</span><br><span class="line"></span><br><span class="line"># Service Monitor 客户端配置替代</span><br><span class="line">&lt;property&gt;&lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;&#x2F;name&gt;&lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;&#x2F;name&gt;&lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;io.compression.codecs&lt;&#x2F;name&gt;&lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line"># YARN 服务 MapReduce 高级配置代码段（安全阀）</span><br><span class="line">&lt;property&gt;&lt;name&gt;mapreduce.map.output.compress&lt;&#x2F;name&gt;&lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;mapred.map.output.compress.codec&lt;&#x2F;name&gt;&lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line"># 以上用户需要包含在yarn acl控制的用户中，最好和hue超级用户一致（可以直接访问hdfs页面的路径，但是允许所有人访问，有风险）</span><br><span class="line">&lt;property&gt;&lt;name&gt;hadoop.http.staticuser.user&lt;&#x2F;name&gt;&lt;value&gt;yarn&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>资源行管参数细节解释说明：</p>
<ul>
<li>AM参数mapreduce.map.memory.mb=1536MB，表示AM要为map Container申请1536MB资源，但RM实际分配的内存却是2048MB，因为yarn.scheduler.mininum-allocation-mb=1024MB，这定义了RM最小要分配1024MB，1536MB超过了这个值，所以实际分配给AM的值为2048MB。</li>
<li>AM参数mapreduce.map.java.opts=-Xmx 1024m，表示运行map任务的jvm内存为1024MB,因为map任务要运行在Container里面，所以这个参数的值略微小于mapreduce.map.memory.mb=1536MB这个值。</li>
<li>NM参数yarn.nodemanager.vmem-pmem-radio=2.1,这表示NodeManager可以分配给map/reduce Container 2.1倍的虚拟内存，安照上面的配置，实际分配给map Container容器的虚拟内存大小为2048 * 2.1=3225.6MB，若实际用到的内存超过这个值，NM就会kill掉这个map Container,任务执行过程就会出现异常。</li>
<li>AM参数mapreduce.reduce.memory.mb=3072MB，表示分配给reduce Container的容器大小为3072MB,而map Container的大小分配的是1536MB，从这也看出，reduce Container容器的大小最好是map Container大小的两倍。</li>
<li>NM参数yarn.nodemanager.resource.mem.mb=24576MB,这个值表示节点分配给NodeManager的可用内存，也就是节点用来执行yarn任务的内存大小。这个值要根据实际服务器内存大小来配置，比如我们hadoop集群机器内存是128GB，我们可以分配其中的80%给yarn，也就是102GB。</li>
</ul>
<p><strong>Yarn动态资源队列设置</strong></p>
<p>在Cloudera Manager界面点击集群选项卡，进入动态资源池配置，设置任务队列、队列资源、访问控制等信息。</p>
<h3 id="Spark">Spark</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># spark-conf&#x2F;spark-defaults.conf 的 Spark 客户端高级配置代码段（安全阀）</span><br><span class="line">spark.driver.extraJavaOptions&#x3D;-Dfile.encoding&#x3D;UTF-8</span><br><span class="line">spark.executor.extraJavaOptions&#x3D;-Dfile.encoding&#x3D;UTF-8</span><br><span class="line">spark.hadoop.mapred.output.compress&#x3D;true</span><br><span class="line">spark.hadoop.mapred.output.compression.codec&#x3D;org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">spark.hadoop.mapred.output.compression.type&#x3D;BLOCK</span><br><span class="line"></span><br><span class="line"># 修改spark默认的python版本</span><br><span class="line"># spark2-conf&#x2F;spark-env.sh 的 Spark 2 客户端高级配置代码段（安全阀）</span><br><span class="line">PYSPARK_PYTHON&#x3D;&#x2F;usr&#x2F;bin&#x2F;python3.6</span><br><span class="line"># spark-shell或者spark-submit使用</span><br><span class="line">spark.pyspark.python</span><br></pre></td></tr></table></figure>
<h3 id="hdfs">hdfs</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 压缩设置</span><br><span class="line">io.compression.codecs&#x3D;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec</span><br><span class="line"></span><br><span class="line"># 存量数据导入后可进行rebalance操作</span><br><span class="line">sudo -u hdfs hadoop balancer -threshold 10 -policy datanode</span><br><span class="line"></span><br><span class="line"># 启用HA</span><br><span class="line"># HDFS服务配置中开启HA设置</span><br></pre></td></tr></table></figure>
<h3 id="Impala">Impala</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hue启用impala</span><br><span class="line"></span><br><span class="line">hue_safety_value.ini</span><br><span class="line">[impala]</span><br><span class="line">server_host&#x3D;</span><br><span class="line">server_port&#x3D;</span><br><span class="line"></span><br><span class="line"># hue管理用户勾选组的impala权限</span><br></pre></td></tr></table></figure>
<h3 id="Kafka">Kafka</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kafka默认分区数修改为8</span><br><span class="line">num.partitions&#x3D;8</span><br></pre></td></tr></table></figure>
<h3 id="使用Sentry进行权限管理">使用Sentry进行权限管理</h3>
<p>Hue、Hive启用Sentry设置：</p>
<ul>
<li>hue开启sentry服务</li>
<li>hive 启用数据库中的存储通知</li>
<li>hue添加hue用户组和用户，超级管理权限</li>
<li>security添加super role给hue组，所有权限</li>
</ul>
<p>添加普通用户流程：</p>
<ul>
<li>linux添加用户和用户组</li>
<li>hue添加用户和用户组</li>
<li>security添加role给到组</li>
</ul>
<p><strong>建议结合LDAP进行账号体系管理。</strong></p>
<h3 id="4-3-其他配置">4.3 其他配置</h3>
<h3 id="CM集群警报配置">CM集群警报配置</h3>
<p>进入Cloudera Manager Service配置界面，搜索alert对邮件警报进行配置。</p>
<ul>
<li>启用电子邮件警报</li>
<li>邮件服务器协议：smtp</li>
<li>邮箱服务器主机名称：邮箱服务器地址</li>
<li>邮箱服务器用户名：邮箱登录用户名</li>
<li>邮箱服务器密码：邮箱登录用户密码</li>
<li>邮件发件人地址：同登录用户名</li>
<li>邮件收件人地址：多个使用逗号隔开</li>
</ul>
<h3 id="使用hdfs纠错码">使用hdfs纠错码</h3>
<p>HDFS纠错码参考 <a href="https://link.zhihu.com/?target=https%3A//cloud.tencent.com/developer/article/1363393">这里</a>。</p>
<h3 id="4-4-集群性能测试">4.4 集群性能测试</h3>
<h3 id="HDFS性能测试">HDFS性能测试</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 写</span><br><span class="line">hadoop jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-tests.jar TestDFSIO -D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark -write -nrFiles 1000 -fileSize 100</span><br><span class="line"># 读</span><br><span class="line">hadoop jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-tests.jar TestDFSIO -D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark -read -nrFiles 1000 -fileSize 100</span><br><span class="line"># 清理数据</span><br><span class="line">hadoop jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-tests.jar TestDFSIO -D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark -clean</span><br></pre></td></tr></table></figure>
<h3 id="hbase性能测试">hbase性能测试</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows&#x3D;100000 --presplit&#x3D;100 sequentialWrite 10</span><br><span class="line">19&#x2F;04&#x2F;11 09:08:19 INFO hbase.PerformanceEvaluation: [SequentialWriteTest]   Min: 14083ms    Max: 14549ms    Avg: 14270ms</span><br><span class="line"></span><br><span class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows&#x3D;100000 --presplit&#x3D;100 randomWrite 10</span><br><span class="line">19&#x2F;04&#x2F;11 09:09:59 INFO hbase.PerformanceEvaluation: [RandomWriteTest]   Min: 20766ms    Max: 21968ms    Avg: 21383ms</span><br><span class="line"></span><br><span class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows&#x3D;100000 sequentialRead 10</span><br><span class="line">19&#x2F;04&#x2F;11 09:12:07 INFO hbase.PerformanceEvaluation: [SequentialReadTest]    Min: 50383ms    Max: 52429ms    Avg: 51691ms</span><br><span class="line"></span><br><span class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --rows&#x3D;100000 randomRead 10</span><br><span class="line">19&#x2F;04&#x2F;11 09:13:46 INFO hbase.PerformanceEvaluation: [RandomReadTest]    Min: 73645ms    Max: 74517ms    Avg: 74130ms</span><br><span class="line"></span><br><span class="line">sudo -u hdfs hbase pe sequentialWrite 1</span><br><span class="line">sudo -u hdfs hbase pe sequentialRead 1</span><br><span class="line">sudo -u hdfs hbase pe randomWrite 1</span><br><span class="line">sudo -u hdfs hbase pe randomRead 1</span><br><span class="line"></span><br><span class="line">count &#39;TestTable&#39;, &#123;INTERVAL &#x3D;&gt; 100000, CACHE &#x3D;&gt; 50000&#125;</span><br></pre></td></tr></table></figure>
<p>查看输出的测试报告，Done！</p>
]]></content>
  </entry>
  <entry>
    <title>DBeaver Enterprise 21.2.0 破解版 安装教程</title>
    <url>/2022/01/29/DBeaver%20Enterprise%2021.2.0%20%E7%A0%B4%E8%A7%A3%E7%89%88%20%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h1>DBeaver Enterprise 21.2.0 破解版 安装教程</h1>
<h2 id="制作绿色包-开箱即用">制作绿色包  开箱即用</h2>
<p><a href="https://blog.csdn.net/u012234419/article/details/120091206" target="_blank" rel="noopener">https://blog.csdn.net/u012234419/article/details/120091206</a></p>
<p>亲测可用，感谢楼主~</p>
]]></content>
  </entry>
  <entry>
    <title>clickhouse实时同步mysql数据</title>
    <url>/2022/01/26/clickhouse%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5mysql%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h1>clickhouse实时同步mysql数据</h1>
<p>clickhouse-client -m --password ‘admin’</p>
<h1>使用 MaterializedMySQL 引擎实时同步</h1>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> allow_experimental_database_materialized_mysql = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> dbtest <span class="keyword">ENGINE</span> = MaterializedMySQL(<span class="string">'10.6.123.23:3306'</span>, <span class="string">'dbtest'</span>, <span class="string">'cs_yangz'</span>, <span class="string">'***'</span>) <span class="keyword">SETTINGS</span> allows_query_when_mysql_lost=<span class="literal">true</span>,max_wait_time_when_mysql_unavailable=<span class="number">10000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看磁盘容量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">name</span>,</span><br><span class="line">    <span class="keyword">path</span>,</span><br><span class="line">    formatReadableSize(free_space) <span class="keyword">AS</span> free,</span><br><span class="line">    formatReadableSize(total_space) <span class="keyword">AS</span> total,</span><br><span class="line">    formatReadableSize(keep_free_space) <span class="keyword">AS</span> reserved</span><br><span class="line"><span class="keyword">FROM</span> system.disks</span><br></pre></td></tr></table></figure>
<h2 id="安装配置-MySQL-同步用户">安装配置 MySQL 同步用户</h2>
<ul>
<li>创建用户</li>
<li>全局赋予 replication client,replication slave, reload 权限</li>
<li>对同步库 db 赋予 select 权限</li>
</ul>
<p>如果赋予权限不正确，会报错，</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">'clickhouse'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'alitrack'</span>;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">select</span> <span class="keyword">ON</span> db.* <span class="keyword">TO</span> <span class="string">'clickhouse'</span>@<span class="string">'%'</span>;</span><br><span class="line"><span class="keyword">GRANT</span>  <span class="keyword">replication</span> <span class="keyword">client</span>,<span class="keyword">replication</span> <span class="keyword">slave</span>, reload <span class="keyword">on</span> *.* <span class="keyword">to</span> <span class="string">'clickhouse'</span>@<span class="string">'%'</span>;</span><br><span class="line"><span class="keyword">FLUSH</span> <span class="keyword">PRIVILEGES</span>;</span><br></pre></td></tr></table></figure>
<p>遇到没有主键的表，</p>
<p>如果没有初始化完成，删掉没有主键的表，直接重启clickhouse就可以了</p>
<p>如果已经初始化，修改GTID，再重启。</p>
<p>vim /data/clickhouse/metadata/dbtest/.metadata   修改GTID 跳过  重启clickhouse服务</p>
<p>– 不支持的数据类型</p>
<h1>MaterializeMySQL don’t support the json,bit,time data type</h1>
<h2 id="–如果没有生成-metadata-文件-查看日志-可能是不支持的类型-，或者没有主键">–如果没有生成 .metadata 文件     查看日志, 可能是不支持的类型 ，或者没有主键</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;var&#x2F;log&#x2F;clickhouse-server&#x2F;clickhouse-server.err.log</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>nginx代理clickhouse</title>
    <url>/2022/01/26/nginx%E4%BB%A3%E7%90%86clickhouse/</url>
    <content><![CDATA[<h2 id="下载编译源码包">下载编译源码包</h2>
<p>下载源码包到指定的目录，</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://nginx.org/download/nginx-1.20.2.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压并编译源码</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf  nginx-1.20.2.tar.gz</span><br><span class="line">cd nginx-1.20.2/</span><br></pre></td></tr></table></figure>
<p>1）配置Nginx编译文件参数</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">./configure --<span class="keyword">with</span>-stream</span><br></pre></td></tr></table></figure>
<p>2）编译、安装，make &amp;&amp; make install</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="built_in">make</span> &amp; <span class="built_in">make</span> install</span><br></pre></td></tr></table></figure>
<p>vim  /usr/local/nginx/conf/nginx.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">worker_processes auto;</span><br><span class="line"></span><br><span class="line">error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log info;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stream&#123;</span><br><span class="line">  upstream clickhouse8123&#123;</span><br><span class="line">    server 172.20.85.141:8123 weight&#x3D;1;</span><br><span class="line">    server 172.20.85.142:8123 weight&#x3D;1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  server&#123;</span><br><span class="line">    listen 8123;</span><br><span class="line">    proxy_pass clickhouse8123;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  upstream clickhouse9002&#123;</span><br><span class="line">    server 172.20.85.141:9002 weight&#x3D;1;</span><br><span class="line">    server 172.20.85.142:9002 weight&#x3D;1;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  server&#123;</span><br><span class="line">    listen 9002;</span><br><span class="line">    proxy_pass clickhouse9002;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="添加到环境变量">添加到环境变量</h2>
<p>1、编辑/etc/profile</p>
<p>vim /etc/profile<br>
2、在最后一行添加配置，:wq保存</p>
<p>PATH=$PATH:/usr/local/nginx/sbin<br>
export PATH<br>
3、使配置立即生效</p>
<p>source /etc/profile</p>
<p>– 查看帮助</p>
<p>nginx -h</p>
<h2 id="遇到的问题">遇到的问题</h2>
<p>[root@bigdata-3 nginx-1.20.2]# nginx -s reload<br>
nginx: [error] open() “/usr/local/nginx/logs/nginx.pid” failed (2: No such file or directory)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;var&#x2F;logs&#x2F;nginx&#x2F;</span><br><span class="line">nginx -c &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>presto集群管理</title>
    <url>/2022/01/26/presto%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<p>在ansible环境下，控制其他机器，除了/etc/profile文件需要修改，还需要修改/root/.bashrc文件中的值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible presto -m copy -a &quot;src&#x3D;&#x2F;root&#x2F;.bashrc dest&#x3D;&#x2F;root&#x2F;.bashrc &quot;</span><br><span class="line"></span><br><span class="line">ansible presto -m shell -a &quot;java -version &quot;</span><br><span class="line"></span><br><span class="line">ansible presto -m copy -a &quot;src&#x3D;&#x2F;opt&#x2F;presto&#x2F;etc&#x2F;catalog  dest&#x3D;&#x2F;opt&#x2F;presto&#x2F;etc&#x2F;&quot;</span><br><span class="line"></span><br><span class="line">ansible presto -m shell -a &quot;ls &#x2F;opt&#x2F;presto&#x2F;etc&#x2F;catalog &quot;</span><br></pre></td></tr></table></figure>
<h3 id="增加新的连接">增加新的连接</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible presto -m copy -a &quot;src&#x3D;&#x2F;opt&#x2F;presto&#x2F;etc&#x2F;catalog&#x2F;mysql_80_20.properties  dest&#x3D;&#x2F;opt&#x2F;presto&#x2F;etc&#x2F;catalog&#x2F;mysql_80_20.properties&quot;</span><br><span class="line"></span><br><span class="line"> ansible presto -m shell -a &quot;&#x2F;opt&#x2F;presto&#x2F;bin&#x2F;launcher restart &quot;</span><br></pre></td></tr></table></figure>
<p><a href="https://repo1.maven.org/maven2/io/prestosql/presto-server/350/" target="_blank" rel="noopener">https://repo1.maven.org/maven2/io/prestosql/presto-server/350/</a></p>
<p>350版本之后更名成trino</p>
]]></content>
  </entry>
  <entry>
    <title>Alluxio运维</title>
    <url>/2022/01/25/Alluxio%E8%BF%90%E7%BB%B4/</url>
    <content><![CDATA[<h2 id="Alluxio命令">Alluxio命令</h2>
<h3 id="alluxio-fsadmin">alluxio fsadmin</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看服务状态</span><br><span class="line">alluxio fsadmin report</span><br><span class="line"></span><br><span class="line"># 查看挂掉的服务ip</span><br><span class="line">alluxio fsadmin report capacity -lost</span><br></pre></td></tr></table></figure>
<h3 id="alluxio-getConf">alluxio getConf</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看配置参数</span><br><span class="line">alluxio getConf --master</span><br></pre></td></tr></table></figure>
<h2 id="Alluxio运维实战">Alluxio运维实战</h2>
<h3 id="Worker节点挂掉">Worker节点挂掉</h3>
<ol>
<li>
<p>查看服务状态，发现有一台worker节点丢失</p>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2022/01/5586288e0f4a956c99cfbba8eb4ffbba.webp" alt="img"></p>
</li>
<li>
<p>查看丢失的节点是哪一台</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ alluxio fsadmin report capacity -lost</span><br><span class="line">sjsysc-hh405-zbhx700w</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>登录到丢失的worker节点，启动worker</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh sjsysc-hh405-zbhx700w</span><br><span class="line">$ alluxio-start.sh worker SudoMount</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="设置子目录挂载点">设置子目录挂载点</h3>
<p>待Alluxio启动完毕之后，用户可以在挂载其他子目录，例如，将另一个hadoop集群的hdfs目录挂载到alluxio中。</p>
<p>当我们挂载配置不同的HDFS时候，可以在挂载的时候特别指定每一个HDFS所对应的配置信息（hdfs-site.xml,core-site.xml）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alluxio fs mount &#x2F;ia_test hdfs:&#x2F;&#x2F;nameservice1&#x2F;ia_test \</span><br><span class="line">--option alluxio.underfs.hdfs.configuration&#x3D;&#x2F;opt&#x2F;alluxio&#x2F;hdfs&#x2F;ia_conf&#x2F;hdfs-site.xml:&#x2F;opt&#x2F;alluxio&#x2F;hdfs&#x2F;ia_conf&#x2F;core-site.xml</span><br><span class="line">挂载要求：</span><br></pre></td></tr></table></figure>
<ol>
<li>
<p>端口打通</p>
<p>（1） 需要打通alluxio集群到hdfs集群namenode 的8020端口</p>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2022/01/e82fc6b9ff733a9de369bad8afbfcbd0.webp" alt="img"></p>
<p>如果不打通此端口，则会报如下错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.net.UnknownHostException: nameservice1</span><br></pre></td></tr></table></figure>
<p>（2）需要打通alluxio集群到hdfs集群datanode的9866、9867端口</p>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2022/01/12e72f5591d5c6e6040eb992310297df.webp" alt="img"></p>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2022/01/9c3da511324be8438bccdae44839c3ec.webp" alt="img"></p>
<p>如果不打通此端口，则操作alluxio 文件时，会报如下错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Attempt 1 to load &#x2F;hive&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000016_0.gz failed because: Task execution failed: Could not obtain block: BP-467187067-10.177.36.3-1591087438300:blk_4563885290_3807183975 file&#x3D;&#x2F;user&#x2F;alluxio_ia&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000016_0.gz (Zero Copy GrpcDataReader)</span><br><span class="line">Attempt 1 to load &#x2F;hive&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000083_0.gz failed because: Task execution failed: Could not obtain block: BP-467187067-10.177.36.3-1591087438300:blk_4564100089_3807398774 file&#x3D;&#x2F;user&#x2F;alluxio_ia&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000083_0.gz (Zero Copy GrpcDataReader)</span><br><span class="line">Attempt 1 to load &#x2F;hive&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000115_0.gz failed because: Task execution failed: Could not obtain block: BP-467187067-10.177.36.3-1591087438300:blk_4564170915_3807469600 file&#x3D;&#x2F;user&#x2F;alluxio_ia&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000115_0.gz (Zero Copy GrpcDataReader)</span><br><span class="line">Attempt 1 to load &#x2F;hive&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000079_0.gz failed because: Task execution failed: Could not obtain block: BP-467187067-10.177.36.3-1591087438300:blk_4564086733_3807385418 file&#x3D;&#x2F;user&#x2F;alluxio_ia&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000079_0.gz (Zero Copy GrpcDataReader)</span><br><span class="line">Attempt 1 to load &#x2F;hive&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000041_0.gz failed because: Task execution failed: Could not obtain block: BP-467187067-10.177.36.3-1591087438300:blk_4563964409_3807263094 file&#x3D;&#x2F;user&#x2F;alluxio_ia&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000041_0.gz (Zero Copy GrpcDataReader)</span><br><span class="line">Attempt 1 to load &#x2F;hive&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000103_0.gz failed because: Task execution failed: Could not obtain block: BP-467187067-10.177.36.3-1591087438300:blk_4564147300_3807445985 file&#x3D;&#x2F;user&#x2F;alluxio_ia&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000103_0.gz (Zero Copy GrpcDataReader)</span><br><span class="line">Attempt 1 to load &#x2F;hive&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000046_0.gz failed because: Task execution failed: Could not obtain block: BP-467187067-10.177.36.3-1591087438300:blk_4563978019_3807276704 file&#x3D;&#x2F;user&#x2F;alluxio_ia&#x2F;dwa_d_ia_basic_user_all&#x2F;month_id&#x3D;202105&#x2F;day_id&#x3D;19&#x2F;prov_id&#x3D;097&#x2F;000046_0.gz (Zero Copy GrpcDataReader)</span><br></pre></td></tr></table></figure>
<p><img src="https://ask3.oss-cn-hangzhou.aliyuncs.com/img/md/2022/01/ff373308af325099cd3da95273f67fbf.webp" alt="img"></p>
</li>
<li>
<p>需要将hdfs配置文件发放到alluxio集群的<strong>所有节点</strong>上，并且配置<strong>文件及其所有父目录具有755权限</strong>。</p>
</li>
</ol>
<p>否则挂载文件时会报如下错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.net.UnknownHostException: nameservice1</span><br></pre></td></tr></table></figure>
<p>如果只是mount hdfs目录，只需要将hdfs 配置文件发放到所有alluxio mastera节点即可，但是当操作alluxio 文件时，如果不讲hdfs配置文件发放到所有alluxio worker节点，则会报如下错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[alluxio@sjsysc-hh405-zbhx1135w ~]$ alluxio fs copyToLocal &#x2F;hive-test&#x2F;dm_m_ia_prefer_label_app_top5&#x2F;month_id&#x3D;202104&#x2F;prov_id&#x3D;084&#x2F;000002_0.gz .</span><br><span class="line">Failed to read block ID&#x3D;287209160704 from tiered storage and UFS tier: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalArgumentException: java.net.UnknownHostException: nameservice1 (Zero Copy GrpcDataReader)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：一般将配置文件放到 /opt 或者 /usr/local 这样的目录下，因为这样的目录都可执行权限，不要将配置文件放到 /home/用户/目录下，因为这个目录给父目录增加755权限的时候，ssh 免密登录会失效！！！</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>破解PyCharm 2018.3.2</title>
    <url>/2022/01/21/%E7%A0%B4%E8%A7%A3PyCharm%202018.3.2/</url>
    <content><![CDATA[<h1>破解PyCharm 2018.3.2</h1>
<p><strong>亲测可以用</strong></p>
<p>破解教程：<a href="https://www.jianshu.com/p/76a30aeb8f9d" target="_blank" rel="noopener">https://www.jianshu.com/p/76a30aeb8f9d</a></p>
<h4 id="1、key">1、key</h4>
<figure class="highlight swift"><table><tr><td class="code"><pre><span class="line"><span class="type">K71U8DBPNE</span>-eyJsaWNlbnNlSWQiOiJLNzFVOERCUE5FIiwibGljZW5zZWVOYW1lIjoibGFuIHl1IiwiYXNzaWduZWVOYW1lIjoiIiwiYXNzaWduZWVFbWFpbCI6IiIsImxpY2Vuc2VSZXN0cmljdGlvbiI6IkZvciBlZHVjYXRpb25hbCB1c2Ugb25seSIsImNoZWNrQ29uY3VycmVudFVzZSI6ZmFsc2UsInByb2R1Y3RzIjpbeyJjb2RlIjoiSUkiLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJSUzAiLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJXUyIsInBhaWRVcFRvIjoiMjAxOS0wNS0wNCJ9LHsiY29kZSI6IlJEIiwicGFpZFVwVG8iOiIyMDE5LTA1LTA0In0seyJjb2RlIjoiUkMiLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJEQyIsInBhaWRVcFRvIjoiMjAxOS0wNS0wNCJ9LHsiY29kZSI6IkRCIiwicGFpZFVwVG8iOiIyMDE5LTA1LTA0In0seyJjb2RlIjoiUk0iLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJETSIsInBhaWRVcFRvIjoiMjAxOS0wNS0wNCJ9LHsiY29kZSI6IkFDIiwicGFpZFVwVG8iOiIyMDE5LTA1LTA0In0seyJjb2RlIjoiRFBOIiwicGFpZFVwVG8iOiIyMDE5LTA1LTA0In0seyJjb2RlIjoiR08iLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJQUyIsInBhaWRVcFRvIjoiMjAxOS0wNS0wNCJ9LHsiY29kZSI6IkNMIiwicGFpZFVwVG8iOiIyMDE5LTA1LTA0In0seyJjb2RlIjoiUEMiLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifSx7ImNvZGUiOiJSU1UiLCJwYWlkVXBUbyI6IjIwMTktMDUtMDQifV0sImhhc2giOiI4OTA4Mjg5LzAiLCJncmFjZVBlcmlvZERheXMiOjAsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-<span class="type">Owt3</span>/+<span class="type">LdCpedvF0eQ8635yYt0</span>+<span class="type">ZLtCfIHOKzSrx5hBtbKGYRPFDrdgQAK6lJjexl2emLBcUq729K1</span>+ukY9Js0nx1NH09l9Rw4c7k9wUksLl6RWx7Hcdcma1AHolfSp79NynSMZzQQLFohNyjD+dXfXM5GYd2OTHya0zYjTNMmAJuuRsapJMP9F1z7UTpMpLMxS/<span class="type">JaCWdyX6qIs</span>+funJdPF7bjzYAQBvtbz+6SANBgN36gG1B2xHhccTn6WE8vagwwSNuM70egpahcTktoHxI7uS1JGN9gKAr6nbp+8DbFz3a2wd+<span class="type">XoF3nSJb</span>/d2f/6zJR8yJF8AOyb30kwg3zf5cWw==-<span class="type">MIIEPjCCAiagAwIBAgIBBTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE1MTEwMjA4MjE0OFoXDTE4MTEwMTA4MjE0OFowETEPMA0GA1UEAwwGcHJvZDN5MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq</span>+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+<span class="type">IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU</span>+<span class="type">Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1</span>+<span class="type">I4ZI</span>+<span class="type">FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB</span>/xVy/<span class="type">VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE</span>/<span class="type">EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD</span>+<span class="type">AFKOetkhnQhI2Qb1t4Lm0oFKLl</span>/<span class="type">GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQC9WZuYgQedSuOc5TOUSrRigMw4</span>/+wuC5EtZBfvdl4HT/8vzMW/oUlIP4YCvA0XKyBaCJ2iX+<span class="type">ZCDKoPfiYXiaSiH</span>+<span class="type">HxAPV6J79vvouxKrWg2XV6ShFtPLP</span>+0gPdGq3x9R3+kJbmAm8w+<span class="type">FOdlWqAfJrLvpzMGNeDU14YGXiZ9bVzmIQbwrBA</span>+<span class="built_in">c</span>/<span class="type">F4tlK</span>/<span class="type">DV07dsNExihqFoibnqDiVNTGombaU2dDup2gwKdL81ua8EIcGNExHe82kjF4zwfadHk3bQVvbfdAwxcDy4xBjs3L4raPLU3yenSzr</span>/<span class="type">OEur1</span>+jfOxnQSmEcMXKXgrAQ9U55gwjcOFKrgOxEdek/<span class="type">Sk1VfOjvS</span>+nuM4eyEruFMfaZHzoQiuw4IqgGc45ohFH0UUyjYcuFxxDSU9lMCv8qdHKm+wnPRb0l9l5vXsCBDuhAGYD6ss+<span class="type">Ga</span>+aDY6f/qXZuUCEUOH3QUNbbCUlviSz6+<span class="type">GiRnt1kA9N2Qachl</span>+2yBfaqUqr8h7Z2gsx5LcIf5kYNsqJ0GavXTVyWh7PYiKX4bs354ZQLUwwa/cG++<span class="number">2</span>+wNWP+<span class="type">HtBhVxMRNTdVhSm38AknZlD</span>+<span class="type">PTAsWGu9GyLmhti2EnVwGybSD2Dxmhxk3IPCkhKAK</span>+pl0eWYGZWG3tJ9mZ7SowcXLWDFAk0lRJnKGFMTggrWjV8GYpw5bq23VmIqqDLgkNzuoog==</span><br></pre></td></tr></table></figure>
<h4 id="2、破解补丁下载">2、破解补丁下载</h4>
<p>链接：<a href="https://pan.baidu.com/s/1EvUvweSG58kFToJXB8ugPw" target="_blank" rel="noopener">https://pan.baidu.com/s/1EvUvweSG58kFToJXB8ugPw</a><br>
提取码：vnkq</p>
<p><strong>亲测  JetBrains PhpStorm 2018.3.2  同样也能用</strong></p>
]]></content>
  </entry>
  <entry>
    <title>python直接操作wordpress</title>
    <url>/2022/01/11/python%E7%9B%B4%E6%8E%A5%E6%93%8D%E4%BD%9Cwordpress/</url>
    <content><![CDATA[<p>111</p>
<h1><a href="https://vulsee.com/archives/vulsee_2021/0119_13534.html" target="_blank" rel="noopener">python直接操作wordpress的模块：python-wordpress-xmlrpc</a></h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from wordpress_xmlrpc import Client, WordPressPost</span><br><span class="line">from wordpress_xmlrpc.methods.posts import GetPosts, NewPost</span><br><span class="line">from wordpress_xmlrpc.methods.users import GetUserInfo</span><br><span class="line"></span><br><span class="line">wp &#x3D; Client(&#39;http:&#x2F;&#x2F;ask3.cn&#x2F;xmlrpc.php&#39;, &#39;A3&#39;, &#39;AAaa11@@&#39;)</span><br><span class="line">wp.call(GetPosts())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">wp.call(GetUserInfo())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">post &#x3D; WordPressPost()</span><br><span class="line">post.title &#x3D; &#39;My new title&#39;</span><br><span class="line">post.content &#x3D; &#39;This is the body of my new post.&#39;</span><br><span class="line">post.terms_names &#x3D; &#123;</span><br><span class="line">&#39;post_tag&#39;: [&#39;test&#39;, &#39;firstpost&#39;],</span><br><span class="line">&#39;category&#39;: [&#39;Introductions&#39;, &#39;Tests&#39;]</span><br><span class="line">&#125;</span><br><span class="line">wp.call(NewPost(post))</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>hexo部署到GitHub出现的问题</title>
    <url>/2022/01/06/hexo%E9%83%A8%E7%BD%B2%E5%88%B0GitHub%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1>Hexo出现错误err: Error: Spawn failed</h1>
<p>在 hexo d之后，出现如下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fatal: Could not read from remote repository.</span><br><span class="line">Please make sure you have the correct access rights</span><br><span class="line">and the repository exists.</span><br><span class="line">FATAL &#123;</span><br><span class="line">  err: Error: Spawn failed</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure>
<p>使用上面的命令查看详细错误信息，得到以下结果</p>
<p>ssh: connect to host <a href="http://github.com" target="_blank" rel="noopener">github.com</a> port 22: Resource temporarily unavailable</p>
<h1>2、解决方法</h1>
<p>然后我们进入.ssh的配置目录查看，发现ssh目录里少了配置文件config。</p>
<p>找到原因后我们进入.ssh的目录，使用命令“touch config”创建一个配置文件，并写入你github的配置信息。（xxxxx@xx.com是你github的注册邮箱）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Host github.com  </span><br><span class="line">User u8888888888@hotmail.com  </span><br><span class="line">Hostname ssh.github.com  </span><br><span class="line">PreferredAuthentications publickey  </span><br><span class="line">IdentityFile ~&#x2F;.ssh&#x2F;id_rsa  </span><br><span class="line">Port 443</span><br></pre></td></tr></table></figure>
<p>现在再使用ssh git@github.com查看与github的连接状态，可能出现错误Bad owner or permissions on</p>
<p>这是因为你创建的配置文件config的权限不够。所以我们需要修改其权限。使用下面命令。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo chmod 600 config</span><br></pre></td></tr></table></figure>
<p>现在我们再尝试查看连接状态发现已经成功连接。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@BF100638:~&#x2F;.ssh# ssh -T git@github.com</span><br><span class="line">The authenticity of host &#39;[ssh.github.com]:443 ([20.205.243.160]:443)&#39; can&#39;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:p2QAMXNIC1TJYWeIOttrVc98&#x2F;R1BUFWu3&#x2F;LiyKgUfQM.</span><br><span class="line">Are you sure you want to continue connecting (yes&#x2F;no)? yes</span><br><span class="line">Warning: Permanently added &#39;[ssh.github.com]:443,[20.205.243.160]:443&#39; (ECDSA) to the list of known hosts.</span><br><span class="line">Hi chinayangze! You&#39;ve successfully authenticated, but GitHub does not provide shell access.</span><br></pre></td></tr></table></figure>
<p>接下来就可以成功的push你的代码到github了。</p>
<p>参考：<a href="https://gugeshenjineng.github.io/2021/07/02/hexo%E9%83%A8%E7%BD%B2%E5%88%B0GitHub%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">https://gugeshenjineng.github.io/2021/07/02/hexo部署到GitHub出现的问题/</a></p>
]]></content>
  </entry>
  <entry>
    <title>mysql重装系统后以前的数据_重装系统后 如何使用之前mysql数据</title>
    <url>/2021/12/31/mysql%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F%E5%90%8E%E4%BB%A5%E5%89%8D%E7%9A%84%E6%95%B0%E6%8D%AE_%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F%E5%90%8E%20%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E4%B9%8B%E5%89%8Dmysql%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h1>mysql重装系统后以前的数据_重装系统后 如何使用之前mysql数据</h1>
<p><a href="https://blog.csdn.net/weixin_28854171/article/details/113467193" target="_blank" rel="noopener">https://blog.csdn.net/weixin_28854171/article/details/113467193</a></p>
]]></content>
  </entry>
  <entry>
    <title>安装不同版本的php</title>
    <url>/2021/12/30/%E5%AE%89%E8%A3%85%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E7%9A%84php/</url>
    <content><![CDATA[<h2 id="移除php">移除php</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum remove php-common -y</span><br></pre></td></tr></table></figure>
<p><a href="https://www.cnblogs.com/hongdada/p/9687330.html" target="_blank" rel="noopener">yum仓库中源的配置与使用</a></p>
<p>yum 仓库管理工具 <code>yum-config-manager</code>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum-config-manager --add-repo repository_url</span><br></pre></td></tr></table></figure>
<p>查询仓库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum repolist enabled # 显示所有启动的仓库</span><br><span class="line">yum repolist disabled # 显示所有禁用的仓库</span><br><span class="line">yum repolist all # 显示所有仓库</span><br></pre></td></tr></table></figure>
<p>修改仓库</p>
<p>最常用的修改操作就是启动和停用, 可以使用以下命令实现:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum-config-manager --enable repository…</span><br><span class="line">yum-config-manager --disable repository…</span><br></pre></td></tr></table></figure>
<h2 id="配置php-yum">配置php yum</h2>
<p>centos yum 安装php7.4 使用remi源</p>
<p>参考：<a href="https://blog.csdn.net/ecba1988/article/details/109991911" target="_blank" rel="noopener">https://blog.csdn.net/ecba1988/article/details/109991911</a></p>
<h2 id="安装php">安装php</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install php php-mysql gd php-gd gd-devel php-xml php-common php-mbstring php-ldap php-pear php-xmlrpc php-imap</span><br></pre></td></tr></table></figure>
<p>systemctl restart httpd</p>
]]></content>
  </entry>
  <entry>
    <title>hexo部署到GitHub</title>
    <url>/2021/12/24/hexo%E9%83%A8%E7%BD%B2%E5%88%B0GitHub/</url>
    <content><![CDATA[<p>参考这篇文章就可以了</p>
<p>GitHub+Hexo 搭建个人网站详细教程  <a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a></p>
<p>其他相关笔记</p>
<p><strong>安装插件</strong></p>
<p>npm i -S hexo-prism-plugin</p>
<p><strong>卸载插件</strong></p>
<p>npm uninstall hexo-prism-plugin --save</p>
<p><strong>next7.X版本关于设置阅读全文</strong></p>
<p>网上查了好多资料始终都没得到解决如何去设置“阅读全文”。最后查了最细的next7.6文档才发现还要下载插件 然后在主题配置文件_config.yml配置下就好了。</p>
<p>一，在博客目录下执行</p>
<p>npm install hexo-excerpt --save</p>
<p>二，在站点配置文件_config.yml添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">excerpt:</span><br><span class="line">depth: 10</span><br><span class="line">excerpt_excludes: []</span><br><span class="line">more_excludes: []</span><br><span class="line">hideWholePostExcerpts: true</span><br></pre></td></tr></table></figure>
<p>使用 Hexo Admin 插件</p>
<p><strong>使用方法</strong></p>
<ol>
<li>在Hexo网站目录下，安装 Hexo Admin 插件</li>
</ol>
<p>npm install --save hexo-admin</p>
<ol>
<li>启动本地服务器并打开管理界面，即可使用</li>
</ol>
<p>hexo server -d open <a href="http://localhost:4000/admin/" target="_blank" rel="noopener">http://localhost:4000/admin/</a></p>
<p><strong>卸载模块</strong></p>
<p>我们可以使用以下命令来卸载 Node.js 模块。</p>
<p>$ npm uninstall express</p>
]]></content>
  </entry>
  <entry>
    <title>presto 连接clickhouse</title>
    <url>/2021/12/23/presto%20%E8%BF%9E%E6%8E%A5clickhouse/</url>
    <content><![CDATA[<h1>presto 连接clickhouse</h1>
<p>presto350版本，不支持clickhouse ，需要升级，</p>
<p>presto350版本之后 presto改名为trino，导致不兼容，升级后 对应的jdbc驱动 ，客户端都需要换成新的。</p>
<p>客户端工具用DBeaver比较好，DbVisualizer 不支持trino</p>
]]></content>
  </entry>
  <entry>
    <title>ckman部署</title>
    <url>/2021/12/08/ckman%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>ckman部署</p>
<h3 id="安装">安装</h3>
<p><code>rpm</code>安装直接使用命令安装即可：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -ivh ckman-1.3.1.x86_64.rpm</span><br></pre></td></tr></table></figure>
<p>安装完成后，在<code>/etc/ckman</code>目录下，会生成工作目录（日志和配置文件等都在该目录下）。</p>
<h3 id="启动">启动</h3>
<p><code>rpm</code>方式安装的<code>ckman</code>有两种启动方式：</p>
<h4 id="方式一：">方式一：</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;bin&#x2F;ckman -c&#x3D;&#x2F;etc&#x2F;ckman&#x2F;conf&#x2F;ckman.yaml -p&#x3D;&#x2F;run&#x2F;ckman&#x2F;ckman.pid -l&#x3D;&#x2F;var&#x2F;log&#x2F;ckman&#x2F;ckman.log -d</span><br></pre></td></tr></table></figure>
<h4 id="方式二：">方式二：</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start ckman</span><br></pre></td></tr></table></figure>
<h2 id="tar-gz包安装">tar.gz包安装</h2>
<h3 id="安装-2">安装</h3>
<p>可以在任意目录进行安装。安装方式为直接解压安装包即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xzvf ckman-1.3.1-210428.Linux.x86_64.tar.gz</span><br></pre></td></tr></table></figure>
<h3 id="启动-2">启动</h3>
<p>进入<code>ckman</code>的工作目录，执行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ckman</span><br><span class="line">bin&#x2F;start</span><br></pre></td></tr></table></figure>
<p>启动之后，在浏览器输入 <a href="http://localhost:8808/" target="_blank" rel="noopener">http://localhost:8808</a> 跳出如下界面，说明启动成功：</p>
<p>– 特别主要：</p>
<p>clickhouse占用9000的端口 需要修改cloudera manager agent 默认端口9000</p>
<p>可以在clickhouse结点上查看/var/log/clickhouse-server/clickhouse-server.log，注意ERROR级别日志</p>
<p>/var/log/clickhouse-server/clickhouse-server.err.log</p>
<p>clickhouse  9000 9004 9005 9009</p>
<p>部署集群:认证方式:<br>
1．密码认证（保存密码）﹔使用密码登录，密码加密保存在本地，后续运维动作无需输入密码<br>
2．密码认证（不保存密码)﹔使用密码登录，但是密码不保存，后续运维动作（增删节点、启停等)，需要输入密码3．公钥认证:使用公钥登录，无需保存密码，后续运维操作可直接操作，是默认的认证方式<br>
公钥认证需要注意:<br>
1）需要提前配置ckman所在服务器到clickhouse各节点之间的互信（使用哪个用户去部署就配置哪个用户的)<br>
ssh-copy-id<br>
2）需要将公钥文件~/.ssh/id_rsa拷贝到ckman/conf目录下，并保证c kman用户有权限访间该支件3)如果是使用普通用户，公钥方式认证，那么该普通用户需要具有sudo权限，且在/etc/sudoers<br>
文件中配置了NOPASSwD</p>
<p>–客户端访问</p>
<p>clickhouse-client --port 9002 --password admin -m</p>
<p>CREATE USER ‘clickhouse’@’%’ IDENTIFIED BY ‘alitrack’;<br>
GRANT select ON db.* TO ‘clickhouse’@’%’;<br>
GRANT replication client,replication slave, reload on <em>.</em> to ‘clickhouse’@’%’;<br>
FLUSH PRIVILEGES;</p>
]]></content>
  </entry>
  <entry>
    <title>升级OpenSSH之后登录不上去</title>
    <url>/2021/12/06/%E5%8D%87%E7%BA%A7OpenSSH%E4%B9%8B%E5%90%8E%E7%99%BB%E5%BD%95%E4%B8%8D%E4%B8%8A%E5%8E%BB/</url>
    <content><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure>
<p>需要在配置文件中添加：</p>
<p>PermitRootLogin yes</p>
]]></content>
  </entry>
  <entry>
    <title>clickhouse之HDFS云存储</title>
    <url>/2021/11/19/clickhouse%E4%B9%8BHDFS%E4%BA%91%E5%AD%98%E5%82%A8/</url>
    <content><![CDATA[<h1>clickhouse之HDFS云存储</h1>
<p>参考： <a href="https://blog.csdn.net/weixin_40104766/article/details/120029525" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40104766/article/details/120029525</a></p>
<p>按照官方文档说明，在config.xml文件中添加如下配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">storage_configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">disks</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">diskname</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">type</span>&gt;</span>hdfs<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">endpoint</span>&gt;</span>hdfs://10.0.19.241:8020/ckdata/<span class="tag">&lt;/<span class="name">endpoint</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">diskname</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">disks</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">policies</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">hdfs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">volumes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">volumename</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">disk</span>&gt;</span>diskname<span class="tag">&lt;/<span class="name">disk</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">volumename</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">volumes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">hdfs</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">policies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">storage_configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>配置完成后，重启clickhouse-server服务，验证是否生效：</p>
<p>1.通过system.storage_policies表查看存储策略是否OK</p>
<p>select * from system.storage_policies;</p>
<p>2.通过settings参数指定storage_policy为我们配置好的hdfs，建表语句如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_data <span class="keyword">engine</span>=MergeTree <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">id</span> <span class="keyword">settings</span> storage_policy=<span class="string">'hdfs'</span> <span class="keyword">as</span> <span class="keyword">with</span> (<span class="keyword">select</span> [<span class="string">'A'</span>,<span class="string">'a'</span>,<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>,<span class="string">'A'</span>,<span class="string">'59'</span>,<span class="string">'90'</span>,<span class="string">'80'</span>,<span class="string">'85'</span>,<span class="string">'90'</span>,<span class="string">'929'</span>,<span class="string">'80'</span>,<span class="string">'72'</span>,<span class="string">'90'</span>,<span class="string">'123'</span>]) <span class="keyword">AS</span> dict <span class="keyword">select</span> dict[<span class="built_in">number</span>%<span class="number">10</span> + <span class="number">1</span>] <span class="keyword">as</span> <span class="keyword">id</span>,dict[<span class="built_in">number</span> + <span class="number">11</span>] <span class="keyword">as</span> val <span class="keyword">from</span> system.numbers <span class="keyword">limit</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>磁盘读写测试</title>
    <url>/2021/10/28/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h2 id="磁盘读写测试">磁盘读写测试</h2>
<p>FIO安装<br>
wget <a href="http://brick.kernel.dk/snaps/fio-2.2.5.tar.gz" target="_blank" rel="noopener">http://brick.kernel.dk/snaps/fio-2.2.5.tar.gz</a></p>
<p>yum install libaio-devel<br>
tar -zxvf fio-2.2.5.tar.gz<br>
cd fio-2.2.5<br>
make<br>
make install</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">测试：</span><br><span class="line">顺序读：</span><br><span class="line">fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=mytest</span><br><span class="line"></span><br><span class="line">随机写：</span><br><span class="line">fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=mytest</span><br><span class="line"></span><br><span class="line">顺序写：</span><br><span class="line">fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=mytest</span><br><span class="line"></span><br><span class="line">混合随机读写：</span><br><span class="line">fio -filename=/dev/sda -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=mytest -ioscheduler=noop</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>智能大数据平台USDP操作部署</title>
    <url>/2021/07/20/%E6%99%BA%E8%83%BD%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0USDP%E6%93%8D%E4%BD%9C%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h2 id="智能大数据平台USDP操作部署">智能大数据平台USDP操作部署</h2>
<p>USDP部署操作指南：<a href="https://mp.weixin.qq.com/s/COnkLXPTWL5OK1PFYooThw?scene=25#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/COnkLXPTWL5OK1PFYooThw?scene=25#wechat_redirect</a></p>
<p>bash <a href="http://repair.sh" target="_blank" rel="noopener">repair.sh</a> initAll   这个步骤大约要30多分钟，重复执行也慢。</p>
<p>注意：</p>
<p>1.在repair-host-info-add.properties文件中，仅需配置每次新增的节点信息即可，若存在已修复过的节点信息时，在下次运行“<a href="http://repair.sh" target="_blank" rel="noopener">repair.sh</a> initSingle”指令前，请清除。</p>
<p>2.jdk 安装在 /opt/module 下面，不允许随意删除，否则 java 环境失效。</p>
<p>3.数据库密码中不得包含 “@” 。</p>
<p>4.主机名设置不得包含 “_”，&quot;-&quot; 。</p>
<p>启动服务  bin/stop-udp-server.sh</p>
<p>查看日志目录  /var/log/udp</p>
<p>一般如果数据库已经安装完毕了的话，初始化是不会修改数据库的，所以重复执行bash <a href="http://repair.sh" target="_blank" rel="noopener">repair.sh</a> initAll 也是没有用的。</p>
<p>不管之前设置什么密码，修改数据库密码的统一操作：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo "skip-grant-tables" &gt;&gt; /etc/my.cnf</span><br><span class="line">systemctl restart mysqld</span><br><span class="line">systemctl enable mysqld</span><br><span class="line">sh /opt/usdp-srv/usdp/repair/bin/repair-modifyMysqlPassword.sh  '密码'</span><br><span class="line">sed -i 's/skip-grant-tables/#&amp;/' /etc/my.cnf</span><br><span class="line">systemctl restart mysqld</span><br><span class="line">sh /opt/usdp-srv/usdp/repair/bin/repair-init-mysql-udp.sh '密码'</span><br></pre></td></tr></table></figure>
<h3 id="1-0-升级到2-0">1.0 升级到2.0</h3>
<p>下载  usdp-01-master-privatization-free-2.0.0.0.tar.gz</p>
<p>解压，把2.0中/opt/usdp-srv/usdp/repair/remove  目录  复制到1.0 /opt/usdp-srv/usdp/repair</p>
<p>sh <a href="http://remove-all.sh" target="_blank" rel="noopener">remove-all.sh</a> 1.0.0.0</p>
<table>
<thead>
<tr>
<th><strong>集群清除模块文件</strong></th>
<th><strong>文件说明</strong></th>
<th><strong>文件位置</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://remove-all.sh" target="_blank" rel="noopener">remove-all.sh</a></td>
<td>执行删除集群入口</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
<tr>
<td>remove-host-info.properties</td>
<td>配置需要删除集群的相关节点信息</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
<tr>
<td>remove-init-mysql- <a href="http://udp.sh" target="_blank" rel="noopener">udp.sh</a></td>
<td>清除数据库脚本文件</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
<tr>
<td>remove_db_udp.sql</td>
<td>删除数据库 sql 文件</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
<tr>
<td>stop-all-residual- <a href="http://process.sh" target="_blank" rel="noopener">process.sh</a></td>
<td>清除残留进程，删除残留目录</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
<tr>
<td>stop-management- <a href="http://program.sh" target="_blank" rel="noopener">program.sh</a></td>
<td>停止 USDP 管理端 server 和agent 进程</td>
<td>/opt/usdp- srv/usdp/repair/remove</td>
</tr>
</tbody>
</table>
<h1><strong>1.1</strong> <em><strong>*.2*</strong></em> <em><strong>*清除集群步骤*</strong></em></h1>
<h2 id="1-1-1-修改-remove-host-info-properties-配置文件"><strong>1.1.1</strong> <em><strong>*修改*</strong></em> <em><strong>*remove-host-info.properties*</strong></em> <em><strong>*配置文件*</strong></em></h2>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><img src="../images/wps1.png" alt="img"></td>
</tr>
</tbody>
</table>
<p><img src="../images/wps2.png" alt="img"></p>
<p>上述配置项解释如下：</p>
<table>
<thead>
<tr>
<th><strong>remove-host-info.properties</strong> <strong>文件配置项</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>usdp.ip.x</td>
<td>配置删除集群的节点 ip</td>
</tr>
<tr>
<td>usdp.ssh.port.x</td>
<td>配置删除集群的节点端口号</td>
</tr>
<tr>
<td>remove.host.num</td>
<td>配置清除集群包含的节点数</td>
</tr>
<tr>
<td>mysql.ip</td>
<td>mysql 所在节点 ip</td>
</tr>
<tr>
<td>mysql.host.ssh.port</td>
<td>mysql 所在节点 ssh 端口号</td>
</tr>
<tr>
<td>mysql.host.ssh.password</td>
<td>mysql 所在节点密码</td>
</tr>
<tr>
<td>mysql.password</td>
<td>mysql 数据库登录密码</td>
</tr>
</tbody>
</table>
<h2 id="1-1-2-执行清除脚本"><strong>1.1.2</strong> <em><strong>*执行清除脚本*</strong></em></h2>
<p>修改完上述配置文件，即可进入 remove 目录，执行一键清除脚本</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><img src="../images/wps3.png" alt="img"></td>
</tr>
</tbody>
</table>
]]></content>
  </entry>
  <entry>
    <title>黑鲨游戏手机解BL锁，刷REC，ROOT 和问题总结</title>
    <url>/2021/07/14/%E9%BB%91%E9%B2%A8%E6%B8%B8%E6%88%8F%E6%89%8B%E6%9C%BA%E8%A7%A3BL%E9%94%81%EF%BC%8C%E5%88%B7REC%EF%BC%8CROOT%20%E5%92%8C%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="黑鲨游戏手机解BL锁，刷REC，ROOT-和问题总结">黑鲨游戏手机解BL锁，刷REC，ROOT 和问题总结</h2>
<p>参考帮助  <a href="https://www.bilibili.com/video/BV1Cf4y1U7gn?from=search&amp;seid=14731760489436726457" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Cf4y1U7gn?from=search&amp;seid=14731760489436726457</a></p>
<h2 id="总的步骤：">总的步骤：</h2>
<p><img src="/images/image-20210714100219695.png" alt="image-20210714100219695"></p>
<p>需要用到的文件</p>
<p><img src="/images/image-20210714110634447.png" alt="image-20210714110634447"></p>
<h3 id="第一步-系统降级">第一步.系统降级</h3>
<p>黑鲨1代<br>
系统版本:G66X1811300CN00MPX安卓版本:8.1下载地址:<a href="https://ota-1256119282.file.myqcloud.com/SKR-A0/2018-11-30_shark_mp_ota11_20180929shark-ota-eng.buildfarm.zip" target="_blank" rel="noopener">https://ota-1256119282.file.myqcloud.com/SKR-A0/2018-11-30_shark_mp_ota11_20180929shark-ota-eng.buildfarm.zip</a><br>
系统版本:G66X1905170CN00MPP安卓版本:P(安卓9)下载地址:<a href="https://ota-1256119282.file.myqcloud.com/SKR-A0/shark-ota-eng.buildfarm_2019-05-17_sdm845_p_mp_ota1_20190430.zip" target="_blank" rel="noopener">https://ota-1256119282.file.myqcloud.com/SKR-A0/shark-ota-eng.buildfarm_2019-05-17_sdm845_p_mp_ota1_20190430.zip</a></p>
<p><strong>刷机的准备工作：</strong></p>
<p>1、根据自己机型下载对应版本型号的黑鲨手机全量包到电脑，将文件重命名为<strong>update.zip</strong></p>
<p>2、将<strong>update.zip</strong>文件放到手机根目录的<strong>ota文件夹</strong>下（如果手机本身有一个OTA文件夹（大写的），就删掉重建一个ota文件夹），</p>
<p><strong>1、打开手机的拨号界面，输入*#*#1027#*#*后，就自动进入本地升级界面</strong></p>
<img src="/images/image-20210714100123909.png" alt="image-20210714100123909" style="zoom:50%;" />
<p>2、当更新的时，图示变成了为</p>
<p><strong>＋＋＋＋＋＋OTA success,please Reboot＋＋＋＋＋＋</strong></p>
<p>字样，说明你已经成功升级，只需重启系统就可以正常的进入新系统了。</p>
<img src="/images/image-20210714100503207.png" alt="image-20210714100503207" style="zoom:50%;" />
<h4 id="遇到的问题：">遇到的问题：</h4>
<p>手机降级后，重启手机需要密码</p>
<img src="/images/image-20210714100618904.png" alt="image-20210714100618904" style="zoom:50%;" />
<p>需要双清手机</p>
<p><strong>按住音量上键不放按住电源键重启，手机一重启就松开电源键，音量键不松</strong></p>
<h2 id="双清：">双清：</h2>
<p>wipe data/ factory reset, 清除用户数据并恢复出厂设置（刷机前必须执行的选项）</p>
<p>wipe cache partition，清除系统缓存（刷机前执行）（系统出问题也可尝试此选项，一般都能够解决）</p>
<p>这样就完成了系统降级，本人是刷到了安卓8.1</p>
<img src="/images/image-20210714101353437.png" alt="image-20210714101353437" style="zoom:50%;" />
<h3 id="第二步-解BL锁">第二步 解BL锁</h3>
<p>用的 黑鲨全机型全版本一键解锁BL，QQ群177600200 的文件里可以下载</p>
<p><img src="/images/image-20210714102642570.png" alt="image-20210714102642570"></p>
<h4 id="遇到的问题">遇到的问题</h4>
<p>出现多个端口</p>
<p><img src="/images/image-20210714101613505.png" alt="image-20210714101613505"></p>
<p>禁用掉多的端口就可以了，刷好之后 再解禁端口。</p>
<p><img src="/images/image-20210714102255099.png" alt="image-20210714102255099"></p>
<h3 id="第三步-刷入第三方中文版TWRP-Recovery，简称-“刷rec”">第三步  刷入第三方中文版TWRP-Recovery，简称 “刷rec”</h3>
<p>recovery下载：<a href="https://pan.baidu.com/s/1QzY2qS7TsTGr-Y-H2-OC5Q" target="_blank" rel="noopener">https://pan.baidu.com/s/1QzY2qS7TsTGr-Y-H2-OC5Q</a></p>
<p>我下载的黑鲨1安卓8的   TWRP-3.3.1-0526-BLACKSHARK_SHARK-CN-wzsx150-fastboot.7z</p>
<p>刷入方法：</p>
<p>下载后解压（存放路径中不要有中文和空格等），</p>
<p>手机连接电脑，</p>
<p>双击运行一键刷入recovery工具.bat文件，按照提示操作</p>
<p>需要提供手机内系统相同版本的boot镜像文件（网盘地址中有提供，也可以自己提取 ），我直接下载网盘里的 boot-G66X1811300CN00MPX.img 文件</p>
<p>进入第三方TWRP_Recovery方法：</p>
<p>手机彻底关机，音量上和开机按键双手一起按住，不要松开<br>
等待手机震动后，松开开机按键，音量上不放手，就进入了recovery</p>
<h4 id="遇到的问题：-2">遇到的问题：</h4>
<p>存放路径中有中文，放到没中文的目录就可以了。</p>
<h3 id="第四步-通过Magisk-刷root">第四步 通过Magisk 刷root</h3>
<p>由于直接用recovery里的清楚Root功能 ，重启后并没有生效。我通过刷面具Magisk-v19.3(19300).zip 实现</p>
<p>参考：<a href="https://www.guanjiaxiaoe.com/130.html" target="_blank" rel="noopener">https://www.guanjiaxiaoe.com/130.html</a></p>
<p>4.1 把Magisk-v19.3(19300).zip导入到手机根目录</p>
<p>4.2 进入rec ，点击安装，选择Magisk-v19.3(19300).zip</p>
<p>安装好面具后，需要root权限的应用，当第一次运行的时候它会要求获取Root权限，此时Magisk会自动弹出，点击允许即可获取Root权限。</p>
<h4 id="遇到的问题-2">遇到的问题</h4>
<p>直接用recovery里的清除Root功能 ，重启后并没有生效。可能是版本低了。</p>
<img src="/images/E7051906A010BA6576A73B7DCD94B818.jpg" alt="img" style="zoom:50%;" />
]]></content>
  </entry>
  <entry>
    <title>服务器管理规范(V1)</title>
    <url>/2021/07/13/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AE%A1%E7%90%86%E8%A7%84%E8%8C%83(V1)/</url>
    <content><![CDATA[<h4 id="服务器管理规范-V1">服务器管理规范(V1)</h4>
<p><strong>一 配置管理规范</strong></p>
<p>所有设备信息必须录入配置管理系统，在配置系统中能随时查询到现网业务的部署分布情况 具体信息待配置管理系统建立后再补充</p>
<p>主机命名规范</p>
<p>网卡vlan规范</p>
<p>安全策略命名规范</p>
<p>监控/部署/插件/模块命名规范</p>
<p>版本命名规范</p>
<p><strong>二 文件系统管理规范</strong></p>
<p>2.1 文件布局</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 系统服务及配置文件优先采用LSF标准，其余文件可采用BSD标准[注1]，例如:</span><br><span class="line">&#x2F;etc&#x2F;vimrc         ---- vim默认配置</span><br><span class="line">&#x2F;etc&#x2F;bash.bashrc   ---- shell环境配置</span><br><span class="line">. 用户脚本</span><br><span class="line">&#x2F;opt&#x2F;admin&#x2F;shell&#x2F;  ---- 自定义脚本，函数</span><br><span class="line">&#x2F;opt&#x2F;admin&#x2F;cron&#x2F;   ---- cron脚本</span><br><span class="line">&#x2F;opt&#x2F;admin&#x2F;firwall&#x2F;   ---- 防火墙脚本</span><br><span class="line">. data分区(&#x2F;data或&#x2F;home, 这里以&#x2F;data为基准)</span><br><span class="line">&#x2F;data&#x2F;db_dir&#x2F;mysql        ---- mysql data目录</span><br><span class="line">&#x2F;data&#x2F;db_dir&#x2F;redis        ---- redis data目录</span><br><span class="line">&#x2F;data&#x2F;www          ---- web目录</span><br><span class="line">&#x2F;data&#x2F;backup       ---- 备份目录</span><br><span class="line">注1: LSF(Linux Stand Filesytem)</span><br><span class="line">     BSD: 软件包通常位于:   &#x2F;usr&#x2F;local&#x2F;$package</span><br><span class="line">     SYS V: 软件包通常位于: &#x2F;opt&#x2F;$package</span><br><span class="line"> 程序原始文件必须放置于&#x2F;data目录下，可以自行加入软连接 如：ln -s &#x2F;data&#x2F;apps &#x2F;opt&#x2F;apps</span><br><span class="line"> 数据放置于&#x2F;data下</span><br></pre></td></tr></table></figure>
<p>2.2 文件修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 默认配置文件    ---- xxxx.orig</span><br><span class="line">  在没有任何改动的情况下以orig扩展名做一次备份并保留注释，以供参考</span><br><span class="line">. 修改配置文件    ---- xxxx.$date</span><br><span class="line">  对当前配置文件做出修改时, 建议首先以xxxx.$date的命名方式对其做一次备份.</span><br><span class="line">. 当前配置文件</span><br><span class="line">  建议移除相关注释及空行, 在有缩进的情况下以四个空格作为缩进，以保证阅读的清爽性.</span><br></pre></td></tr></table></figure>
<p><strong>三 软件包管理规范</strong></p>
<p>3.1 包管理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 采用自动部署工具(salt, puppet等)管理相应软件包，应避免手动直接安装。</span><br><span class="line">. 只保留安全补丁升级，应避免系统库及相应服务升级。</span><br><span class="line">. 建立官方仓库本地镜像及私有仓库。</span><br></pre></td></tr></table></figure>
<p>3.2 包安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 尽量采用官方源，及稳定的三方源安装相应软件包。</span><br><span class="line">. 如有必须源码编译[注1]，务必遵照Debian官方打包方式进行打包[注2]，以保持LSF规范及自动化管理。</span><br><span class="line">. 自打包程序通过测试及审核后放入私有仓库。</span><br><span class="line">注1: GCC保持默认的o2就好，不要修改CFLAG，以稳定为优先原则。</span><br><span class="line">注2: 勿用checkinstall直接打包。</span><br></pre></td></tr></table></figure>
<p><strong>四 日志管理规范</strong></p>
<p>4.1 系统日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 系统日志服务统一采用syslog-ng, 不应与rsyslogd混用</span><br><span class="line">. 针对不同日志类型, 存于不同的文件. 例如</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;auth.log      ---- 安全日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;kern.log      ---- 内核日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;user.log      ---- 用户日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;daemon.log    ---- 守护进程日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;misc.log      ---- </span><br><span class="line">&#x2F;var&#x2F;log&#x2F;cron.log      ---- 计划任务日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;syslog        ---- 系统日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;boot.log      ---- 引导日志</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;messages      ---- 所有日志</span><br><span class="line">. 系统级日志保留7天回滚, 服务级日志保留15天回滚, 并做定期检查</span><br></pre></td></tr></table></figure>
<p>4.2 操作日志</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">. 将所有ssh操作日志记录于文件, 方便系统管理员定位具体时间点的操作, 例如:</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;audit&#x2F;sysop   </span><br><span class="line">&#x2F;var&#x2F;log&#x2F;audit&#x2F;dba</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;audit&#x2F;root</span><br></pre></td></tr></table></figure>
<p><strong>五 安全管理规范</strong></p>
<p>5.1 .网络访问规则 业务模块类型不同策略组尽量不通，比如db类，前端类，下载类等等。（这样做的目的是尽量把权限控制死，减少黑客入口） ACL 外网策略默认入默认全关，针对访问需求开放 外网策略默认出全开 内网默认出和入均开放 不同vlan之间做绝对隔离 iptables 外网策略默认入全关，针对访问需求开放 外网策略默认出全开 内网默认出和入均开放 5.2 .程序监听端口 非特殊需求1024以下端口禁止使用，且定义为高危端口，若发现高危端口暴露公网则进行罚款警告 数据库端口和ssh进程端口严谨暴露外网 只使用内网访问的程序禁止使用0.0.0.0监听</p>
<p><strong>六 DB操作规范</strong></p>
<p>6.1 用户权限分级 业务账户 备份账户 管理账户 其他需求账户（主要指查询） 主从复制账户 6.2 修改db和数据前先备份,大型db变更可以先停掉slave,待变更完成后再开启。 6.3 禁止擅自修改数据，若要修改需要提交需求 6.4 db 备份必须要有异地备份,db 需要打开binlog,备份需要有slave. 6.5 不确定情况请找DBA 确认。</p>
<p><strong>七 版本更新规范</strong></p>
<p>7.4 版本更新checklist模板制定 每次版本更新需要针对实际操作情况根据checklist模板进行细化制定。 7.5 禁止开发登陆服务器进行更新和修改操作（特殊情况请说明） 7.6 未经测试通过或者有严重bug的版本禁止对外发布，如需要发布，需要项目PM和QA确认。 7.7 临时修改发布内容视实际情况自行评估，原则上确定的内容临时调整不接受。</p>
<p><strong>八 故障处理规范 .</strong></p>
<p>大型故障处理 .服务器故障 .业务故障处 主要是checklist,需要包含故障现象，分析问题过程和故障处理恢复过程</p>
<p>**九 监控规范 **</p>
<p>业务上线后必须马上加入监控,此作为上线的其他步骤同等重要 监控中必需监控指标项必须加入（之前lorin有提供文档）</p>
<p><strong>十 其他变更规范</strong></p>
<p>搬迁，开服，新功能上线等都属于变更范畴。</p>
<p>10.1 新服开放 需要提前主动搜集运营需求，进行资源的准备和规划 需要准备变更所需checklist 10.2 搬迁及升级 升级扩容预案 搬迁方案的准备 回滚方案准备 数据一致性校验 以上方案在操作前需要提交运维团队进行评审确认。</p>
]]></content>
  </entry>
  <entry>
    <title>Docker常用命令</title>
    <url>/2021/06/29/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th>COMMAND</th>
<th>DESC</th>
</tr>
</thead>
<tbody>
<tr>
<td>查看</td>
<td></td>
</tr>
<tr>
<td>docker images</td>
<td>列出所有镜像(images)</td>
</tr>
<tr>
<td>docker ps</td>
<td>列出正在运行的容器(containers)</td>
</tr>
<tr>
<td>docker ps -a</td>
<td>列出所有的容器</td>
</tr>
<tr>
<td>docker pull centos</td>
<td>下载centos镜像</td>
</tr>
<tr>
<td>docker top ‘container’</td>
<td>查看容器内部运行程序</td>
</tr>
<tr>
<td>容器</td>
<td></td>
</tr>
<tr>
<td>docker exec -it 容器ID sh</td>
<td>进入容器</td>
</tr>
<tr>
<td>docker stop ‘container’</td>
<td>停止一个正在运行的容器，‘container’可以是容器ID或名称</td>
</tr>
<tr>
<td>docker start ‘container’</td>
<td>启动一个已经停止的容器</td>
</tr>
<tr>
<td>docker restart ‘container’</td>
<td>重启容器</td>
</tr>
<tr>
<td>docker rm ‘container’</td>
<td>删除容器</td>
</tr>
<tr>
<td>docker run -i -t -p :80 LAMP /bin/bash</td>
<td>运行容器并做http端口转发</td>
</tr>
<tr>
<td>docker exec -it ‘container’ /bin/bash</td>
<td>进入ubuntu类容器的bash</td>
</tr>
<tr>
<td>docker exec -it /bin/sh</td>
<td>进入alpine类容器的sh</td>
</tr>
<tr>
<td>docker rm <code>docker ps -a -q</code></td>
<td>删除所有已经停止的容器</td>
</tr>
<tr>
<td>docker kill $(docker ps -a -q)</td>
<td>杀死所有正在运行的容器，$()功能同``</td>
</tr>
<tr>
<td>镜像</td>
<td></td>
</tr>
<tr>
<td>docker build -t wp-api .</td>
<td>构建1个镜像,-t(镜像的名字及标签) wp-api(镜像名) .(构建的目录)</td>
</tr>
<tr>
<td>docker run -i -t wp-api</td>
<td>-t -i以交互伪终端模式运行,可以查看输出信息</td>
</tr>
<tr>
<td>docker run -d -p 80:80 wp-api</td>
<td>镜像端口 -d后台模式运行镜像</td>
</tr>
<tr>
<td>docker rmi [image-id]</td>
<td>删除镜像</td>
</tr>
<tr>
<td>docker rmi $(docker images -q)</td>
<td>删除所有镜像</td>
</tr>
<tr>
<td>docker rmi $(sudo docker images --filter “dangling=true” -q --no-trunc)</td>
<td>删除无用镜像</td>
</tr>
<tr>
<td>docker run --help</td>
<td>帮助</td>
</tr>
</tbody>
</table>
<blockquote>
<p>更多命令查看Docker 命令大全 | 菜鸟教程</p>
</blockquote>
<h3 id="场景二：下载镜像并直接运行">场景二：下载镜像并直接运行</h3>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">docker run  --name ubuntu -it ubuntu bash </span><br><span class="line">docker cp dd ubuntu:tmp/ #复制文件dd 到容器的/tmp 目录</span><br><span class="line">Ctrl-p Ctrl-q  #退出</span><br></pre></td></tr></table></figure>
<h3 id="场景三：修改镜像，并保存到私有仓库">场景三：修改镜像，并保存到私有仓库</h3>
<blockquote>
<p>期望结果:在ubuntu 镜像中添加 apache，将新的镜像保存到私有仓库中</p>
</blockquote>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">docker exec -it ubuntu bash </span><br><span class="line">apt-<span class="keyword">get</span> update</span><br><span class="line">apt-<span class="keyword">get</span> install apache2</span><br><span class="line">Ctrl-p Ctrl-q  #退出</span><br><span class="line">docker commit -a "mir355" -m "ubuntu add apache2" &#123;ID&#125;  private/ubuntu_apache:v1   #保存镜像</span><br><span class="line">docker stop ubuntu </span><br><span class="line">docker rm ubuntu</span><br><span class="line">docker run -i -t --name apache2 -p <span class="number">8080</span>:<span class="number">80</span> private/ubuntu_apache:v1 /bin/bash</span><br><span class="line">/etc/init.d/apache2 start</span><br><span class="line">Ctrl-p Ctrl-q  #退出</span><br><span class="line">#通过 docker tag重命名镜像，使之与registry匹配</span><br><span class="line">docker tag private/ubuntu_apache:v1 <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">5000</span>/private/ubuntu_apache:v1</span><br><span class="line">#保存到私有仓库</span><br><span class="line">docker push <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">5000</span>/private/ubuntu_apache:v1</span><br><span class="line">curl http:<span class="comment">//127.0.0.1:5000/v2/_catalog</span></span><br><span class="line"></span><br><span class="line">#下载镜像</span><br><span class="line">docker pull <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">5000</span>/private/ubuntu_apache:v1</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>mapreduce、spark的样例</title>
    <url>/2021/06/25/mapreduce%E3%80%81spark%E7%9A%84%E6%A0%B7%E4%BE%8B/</url>
    <content><![CDATA[<h1>mapreduce、spark的样例</h1>
<h3 id="指定队列和优先级">指定队列和优先级</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi \</span><br><span class="line">-D mapreduce.job.queuename=bf_yarn_pool.development \</span><br><span class="line">-D mapreduce.job.priority=VERY_HIGH 5 5</span><br></pre></td></tr></table></figure>
<h3 id="TestDFSIO">TestDFSIO</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -u hdfs hadoop jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.0.1-tests.jar  TestDFSIO \</span><br><span class="line">-D mapreduce.job.queuename&#x3D;bf_yarn_pool.production \</span><br><span class="line">-D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark \</span><br><span class="line">-D mapreduce.output.fileoutputformat.compress&#x3D;false \</span><br><span class="line">-write -nrFiles 10 -fileSize 1000</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Windows Server 2019和Win10 安装 Linux 子系统</title>
    <url>/2021/05/18/Windows%20Server%202019%E5%92%8CWin10%20%E5%AE%89%E8%A3%85%20Linux%20%E5%AD%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p><strong>Windows Server</strong>版本 安装参考  <a href="https://blog.51cto.com/professor/2411436" target="_blank" rel="noopener">https://blog.51cto.com/professor/2411436</a></p>
<p>注意：如果先安装的ConEmu ，可能没有bash  ，刷新一系default tasks，如下图：</p>
<p><img src="/images/image-20210518154415530.png" alt="image-20210518154415530"></p>
<p>win10版本 参考之前写的文档：</p>
<p><strong>Win10 Subsystem Linux : Ubuntu 的root密码</strong></p>
<p>每次开机都有一个新的root密码。我们可以在终端输入命令 sudo passwd，</p>
<p>然后输入当前用户的密码，enter，终端会提示我们输入新的密码并确认，</p>
<p>此时的密码就是root新密码。修改成功后，输入命令 su root，再输入新的密码就ok了。</p>
<p><strong>win10 linux子系统设置默认用户</strong></p>
<p><a href="https://blog.csdn.net/ijiabao520/article/details/79285041" target="_blank" rel="noopener">https://blog.csdn.net/ijiabao520/article/details/79285041</a></p>
<p>lxrun是旧版的，不可使用了。</p>
<p>在cmd终端输入：</p>
<p>$ ubuntu1804 config --default-user root</p>
<p>window终端推荐 ConEmu  超级好用</p>
<p><a href="https://conemu.github.io/en/" target="_blank" rel="noopener">https://conemu.github.io/en/</a></p>
<p>添加到右键菜单</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;REUSE &#x2F;REUSE -run &#123;Bash::bash&#125; -cur_console:n</span><br></pre></td></tr></table></figure>
<p><img src="/images/clipboard.png" alt="img"></p>
<p><strong>win10下Linux子系统开启ssh服务</strong></p>
<p>Windows10开启Ubuntu子系统简易步骤  (现在不需要开启开发者模式了)</p>
<p><a href="https://zhuanlan.zhihu.com/p/34133795" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34133795</a></p>
<p><strong>开启win10下Ubuntu子系统的SSH服务</strong></p>
<p><a href="https://blog.csdn.net/zhouzme/article/details/81087837" target="_blank" rel="noopener">https://blog.csdn.net/zhouzme/article/details/81087837</a></p>
<p>打开/etc/rc.local文件加入：</p>
<p>/etc/init.d/ssh start</p>
]]></content>
  </entry>
  <entry>
    <title>运维和发展线路</title>
    <url>/2021/05/18/%E8%BF%90%E7%BB%B4%E5%92%8C%E5%8F%91%E5%B1%95%E7%BA%BF%E8%B7%AF/</url>
    <content><![CDATA[<h2 id="运维和发展的一个线路">运维和发展的一个线路</h2>
<ul>
<li>1.搭建服务（部署并运行起来）</li>
<li>2.用好服务（监控、管理、优化）</li>
<li>3.自动化（服务直接的关联和协同工作）</li>
<li>4.产品设计（如何设计一个监控系统）</li>
</ul>
<p><strong>云计算的核心竞争力是运维！</strong></p>
<p>系统架构师（偏管理）：网络  系统  数据库  开发  云计算  自动化  运维管理  服务管理 项目管理  测试  业务<br>
专注于某一领域<br>
解决方案架构师</p>
<h2 id="运维工作内容分类">运维工作内容分类</h2>
<ul>
<li>监控运维（7x24运维值班、故障处理）</li>
<li>应用运维（业务熟悉、服务部署、业务部署、版本管理、灰度发布、应用监控）</li>
<li>安全运维（整体的安全方案、规范、漏洞监测、安全防护等）</li>
<li>系统运维（架构层面的分布式缓存、分布式文件系统、日志收集、环境规划（测试、开发、生产）、架构设计、性能优化）</li>
<li>基础服务运维（包含运维开发）（内部DNS、负载均衡、系统监控、资产管理、运维平台）</li>
<li>基础设施运维（系统初始化、网络维护）</li>
<li>机房运维（负责设备上下架、巡检、报修、硬件监控）</li>
</ul>
<p>阿里云:<br>
SLB  LVS + Tengine（Nginx）<br>
ECS  KVM</p>
<h2 id="运维标准化">运维标准化</h2>
<p><strong>物理设备层面：</strong><br>
1.服务器标签化、设备负责人、设备采购详情、设备摆放标准<br>
2.网络划分、远程控制卡、网卡端口<br>
3.服务器机型、硬盘、内存统一，根据业务分类<br>
4.资产命名规范、编号规范、类型规范<br>
5.监控标准</p>
<p><strong>操作系统层面</strong><br>
1.操作系统版本<br>
2.系统初始化（配置DNS、NTP、内核参数调优）<br>
3.基础Agent配备（Zabbix agent、logstash agent、salt minion）<br>
4.系统监控标准（CPU、内存、硬盘、网络、进程）</p>
<p><strong>应用服务层面：</strong><br>
1.Web服务器选型（nginx、Apache）<br>
2.进程启动用户、端口监听规范、日志收集规范（访问日志、错误日志、运行日志）<br>
3.配置管理（配置文件规范、脚本规范）<br>
4.架构规范（Nginx+keepalived、LVS+keepalived等等）<br>
5.部署规范（位置、包命名等）</p>
<p><strong>运维操作层面：</strong><br>
1.机房巡检流程（周期、内容、保修流程）<br>
2.业务部署流程（先测试、后生产。回滚）<br>
3.故障处理流程（紧急处理、故障升级、重大故障处理）<br>
4.工作日志流程（如何编写工作日志）<br>
5.业务上线流程（1.项目发起人 2.系统安装 3.部署nginx 4.解析域名 5.测试 6.加监控）<br>
6.业务下线流程（谁发起，数据如何处理）<br>
7.运维安全规范（密码复杂度、更改周期、VPN使用规范、服务登陆规范、rm命令的参数写在最后面）</p>
<p>标准化：规范化  流程化  文档化</p>
<p>目标：文档化</p>
<h2 id="运维自动化发展-工具化">运维自动化发展-工具化</h2>
<p><strong>工具化：</strong></p>
<ul>
<li>1.shell脚本（功能性（流程）脚本、检查性、报表性）</li>
<li>2.开源工具：zabbix、elkstack、saltstack、cobbler</li>
</ul>
<p><strong>目标：</strong></p>
<ul>
<li>1.促进标准化的实施</li>
<li>2.将重复的操作简单化</li>
<li>3.将多次操作流程化</li>
<li>4.减少人为操作的低效和降低故障率</li>
</ul>
<p>工具化和标准化是好搭档</p>
<p><strong>痛点：</strong></p>
<ul>
<li>1.你至少要ssh到服务器执行，可能出错</li>
<li>2.多个脚本有执行顺序的时候，可能出错</li>
<li>3.权限不好管理，日志没法统计</li>
<li>4.无法避免手工操作</li>
</ul>
<p><strong>例子：</strong></p>
<p>比如某天我们要对一个数据库从库进行版本停机升级。那么要求评估：<br>
停机影响：<br>
3：00  晚上有定时任务连接该数据库，做数据报表统计</p>
<ul>
<li>1.凌晨3：00 我们所有系统的定时任务有哪些crontab</li>
<li>2.这些crontab哪些要连接我们要停止的从库</li>
<li>3.哪些可以停，哪些不能停（修改到主库），哪些可以后补</li>
<li>4.这些需要后补的脚本哪个业务的，谁加的，什么时候加的</li>
</ul>
<h2 id="运维自动化发展-web化">运维自动化发展-web化</h2>
<p>运维平台<br>
例子：Job管理平台</p>
<ul>
<li>1.做成web界面</li>
<li>2.权限控制</li>
<li>3.日志记录</li>
<li>4.弱化流程</li>
<li>5.不用ssh到服务器，减少人为操作造成故障 Web ssh</li>
</ul>
<p>DNSWeb管理 bind-DLZ<br>
负载均衡Web管理<br>
Job管理平台<br>
监控平台  zabbix<br>
操作系统安装平台</p>
<h2 id="运维自动化发展-服务化（API）">运维自动化发展-服务化（API）</h2>
<ul>
<li>DNSWeb管理 bind-DLZ  dns-api</li>
<li>负载均衡Web管理     slb-api</li>
<li>Job管理平台    job-api</li>
<li>监控平台  zabbix  zabbix-api</li>
<li>操作系统安装平台   cobbler-api</li>
<li>部署平台    deploy-api</li>
<li>配置管理    saltstack-api</li>
</ul>
<p><strong>智能化实现</strong></p>
<ul>
<li>1.调用cobbler-api安装操作系统</li>
<li>2.调用saltstack-api进行系统初始化</li>
<li>3.调用dns-api解析主机名</li>
<li>4.调用zabbix-api将该新上线机器加上监控</li>
<li>5.再次调用saltstack-api部署软件（安装nginx+php）</li>
<li>6.调用deploy-api将当前版本的代码部署到服务器上</li>
<li>7.调用test-api 测试当前服务器运行是否正常</li>
<li>8.调用slb-api 将该节点加入集群</li>
</ul>
<h2 id="运维自动化发展-智能化">运维自动化发展-智能化</h2>
<p>运维自动化发展层级：</p>
<ul>
<li>标准化、工具化</li>
<li>Web化、平台化</li>
<li>服务化、API化</li>
<li>智能化</li>
</ul>
<p>智能化的自动化扩容、缩容、服务降级、故障自愈</p>
<p><strong>自动化扩容</strong><br>
1.zabbix触发Action<br>
触发条件和决策：</p>
<ul>
<li>1.当某个集群的访问量超过最大支撑量，比如10000</li>
<li>2.并持续5分钟</li>
<li>3.不是攻击</li>
<li>4.资源池有可用资源
<ul>
<li>当前网络带宽使用率</li>
<li>如果是公有云–钱够不够</li>
</ul>
</li>
<li>5.当前后端服务支撑量是否超过阈值  如果超过应该后端先扩容</li>
<li>6.数据库是否可以支撑当前并发</li>
<li>7.当前自动化扩展队列，是否有正在扩容的节点</li>
<li>其他业务相关的</li>
</ul>
<p>创建虚拟机之前，先判断Buffer是否有最近X小时已经存在之前已经移除的虚拟机，并查询软件版本是否和当前一致，如果一致，跳过234步，如果不一致，跳过23步</p>
<p>2.Openstack 创建虚拟机<br>
3.Saltstack 配置环境<br>
4.部署系统 部署当前代码<br>
5.测试服务是否可用(注意间隔和次数)<br>
6.加入集群<br>
7.通知(短信、邮件)</p>
<p><strong>自动化缩容</strong></p>
<ul>
<li>1.触发条件和决策</li>
<li>2.从集群中移除节点</li>
<li>3.通知</li>
<li>4.移除的节点存放于Buffer里面</li>
<li>5.Buffer里面超过1天的虚拟机，自动关闭，存放于xx区</li>
<li>6.xx区的虚拟机，超过7天的清理删除</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>CDH Disk Balancer 磁盘数据均衡</title>
    <url>/2021/04/30/CDH%20Disk%20Balancer%20%E7%A3%81%E7%9B%98%E6%95%B0%E6%8D%AE%E5%9D%87%E8%A1%A1/</url>
    <content><![CDATA[<h3 id="CDH-Disk-Balancer-磁盘数据均衡">CDH Disk Balancer 磁盘数据均衡</h3>
<p>1.设置dfs.disk.balancer.enabled  为true , 可以单个datanode设置，重启单个datanode生效。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.disk.balancer.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<img src="/images/image-20210429175922996.png" alt="image-20210429175922996" >
<p>1.创建均衡任务并生成计划任务配置文件</p>
<p>sudo -u hdfs hdfs diskbalancer -plan cdh85-73</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@cdh85-73 tmp]# sudo -u hdfs hdfs diskbalancer -plan cdh85-73</span><br><span class="line">21/04/29 17:02:45 INFO planner.GreedyPlanner: Starting plan for Node : cdh85-73:9867</span><br><span class="line">21/04/29 17:02:45 INFO planner.GreedyPlanner: Disk Volume set f4da1504-1dfc-42d0-ac61-020dd7b02620 Type : DISK plan completed.</span><br><span class="line">21/04/29 17:02:45 INFO planner.GreedyPlanner: Compute Plan for Node : cdh85-73:9867 took 7 ms </span><br><span class="line">21/04/29 17:02:46 INFO command.Command: Writing plan to:</span><br><span class="line">21/04/29 17:02:46 INFO command.Command: /system/diskbalancer/2021-Apr-29-17-02-45/cdh85-73.plan.json</span><br><span class="line">Writing plan to:</span><br><span class="line">/system/diskbalancer/2021-Apr-29-17-02-45/cdh85-73.plan.json</span><br></pre></td></tr></table></figure>
<p>注意： 这个路径是HDFS的路径，不是本地路径</p>
<p>2.查看配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@cdh85-73 tmp]# hdfs dfs -ls  /system/diskbalancer/2021-Apr-29-16-45-51</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 hdfs supergroup     192586 2021-04-29 16:45 /system/diskbalancer/2021-Apr-29-16-45-51/cdh85-73.before.json</span><br><span class="line">-rw-r--r--   3 hdfs supergroup       4546 2021-04-29 16:45 /system/diskbalancer/2021-Apr-29-16-45-51/cdh85-73.plan.json</span><br></pre></td></tr></table></figure>
<p>3.启动均衡任务</p>
<p>sudo -u hdfs hdfs diskbalancer -execute /system/diskbalancer/2021-Apr-29-16-51-42/cdh85-73.plan.json</p>
<p>注意：这里执行后  立马结束了  看不到效果，实际上是生效的</p>
<p>4.查看状态</p>
<p>sudo -u hdfs hdfs diskbalancer -query cdh85-73</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@cdh85-73 tmp]# sudo -u hdfs hdfs diskbalancer -query cdh85-73</span><br><span class="line">21/04/29 17:42:24 INFO command.Command: Executing "query plan" command.</span><br><span class="line">Plan File: /system/diskbalancer/2021-四月-29-16-40-04/cdh85-73.plan.json</span><br><span class="line">Plan ID: bb2087b0d6dfbe65835e2831836fd814cf70e605</span><br><span class="line">Result: PLAN_UNDER_PROGRESS</span><br></pre></td></tr></table></figure>
<p>完成后的状态，耗时大约20几个Hours</p>
]]></content>
  </entry>
  <entry>
    <title>chia挖矿</title>
    <url>/2021/04/27/chia%E6%8C%96%E7%9F%BF/</url>
    <content><![CDATA[<h4 id="用家里的老电脑做一个测试-，p了一块k32的盘">用家里的老电脑做一个测试 ，p了一块k32的盘</h4>
<p>矿池的具体接入教程描述得非常浅显易懂了，这里我就不做重复搬运了，直接参考 <a href="https://www.hpool.com/help/tutorial/Chia%E6%8C%96%E7%9F%BF%E6%95%99%E7%A8%8B" target="_blank" rel="noopener">Chia挖矿教程</a></p>
<p>linux P盘</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup &#x2F;opt&#x2F;chia-plotter&#x2F;chia-plotter-linux-amd64 -action plotting -plotting-fpk 0x85f80829a93d960313a99ca5482703fea2caae1d07db589344e76eba135db14c8f70d08dadc991805ae917d61626fd8d -plotting-ppk 0x970214947045bd1c6fbb0b3b3499dafab837eb26b58356504aea4d8ee19e9c5c064a5dfdad0cb5f7f047e0030f088a65 -plotting-n 1 -b 8000 -t &#x2F;data1&#x2F;chia -d &#x2F;opt&#x2F;chia  &gt;&gt; plots2.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
<p>晒一下收益效果：</p>
<p><img src="/images/image-20210427101445602.png" alt="image-20210427101445602"></p>
<h4 id="相关资料：">相关资料：</h4>
<p><strong>谋杀SSD磁盘</strong></p>
<p><a href="https://www.expreview.com/78802.html" target="_blank" rel="noopener">https://www.expreview.com/78802.html</a></p>
<blockquote>
<p>生成一个K=32文件大概需要6.5小时，生成三个K33两个K32文件总共占了829GB的硬盘空间，而HDD的写入量是840.1GB，但SSD的读写非常厉害，整个P盘过程，SSD读取11.8TB，写入12.06TB，因为SSD的写入次数是有限的，把这个6TB的红盘P满虽然不至于把SSD写死，但磨损也很厉害。</p>
</blockquote>
<p><strong>阿里云Chia挖矿，很不划算</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/365806867" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/365806867</a></p>
<blockquote>
<p>0.1个币提现。因为真的是亏到姥姥家哈哈。4月17到4月24，一共300块钱。估计挖到0.1个币还要再等2天。</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>hadoop decommission 卡住</title>
    <url>/2021/04/27/hadoop%20decommission%20%E5%8D%A1%E4%BD%8F/</url>
    <content><![CDATA[<p>hadoop decommission 卡住</p>
<p>hadoop decommission一个节点Datanode，几万个block都同步过去了，但是唯独剩下2个block一直停留在哪，导致该节点几个小时也无法 下线。hadoop UI中显示在Under Replicated Blocks里面有2个块始终无法消除.</p>
<p>是一个hadoop的bug，<a href="https://issues.apache.org/jira/browse/HDFS-5579" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/HDFS-5579</a></p>
<p>根据blockid 查找文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs fsck -blockId blk_2050561344</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>hdfs 如何实现退役节点快速下线</title>
    <url>/2021/04/21/hdfs%20%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E9%80%80%E5%BD%B9%E8%8A%82%E7%82%B9%E5%BF%AB%E9%80%9F%E4%B8%8B%E7%BA%BF/</url>
    <content><![CDATA[<p><strong>hdfs 如何实现退役节点快速下线（也就是退役节点上的数据块快速迁移）</strong></p>
<p>参考  <a href="https://www.cnblogs.com/jiangxiaoxian/p/9665588.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangxiaoxian/p/9665588.html</a></p>
<p>进度可在HDFS的50070可视化界面的Decommissioning处查看**</p>
<p>Under replicated blocks	： 有备份的blocks</p>
<p>Blocks with no live replicas	： 没有存活备份的blocks(存备份的datanode下线了)</p>
<p>Under Replicated Blocks In files under construction   ： 备份数不够的blocks</p>
<p>可调整集群参数，对退服进行调优，注意，更改参数需要重启服务。需要修改的参数如下：</p>
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>实例</strong></th>
<th><strong>参数类别</strong></th>
<th style="text-align:left"><strong>参数名称</strong></th>
<th><strong>默认值</strong></th>
<th><strong>修改值</strong></th>
<th><strong>参数含义</strong></th>
<th><strong>调整场景</strong></th>
<th><strong>是否可以默认值调整</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>balance/退服性能参数</td>
<td style="text-align:left">dfs.datanode.balance.bandwidthPerSec</td>
<td>20971520</td>
<td>209715200</td>
<td>【说明】每个DataNode可用于负载均衡的最大带宽量（每秒的字节数）。</td>
<td>balance-性能调优</td>
<td>不建议调整默认值</td>
</tr>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>balance/退服性能参数</td>
<td style="text-align:left">dfs.datanode.balance.max.concurrent.moves</td>
<td>5</td>
<td>30</td>
<td>允许在DataNode上进行负载均衡的最大线程数。</td>
<td></td>
<td>有必要调整</td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>balance/退服性能参数</td>
<td style="text-align:left">dfs.namenode.replication.max-streams</td>
<td>10</td>
<td>64</td>
<td>DataNode上复制线程的最大数。</td>
<td></td>
<td>C70默认值已调整为64，有必要继续调整</td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>balance/退服性能参数</td>
<td style="text-align:left">dfs.namenode.replication.max-streams-hard-limit</td>
<td>20</td>
<td>500</td>
<td>对DataNode上复制线程数的硬限制。</td>
<td></td>
<td>C70默认值已调整为128，有必要继续调整</td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>balance/退服性能参数</td>
<td style="text-align:left">dfs.namenode.replication.work.multiplier.per.iteration</td>
<td>10</td>
<td>500</td>
<td>高级属性。修改时需谨慎。该参数表示NameNode通过DataNode心跳发送这样一个命令列表时DataNode上并行开始的用于复制的块传输的总量。</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>HDFS</th>
<th>NameNode</th>
<th>运行-性能调优</th>
<th>dfs.namenode.handler.count</th>
<th>64</th>
<th>192</th>
<th>NameNode处理线程数</th>
<th>大集群，性能调优</th>
<th>可以调整/更耗内存</th>
</tr>
</thead>
<tbody>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>运行-性能调优</td>
<td>dfs.datanode.handler.count</td>
<td>8</td>
<td>24</td>
<td>DataNode处理线程数</td>
<td>大集群，性能调优</td>
<td>可以调整/更耗内存</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>HDFS</th>
<th>NameNode</th>
<th>运行-性能调优</th>
<th>ipc.server.read.threadpool.size</th>
<th>1</th>
<th>10</th>
<th>NameNode处理请求线程池大小</th>
<th>大集群，性能调优</th>
<th>可以调整/更耗内存</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>HDFS</th>
<th>DataNode</th>
<th>运行-性能调优</th>
<th>dfs.datanode.max.transfer.threads</th>
<th>4096</th>
<th>8192</th>
<th>与DataNode间传输数据的线程的最大数。</th>
<th>负载高集群，性能调优</th>
<th>C70默认值已调整</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="处理“没有实时副本的块”">处理“没有实时副本的块”</h3>
<h3 id="Dealing-with-‘Blocks-with-no-live-replicas’-in-the-HDFS">Dealing with ‘Blocks with no live replicas’ in the HDFS</h3>
<p>参考： <a href="https://piyushroutray.com/2019/06/04/dealing-with-blocks-with-no-live-replicas-in-the-hdfs/" target="_blank" rel="noopener">https://piyushroutray.com/2019/06/04/dealing-with-blocks-with-no-live-replicas-in-the-hdfs/</a></p>
]]></content>
  </entry>
  <entry>
    <title>xfs_repair 磁盘修复</title>
    <url>/2021/04/19/xfs_repair%20%E7%A3%81%E7%9B%98%E4%BF%AE%E5%A4%8D/</url>
    <content><![CDATA[<h3 id="1-现状">1.现状</h3>
<p>目前网上出现大量的主机输入输出错误，原因是由于主机文件系统损坏。一线人员大部分采用的是umont 和 mount的方式恢复，这种恢复方式不能真正修复已经损坏的文件系统，在后续使用过程中，仍然会再次出现主机端输入输出错误。</p>
<h3 id="2-需要修复的场景">2.需要修复的场景</h3>
<p>&lt;1&gt;.主机侧发现存在文件系统不可读写的情况，也可以通过查看主机端日志来确认是否有文件系统异常发生： xfs_force_shutdown 、I/O error <br>
&lt;2&gt;.出现异常停电，供电恢复正常，主机和阵列系统重起之后<br>
&lt;3&gt;.存储介质故障：出现LUN失效、RAID失效、以及IO超时或者出现慢盘，对慢盘进行更换，系统恢复正常之后<br>
&lt;4&gt;.传输介质故障：如光纤、网线等损坏等，数据传输链路断开后又恢复正常之后</p>
<h3 id="3-检查文件系统">3.检查文件系统</h3>
<p>注：检查文件系统必须保证将文件系统umount成功。<br>
在根目录下输入“xfs_check /dev/sdd（盘符）；echo $?”（注意：在执行 此命令之前，必须将文件系统umount，否则会出现警告信 “xfs_check: /dev/sdd contains a mounted and writable filesystem ”）敲回车键，查看命令执行返回值：0表示正常，其他为不正常，说明文件系统 损坏，需要修复。</p>
<h3 id="4-修复过程">4.修复过程</h3>
<p>注：修复时需要暂停主机侧的业务，umount 和 mount 无法修复文件系统 。</p>
<ol>
<li>先umount要修复的文件系统的分区</li>
<li>然后输入 “xfs_repair /dev/sdd（盘符）”执行修复命令。<br>
xfs_check /dev/sdd; echo $?<br>
A）如果为0===》成功修复。<br>
B) 如果不为0===》没有成功：请执行 <strong>xfs_repair –L /dev/sdd</strong> 命令，再执 行xfs_repair（反复多修复几次）</li>
</ol>
<h3 id="5-xfs常用命令">5.xfs常用命令</h3>
<p>xfs_admin: 调整 xfs 文件系统的各种参数<br>
xfs_copy: 拷贝 xfs 文件系统的内容到一个或多个目标系统（并行方式）<br>
xfs_db: 调试或检测 xfs 文件系统（查看文件系统碎片等）<br>
xfs_check: 检测 xfs 文件系统的完整性<br>
xfs_bmap: 查看一个文件的块映射<br>
xfs_repair: 尝试修复受损的 xfs 文件系统<br>
xfs_fsr: 碎片整理<br>
xfs_quota: 管理 xfs 文件系统的磁盘配额<br>
xfs_metadump: 将 xfs 文件系统的元数据 (metadata) 拷贝到一个文件中<br>
xfs_mdrestore: 从一个文件中将元数据 (metadata) 恢复到 xfs 文件系统<br>
xfs_growfs: 调整一个 xfs 文件系统大小（只能扩展）<br>
xfs_logprint: print the log of an XFS filesystem<br>
xfs_mkfile: create an XFS file<br>
xfs_info: expand an XFS filesystem<br>
xfs_ncheck: generate pathnames from i-numbers for XFS<br>
xfs_rtcp: XFS realtime copy command<br>
xfs_freeze: suspend access to an XFS filesystem<br>
xfs_io: debug the I/O path of an XFS filesystem</p>
<h3 id="6-具体应用：">6.具体应用：</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看文件块状况: xfs_bmap -v sarubackup.tar.bz2 </span><br><span class="line">查看磁盘碎片状况: xfs_db -c frag -r &#x2F;dev&#x2F;sda1 </span><br><span class="line">文件碎片整理: xfs_fsr sarubackup.tar.bz2 </span><br><span class="line">磁盘碎片整理: xfs_fsr &#x2F;dev&#x2F;sda1</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>DolphinScheduler集群部署</title>
    <url>/2021/04/08/DolphinScheduler%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>官网部署文档</p>
<p><a href="https://dolphinscheduler.apache.org/zh-cn/docs/latest/user_doc/cluster-deployment.html" target="_blank" rel="noopener">https://dolphinscheduler.apache.org/zh-cn/docs/latest/user_doc/cluster-deployment.html</a></p>
<h1>1、基础软件安装(必装项请自行安装)</h1>
<ul>
<li>PostgreSQL (8.2.15+) or MySQL (5.7系列) : 两者任选其一即可, 如MySQL则需要JDBC Driver 5.1.47+</li>
<li><a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">JDK</a> (1.8+) : 必装，请安装好后在/etc/profile下配置 JAVA_HOME 及 PATH 变量</li>
<li>ZooKeeper (3.4.6+) ：必装</li>
<li>Hadoop (2.6+) or MinIO ：选装，如果需要用到资源上传功能，可以选择上传到Hadoop or MinIO上</li>
</ul>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">注意：DolphinScheduler本身不依赖Hadoop、Hive、Spark,仅是会调用他们的Client，用于对应任务的提交。</span><br></pre></td></tr></table></figure>
<h1>2、下载二进制tar.gz包</h1>
<ul>
<li>请下载最新版本的后端安装包至服务器部署目录,比如创建 /tmp/dolphinscheduler 做为安装部署目录，下载地址： <a href="https://dolphinscheduler.apache.org/zh-cn/download/download.html" target="_blank" rel="noopener">下载</a>，下载后上传tar包到该目录中，并进行解压</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建部署目录,部署目录请不要创建在/root、/home等高权限目录 </span></span><br><span class="line">mkdir -p /tmp/app/dolphinscheduler;  #安装文件别放到/tmp/dolphinscheduler , 这是特殊路径  创建文件时会用到的临时路径</span><br><span class="line">cd /tmp/app/dolphinscheduler;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压缩</span></span><br><span class="line">tar -zxvf apache-dolphinscheduler-incubating-1.3.5-dolphinscheduler-bin.tar.gz -C /opt/dolphinscheduler;</span><br><span class="line"></span><br><span class="line">mv apache-dolphinscheduler-incubating-1.3.5-dolphinscheduler-bin  dolphinscheduler-bin</span><br></pre></td></tr></table></figure>
<h1>3、创建部署用户和hosts映射</h1>
<ul>
<li>在<strong>所有</strong>部署调度的机器上创建部署用户，并且一定要配置sudo免密。假如我们计划在ds1,ds2,ds3,ds4这4台机器上部署调度，首先需要在每台机器上都创建部署用户</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建用户需使用root登录，设置部署用户名，请自行修改，后面以dolphinscheduler为例</span></span><br><span class="line">useradd dolphinscheduler;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置用户密码，请自行修改，后面以dolphinscheduler123为例</span></span><br><span class="line">echo "dolphinscheduler123" | passwd --stdin dolphinscheduler</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置sudo免密</span></span><br><span class="line">echo 'dolphinscheduler  ALL=(ALL)  NOPASSWD: NOPASSWD: ALL' &gt;&gt; /etc/sudoers</span><br><span class="line">sed -i 's/Defaults    requirett/#Defaults    requirett/g' /etc/sudoers</span><br><span class="line"> 注意：</span><br><span class="line"> - 因为是以 sudo -u &#123;linux-user&#125; 切换不同linux用户的方式来实现多租户运行作业，所以部署用户需要有 sudo 权限，而且是免密的。</span><br><span class="line"> - 如果发现/etc/sudoers文件中有"Default requiretty"这行，也请注释掉</span><br><span class="line"> - 如果用到资源上传的话，还需要在`HDFS或者MinIO`上给该部署用户分配读写的权限</span><br></pre></td></tr></table></figure>
<h1>4、配置hosts映射和ssh打通及修改目录权限</h1>
<ul>
<li>
<p><em>备注：当然 通过<code>sshpass -p xxx sudo scp -r /etc/hosts $ip:/etc/</code>就可以省去输入密码了</em></p>
<blockquote>
<p>centos下sshpass的安装：</p>
<ol>
<li>
<p>先安装epel</p>
<p>yum install -y epel-release</p>
<p>yum repolist</p>
</li>
<li>
<p>安装完成epel之后，就可以按照sshpass了</p>
<p>yum install -y sshpass</p>
</li>
</ol>
</blockquote>
</li>
<li>
<p>在ds1上，切换到部署用户并配置ssh本机免密登录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> su dolphinscheduler;</span><br><span class="line"></span><br><span class="line">ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa</span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>注意：<em>正常设置后，dolphinscheduler用户在执行命令<code>ssh localhost</code> 是不需要再输入密码的</em></p>
<ul>
<li>
<p>在ds1上，配置部署用户dolphinscheduler ssh打通到其他待部署的机器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">su dolphinscheduler;</span><br><span class="line">for ip in bigdata-9.baofoo.cn bigdata-8.baofoo.cn bigdata-7.baofoo.cn ;   </span><br><span class="line">do</span><br><span class="line">    sshpass -p dolphinscheduler123 ssh-copy-id $ip  </span><br><span class="line">done</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>在ds1上，修改目录权限，使得部署用户对dolphinscheduler-bin目录有操作权限</p>
</li>
</ul>
<h1>5、数据库初始化</h1>
<ul>
<li>进入数据库，默认数据库是PostgreSQL，如选择MySQL的话，后续需要添加mysql-connector-java驱动包到DolphinScheduler的lib目录下，这里以MySQL为例</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -h192.168.xx.xx -P3306 -uroot -p</span><br></pre></td></tr></table></figure>
<ul>
<li>进入数据库命令行窗口后，执行数据库初始化命令，设置访问账号和密码。<strong>注: {user} 和 {password} 需要替换为具体的数据库用户名和密码</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE dolphinscheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON dolphinscheduler.* TO &#39;&#123;user&#125;&#39;@&#39;%&#39; IDENTIFIED BY &#39;&#123;password&#125;&#39;;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON dolphinscheduler.* TO &#39;&#123;user&#125;&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;&#123;password&#125;&#39;;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>创建表和导入基础数据</p>
<ul>
<li>修改 conf 目录下 datasource.properties 中的下列配置</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vi conf/datasource.properties</span><br></pre></td></tr></table></figure>
<ul>
<li>如果选择 MySQL，请注释掉 PostgreSQL 相关配置(反之同理), 还需要手动添加 [<a href="https://downloads.mysql.com/archives/c-j/" target="_blank" rel="noopener"> mysql-connector-java 驱动 jar </a>] 包到 lib 目录下，这里下载的是mysql-connector-java-5.1.47.jar，然后正确配置数据库连接相关信息</li>
</ul>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#postgre</span></span><br><span class="line"><span class="comment">#spring.datasource.driver-class-name=org.postgresql.Driver</span></span><br><span class="line"><span class="comment">#spring.datasource.url=jdbc:postgresql://localhost:5432/dolphinscheduler</span></span><br><span class="line"><span class="comment"># mysql</span></span><br><span class="line"><span class="meta">spring.datasource.driver-class-name</span>=<span class="string">com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="meta">spring.datasource.url</span>=<span class="string">jdbc:mysql://10.0.20.107:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=true    </span></span><br><span class="line"><span class="meta">spring.datasource.username</span>=<span class="string">cs_yangz						需要修改为上面的&#123;user&#125;值</span></span><br><span class="line"><span class="meta">spring.datasource.password</span>=<span class="string">xxx						需要修改为上面的&#123;password&#125;值</span></span><br></pre></td></tr></table></figure>
<ul>
<li>修改并保存完后，执行 script 目录下的创建表及导入基础数据脚本</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh script/create-dolphinscheduler.sh</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1>6、修改运行参数</h1>
<ul>
<li>
<p>修改 conf/env 目录下的 <code>dolphinscheduler_env.sh</code> 环境变量(以相关用到的软件都安装在/opt/soft下为例)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">   export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</span><br><span class="line">   export HADOOP_CONF_DIR=/etc/hadoop/conf</span><br><span class="line"><span class="meta">   #</span><span class="bash"><span class="built_in">export</span> SPARK_HOME1=/opt/soft/spark1</span></span><br><span class="line">   export SPARK_HOME2=/opt/cloudera/parcels/CDH/lib/spark</span><br><span class="line">   export PYTHON_HOME=/usr/bin/python</span><br><span class="line">   export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera</span><br><span class="line">   export HIVE_HOME=/opt/cloudera/parcels/CDH/lib/hive</span><br><span class="line"><span class="meta">   #</span><span class="bash"><span class="built_in">export</span> FLINK_HOME=/opt/soft/flink</span></span><br><span class="line"><span class="meta">   #</span><span class="bash"><span class="built_in">export</span> DATAX_HOME=/opt/soft/datax/bin/datax.py</span></span><br><span class="line">   export PATH=$HADOOP_HOME/bin:$SPARK_HOME2/bin:$PYTHON_HOME:$JAVA_HOME/bin:$HIVE_HOME/bin:$PATH</span><br><span class="line">  </span><br><span class="line">`注: 这一步非常重要,例如 JAVA_HOME 和 PATH 是必须要配置的，没有用到的可以忽略或者注释掉`</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>修改一键部署配置文件 <code>conf/config/install_config.conf</code>中的各参数，特别注意以下参数的配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> postgresql or mysql</span></span><br><span class="line">dbtype="mysql"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> db config</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> db address and port</span></span><br><span class="line">dbhost="10.0.20.107:3306"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> db username</span></span><br><span class="line">username="cs_yangz"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> database name</span></span><br><span class="line">dbname="dolphinscheduler"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> db passwprd</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> NOTICE: <span class="keyword">if</span> there are special characters, please use the \ to escape, <span class="keyword">for</span> example, `[` escape to `\[`</span></span><br><span class="line">password="***"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> zk cluster</span></span><br><span class="line">zkQuorum="10.0.19.130:2181,10.0.19.131:2181,10.0.19.132:2181"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: the target installation path <span class="keyword">for</span> dolphinscheduler, please not config as the same as the current path (<span class="built_in">pwd</span>)</span></span><br><span class="line">installPath="/opt/dolphinscheduler"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> deployment user</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: the deployment user needs to have sudo privileges and permissions to operate hdfs. If hdfs is enabled, the root directory needs to be created by itself</span></span><br><span class="line">deployUser="dolphinscheduler"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> alert config</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mail server host</span></span><br><span class="line">mailServerHost="smtp.qiye.163.com"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> mail server port</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> note: Different protocols and encryption methods correspond to different ports, when SSL/TLS is enabled, make sure the port is correct.</span></span><br><span class="line">mailServerPort="25"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> sender</span></span><br><span class="line">mailSender="hadooper@baofoo.com"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> user</span></span><br><span class="line">mailUser="hadooper@baofoo.com"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> sender password</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> note: The mail.passwd is email service authorization code, not the email login password.</span></span><br><span class="line">mailPassword="***"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> TLS mail protocol support</span></span><br><span class="line">starttlsEnable="true"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> SSL mail protocol support</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> only one of TLS and SSL can be <span class="keyword">in</span> the <span class="literal">true</span> state.</span></span><br><span class="line">sslEnable="false"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">note: sslTrust is the same as mailServerHost</span></span><br><span class="line">sslTrust="smtp.qiye.163.com"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> resource storage <span class="built_in">type</span>：HDFS,S3,NONE</span></span><br><span class="line">resourceStorageType="HDFS"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> resourceStorageType is HDFS，defaultFS write namenode address，HA you need to put core-site.xml and hdfs-site.xml <span class="keyword">in</span> the conf directory.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> S3，write S3 address，HA，<span class="keyword">for</span> example ：s3a://dolphinscheduler，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note，s3 be sure to create the root directory /dolphinscheduler</span></span><br><span class="line">defaultFS="hdfs://bigdata-7.baofoo.cn:8020"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> resourceStorageType is S3, the following three configuration is required, otherwise please ignore</span></span><br><span class="line">s3Endpoint="http://192.168.xx.xx:9010"</span><br><span class="line">s3AccessKey="xxxxxxxxxx"</span><br><span class="line">s3SecretKey="xxxxxxxxxx"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> resourcemanager HA <span class="built_in">enable</span>, please <span class="built_in">type</span> the HA ips ; <span class="keyword">if</span> resourcemanager is single, make this value empty</span></span><br><span class="line">yarnHaIps="bigdata-7.baofoo.cn"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> resourcemanager HA <span class="built_in">enable</span> or not use resourcemanager, please skip this value setting; If resourcemanager is single, you only need to replace yarnIp1 to actual resourcemanager hostname.</span></span><br><span class="line">singleYarnIp="bigdata-7.baofoo.cn"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> resource store on HDFS/S3 path, resource file will store to this hadoop hdfs path, self configuration, please make sure the directory exists on hdfs and have <span class="built_in">read</span> write permissions。/dolphinscheduler is recommended</span></span><br><span class="line">resourceUploadPath="/dolphinscheduler"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> who have permissions to create directory under HDFS/S3 root path</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: <span class="keyword">if</span> kerberos is enabled, please config hdfsRootUser=</span></span><br><span class="line">hdfsRootUser="hdfs"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kerberos config</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> whether kerberos starts, <span class="keyword">if</span> kerberos starts, following four items need to config, otherwise please ignore</span></span><br><span class="line">kerberosStartUp="false"</span><br><span class="line"><span class="meta">#</span><span class="bash"> kdc krb5 config file path</span></span><br><span class="line">krb5ConfPath="$installPath/conf/krb5.conf"</span><br><span class="line"><span class="meta">#</span><span class="bash"> keytab username</span></span><br><span class="line">keytabUserName="hdfs-mycluster@ESZ.COM"</span><br><span class="line"><span class="meta">#</span><span class="bash"> username keytab path</span></span><br><span class="line">keytabPath="$installPath/conf/hdfs.headless.keytab"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> api server port</span></span><br><span class="line">apiServerPort="12345"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> install hosts</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: install the scheduled hostname list. If it is pseudo-distributed, just write a pseudo-distributed hostname</span></span><br><span class="line">ips="bigdata-7.baofoo.cn,bigdata-8.baofoo.cn,bigdata-9.baofoo.cn"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ssh port, default 22</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: <span class="keyword">if</span> ssh port is not default, modify here</span></span><br><span class="line">sshPort="22"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> run master machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: list of hosts hostname <span class="keyword">for</span> deploying master</span></span><br><span class="line">masters="bigdata-9.baofoo.cn,bigdata-8.baofoo.cn"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> run worker machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> note: need to write the worker group name of each worker, the default value is <span class="string">"default"</span></span></span><br><span class="line">workers="bigdata-7.baofoo.cn:default,bigdata-8.baofoo.cn:default,bigdata-9.baofoo.cn:default"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> run alert machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> note: list of machine hostnames <span class="keyword">for</span> deploying alert server</span></span><br><span class="line">alertServer="bigdata-8.baofoo.cn"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> run api machine</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> note: list of machine hostnames <span class="keyword">for</span> deploying api server</span></span><br><span class="line">apiServers="bigdata-9.baofoo.cn"</span><br></pre></td></tr></table></figure>
<p><em>特别注意：</em></p>
<ul>
<li>如果需要用资源上传到Hadoop集群功能， 并且Hadoop集群的NameNode 配置了 HA的话 ，需要开启 HDFS类型的资源上传，同时需要将Hadoop集群下的core-site.xml和hdfs-site.xml复制到/opt/dolphinscheduler/conf，非NameNode HA跳过次步骤</li>
</ul>
<h1>7、一键部署</h1>
<ul>
<li>
<p><strong>切换到部署用户dolphinscheduler</strong>，然后执行一键部署脚本</p>
<p><code>sh install.sh</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">注意：</span><br><span class="line">第一次部署的话，在运行中第3步&#96;3,stop server&#96;出现5次以下信息，此信息可以忽略</span><br><span class="line">sh: bin&#x2F;dolphinscheduler-daemon.sh: No such file or directory</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>脚本完成后，会启动以下5个服务，使用<code>jps</code>命令查看服务是否启动(<code>jps</code>为<code>java JDK</code>自带)</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MasterServer         ----- master服务</span><br><span class="line">WorkerServer         ----- worker服务</span><br><span class="line">LoggerServer         ----- logger服务</span><br><span class="line">ApiApplicationServer ----- api服务</span><br><span class="line">AlertServer          ----- alert服务</span><br></pre></td></tr></table></figure>
<p>如果以上服务都正常启动，说明自动部署成功</p>
<p>部署成功后，可以进行日志查看，日志统一存放于logs文件夹内</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">logs&#x2F;</span><br><span class="line">   ├── dolphinscheduler-alert-server.log</span><br><span class="line">   ├── dolphinscheduler-master-server.log</span><br><span class="line">   |—— dolphinscheduler-worker-server.log</span><br><span class="line">   |—— dolphinscheduler-api-server.log</span><br><span class="line">   |—— dolphinscheduler-logger-server.log</span><br></pre></td></tr></table></figure>
<h1>8、登录系统</h1>
<ul>
<li>访问前端页面地址,接口ip(自行修改) <a href="http://bigdata-9.baofoo.cn:12345/dolphinscheduler" target="_blank" rel="noopener">http://bigdata-9.baofoo.cn:12345/dolphinscheduler</a></li>
</ul>
<p>默认的用户是<code>admin</code>，默认的密码是<code>dolphinscheduler123</code></p>
<h4 id="修改-JVM-参数">修改 JVM 参数</h4>
<ul>
<li>两个文件</li>
<li>/bin/dolphinscheduler-daemon.sh</li>
<li>/scripts/dolphinscheduler-daemon.sh</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export DOLPHINSCHEDULER_OPTS&#x3D;&quot;-server -Xmx16g -Xms1g -Xss512k -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:LargePageSizeInBytes&#x3D;128m -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction&#x3D;70&quot;</span><br></pre></td></tr></table></figure>
<p>复制代码</p>
<ul>
<li>一键部署</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su  dolphinscheduler</span><br><span class="line"></span><br><span class="line">sh install.sh</span><br></pre></td></tr></table></figure>
<p>复制代码</p>
<ul>
<li>进程检查</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su dolphinscheduler;jps;</span><br></pre></td></tr></table></figure>
<p>复制代码</p>
<p><img src="../images/56bd851533e862beaedd00b76e45409e.png" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 一键停止</span><br><span class="line">sh .&#x2F;bin&#x2F;stop-all.sh</span><br><span class="line"># 一键开启</span><br><span class="line">sh .&#x2F;bin&#x2F;start-all.sh</span><br><span class="line"># 启停master</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh start master-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh stop master-server</span><br><span class="line"># 启停worker</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line"># 启停api-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh start api-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh stop api-server</span><br><span class="line"># 启停logger</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line"># 启停alert</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">sh .&#x2F;bin&#x2F;dolphinscheduler-daemon.sh stop alert-server</span><br></pre></td></tr></table></figure>
<p><strong>遇到的坑：</strong></p>
<p>1.安装文件别放到/tmp/dolphinscheduler    ， 这是特殊路径</p>
<p>2.用root用户启动服务导致一系列问题，要用新建的用户操作所有的一切。</p>
]]></content>
  </entry>
  <entry>
    <title>ClickHouse 部署</title>
    <url>/2021/04/06/ClickHouse%20%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h1>ClickHouse 部署</h1>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install yum-utils</span><br><span class="line">sudo rpm --import https:&#x2F;&#x2F;repo.clickhouse.tech&#x2F;CLICKHOUSE-KEY.GPG</span><br><span class="line">sudo yum-config-manager --add-repo https:&#x2F;&#x2F;repo.clickhouse.tech&#x2F;rpm&#x2F;stable&#x2F;x86_64</span><br></pre></td></tr></table></figure>
<p>如果您想使用最新的版本，请用<code>testing</code>替代<code>stable</code>(我们只推荐您用于测试环境)。<code>prestable</code>有时也可用。</p>
<p>然后运行命令安装：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install clickhouse-server clickhouse-client</span><br></pre></td></tr></table></figure>
<p>启动服务</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo systemctl start clickhouse-server</span><br><span class="line">sudo systemctl stop clickhouse-server</span><br><span class="line">sudo systemctl status clickhouse-server</span><br></pre></td></tr></table></figure>
<p>快速开始</p>
<p>clickhouse-client -m</p>
]]></content>
  </entry>
  <entry>
    <title>快速试用 DolphinScheduler</title>
    <url>/2021/03/31/%E5%BF%AB%E9%80%9F%E8%AF%95%E7%94%A8%20DolphinScheduler/</url>
    <content><![CDATA[<p>快速试用 DolphinScheduler</p>
<h5 id="1、下载源码-zip-包">1、下载源码 zip 包</h5>
<ul>
<li>请下载最新版本的源码包并进行解压</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建源码存放目录</span></span><br><span class="line">mkdir -p /opt/soft/dolphinscheduler;</span><br><span class="line">cd /opt/soft/dolphinscheduler;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过wget下载源码包</span></span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/incubator/dolphinscheduler/1.3.5/apache-dolphinscheduler-incubating-1.3.5-src.zip</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过curl下载源码包</span></span><br><span class="line">curl -O https://mirrors.tuna.tsinghua.edu.cn/apache/incubator/dolphinscheduler/1.3.5/apache-dolphinscheduler-incubating-1.3.5-src.zip</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压缩</span></span><br><span class="line">unzip apache-dolphinscheduler-incubating-1.3.5-src.zip</span><br><span class="line"></span><br><span class="line">mv apache-dolphinscheduler-incubating-1.3.5-src-release dolphinscheduler-src</span><br></pre></td></tr></table></figure>
<h5 id="2、安装并启动服务">2、安装并启动服务</h5>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd dolphinscheduler-src&#x2F;docker&#x2F;docker-swarm</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure>
<h5 id="3、登录系统">3、登录系统</h5>
<p>访问前端页面： <a href="http://bigdata-3.baofoo.cn:12345/dolphinscheduler" target="_blank" rel="noopener">http://bigdata-3.baofoo.cn:12345/dolphinscheduler</a></p>
<p>默认的用户是<code>admin</code>，默认的密码是<code>dolphinscheduler123</code></p>
<p>停止所有容器:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker-compose stop</span><br></pre></td></tr></table></figure>
<p>停止所有容器并移除所有容器，网络和存储卷:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker-compose down -v</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>conda笔记</title>
    <url>/2021/03/30/conda%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p><strong>Conda常用命令整理</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda --version #查看conda版本，验证是否安装</span><br><span class="line"></span><br><span class="line">conda update conda #更新至最新版本，也会更新其它相关包</span><br><span class="line"></span><br><span class="line">conda update --all #更新所有包</span><br><span class="line"></span><br><span class="line">conda update package_name #更新指定的包</span><br><span class="line"></span><br><span class="line">conda create -n env_name package_name #创建名为env_name的新环境，并在该环境下安装名为package_name 的包，可以指定新环境的版本号，例如：conda create -n python2 python=python2.7 numpy pandas，创建了python2环境，python版本为2.7，同时还安装了numpy pandas包</span><br><span class="line"></span><br><span class="line">conda activate env_name #切换至env_name环境</span><br><span class="line"></span><br><span class="line">conda deactivate #退出环境</span><br><span class="line"></span><br><span class="line">conda info -e #显示所有已经创建的环境</span><br><span class="line"></span><br><span class="line">conda create --name new_env_name --clone old_env_name #复制old_env_name为new_env_name</span><br><span class="line"></span><br><span class="line">conda remove --name env_name –all #删除环境</span><br><span class="line"></span><br><span class="line">conda list #查看所有已经安装的包</span><br><span class="line"></span><br><span class="line">conda install package_name #在当前环境中安装包</span><br><span class="line"></span><br><span class="line">conda install --name env_name package_name #在指定环境中安装包</span><br><span class="line"></span><br><span class="line">conda remove -- name env_name package #删除指定环境中的包</span><br><span class="line"></span><br><span class="line">conda remove package #删除当前环境中的包</span><br><span class="line"></span><br><span class="line">conda create -n tensorflow_env tensorflow</span><br><span class="line"></span><br><span class="line">conda activate tensorflow_env #conda 安装tensorflow的CPU版本</span><br><span class="line"></span><br><span class="line">conda create -n tensorflow_gpuenv tensorflow-gpu</span><br><span class="line"></span><br><span class="line">conda activate tensorflow_gpuenv #conda安装tensorflow的GPU版本</span><br><span class="line"></span><br><span class="line">conda env remove -n env_name #采用第10条的方法删除环境失败时，可采用这种方法</span><br></pre></td></tr></table></figure>
<p><strong>Conda常用命令整理</strong></p>
<p><a href="https://blog.csdn.net/menc15/article/details/71477949/" target="_blank" rel="noopener">https://blog.csdn.net/menc15/article/details/71477949/</a></p>
]]></content>
  </entry>
  <entry>
    <title>spark打包提交python程序</title>
    <url>/2021/03/10/spark%E6%89%93%E5%8C%85%E6%8F%90%E4%BA%A4python%E7%A8%8B%E5%BA%8F/</url>
    <content><![CDATA[<p><strong>spark-python版本依赖与三方模块方案</strong></p>
<p>（1）使用conda创建python虚拟环境、安装第三方库</p>
<p>假设虚拟环境是pyspark_py36，安装位置是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;root&#x2F;miniconda3&#x2F;envs&#x2F;pyspark_py36</span><br><span class="line">此处省略1w个字。</span><br></pre></td></tr></table></figure>
<p>安装的第三方库是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source activate pyspark_py36</span><br><span class="line"></span><br><span class="line">pip install pandas </span><br><span class="line">pip install sklearn</span><br><span class="line">pip install lightgbm</span><br></pre></td></tr></table></figure>
<p>其他省略1w字。</p>
<p>（2）打包整个虚拟环境</p>
<p>进入虚拟环境目录，压缩整个文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;root&#x2F;miniconda3&#x2F;envs&#x2F;</span><br><span class="line">zip -r -9 -q pyspark_py36.zip pyspark_py36&#x2F;</span><br></pre></td></tr></table></figure>
<p>压缩后得到压缩包pyspark_py36.zip。</p>
<p>（3）将压缩是虚拟环境上传到hdfs</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 上传</span><br><span class="line">hdfs dfs –put pyspark_py36.zip &#x2F;tmp&#x2F;</span><br></pre></td></tr></table></figure>
<p>（4）新建pyspark程序</p>
<p>没什么好说的，就是普通的pyspark程序，简单的例子如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试刚才安装的第三方库是否正常导入</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"></span><br><span class="line">dates = pd.date_range(<span class="string">'20130101'</span>,periods=<span class="number">6</span>)</span><br><span class="line">print(dates)</span><br><span class="line">print(platform.python_version())</span><br></pre></td></tr></table></figure>
<p>(5) 提交pyspark程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo -u yarn spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--queue bf_yarn_pool.development \</span><br><span class="line">--num-executors 1 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--archives hdfs://ns1/tmp/pyspark_py36.zip#pyenv \</span><br><span class="line">--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=pyenv/pyspark_py36/bin/python \</span><br><span class="line">hdfs://ns1/tmp/test_spark_env.py</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>vi模式下查找和替换</title>
    <url>/2021/03/10/vi%E6%A8%A1%E5%BC%8F%E4%B8%8B%E6%9F%A5%E6%89%BE%E5%92%8C%E6%9B%BF%E6%8D%A2/</url>
    <content><![CDATA[<p>linux基础命令之：vi模式下查找和替换</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">一、查找</span><br><span class="line">查找命令</span><br><span class="line">/pattern&lt;Enter&gt; ：向下查找pattern匹配字符串</span><br><span class="line">?pattern&lt;Enter&gt;：向上查找pattern匹配字符串</span><br><span class="line">使用了查找命令之后，使用如下两个键快速查找：</span><br><span class="line">n：按照同一方向继续查找</span><br><span class="line">N：按照反方向查找</span><br><span class="line">pattern是需要匹配的字符串，例如：</span><br><span class="line">/hello&lt;Enter&gt;      #查找hello</span><br><span class="line">/hello&lt;Enter&gt;    #查找hello单词（注意前后的空格）</span><br><span class="line">除此之外，pattern还可以使用一些特殊字符，包括（/、^、$、*、.），其中前三个这两个是vi与vim通用的，“/”为转义字符。</span><br><span class="line">/^hello&lt;Enter&gt;    #查找以hello开始的行</span><br><span class="line"><span class="meta">/hello$</span><span class="bash">&lt;Enter&gt;    <span class="comment">#查找以hello结束的行</span></span></span><br><span class="line">//^hello&lt;Enter&gt;    #查找^hello字符串</span><br><span class="line"> </span><br><span class="line">二、替换</span><br><span class="line">1.:[range]s/pattern/string/[c,e,g,i]</span><br><span class="line">range     指的是范围，1,7 指从第一行至第七行，1,$ 指从第一行至最后一行，也就是整篇文章，也可以 % 代表。 % 是目前编辑的文章，# 是前一次编辑的文章。</span><br><span class="line">pattern     就是要被替换掉的字串，可以用 regexp 来表示。</span><br><span class="line">string     将 pattern 由 string 所取代。</span><br><span class="line">c     confirm，每次替换前会询问。</span><br><span class="line">e     不显示 error。</span><br><span class="line">g     globe，不询问，整行替换。</span><br><span class="line">i     ignore 不分大小写。</span><br><span class="line">I     ignore 大小写敏感。</span><br><span class="line"> </span><br><span class="line">2.基本替换</span><br><span class="line">:s/lantian/sky/         #替换当前行第一个 lantian 为 sky</span><br><span class="line">:s/lantian/sky/g     #替换当前行所有 lantian 为 sky</span><br><span class="line">:n,$s/lantian/sky/     #替换第 n 行开始到最后一行中每一行的第一个 lantian 为 sky</span><br><span class="line">:n,$s/lantian/sky/g     #替换第 n 行开始到最后一行中每一行所有 lantian 为 sky</span><br><span class="line"><span class="meta">#</span><span class="bash">（n 为数字，若 n 为 .，表示从当前行开始到最后一行）</span></span><br><span class="line">:%s/lantian/sky/        #（等同于 :g/lantian/s//sky/） 替换每一行的第一个 lantian 为 sky</span><br><span class="line">:%s/lantian/sky/g    #（等同于 :g/lantian/s//sky/g） 替换每一行中所有 lantian 为 sky</span><br><span class="line">可以使用 #或+ 作为分隔符，此时中间出现的 / 不会作为分隔符</span><br><span class="line">:s#lantian/#sky/#         替换当前行第一个 lantian/ 为 sky/</span><br><span class="line">:%s+/oradata/apras/+/user01/apras1+ （</span><br><span class="line">使用+ 来 替换 / ）： /oradata/apras/替换成/user01/apras1/</span><br><span class="line"> </span><br><span class="line">3.删除文本中的^M</span><br><span class="line">问题描述：对于换行，window下用回车换行（0A0D）来表示，linux下是回车（0A）来表示。这样，将window上的文件拷到unix上用时，总会有个^M，请写个用在unix下的过滤windows文件的换行符（0D）的shell或c程序。</span><br><span class="line">使用命令：cat filename1 | tr -d “^V^M” &gt; newfile;</span><br><span class="line">使用命令：sed -e “s/^V^M//” filename &gt; outputfilename</span><br><span class="line">需要注意的是在1、2两种方法中，^V和^M指的是Ctrl+V和Ctrl+M。你必须要手工进行输入，而不是粘贴。</span><br><span class="line">在vi中处理：首先使用vi打开文件，然后按ESC键，接着输入命令：</span><br><span class="line">:%s/^V^M//</span><br><span class="line">:%s/^M$//g</span><br><span class="line">如果上述方法无用，则正确的解决办法是：</span><br><span class="line">tr -d “/r” &lt; src &gt;dest</span><br><span class="line">tr -d “/015″ dest</span><br><span class="line">strings A&gt;B</span><br><span class="line"> </span><br><span class="line">4.其它用法</span><br><span class="line">:s/str1/str2/          #用字符串 str2 替换行中首次出现的字符串 str1</span><br><span class="line">:s/str1/str2/g         #用字符串 str2 替换行中所有出现的字符串 str1</span><br><span class="line">:.,$ s/str1/str2/g     #用字符串 str2 替换正文当前行到末尾所有出现的字符串 str1</span><br><span class="line">:1,$ s/str1/str2/g     #用字符串 str2 替换正文中所有出现的字符串 str1</span><br><span class="line">:g/str1/s//str2/g      #功能同上</span><br><span class="line"> </span><br><span class="line">5.g的总结说明</span><br><span class="line">从上述替换命令可以看到：</span><br><span class="line">g 放在命令末尾，表示对指定行的搜索字符串的每次出现进行替换；不加 g，表示只对指定行的搜索字符串的首次出现进行替换；</span><br><span class="line">g 放在命令开头，表示对正文中所有包含搜索字符串的行进行替换操作。</span><br><span class="line">也就是说命令的开始可以添加影响的行，如果为g表示对所有行；命令的结尾可以使用g来表示是否对每一行的所有字符串都有影响。</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>用python进行量化投资</title>
    <url>/2021/03/09/%E7%94%A8python%E8%BF%9B%E8%A1%8C%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/</url>
    <content><![CDATA[<p><strong>用python进行量化投资：</strong></p>
<p>自己编写:NumPy+pandas+Matplotlib+…</p>
<p>在线平台:聚宽、优矿、米筐、Quantopian、…</p>
<p>开源框架:RQAlpha、QUANTAXIS</p>
<p>聚宽 <a href="https://www.joinquant.com/" target="_blank" rel="noopener">https://www.joinquant.com/</a></p>
<p>优矿 <a href="https://uqer.datayes.com/" target="_blank" rel="noopener">https://uqer.datayes.com/</a></p>
<p>rqalpha <a href="https://rqalpha.readthedocs.io/zh_CN/latest/index.html" target="_blank" rel="noopener">https://rqalpha.readthedocs.io/zh_CN/latest/index.html</a></p>
<p>quantaxis <a href="https://doc.yutiansut.com/" target="_blank" rel="noopener">https://doc.yutiansut.com/</a></p>
]]></content>
  </entry>
  <entry>
    <title>yarn webUI 看不到日志</title>
    <url>/2021/03/03/yarn%20webUI%20%E7%9C%8B%E4%B8%8D%E5%88%B0%E6%97%A5%E5%BF%97/</url>
    <content><![CDATA[<p>yarn webUI 看不到日志</p>
<p>解决办法：</p>
<p>Resource Manager webUI No logs available for container</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1. Please verify once if you could collect logs using "yarn logs" command as below?</span><br><span class="line">&#123;&#123;</span><br><span class="line">yarn logs -applicationId &lt;appID&gt; -appOwner &lt;user&gt;</span><br><span class="line">&#125;&#125;</span><br><span class="line"></span><br><span class="line">2.  please verify the below directories if they have right permissions</span><br><span class="line">&#123;&#123;</span><br><span class="line">hdfs dfs -ls /user/history/done</span><br><span class="line">hdfs dfs -ls  /user/history/done_intermediate</span><br><span class="line">&#125;&#125;</span><br><span class="line"></span><br><span class="line">3. Please verify if "Enable Log Aggregation" has been enabled or not</span><br><span class="line">&#123;&#123;</span><br><span class="line"><span class="meta">ClouderaManager--&gt;</span><span class="bash">Yarn--&gt;Configurations--&gt;<span class="string">"Enable Log Aggregation"</span></span></span><br><span class="line">&#125;&#125;</span><br><span class="line"></span><br><span class="line">4. Verify the permissions for /tmp/logs as below,</span><br><span class="line">&#123;&#123;</span><br><span class="line">hadoop fs -chown mapred:hadoop /tmp/logs</span><br><span class="line">hadoop fs -chown -R :hadoop /tmp/logs/*</span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>用Hue中的Oozie执行Impala Sheel脚本</title>
    <url>/2021/03/02/%E7%94%A8Hue%E4%B8%AD%E7%9A%84Oozie%E6%89%A7%E8%A1%8CImpala%20Sheel%E8%84%9A%E6%9C%AC/</url>
    <content><![CDATA[<h3 id="用Hue中的Oozie执行Impala-Sheel脚本">用Hue中的Oozie执行Impala Sheel脚本</h3>
<p>在Oozie中不能像执行hive SQL那样直接执行impala SQL脚本。目前没有Impala操作，因此你必须使用调用impala-shell的shell操作。调用impala-shell的shell脚本中还必须包含设置PYTHON EGGS位置的环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export PYTHON_EGG_CACHE=.python-eggs   </span><br><span class="line"></span><br><span class="line">impala-shell -i 172.20.15.10:21000 -u hpt -l --auth_creds_ok_in_clear --ldap_password_cmd='echo -n ***' -q 'SET request_pool=development;'</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>hudi</title>
    <url>/2021/03/02/hudi/</url>
    <content><![CDATA[<p><strong>注意：<strong>目前Hudi使用的是</strong>hadoop2.7.3</strong>版本，CDH6.3.0 环境使用的是<strong>hadoop3.0.0</strong>， 所以在打包的时候需要加上**-Dhadoop.version=3.0.0** 参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean install -DskipTests -DskipITs -Dcheckstyle.skip&#x3D;true -Drat.skip&#x3D;true -Dhadoop.version&#x3D;3.0.0</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo -u yarn spark-shell  \</span><br><span class="line">  --queue bf_yarn_pool.development \</span><br><span class="line">  --packages org.apache.spark:spark-avro_2.11:2.4.0 \</span><br><span class="line">  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \</span><br><span class="line">  --jars `ls /opt/hudi/packaging/hudi-spark-bundle/target/hudi-spark-bundle_2.11-*.*.*-SNAPSHOT.jar`</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>用FsImage查找hadoop集群小文件</title>
    <url>/2021/02/20/%E7%94%A8FsImage%E6%9F%A5%E6%89%BEhadoop%E9%9B%86%E7%BE%A4%E5%B0%8F%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /tmp</span><br><span class="line">hdfs dfsadmin -fetchImage ./tmp_meta</span><br><span class="line">hdfs oiv -i ./tmp_meta -o ./fsimage.csv -p Delimited</span><br><span class="line">hdfs dfs -mkdir -p /tmp/hdfs_metadata/fsimage</span><br><span class="line">hdfs dfs -copyFromLocal ./fsimage.csv /tmp/hdfs_metadata/fsimage</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> hdfs_meta_temp;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> hdfs_meta_temp (<span class="keyword">path</span> <span class="keyword">string</span> ,</span><br><span class="line">repl <span class="built_in">int</span> ,</span><br><span class="line">modification_time <span class="keyword">string</span> ,accesstime <span class="keyword">string</span> ,</span><br><span class="line">preferredblocksize <span class="built_in">int</span> ,blockcount <span class="keyword">double</span>,</span><br><span class="line">filesize <span class="keyword">double</span> ,nsquota <span class="built_in">int</span> ,</span><br><span class="line">dsquota <span class="built_in">int</span> ,</span><br><span class="line">permission <span class="keyword">string</span> ,username <span class="keyword">string</span> ,groupname <span class="keyword">string</span>)<span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span> location <span class="string">'/tmp/hdfs_metadata/fsimage/'</span> ;</span><br><span class="line"></span><br><span class="line"><span class="comment">--将临时表转换为Impala的 Parquet表</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> hdfs_meta</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> hdfs_meta <span class="keyword">stored</span> <span class="keyword">as</span> parquet <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">path</span>,</span><br><span class="line">repl,</span><br><span class="line"><span class="keyword">cast</span>(<span class="keyword">concat</span>(modification_time, <span class="string">' :00'</span>) <span class="keyword">as</span> <span class="built_in">timestamp</span>) modification_time,</span><br><span class="line"><span class="keyword">cast</span>(<span class="keyword">concat</span>(accesstime,<span class="string">':00'</span>) <span class="keyword">as</span> <span class="built_in">timestamp</span>) accesstime,</span><br><span class="line">preferredblocksize,</span><br><span class="line">blockcount,</span><br><span class="line">filesize,nsquota,dsquota,permission,username,groupname</span><br><span class="line"><span class="keyword">from</span> hdfs_meta_temp;</span><br></pre></td></tr></table></figure>
<p>instr(path,’/’,1,2)这两个参数主要表示指定统计的HDFS目录以及目录钻取深度，instr()函数中的最后一个参数即为目录钻取深度</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line"><span class="comment">--concat('/',split_part(path,'/',2))  basepath,</span></span><br><span class="line"><span class="comment">--concat('/',split_part(path,'/',2),'/',split_part(path,'/',3))  basepath,</span></span><br><span class="line"><span class="comment">--concat('/',split_part(path,'/',2),'/',split_part(path,'/',3),'/',split_part(path,'/',4))  basepath,</span></span><br><span class="line"><span class="keyword">concat</span>(<span class="string">'/'</span>,split_part(<span class="keyword">path</span>,<span class="string">'/'</span>,<span class="number">2</span>),<span class="string">'/'</span>,split_part(<span class="keyword">path</span>,<span class="string">'/'</span>,<span class="number">3</span>),<span class="string">'/'</span>,split_part(<span class="keyword">path</span>,<span class="string">'/'</span>,<span class="number">4</span>),<span class="string">'/'</span>,split_part(<span class="keyword">path</span>,<span class="string">'/'</span>,<span class="number">5</span>))  basepath,</span><br><span class="line"><span class="keyword">sum</span>(blockcount) blockcounts,</span><br><span class="line"><span class="keyword">cast</span>(<span class="keyword">sum</span>(filesize)/<span class="number">1024</span>/<span class="number">1024</span>/<span class="number">1024</span> <span class="keyword">as</span> <span class="built_in">decimal</span>(<span class="number">18</span>,<span class="number">2</span>) ) filesizes,</span><br><span class="line"><span class="keyword">count</span>(*) file_nums,</span><br><span class="line"><span class="keyword">cast</span>(<span class="keyword">count</span>(*)/<span class="keyword">sum</span>(blockcount) <span class="keyword">as</span> <span class="built_in">decimal</span>(<span class="number">18</span>,<span class="number">2</span>) ) <span class="keyword">as</span> avg_block ,</span><br><span class="line"><span class="keyword">cast</span>(<span class="keyword">sum</span>(filesize)/<span class="keyword">count</span>(*)/<span class="number">1024</span> <span class="keyword">as</span> <span class="built_in">decimal</span>(<span class="number">18</span>,<span class="number">2</span>) ) <span class="keyword">AS</span> avg_filesize</span><br><span class="line"><span class="keyword">FROM</span> tmp.hdfs_meta </span><br><span class="line"><span class="keyword">where</span> <span class="keyword">instr</span>(<span class="keyword">path</span>,<span class="string">'/'</span>,<span class="number">1</span>,<span class="number">4</span>)&gt;<span class="number">0</span></span><br><span class="line"><span class="comment">--and strleft(path, instr(path,'/',1,4)-1)='/user/hive/warehouse'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> basepath  </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> file_nums <span class="keyword">desc</span>, avg_filesize </span><br><span class="line"><span class="keyword">limit</span> <span class="number">200</span></span><br></pre></td></tr></table></figure>
<p><strong>总结</strong></p>
<p>如上SQL的统计分析可以看到有三个比较重要的统计指标file_nums、blockcounts和avg_filesize。通过这三个指标进行小文件分析，进行如下分析：</p>
<p>如果file_nums/blockcounts的值越大且avg_filesize越小则说明该HDFS或Hive表的小文件越多。</p>
<p><strong>方法二、</strong></p>
<p>使用Sqoop脚本将Hive元数据中关于Hive库和表的信息抽取的Hive中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">-D mapred.job.queue.name=bf_yarn_pool.development \</span><br><span class="line">--connect "jdbc:mysql://10.0.20.107:3306/baofoo_hive_2" \</span><br><span class="line">--username cs_yangz \</span><br><span class="line">--password *** \</span><br><span class="line">--query 'select c.NAME,c.DB_LOCATION_URI,a.TBL_NAME,a.OWNER,a.TBL_TYPE,b.LOCATION from TBLS a,SDS b,DBS c where a.SD_ID=b.SD_ID and a.DB_ID=c.DB_ID and $CONDITIONS' \</span><br><span class="line">--fields-terminated-by ',' \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--hive-database default \</span><br><span class="line">--target-dir /tmp/hive_tables_temp \</span><br><span class="line">--hive-import \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table hive_tables_temp \</span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>hdfs性能测试</title>
    <url>/2021/01/19/hdfs%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h2 id="hdfs性能测试">hdfs性能测试</h2>
<p>hadoop自带TestDFSIO测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;tmp</span><br><span class="line">sudo -u hdfs hadoop jar \</span><br><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.0.1-tests.jar TestDFSIO \</span><br><span class="line">-D mapreduce.job.queuename&#x3D;bf_yarn_pool.production \</span><br><span class="line">-D test.build.data&#x3D;&#x2F;tmp&#x2F;benchmark \</span><br><span class="line">-D mapreduce.output.fileoutputformat.compress&#x3D;false \</span><br><span class="line">-write -nrFiles 10 -fileSize 1000</span><br></pre></td></tr></table></figure>
<p>–结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:             Date &amp; time: Tue Jan 19 15:29:04 CST 2021</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:         Number of files: 10</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:  Total MBytes processed: 10000</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:       Throughput mb&#x2F;sec: 23.96</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:  Average IO rate mb&#x2F;sec: 32.37</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:   IO rate std deviation: 29.51</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:      Test exec time sec: 68</span><br><span class="line">21&#x2F;01&#x2F;19 15:29:04 INFO fs.TestDFSIO:</span><br></pre></td></tr></table></figure>
<p>结果说明：</p>
<blockquote>
<p>Total MBytes processed ： 总共需要写入的数据量 ＝＝》 256*1000</p>
<p>Throughput mb/sec ：总共需要写入的数据量/（每个map任务实际写入数据的执行时间之和（这个时间会远小于Test exec time sec））＝＝》256000/(map1写时间+map2写时间+…)</p>
<p>Average IO rate mb/sec ：（每个map需要写入的数据量/每个map任务实际写入数据的执行时间）之和/任务数＝＝》(1000/map1写时间＋1000/map2写时间+…)/256，所以这个值跟上面一个值总是存在差异。</p>
<p>IO rate std deviation ：上一个值的标准差</p>
<p>Test exec time sec ：整个job的执行时间</p>
</blockquote>
<p>testDFSIO的参数如下：</p>
<table>
<thead>
<tr>
<th>read</th>
<th>读测试。执行该测试之前，需要先做write测试</th>
</tr>
</thead>
<tbody>
<tr>
<td>write</td>
<td>写测试</td>
</tr>
<tr>
<td>nfFiles</td>
<td>文件个数，默认为1</td>
</tr>
<tr>
<td>fileSize</td>
<td>文件大小，默认为1MB</td>
</tr>
<tr>
<td>resFile</td>
<td>结果文件名，默认为” TestDFSIO_results.log”</td>
</tr>
<tr>
<td>bufferSize</td>
<td>设置缓存大小，默认为1000000</td>
</tr>
<tr>
<td>clean</td>
<td>清理数据</td>
</tr>
<tr>
<td>seq</td>
<td>数据是否有序，默认无序</td>
</tr>
</tbody>
</table>
<p><strong>备注</strong>：</p>
<p>如果不到/tmp目录执行 会报TestDFSIO_results.log没有写入权限</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ava.io.FileNotFoundException: TestDFSIO_results.log (Permission denied)</span><br><span class="line">	at java.io.FileOutputStream.open0(Native Method)</span><br><span class="line">	at java.io.FileOutputStream.open(FileOutputStream.java:270)</span><br><span class="line">	at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213)</span><br><span class="line">	at org.apache.hadoop.fs.TestDFSIO.analyzeResult(TestDFSIO.java:1068)</span><br><span class="line">	at org.apache.hadoop.fs.TestDFSIO.run(TestDFSIO.java:891)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)</span><br><span class="line">	at org.apache.hadoop.fs.TestDFSIO.main(TestDFSIO.java:742)</span><br></pre></td></tr></table></figure>
<p>如果不关闭压缩，会报part-00000不存在，因为默认启用了snappy压缩 ，文件是part-00000.snappy</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: &#x2F;tmp&#x2F;benchmark&#x2F;io_write&#x2F;part-00000</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:85)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:75)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1937)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:728)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>HDFS ACL权限设置</title>
    <url>/2021/01/13/HDFS%20ACL%E6%9D%83%E9%99%90%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<h1>HDFS ACL权限设置</h1>
<p>今天主要给大家说一下HDFS文件权限的问题。当一个<code>普通用户</code>去访问<code>HDFS文件</code>时，可能会报<code>Permission denied</code>的错误。那么你会怎么做呢？</p>
<p>像修改linux文件似的，可能的做法有：</p>
<ul>
<li>修改文件所有者</li>
<li>直接将文件赋予全部的权限，即rwx权限。</li>
</ul>
<p>上面的做法虽然可以达到目的，但是相对来说对权限的把握不是很精准，不适用于生产环境。</p>
<p><strong>本文主要讲解HDFS的ACL(Access Control List)权限，通过hdfs超级用户，来为普通用户分配权限。</strong></p>
<h3 id="一、背景"><strong>一、背景</strong></h3>
<p>如下图所示，</p>
<p><img src="/images/image-20210113155315046.png" alt="image-20210113155315046"></p>
<p>目录没有权限，所以创建失败了。</p>
<p>这里就用到了HDFS的ACL权限设置。</p>
<h3 id="二、前提条件"><strong>二、前提条件</strong></h3>
<p>需要确定<code>hdfs-site.xml</code>文件的两个配置项为<code>true</code>：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.permissions.enabled&lt;<span class="regexp">/name&gt;</span></span><br><span class="line"><span class="regexp">    &lt;value&gt;true&lt;/</span>value&gt;</span><br><span class="line">&lt;<span class="regexp">/property&gt;</span></span><br><span class="line"><span class="regexp">&lt;property&gt;</span></span><br><span class="line"><span class="regexp">    &lt;name&gt;dfs.namenode.acls.enabled&lt;/</span>name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">true</span>&lt;<span class="regexp">/value&gt;</span></span><br><span class="line"><span class="regexp">&lt;/</span>property&gt;</span><br></pre></td></tr></table></figure>
<h3 id="三、语法"><strong>三、语法</strong></h3>
<h4 id="1-setfacl"><strong>1. setfacl</strong></h4>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">Usage: hdfs dfs -setfacl -R|[--<span class="keyword">set</span> &lt;acl_spec&gt; &lt;path&gt;]</span><br></pre></td></tr></table></figure>
<p>设置文件和目录的访问控制列表（ACL）。</p>
<p>选项：</p>
<ul>
<li>-b: 删除基本ACL条目以外的所有条目。保留用户，组和其他条目以与权限位兼容。</li>
<li>-k: 删除默认ACL。default</li>
<li>-R: 以递归方式将操作应用于所有文件和目录。<strong>常用。</strong></li>
<li>-m: 修改ACL。新条目将添加到ACL，并保留现有条目。<strong>常用。</strong></li>
<li>-x: 删除指定的ACL条目。保留其他ACL条目。<strong>常用。</strong></li>
<li>–set: 完全替换ACL，丢弃所有现有条目。 acl_spec必须包含用户，组和其他条目，以便与权限位兼容。</li>
<li>acl_spec: 逗号分隔的ACL条目列表。</li>
<li>path: 要修改的文件或目录。</li>
</ul>
<p>示例：</p>
<ul>
<li>hdfs dfs -setfacl -m user:xy_hpt:rw- /user/hive/warehouse/file_record</li>
<li>hdfs dfs -setfacl -x user:hadoop /file</li>
<li>hdfs dfs -setfacl -b /file</li>
<li>hdfs dfs -setfacl -k /dir</li>
<li>hdfs dfs -setfacl --set user::rw-,user:hadoop:rw-,group::r–,other::r-- /file</li>
<li>hdfs dfs -setfacl -R -m user:hadoop:r-x /dir</li>
<li>hdfs dfs -setfacl -m default:user:hadoop:r-x /dir</li>
</ul>
<h4 id="2-getfacl"><strong>2. getfacl</strong></h4>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">Usage: hdfs dfs -getfacl [-R] &lt;path&gt;</span><br></pre></td></tr></table></figure>
<p>显示文件和目录的访问控制列表（ACL）。如果目录具有默认ACL，则getfacl还会显示默认ACL。</p>
<p>选项：</p>
<ul>
<li>-R: 以递归方式列出所有文件和目录的ACL。</li>
<li>path: 要列出的文件或目录。</li>
</ul>
<p>示例：</p>
<ul>
<li>hdfs dfs -getfacl /file</li>
<li>hdfs dfs -getfacl -R /dir</li>
</ul>
<h3 id="四、为hue用户赋予权限"><strong>四、为hue用户赋予权限</strong></h3>
<p>使用hdfs超级用户来设置acl：使用-m参数</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">sudo -u hdfs hdfs dfs -setfacl -m user:hue:rwx /user/hive/warehouse</span><br></pre></td></tr></table></figure>
<p>查看文件目录的acl权限：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">hdfs dfs -getfacl /user/hive/warehouse</span><br></pre></td></tr></table></figure>
<p>文件acl权限如下图所示：</p>
<p><img src="/images/image-20210113155726266.png" alt="image-20210113155726266"></p>
<p>现在<code>hue用户</code>就对<code>/user/hive/warehouse这个目录有了</code>rwx全部权限`了。</p>
<p><strong>备注：</strong></p>
<p>不过是仅限于hive这个目录，对于里面的子文件不是hue用户创建的，hue用户还是无权访问。 如果需要访问递归的子文件，可以使用<code>-R</code>参数，再次授权。</p>
<h3 id="五、总结"><strong>五、总结</strong></h3>
<p>其实这次分享的知识点很简单，但是却很实用。就安全的角度来看，比起<code>chmod 777</code>来说，也比较严谨。</p>
<p>还是希望大家多多练习本文讲述的两个命令：</p>
<ul>
<li>setfacl</li>
<li>getfacl</li>
</ul>
<p>看看这两个命令的其它参数具体什么意思。</p>
<p>关于HDFS shell其它命令，可以查看官网链接：<a href="http://hadoop.apache.org/docs/r2.6.5/hadoop-project-dist/hadoop-common/FileSystemShell.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.6.5/hadoop-project-dist/hadoop-common/FileSystemShell.html</a></p>
]]></content>
  </entry>
  <entry>
    <title>cdh6_hbase新集群配置项整理</title>
    <url>/2020/12/29/cdh6_hbase%E6%96%B0%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E9%A1%B9%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<p>hbase-conf:</p>
<ul>
<li>RS堆栈大小: 32G</li>
<li></li>
<li>hbase.bucketcache.size=64 =64 * 1024M: 堆外缓存大小，单位为M</li>
<li></li>
<li>dfs.replication=3on=3: hdfs副本数</li>
<li></li>
<li>hbase.hregion.max.filesize=20G=20G: Region大小</li>
<li></li>
<li>hbase.hregion.memstore.flush.size=256=256M: Memstore刷新大小</li>
<li></li>
<li>hbase.regionserver.global.memstore.upperLimit=0.t=0.55: 整个RS中Memstore最大比例</li>
<li></li>
</ul>
<h2 id="hbase-regionserver-global-memstore-lowerLimit-0-t-0-5-整个RS中Memstore最小比例-默认0-95">#- hbase.regionserver.global.memstore.lowerLimit=0.t=0.5: 整个RS中Memstore最小比例       默认0.95</h2>
<ul>
<li>hbase.bucketcache.ioengine=off=offheap: 使用堆外缓存</li>
<li></li>
</ul>
<h2 id="hbase-bucketcache-percentage-in-combinebinedcache-0-9-堆外读缓存所占比例，剩余为堆内元数据缓存大小">#- hbase.bucketcache.percentage.in.combinebinedcache=0.9: 堆外读缓存所占比例，剩余为堆内元数据缓存大小</h2>
<ul>
<li>hfile.block.cache.size=0.2=0.2: 校验项,+upperLimit需要小于0.8</li>
<li></li>
<li>hbase.master.handler.count=256=256: Master处理客户端请求最大线程数</li>
<li></li>
<li>hbase.regionserver.handler.count=256=256: RS处理客户端请求最大线程数</li>
<li></li>
<li>hbase.hstore.blockingStoreFiles=100: storefile个数达到该值则block写入</li>
<li></li>
<li>hbase.hregion.memstore.block.multiplier=3:r=3: 强制刷新Memstore大小的倍数</li>
<li></li>
<li>hbase.client.retries.number: 3 : 3</li>
<li></li>
<li>hbase.rpc.timeout: 50: 5000</li>
</ul>
<p>hbase-jvm:</p>
<p>HBASE_OFFHEAPSIZE=??G<br>
HBASE_OPTS=&quot;-XX:MaxDirectMemorySize=??G -Xmx??G -Xms??G -Xmn1g -Xss256k -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m -XX:+UseParNewGC -XX:MaxTenuringThreshold=15  -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSFullGCsBeforeCompaction=0 -XX:CMSInitiatingOccupancyFraction=70 -XX:+PrintTenuringDistribution -XX:SurvivorRatio=2 -XX:+UseConcMarkSweepGC -XX:-DisableExplicitGC $HBASE_OPTS&quot;</p>
<p>-XX:+UseG1GC<br>
-XX:InitiatingHeapOccupancyPercent=65<br>
-XX:-ResizePLAB<br>
-XX:MaxGCPauseMillis=90<br>
-XX:+UnlockDiagnosticVMOptions<br>
-XX:+G1SummarizeConcMark<br>
-XX:+ParallelRefProcEnabled<br>
-XX:G1HeapRegionSize=32m<br>
-XX:G1HeapWastePercent=20<br>
-XX:ConcGCThreads=4<br>
-XX:ParallelGCThreads=16<br>
-XX:MaxTenuringThreshold=1<br>
-XX:G1MixedGCCountTarget=64<br>
-XX:+UnlockExperimentalVMOptions<br>
-XX:G1NewSizePercent=2<br>
-XX:G1OldCSetRegionThresholdPercent=5</p>
<p>HDFS:</p>
<ul>
<li></li>
<li>dfs.datanode.handler.count 64<br>
64</li>
<li></li>
<li>dfs.datanode.max.xcievers,</li>
<li>,</li>
<li></li>
<li>dfs.datanode.max.transfer.threads  12  12288</li>
<li></li>
<li>dfs.namenode.handler.count 256 256</li>
<li></li>
<li>dfs.namenode.service.handler.count 256 256</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>hbase新集群配置项整理</title>
    <url>/2020/12/29/hbase%E6%96%B0%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E9%A1%B9%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h3 id="机器概况">机器概况</h3>
<ul>
<li>总内存:256G</li>
<li>可分配内存:256*0.75=192G</li>
<li>总硬盘:1.8T*12=21.6T</li>
<li>可用硬盘空间:21.6T*0.85=18.36T</li>
</ul>
<h3 id="内存规划">内存规划</h3>
<h4 id="Disk-Java-Heap-Ratio">Disk / Java Heap Ratio</h4>
<p><strong>Disk / Java Heap Ratio=Disk Size / Java Heap = RegionSize / MemstoreSize * ReplicationFactor * HeapFractionForMemstore * 2</strong><br>
一台RegionServer上1bytes的Java内存大小需要搭配多大的硬盘大小最合理。</p>
<p>公式解释：</p>
<ul>
<li>硬盘容量维度下Region个数： Disk Size / (RegionSize ＊ReplicationFactor)</li>
<li>Java Heap维度下Region个： Java Heap * HeapFractionForMemstore / (MemstoreSize / 2 )</li>
<li>硬盘维度和Java Headp维度理论相等：Disk Size / (RegionSize ＊ReplicationFactor)  ＝ Java Heap * HeapFractionForMemstore / (MemstoreSize / 2 ) ＝&gt; Disk Size / Java Heap = RegionSize / MemstoreSize * ReplicationFactor * HeapFractionForMemstore * 2</li>
</ul>
<p>默认配置：</p>
<ul>
<li>RegionSize: hbase.hregion.max.filesize=10G</li>
<li>MemstoreSize: hbase.hregion.memstore.flush.size=128M</li>
<li>ReplicationFactor: dfs.replication=3</li>
<li>HeapFractionForMemstore: hbase.regionserver.global.memstore.lowerLimit = 0.4</li>
</ul>
<p>计算为：10G / 128M * 3 * 0.4 * 2 = 192，即RegionServer上1bytes的Java内存大小需要搭配192bytes的硬盘大小最合理。</p>
<p>默认配置为例，新集群可用内存为192G，即对应的硬盘空间需要为192G * 192 = 36T</p>
<p><strong>默认配置下1:192，硬盘空间不足，可以将内存减少，通过修改HBase配置将多余的内存资源分配给HBase读缓存的BucketCache，这样就可以保证Java Heap并没有实际浪费。</strong></p>
<h4 id="读缓存BucketCache">读缓存BucketCache</h4>
<p>BucketCache模式下HBase的内存布局如图所示：</p>
<p><img src="/Users/xiaohei/Downloads/hbase.png" alt="image"></p>
<p>该模式主要应用于线上读多写少型应用，整个RegionServer内存（Java进程内存）分为两部分：JVM内存和堆外内存。</p>
<ul>
<li>读缓存CombinedBlockCache = LRUBlockCache + 堆外内存BucketCache，用于缓存读到的Block数据</li>
<li>LRUBlockCache，用于缓存元数据Block</li>
<li>BucketCache用于缓存实际用户数据Block</li>
<li>写缓存MemStore，缓存用户写入KeyValue数据</li>
<li>其他部分用于RegionServer正常运行所必须的内存</li>
</ul>
<h4 id="配置说明">配置说明</h4>
<p><img src="/Users/xiaohei/Downloads/hbase1.png" alt="image"></p>
<p>RegionServer 堆栈大小为192G<br>
Java_Heap大小为72G</p>
<ul>
<li>dfs.replication=3: hdfs副本数</li>
<li>hbase.hregion.max.filesize=18G: Region大小</li>
<li>hbase.hregion.memstore.flush.size=256M: Memstore刷新大小</li>
<li>hbase.regionserver.global.memstore.upperLimit=0.58: 整个RS中Memstore最大比例</li>
<li>hbase.regionserver.global.memstore.lowerLimit=0.53: 整个RS中Memstore最小比例</li>
<li>hbase.bucketcache.ioengine=offheap: 使用堆外缓存</li>
<li>hbase.bucketcache.size=(118+16) * 1024M: 堆外缓存大小，单位为M</li>
<li>hbase.bucketcache.percentage.in.combinedcache=0.88: 堆外读缓存所占比例，剩余为堆内元数据缓存大小</li>
<li>hfile.block.cache.size=0.2: 校验项</li>
<li>hbase.regionserver.handler.count=100: RS处理客户端请求最大线程数</li>
<li>hbase.hstore.blockingStoreFiles=100: storefile个数达到该值则block写入</li>
<li>hbase.hregion.memstore.block.multiplier=3: 强制刷新Memstore大小的倍数</li>
</ul>
<p><strong>校验项</strong></p>
<ul>
<li>LRUBlockCache + MemStore &lt; 80% * JVM_HEAP -&gt; (16+40)/72=0.77 &lt;= 0.8</li>
<li>RegionSize / MemstoreSize * ReplicationFactor * HeapFractionForMemstore * 2 -&gt; 18 * 1024 / 256 * 3 * 0.58 * 2 = 250 -&gt; 72G * 250 = 18T &lt;= 18T</li>
<li>Memstore可能的最大大小 -&gt; 341 * 256 / 1024 = 85.25G &gt; 41.76G</li>
<li>hfile.block.cache.size + hbase.regionserver.global.memstore.upperLimit = 0.78 &lt;= 0.8</li>
</ul>
<p><strong>可能存在的风险：Memstore数量同时存在160个以上且写满，将会出现RegionServer级别的强制刷写，造成节点阻塞。</strong></p>
<h3 id="其他">其他</h3>
<h4 id="hbase-env-sh-的-HBase-客户端环境高级配置代码段"><a href="http://hbase-env.sh" target="_blank" rel="noopener">hbase-env.sh</a> 的 HBase 客户端环境高级配置代码段</h4>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">HBASE_OFFHEAPSIZE=??G</span><br><span class="line">HBASE_OPTS="-XX:MaxDirectMemorySize=??G -Xmx??G -Xms??G -Xmn1g -Xss256k -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m -XX:+UseParNewGC -XX:MaxTenuringThreshold=15  -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSFullGCsBeforeCompaction=0 -XX:CMSInitiatingOccupancyFraction=70 -XX:+PrintTenuringDistribution -XX:SurvivorRatio=2 -XX:+UseConcMarkSweepGC -XX:-DisableExplicitGC $HBASE_OPTS"</span><br></pre></td></tr></table></figure>
<h4 id="hbase-site-xml-的-RegionServer-高级配置代码段（安全阀）">hbase-site.xml 的 RegionServer 高级配置代码段（安全阀）</h4>
<p>手动split region</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.wal.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.region.server.rpc.scheduler.factory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rpc.controllerfactory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.thread.compaction.large<span class="tag">&lt;/<span class="name">name</span>&gt;</span><span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">property</span>&gt;</span><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.region.split.policy<span class="tag">&lt;/<span class="name">name</span>&gt;</span><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>修改Kudu表名,和映射在impala的表名</title>
    <url>/2020/12/08/%E4%BF%AE%E6%94%B9Kudu%E8%A1%A8%E5%90%8D,%E5%92%8C%E6%98%A0%E5%B0%84%E5%9C%A8impala%E7%9A%84%E8%A1%A8%E5%90%8D/</url>
    <content><![CDATA[<h1>修改Kudu表名,和映射在impala的表名</h1>
<p>kudu的表名和impala的表名是两码事</p>
<h2 id="修改kudu表名">修改kudu表名</h2>
<h4 id="方法一、在linux中kudu节点执行">方法一、在linux中kudu节点执行</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kudu table rename_table 172.20.85.113:7051 impala::kd_baofoo_cm.cm_entry_tmp impala::kd_baofoo_cm.cm_entry</span><br></pre></td></tr></table></figure>
<h4 id="方法二、在presto中执行">方法二、在presto中执行</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE kudu.default.&quot;impala::kd_baofoo_cm.cm_entry&quot; RENAME TO kudu.default.&quot;impala::kd_baofoo_cm.cm_entry_tmp&quot;</span><br></pre></td></tr></table></figure>
<p>修改了kudu的表名后 在impala查询kudu会报错，需要修改</p>
<h2 id="修改映射的impala表名">修改映射的impala表名</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter table kd_baofoo_cm.cm_entry set tblproperties(&#39;kudu.table_name&#39;&#x3D;&#39;impala::kd_baofoo_cm.cm_entry_tmp&#39;);</span><br><span class="line">alter table kd_baofoo_cm.cm_entry rename to kd_baofoo_cm.cm_entry_tmp;</span><br></pre></td></tr></table></figure>
<h2 id="其他参考：改成外部表，删除重建。">其他参考：改成外部表，删除重建。</h2>
<p>1.将表从内部切换到外部，并删除。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter table kd_baofoo_cm.cm_entry  set tblproperties(&#39;EXTERNAL&#39;&#x3D;&#39;true&#39;);</span><br><span class="line">drop table kd_baofoo_cm.cm_entry</span><br></pre></td></tr></table></figure>
<p>2.将kudu中的表映射到impala中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE kd_baofoo_cm.cm_entry_tmp</span><br><span class="line">STORED AS KUDU</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  &#39;kudu.master_addresses&#39; &#x3D; &#39;cdh85-111:7051,cdh85-112:7051,cdh85-113:7051&#39;, </span><br><span class="line">  &#39;kudu.table_name&#39; &#x3D; &#39;impala::kd_baofoo_cm.cm_entry_tmp&#39;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>解决hdfs文件大小为0</title>
    <url>/2020/11/23/%E8%A7%A3%E5%86%B3hdfs%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E4%B8%BA0/</url>
    <content><![CDATA[<p><strong>问题： 存在文件大小为0，处于打开状态的文件，程序读取这些文件会报错</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cdh85-29 ~]# hadoop fs -du -h  hdfs:&#x2F;&#x2F;ns1&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak</span><br><span class="line">0  1.1 G  hdfs:&#x2F;&#x2F;ns1&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594769101120.log.gz</span><br><span class="line">0  1.1 G  hdfs:&#x2F;&#x2F;ns1&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594856701472.log.gz</span><br><span class="line">0  1.1 G  hdfs:&#x2F;&#x2F;ns1&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594941000485.log.gz</span><br></pre></td></tr></table></figure>
<p>cloudera论坛也有类型的错误 ：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cannot obtain block length for LocatedBlock</span><br></pre></td></tr></table></figure>
<p><a href="https://community.cloudera.com/t5/Support-Questions/Cannot-obtain-block-length-for-LocatedBlock/td-p/117517" target="_blank" rel="noopener">https://community.cloudera.com/t5/Support-Questions/Cannot-obtain-block-length-for-LocatedBlock/td-p/117517</a></p>
<p>但是这个方法并没有解决我的问题。 hdfs debug recoverLease -path  这样也关闭不了文件  ，纠删码策略下 不知道什么bug   这些文件关闭不了。</p>
<h3 id="我的解决方法：">我的解决方法：</h3>
<p><strong>获取hdfs没有正常关闭的文件并删除</strong>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fsck &#x2F;flume&#x2F; -files -openforwrite | grep &quot;OPENFORWRITE&quot;  &gt;tmp.txt</span><br></pre></td></tr></table></figure>
<p>tmp.txt 内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Connecting to namenode via http:&#x2F;&#x2F;cdh85-39:9870&#x2F;fsck?ugi&#x3D;root&amp;files&#x3D;1&amp;openforwrite&#x3D;1&amp;path&#x3D;%2Fflume</span><br><span class="line">&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE&#x2F;pk_day&#x3D;2020-11-23&#x2F;pk_hour&#x3D;16&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1606118400268.snappy.tmp 89401 bytes, replicated: replication&#x3D;3, 1 block(s), OPENFORWRITE:  OK</span><br><span class="line">&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594769101120.log.gz 0 bytes, erasure-coded: policy&#x3D;RS-6-3-1024k, 1 block(s), OPENFORWRITE:  OK</span><br><span class="line">&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594856701472.log.gz 0 bytes, erasure-coded: policy&#x3D;RS-6-3-1024k, 1 block(s), OPENFORWRITE:  OK</span><br><span class="line">&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594941000485.log.gz 0 bytes, erasure-coded: policy&#x3D;RS-6-3-1024k, 1 block(s), OPENFORWRITE:  OK</span><br><span class="line">&#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE-1.1594956902326.log.gz 0 bytes, erasure-coded: policy&#x3D;RS-6-3-1024k, 1 block(s), OPENFORWRITE:  OK</span><br></pre></td></tr></table></figure>
<p>cat tmp.txt | awk -F ’ ’ ‘{print $1}’</p>
<h3 id="移动损坏的文件：">移动损坏的文件：</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat tmp.txt | awk -F &#39; &#39; &#39;&#123;print $1&#125;&#39; | xargs -t -I &#39;&#123;&#125;&#39; sudo -u hdfs hdfs dfs -mv &#123;&#125; &#x2F;flume&#x2F;BankCardAuthReqDTO&#x2F;CREDIT-PRODUCT-RESULT-LOG-MEMBER-RESPONSE_bak&#x2F;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>xargs命令</title>
    <url>/2020/11/23/xargs%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<p>xargs可以将输入内容（通常通过命令行管道传递），转成后续命令的参数，通常用途有：</p>
<ol>
<li>命令组合：尤其是一些命令不支持管道输入，比如<code>ls</code>。</li>
<li>避免参数过长：xargs可以通过<code>-nx</code>来将参数分组，避免参数过长。</li>
</ol>
<p>使用语法如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Usage: xargs [OPTION]... COMMAND INITIAL-ARGS...</span><br><span class="line">Run COMMAND with arguments INITIAL-ARGS and more arguments read from input.</span><br></pre></td></tr></table></figure>
<h2 id="入门例子">入门例子</h2>
<p>首先，创建测试文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">touch a.js b.js c.js</span><br></pre></td></tr></table></figure>
<p>接着，运行如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls *.js | xargs ls -al</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 a.js</span><br><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 b.js</span><br><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 c.js</span><br></pre></td></tr></table></figure>
<p>命令解释：</p>
<ol>
<li>首先，<code>ls *.js</code>的输出为<code>a.js b.js c.js</code>。</li>
<li>通过管道，将<code>a.js b.js c.js</code>作为<code>xargs</code>的输入参数。</li>
<li><code>xargs</code>命令收到输入参数后，对参数进行解析，以空格/换行作为分隔符，拆分成多个参数，这里变成<code>a.js</code>、<code>b.js</code>、<code>c.js</code>。</li>
<li><code>xargs</code>将拆分后的参数，传递给后续的命令，作为后续命令的参数，也就是说，组成这样的命令<code>ls -al a.js b.js c.js</code>。</li>
</ol>
<p>可以加上<code>-t</code>参数，在执行后面的命令前，先将命令打印出来。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls *.js | xargs -t ls -al</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看到多了一行内容<code>ls -al a.js b.js c.js</code>，这就是实际运行的命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls -al a.js b.js c.js</span><br><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 a.js</span><br><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 b.js</span><br><span class="line">-rw-r--r--  1 a  wheel  0 12 18 16:18 c.js</span><br></pre></td></tr></table></figure>
<h2 id="例子：参数替换">例子：参数替换</h2>
<p>有的时候，我们需要用到原始的参数，可以通过参数<code>-i</code>或<code>-I</code>实现。参数说明如下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-I R                         same as --replace=R (R must be specified)</span><br><span class="line">-i,--replace=[R]             Replace R <span class="keyword">in</span> initial arguments with names</span><br><span class="line">                             <span class="built_in">read</span> from standard input. If R is</span><br><span class="line">                             unspecified, assume &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>例子如下，将所有的<code>.js</code>结尾的文件，都加上<code>.backup</code>后缀。<code>-I '{}'</code>表示将后面命令行的<code>{}</code>替换成前面解析出来的参数。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls *.js | xargs -t -I <span class="string">'&#123;&#125;'</span> mv &#123;&#125; &#123;&#125;.backup</span><br></pre></td></tr></table></figure>
<p>展开后的命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv a.js a.js.backup</span><br><span class="line">mv b.js b.js.backup</span><br><span class="line">mv c.js c.js.backup</span><br></pre></td></tr></table></figure>
<h2 id="例子：参数分组">例子：参数分组</h2>
<p>命令行对参数最大长度有限制，xargs通过<code>-nx</code>对参数进行分组来解决这个问题。</p>
<p>首先，创建4个文件用来做实验。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">touch a.js b.js c.js d.js</span><br></pre></td></tr></table></figure>
<p>然后运行如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls *.js | xargs -t -n2 ls -al</span><br></pre></td></tr></table></figure>
<p>输出如下，<code>-n2</code>表示，将参数以2个为一组，传给后面的命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls -al a.js b.js </span><br><span class="line">-rw-r--r-- 1 root root 0 Dec 18 16:52 a.js</span><br><span class="line">-rw-r--r-- 1 root root 0 Dec 18 16:52 b.js</span><br><span class="line">ls -al c.js d.js </span><br><span class="line">-rw-r--r-- 1 root root 0 Dec 18 16:52 c.js</span><br><span class="line">-rw-r--r-- 1 root root 0 Dec 18 16:52 d.js</span><br></pre></td></tr></table></figure>
<h2 id="例子：特殊文件名">例子：特殊文件名</h2>
<p>有的时候，文件名可能存在特殊字符，比如下面的文件名中存在空格。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">touch &#39;hello 01.css&#39; &#39;hello 02.css&#39;</span><br></pre></td></tr></table></figure>
<p>运行之前的命令会报错，因为<code>xargs</code>是以空格/换行作为分隔符，于是就会出现预期之外的行为。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 命令</span></span><br><span class="line">find . -name <span class="string">'*.css'</span> | xargs -t ls -al</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line">ls -al ./hello 01.css ./hello 02.css <span class="comment"># 展开后的命令</span></span><br><span class="line">ls: cannot access ./hello: No such file or directory</span><br><span class="line">ls: cannot access 01.css: No such file or directory</span><br><span class="line">ls: cannot access ./hello: No such file or directory</span><br><span class="line">ls: cannot access 02.css: No such file or directory</span><br></pre></td></tr></table></figure>
<p><code>xargs</code>是这样解决这个问题的。</p>
<ol>
<li><code>-print0</code>：告诉<code>find</code>命令，在输出文件名之后，跟上<code>NULL</code>字符，而不是换行符；</li>
<li><code>-0</code>：告诉<code>xargs</code>，以<code>NULL</code>作为参数分隔符；</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find . -name <span class="string">'*.css'</span> -print0 | xargs -0 -t ls -al</span><br></pre></td></tr></table></figure>
<h2 id="例子：日志备份">例子：日志备份</h2>
<p>将7天前的日志备份到特定目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find . -mtime +7 | xargs -I <span class="string">'&#123;&#125;'</span> mv &#123;&#125; /tmp/otc-svr-logs/</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>could only be written to 0 of the 1 minReplication nodes</title>
    <url>/2020/11/20/could%20only%20be%20written%20to%200%20of%20the%201%20minReplication%20nodes/</url>
    <content><![CDATA[<p>重启dn导致集群不能写入数据，几百个任务都失败了， 经过一个通宵的折腾，记录一下这次重大事故。</p>
<h3 id="报错日志">报错日志</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DataStreamer Exception</span><br><span class="line">20-11-2020 09:48:59 CST mysql2sqoop-1-RD202006060009 INFO - org.apache.hadoop.ipc.RemoteException(java.io.IOException): File &#x2F;user&#x2F;yarn&#x2F;.staging&#x2F;job_1605809645188_1956&#x2F;job.jar could only be written to 0 of the 1 minReplication nodes. There are 66 datanode(s) running and no node(s) are excluded in this operation.</span><br></pre></td></tr></table></figure>
<h3 id="报错的日志各种误导，走了很多弯路，只说一下，最终解决了这个问题的方法：">报错的日志各种误导，走了很多弯路，只说一下，最终解决了这个问题的方法：</h3>
<p>网上有人出现这个问题是格式化解决，它们日志是There are 0 datanode(s) running and no node(s) are excluded in this operation.我的hdfs集群是正常的所有节点都在，只是不能写入数据。</p>
<p>我用的分层策略是One_SSD，查看了DFS Storage Types，发现disk的空间不够。</p>
<p><img src="/images/image-20201120151830207.png" alt="image-20201120151830207"></p>
<h3 id="解决步骤">解决步骤</h3>
<p>ssd磁盘的机器和普通磁盘的机器分2个角色组</p>
<p>普通组不加[SSD]</p>
<p><img src="/images/image-20201120151354085.png" alt="image-20201120151354085"></p>
<p>SSD组</p>
<p><img src="/images/image-20201120151421358.png" alt="image-20201120151421358"></p>
<p>重启datanode</p>
<h3 id="坑、-HDFS分层存储">坑、 HDFS分层存储</h3>
<blockquote>
<p>通过在目录路径开头的括号中添加存储类型，为每个不是标准磁盘的DataNode数据目录指定存储类型。例如：</p>
<p>[SSD]/dfs/dn1</p>
<p>[DISK]/dfs/dn2</p>
<p>[ARCHIVE]/dfs/dn3</p>
</blockquote>
<p><strong>分层存储，官网并没有要求重启datanode，而且也没有说明要分组设置。只是刷新集群配置。其实并没有生效，给以后重启datanode留下了隐患。</strong></p>
]]></content>
  </entry>
  <entry>
    <title>python时间strftime格式化去除前导0</title>
    <url>/2020/11/10/python%E6%97%B6%E9%97%B4strftime%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%8E%BB%E9%99%A4%E5%89%8D%E5%AF%BC0/</url>
    <content><![CDATA[<h1>python时间strftime格式化去除前导0</h1>
<p><strong>解决方案：</strong></p>
<h3 id="linux-加一个“-”符号">linux  (加一个“-”符号)</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">datetime.date(2020, 11, 9).strftime(&quot;%-m月%-d日&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="windows-加一个“-”符号">windows  (加一个“#”符号)</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">datetime.date(2020, 11, 9).strftime(&quot;%#m月%#d日&quot;)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>hive数据导出，并指定分隔符，元素包含引号等</title>
    <url>/2020/10/23/hive%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA%EF%BC%8C%E5%B9%B6%E6%8C%87%E5%AE%9A%E5%88%86%E9%9A%94%E7%AC%A6%EF%BC%8C%E5%85%83%E7%B4%A0%E5%8C%85%E5%90%AB%E5%BC%95%E5%8F%B7%E7%AD%89/</url>
    <content><![CDATA[<h3 id="hive数据导出，并指定分隔符，元素包含引号等">hive数据导出，并指定分隔符，元素包含引号等</h3>
<p>语法格式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">beeline -n username -p password -u jdbc:hive2://host:10000 --verbose=<span class="literal">true</span>  --showHeader=<span class="literal">false</span> --outputformat=tsv2  --color=<span class="literal">true</span>  -e <span class="string">"select * from <span class="variable">$&#123;database&#125;</span>.<span class="variable">$&#123;tablename&#125;</span>"</span> &gt; <span class="variable">$&#123;tableName&#125;</span>.csv</span><br></pre></td></tr></table></figure>
<p>通过 outputformat 指定输出格式</p>
<blockquote>
<p>–outputformat=[table/vertical/csv/tsv/dsv/csv2/tsv2] == 指定输出格式</p>
<p>–delimiterForDSV=&quot;*&quot; ‘&amp;’ 前提（–outputformat=dsv） 指定分隔符</p>
</blockquote>
<p>不同格式对应的分隔符如下表：</p>
<table>
<thead>
<tr>
<th>格式</th>
<th>分隔符</th>
</tr>
</thead>
<tbody>
<tr>
<td>table</td>
<td>表格式</td>
</tr>
<tr>
<td>vertical</td>
<td>如下所示</td>
</tr>
<tr>
<td>csv</td>
<td>‘,’ 逗号(元素包含引号)</td>
</tr>
<tr>
<td>tsv</td>
<td>‘\t’ 制表符(元素包含逗号)</td>
</tr>
<tr>
<td>dsv</td>
<td>默认‘|’ 竖线分割，可通过delimiterForDSV指定分隔符</td>
</tr>
<tr>
<td>csv2</td>
<td>‘,’ 逗号(不含引号)</td>
</tr>
<tr>
<td>tsv2</td>
<td>‘\t’ 制表符(不含引号)</td>
</tr>
</tbody>
</table>
<p>说明：</p>
<blockquote>
<p>csv格式 == 查询元素有’'单引号</p>
<p>csv2格式没有单引号</p>
<p>tsv，tsv2同上</p>
</blockquote>
<h3 id="实例">实例</h3>
<p><strong>impala  ,   ‘|’ 竖线分割 (元素不包含引号)</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">impala-shell -i cdh85-43:21000 -u yarn -l <span class="comment">--auth_creds_ok_in_clear   -B -o /opt/Z2007931000018_S3_N_20200311_00000001.TXT  --output_delimiter='|' -q " select c1,c3,c4,c5,c6,c7,COALESCE(c8,0),COALESCE(c9,0), COALESCE(c10,0),COALESCE(c11,0),'156' from baofoo_rm_regulator.hadoop_pbc_trans_order_aggregate"</span></span><br></pre></td></tr></table></figure>
<p><strong>hive      ‘|’ 竖线分割 (元素包含双引号)</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">beeline -u "jdbc:hive2://172.20.15.12:10000/" -n yarn -p <span class="comment">--outputformat=dsv --showHeader=false -e ' set mapreduce.job.queuename=bf_yarn_pool.production; SELECT  concat("\"", self_acc_name,"\"") ,  concat("\"",self_acc_no ,"\"") ,  concat("\"",bank_acc_name ,"\"") ,  concat("\"",join_code ,"\"") ,  concat("\"",`date` ,"\"") ,  concat("\"", `time`,"\"") ,  concat("\"", cur,"\"") ,  concat("\"",cast(amt as string) ,"\"") ,  concat("\"",cast(usd_amt as string) ,"\"") ,  concat("\"",lend_flag ,"\"") ,  concat("\"",prof_type ,"\"") ,  concat("\"", part_acc_name,"\"") ,  concat("\"",part_acc_no ,"\"") ,  concat("\"", acc_flag,"\"") ,  concat("\"",tran_flag ,"\"") ,  concat("\"",open_bank_name ,"\"") ,  concat("\"",ip_code ,"\"") ,  concat("\"",purpose ,"\"") ,  concat("\"",bord_flag ,"\"") ,  concat("\"",trade_order ,"\"") ,  concat("\"",trans_no ,"\"") FROM BAOFOO_STAT.tb_con_txn_2019_2020 ' &gt; tb_con_txn_2019_2020.csv</span></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>插入图片模板</title>
    <url>/2020/10/14/%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[<hr>
<h2 id="typora-root-url-…">typora-root-url:…</h2>
<p><img src="/images/image-20200116143338485.png" alt="image-20200116143338485"></p>
]]></content>
  </entry>
  <entry>
    <title>hive建表create table xxx as select的问题</title>
    <url>/2020/10/14/hive%E5%BB%BA%E8%A1%A8create%20table%20xxx%20as%20select%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1>hive建表create table xxx as select的问题</h1>
<p>create table xxx as select的方式创建的表默认存储格式是text，所以要注意了假如as select的是其他格式的比如RCFile，<strong>则可能会导致一行变多行的情况</strong>（因为RCFile格式的可能字段包含换行符等），所以必须要加上<br>
create table xxx stored as RCFile as select…<br>
所以使用这种方式建表注意加上指定的存储格式。</p>
<p>测试示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> decision_model.member_close_reason;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> decision_model.member_close_reason <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">'mongodb_member'</span> <span class="keyword">as</span> intype, *</span><br><span class="line"><span class="keyword">from</span> mongo_baofoo_log.log_update_member_state</span><br><span class="line">;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> decision_model.member_close_reason <span class="keyword">where</span> intype &lt;&gt; <span class="string">'mongodb_member'</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/images/image-20201014132306707.png" alt="img"></p>
<p>正确的应该加上指定的存储格式。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> decision_model.member_close_reason;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> decision_model.member_close_reason </span><br><span class="line">    <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> </span><br><span class="line">    <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'|'</span> </span><br><span class="line">    <span class="keyword">STORED</span> <span class="keyword">AS</span> RCFile </span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="string">'mongodb_member'</span> <span class="keyword">as</span> intype, *</span><br><span class="line"><span class="keyword">from</span> mongo_baofoo_log.log_update_member_state </span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> decision_model.member_close_reason <span class="keyword">where</span> intype &lt;&gt; <span class="string">'mongodb_member'</span>;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>解决spark streaming长时间运行日志不断增长问题</title>
    <url>/2020/10/12/%E8%A7%A3%E5%86%B3spark%20streaming%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97%E4%B8%8D%E6%96%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1>解决spark streaming长时间运行日志不断增长问题</h1>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">log4j.rootLogger=WARN,stdout,A1</span><br><span class="line"></span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.Threshold=WARN</span><br><span class="line">log4j.appender.stdout.encoding=UTF-8</span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=[%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;] %m %n[%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;] %p | %F:%L | %M%n%n</span><br><span class="line"></span><br><span class="line">log4j.appender.A1=org.apache.log4j.RollingFileAppender</span><br><span class="line">log4j.appender.A1.BufferedIO=true</span><br><span class="line">log4j.appender.A1.BufferSize=8192</span><br><span class="line">log4j.appender.A1.File=$&#123;spark.yarn.app.container.log.dir&#125;/stderr</span><br><span class="line">log4j.appender.A1.MaxFileSize=10MB</span><br><span class="line">log4j.appender.A1.MaxBackupIndex=9</span><br><span class="line">log4j.appender.A1.encoding=UTF-8</span><br><span class="line">log4j.appender.A1.Append=true</span><br><span class="line">log4j.appender.A1.Threshold=ERROR</span><br><span class="line">log4j.appender.A1.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.A1.layout.ConversionPattern=[%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;] %m %n[%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;] %p | %F:%L | %M%n%n</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo  "================= spark job:CbDimStreamDriver  start!!!========================"</span><br><span class="line"></span><br><span class="line">spark-submit \</span><br><span class="line">--master yarn  \</span><br><span class="line">--deploy-mode  cluster  \</span><br><span class="line">--name  stream-rm-cb-dim  \</span><br><span class="line">--queue bf_yarn_pool.production  \</span><br><span class="line">--class com.baofu.rm.streaming.CbDimStreamDriver  \</span><br><span class="line">--num-executors  32  \</span><br><span class="line">--driver-memory  3G  \</span><br><span class="line">--executor-memory  4G  \</span><br><span class="line">--executor-cores  1  \</span><br><span class="line">--conf spark.dynamicAllocation.enabled=false \</span><br><span class="line">--conf spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC \</span><br><span class="line">--conf spark.streaming.backpressure.enabled=true \</span><br><span class="line">--conf spark.streaming.kafka.maxRatePerPartition=1000 \</span><br><span class="line">--conf spark.eventLog.enabled=false \</span><br><span class="line">--conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \</span><br><span class="line">--conf spark.executor.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties \</span><br><span class="line">--files ./log4j.properties \</span><br><span class="line">/home/bf_app_spark/spark-jobs/streams/fxJob/cbdim/rm-streaming-analysis-pro.jar</span><br><span class="line"></span><br><span class="line">rc=$?</span><br><span class="line">if [[ $rc != 0 ]]; then</span><br><span class="line">    echo "spark task: $0  failed,please check......"  </span><br><span class="line">    exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo "end run spark: `date "+%Y-%m-%d %H:%M:%S"`"  </span><br><span class="line">echo "================== spark job:CbDimStreamDriver  end!!!===================="</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup sh .&#x2F;stream_CbDimStreamDriver.sh &gt; &#x2F;dev&#x2F;null 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
<p>参考： <a href="http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/" target="_blank" rel="noopener">http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/</a></p>
]]></content>
  </entry>
  <entry>
    <title>有道笔记文件备份</title>
    <url>/2020/09/30/%E6%9C%89%E9%81%93%E7%AC%94%E8%AE%B0%E6%96%87%E4%BB%B6%E5%A4%87%E4%BB%BD/</url>
    <content><![CDATA[<h1>有道笔记文件备份</h1>
<ul>
<li><a href="http://www.ask3.cn/files/(%E8%B6%85%E8%AF%A6%E7%BB%86,%E5%B8%A6%E4%BD%A0%E8%B8%A9%E5%9D%91)linux_centos7_%E5%9F%BA%E4%BA%8ECDH6.0.1%E9%85%8D%E7%BD%AEhive_on_tez_%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E(%E4%BA%B2%E6%B5%8B%E6%9C%89%E6%95%88).pdf">(超详细,带你踩坑)linux_centos7_基于CDH6.0.1配置hive_on_tez_执行引擎(亲测有效).pdf</a></li>
<li><a href="http://www.ask3.cn/files/01%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90.pdf">01需求分析.pdf</a></li>
<li><a href="http://www.ask3.cn/files/1.0_OpenLDAP%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3%E5%BC%80%E5%90%AFtls%EF%BC%8C%E4%B8%BB%E4%B8%BB%E5%90%8C%E6%AD%A5.pdf">1.0_OpenLDAP安装文档开启tls，主主同步.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Alluxio_%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95.pdf">Alluxio_效果测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Apache%E8%AE%BE%E7%BD%AE%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E8%BD%AC%E5%8F%91%E7%AB%AF%E5%8F%A3.pdf">Apache设置反向代理转发端口.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6_%E4%BB%8B%E7%BB%8D%EF%BC%8C%E5%AE%89%E8%A3%85%E5%92%8C%E6%B5%8B%E8%AF%95.pdf">CDH6_介绍，安装和测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6%E4%B8%8D%E8%83%BD%E5%85%B3%E6%8E%89_Auto-TLS_%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95.pdf">CDH6不能关掉_Auto-TLS_的解决办法.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6%E5%9C%A8%E5%AE%89%E8%A3%85agent%E6%97%B6%EF%BC%8C%E6%8F%90%E7%A4%BA%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6_Agent%E5%8F%91%E5%87%BA%E7%9A%84%E6%A3%80%E6%B5%8B%E4%BF%A1%E5%8F%B7.pdf">CDH6在安装agent时，提示安装失败无法接收_Agent发出的检测信号.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6%E7%8E%AF%E5%A2%83%E4%B8%ADPhoenix%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8C%E4%BD%BF%E7%94%A8.pdf">CDH6环境中Phoenix的搭建和使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3(1).pdf">CDH6部署文档(1).pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH6%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3.pdf">CDH6部署文档.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH_5.10.2_%E5%AE%89%E8%A3%85kudu_%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E6%95%B4%E7%90%86(1).pdf">CDH_5.10.2_安装kudu_和常见错误整理(1).pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH_5.10.2_%E5%AE%89%E8%A3%85kudu_%E5%92%8C%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E6%95%B4%E7%90%86.pdf">CDH_5.10.2_安装kudu_和常见错误整理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH_%E4%BA%A4%E6%8D%A2%E5%86%85%E5%AD%98%E8%AD%A6%E5%91%8A%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3.pdf">CDH_交换内存警告问题解决.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH_%E7%9A%84Cloudera_Manager%E5%85%8D%E8%B4%B9%E4%B8%8E%E6%94%B6%E8%B4%B9%E7%89%88%E7%9A%84%E5%AF%B9%E6%AF%94%E8%A1%A8_-_Hi%EF%BC%8C%E7%8E%8B%E6%9D%BE%E6%9F%8F_-_%E5%8D%9A%E5%AE%A2%E5%9B%AD.pdf">CDH_的Cloudera_Manager免费与收费版的对比表_-<em>Hi，王松柏</em>-_博客园.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E5%85%83%E6%95%B0%E6%8D%AE%E5%B0%8F%E7%BB%93.pdf">CDH元数据小结.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E5%90%AF%E7%94%A8Kerberos%E5%AF%BC%E8%87%B4hdfs,yarn%E7%AD%89%E9%A1%B5%E9%9D%A2%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E8%A7%A3%E5%86%B3.pdf">CDH启用Kerberos导致hdfs,yarn等页面无法访问解决.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E9%9B%86%E6%88%90%E7%9A%84KDC%E8%BF%81%E7%A7%BB%E8%87%B3FreeIPA%E7%9A%84Kerberos%E8%AE%A4%E8%AF%81.pdf">CDH集成的KDC迁移至FreeIPA的Kerberos认证.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E7%AE%80%E4%BB%8B.pdf">CDH集群搭建简介.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E9%9B%86%E7%BE%A4%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98.pdf">CDH集群时区问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CDH%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2Livy.pdf">CDH集群部署Livy.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Cannot_obtain_block_length_for_LocatedBlock%E6%95%85%E9%9A%9C%E5%88%86%E8%A7%A3%E5%86%B3.pdf">Cannot_obtain_block_length_for_LocatedBlock故障分解决.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CentOS6.X_%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8.pdf">CentOS6.X_升级内核.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CentOS7%E4%B8%8A%E6%89%8B%E5%8A%A8%E9%87%8A%E6%94%BE%E5%86%85%E5%AD%98cache.pdf">CentOS7上手动释放内存cache.pdf</a></li>
<li><a href="http://www.ask3.cn/files/CentOS7%E4%B8%8B%E5%AE%89%E8%A3%85Anaconda3%E5%92%8CTensorflow.pdf">CentOS7下安装Anaconda3和Tensorflow.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Centos7%E8%A3%85NVIDIA%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8(GPU).pdf">Centos7装NVIDIA显卡驱动(GPU).pdf</a></li>
<li><a href="http://www.ask3.cn/files/ClickHouse.pdf">ClickHouse.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ClickHouse_%E9%83%A8%E7%BD%B2%E4%B8%8E%E4%BD%BF%E7%94%A8.pdf">ClickHouse_部署与使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ClouderaManager_(cm)_%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98.pdf">ClouderaManager_(cm)_时区问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Cloudera_Manager%E4%B8%AD%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2Zeppelin%E6%9C%8D%E5%8A%A1.pdf">Cloudera_Manager中安装部署Zeppelin服务.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Cloudera_Manager%E5%9B%9E%E9%80%80.pdf">Cloudera_Manager回退.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Cloudera%E5%B9%B3%E5%8F%B0%E8%BD%AF%E4%BB%B6%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84.pdf">Cloudera平台软件体系结构.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Could_not_open_client_transport_with_JDBC_Uri_jdbchive2.pdf">Could_not_open_client_transport_with_JDBC_Uri_jdbchive2.pdf</a></li>
<li><a href="http://www.ask3.cn/files/DataX_Hdfs_HA(%E9%AB%98%E5%8F%AF%E7%94%A8)%E9%85%8D%E7%BD%AE%E6%94%AF%E6%8C%81.pdf">DataX_Hdfs_HA(高可用)配置支持.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ELK+filebeat_%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.pdf">ELK+filebeat_安装问题总结.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Elasticsearch5.0%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2_%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.pdf">Elasticsearch5.0集群部署_问题总结.pdf</a></li>
<li><a href="http://www.ask3.cn/files/FTP%E7%9A%84%E6%96%B9%E5%BC%8F%E8%AE%BF%E9%97%AECDH%E4%B8%ADHDFS%E6%96%87%E4%BB%B6.pdf">FTP的方式访问CDH中HDFS文件.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Flink%E9%9B%86%E7%BE%A4.pdf">Flink集群.pdf</a></li>
<li><a href="http://www.ask3.cn/files/FreeIPA_%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86.pdf">FreeIPA_常用命令整理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/FreeIPA%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.pdf">FreeIPA部署及基本使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Full_GC__%E5%AF%BC%E8%87%B4RegionServer%E6%8C%82%E4%BA%86.pdf">Full_GC__导致RegionServer挂了.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase2.0_%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4%E8%A1%A8.pdf">HBase2.0_强制删除表.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase_2.0.0_META_%E6%95%B0%E6%8D%AE%E4%BF%AE%E5%A4%8D%E5%B7%A5%E5%85%B7.pdf">HBase_2.0.0_META_数据修复工具.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase_%E7%94%A8phoenix%E5%88%9B%E5%BB%BA%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95.pdf">HBase_用phoenix创建二级索引.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase_%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87%E8%AF%8A%E6%B2%BB.pdf">HBase_疑难杂症诊治.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase%E6%95%B0%E6%8D%AE%E5%9D%97NotServingRegionException%E6%8E%92%E6%9F%A5.pdf">HBase数据块NotServingRegionException排查.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HBase%E8%BF%90%E7%BB%B4%E5%AE%9E%E8%B7%B5.pdf">HBase运维实践.pdf</a></li>
<li><a href="http://www.ask3.cn/files/HDFS%E5%88%86%E5%B1%82%E5%AD%98%E5%82%A8.pdf">HDFS分层存储.pdf</a></li>
<li>[HIVE删除分区表FAILED_Execution_Error,_return_code_1…_MetaException(messageInvalid_p.pdf](<a href="http://www.ask3.cn/files/HIVE%E5%88%A0%E9%99%A4%E5%88%86%E5%8C%BA%E8%A1%A8FAILED_Execution_Error,_return_code_1">http://www.ask3.cn/files/HIVE删除分区表FAILED_Execution_Error,_return_code_1</a>…_MetaException(messageInvalid_p.pdf)</li>
<li><a href="http://www.ask3.cn/files/Hadoop%E8%B0%83%E4%BC%98.pdf">Hadoop调优.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hbase_%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E4%B8%8E%E8%BF%98%E5%8E%9F.pdf">Hbase_数据迁移与还原.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hbase%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6.pdf">Hbase权限控制.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hbase%E8%87%AA%E5%B8%A6%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E5%91%BD%E4%BB%A4.pdf">Hbase自带压力测试命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hive_SQL_Syntax_for_Use_with_Sentry.pdf">Hive_SQL_Syntax_for_Use_with_Sentry.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hive%E4%B9%8B%E2%80%94%E2%80%94%E5%86%85%E9%83%A8%E8%A1%A8%E4%B8%8E%E5%A4%96%E9%83%A8%E8%A1%A8%E7%9B%B8%E4%BA%92%E7%9B%B8%E4%BA%92%E8%BD%AC%E5%8C%96.pdf">Hive之——内部表与外部表相互相互转化.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hive%E5%88%86%E5%8C%BA%E8%A1%A8%E6%96%B0%E5%A2%9E%E5%AD%97%E6%AE%B5.pdf">Hive分区表新增字段.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hive%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%A2%9E%E5%88%97.pdf">Hive实现自增列.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Hive%E8%A1%A8%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81.pdf">Hive表中文乱码.pdf</a></li>
<li><a href="http://www.ask3.cn/files/IIS%E5%86%85%E9%83%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%94%99%E8%AF%AF_dedecms%E7%94%9F%E6%88%90%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E8%B6%85%E6%97%B6%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.pdf">IIS内部服务器错误_dedecms生成静态页面超时解决方案.pdf</a></li>
<li><a href="http://www.ask3.cn/files/IPFS%E9%9F%B3%E4%B9%90%E6%92%AD%E6%94%BE%E5%99%A8.pdf">IPFS音乐播放器.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Impala%E9%85%8D%E7%BD%AELDAP%E8%BA%AB%E4%BB%BD%E8%AE%A4%E8%AF%81.pdf">Impala配置LDAP身份认证.pdf</a></li>
<li><a href="http://www.ask3.cn/files/JanusGraph%E5%8D%95%E6%9C%BA%E6%B5%8B%E8%AF%95.pdf">JanusGraph单机测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Kerberos%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.pdf">Kerberos常用命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Linux_%E5%91%BD%E4%BB%A4%E7%A7%AF%E7%B4%AF.pdf">Linux_命令积累.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Linux_%E7%BB%99%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E6%B7%BB%E5%8A%A0_%E6%96%87%E4%BB%B6%E5%A4%B4.pdf">Linux_给文件内容添加_文件头.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Linux%E4%B8%8B%E4%BD%BF%E7%94%A8Webmin%E6%90%AD%E5%BB%BADNS%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%80%8C%E4%B8%8D%E6%98%AFhosts%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90%E4%B8%BB%E6%9C%BA%E5%90%8D.pdf">Linux下使用Webmin搭建DNS服务器而不是hosts文件解析主机名.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Linux%E4%B8%8B%E6%9B%B4%E6%94%B9%E8%BD%AC%E7%A7%BBmysql%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%AE%E5%BD%95.pdf">Linux下更改转移mysql数据库目录.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Livy_%E5%A4%9A%E7%94%A8%E6%88%B7%E4%BD%BF%E7%94%A8.pdf">Livy_多用户使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/NFS%E5%BC%82%E5%B8%B8%E5%AF%BC%E8%87%B4Host_Monitor%E5%8F%8AAgent%E6%9C%8D%E5%8A%A1%E9%94%99%E8%AF%AF.pdf">NFS异常导致Host_Monitor及Agent服务错误.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Navicat_for_MySQL_%E5%BF%AB%E6%8D%B7%E9%94%AE.pdf">Navicat_for_MySQL_快捷键.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Neo4j%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.pdf">Neo4j安装与配置.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Phoenix%E5%AE%89%E8%A3%85%E5%8F%8A%E5%85%B6%E4%BD%BF%E7%94%A8.pdf">Phoenix安装及其使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Presto_%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%B7%A5%E5%85%B7%E5%92%8C%E7%95%8C%E9%9D%A2.pdf">Presto_客户端工具和界面.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Presto%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E5%8F%8A%E4%BD%BF%E7%94%A8.pdf">Presto安装部署及使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Ranger_%E9%85%8D%E7%BD%AE_LDAP_%E8%B4%A6%E5%8F%B7%EF%BC%88FreeIPA%EF%BC%89%E5%90%8C%E6%AD%A5%E8%B4%A6%E5%8F%B7.pdf">Ranger_配置_LDAP_账号（FreeIPA）同步账号.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Redis5.0%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85.pdf">Redis5.0集群安装.pdf</a></li>
<li><a href="http://www.ask3.cn/files/R%E8%AF%AD%E8%A8%80%E6%93%8D%E4%BD%9Chive,%E5%B9%B6%E8%B0%83%E5%BA%A6.pdf">R语言操作hive,并调度.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Shell_%E6%8C%89%E6%97%A5%E6%9C%9F%E5%BE%AA%E7%8E%AF%E6%89%A7%E8%A1%8C.pdf">Shell_按日期循环执行.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Solr%E5%BC%82%E5%B8%B8%E5%85%B3%E9%97%AD%E5%AF%BC%E8%87%B4index_locked.pdf">Solr异常关闭导致index_locked.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Spark,Impala,Hive_%E8%AE%BE%E7%BD%AE%E9%98%9F%E5%88%97.pdf">Spark,Impala,Hive_设置队列.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Spark_on_Yarn_%E4%B9%8BPython%E7%8E%AF%E5%A2%83%E5%AE%9A%E5%88%B6.pdf">Spark_on_Yarn_之Python环境定制.pdf</a></li>
<li><a href="http://www.ask3.cn/files/TiDB%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E5%92%8C%E6%B5%8B%E8%AF%95.pdf">TiDB快速部署和测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/WORDPRESS_%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E6%98%BE%E7%A4%BA%E5%9B%9E%E8%BD%A6%E6%8D%A2%E8%A1%8C%E7%A9%BA%E8%A1%8C%E5%9B%9E%E8%A1%8C%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.pdf">WORDPRESS_无法正常显示回车换行空行回行的解决方法.pdf</a></li>
<li><a href="http://www.ask3.cn/files/Windows_%E4%B8%8B_Confluence_%E9%AA%8C%E8%AF%81%E7%A0%81%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA_%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95.pdf">Windows_下_Confluence_验证码无法显示_解决办法.pdf</a></li>
<li><a href="http://www.ask3.cn/files/YAML%E6%A0%BC%E5%BC%8F%E8%A7%A3%E6%9E%90.pdf">YAML格式解析.pdf</a></li>
<li><a href="http://www.ask3.cn/files/alluxio%E5%AE%89%E8%A3%85%E5%92%8C_%E7%BB%93%E5%90%88cdh%E4%BD%BF%E7%94%A8%EF%BC%8Calluxio%E5%92%8C_spark%E6%95%B4%E5%90%88.pdf">alluxio安装和_结合cdh使用，alluxio和_spark整合.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ambari_hortonworks_%EF%BC%88hdp%EF%BC%89%E5%AE%89%E8%A3%85%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9.pdf">ambari_hortonworks_（hdp）安装注意事项.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ansible_ansible-demo3_-m_copy_-a_src=testdircopytest_dest=testdir.pdf">ansible_ansible-demo3_-m_copy_-a_src=testdircopytest_dest=testdir.pdf</a></li>
<li><a href="http://www.ask3.cn/files/azkaban%E9%80%9A%E8%BF%87%E8%84%9A%E6%9C%AC%E6%89%93%E5%8C%85%E5%8F%91%E5%B8%83%E5%B7%A5%E7%A8%8B.pdf">azkaban通过脚本打包发布工程.pdf</a></li>
<li><a href="http://www.ask3.cn/files/bat_%E8%BE%93%E5%85%A5%E6%83%B3%E6%89%A7%E8%A1%8C%E7%9A%84%E6%AC%A1%E6%95%B0_%E6%AF%8F%E9%9A%943%E7%A7%92%E9%92%9F%E5%BE%AA%E7%8E%AF%E6%89%A7%E8%A1%8C%E4%B8%80%E6%AC%A1%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F.pdf">bat_输入想执行的次数_每隔3秒钟循环执行一次应用程序.pdf</a></li>
<li><a href="http://www.ask3.cn/files/can_only_run_host_inspector_when_host_is_healthy_cloudera.pdf">can_only_run_host_inspector_when_host_is_healthy_cloudera.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh6.0.1_spark%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%AE%BE%E7%BD%AE.pdf">cdh6.0.1_spark客户端设置.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh6.3.2-%E6%96%B0%E5%8A%9F%E8%83%BD%E6%B5%8B%E8%AF%95.pdf">cdh6.3.2-新功能测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh6.3.2%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2.pdf">cdh6.3.2安装部署.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh6_hbase%E6%96%B0%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E9%A1%B9%E6%95%B4%E7%90%86.pdf">cdh6_hbase新集群配置项整理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh6_oozie%E8%B0%83%E5%BA%A6shell%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81.pdf">cdh6_oozie调度shell中文乱码.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh%E6%9C%8D%E5%8A%A1_%E6%89%8B%E5%8A%A8%E6%93%8D%E4%BD%9C.pdf">cdh服务_手动操作.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdh%E6%A0%B9%E6%8D%AE%E6%9C%BA%E5%99%A8%E6%95%B0%E9%87%8F%E5%88%92%E5%88%86%E8%A7%92%E8%89%B2.pdf">cdh根据机器数量划分角色.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdp_+freeipa___kerberos%E8%AE%A4%E8%AF%81.pdf">cdp_+freeipa___kerberos认证.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cdp%EF%BC%88cdh7%EF%BC%89.pdf">cdp（cdh7）.pdf</a></li>
<li><a href="http://www.ask3.cn/files/centos7.%E4%BD%BF%E7%94%A8Tor_%E5%88%9B%E5%BB%BA%E5%8C%BF%E5%90%8D%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%8C%BF%E5%90%8D%E7%BD%91%E7%AB%99%EF%BC%88.onion%EF%BC%89.pdf">centos7.使用Tor_创建匿名服务和匿名网站（.onion）.pdf</a></li>
<li><a href="http://www.ask3.cn/files/centos7_openldap%E5%8F%8C%E4%B8%BB%E9%83%A8%E7%BD%B2.pdf">centos7_openldap双主部署.pdf</a></li>
<li><a href="http://www.ask3.cn/files/centos_7%E8%AE%BE%E7%BD%AE%E6%9C%80%E5%A4%A7%E6%96%87%E4%BB%B6%E6%89%93%E5%BC%80%E6%95%B0%EF%BC%8C%E4%B8%8D%E7%94%9F%E6%95%88%E9%97%AE%E9%A2%98.pdf">centos_7设置最大文件打开数，不生效问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/clickhouse%E4%BD%BF%E7%94%A8.pdf">clickhouse使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/clouder_manager_(cm)%E9%99%8D%E7%BA%A7.pdf">clouder_manager_(cm)降级.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cloudera_solr_%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.pdf">cloudera_solr_集群搭建.pdf</a></li>
<li><a href="http://www.ask3.cn/files/cloudera_%E6%94%B6%E8%B4%B9%E7%89%88%E5%8A%9F%E8%83%BD.pdf">cloudera_收费版功能.pdf</a></li>
<li><a href="http://www.ask3.cn/files/css_url_%E7%9B%B8%E5%AF%B9%E8%B7%AF%E5%BE%84.pdf">css_url_相对路径.pdf</a></li>
<li><a href="http://www.ask3.cn/files/dbvisualizer_pro_64%E4%BD%8D%E7%A0%B4%E8%A7%A3%E7%89%88_v10.0.20%E4%B8%93%E4%B8%9A%E7%89%88.pdf">dbvisualizer_pro_64位破解版_v10.0.20专业版.pdf</a></li>
<li><a href="http://www.ask3.cn/files/django%E5%81%9A%E4%B8%80%E4%B8%AA%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E5%BA%97%E5%95%86%E7%BD%91%E7%AB%99.pdf">django做一个最简单的店商网站.pdf</a></li>
<li><a href="http://www.ask3.cn/files/dokuwiki_%E5%AE%89%E8%A3%85%E4%B8%8E%E8%AE%BE%E7%BD%AE%E5%92%8C%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9.pdf">dokuwiki_安装与设置和注意事项.pdf</a></li>
<li><a href="http://www.ask3.cn/files/elasticsearch6.4.2%E8%AE%B8%E5%8F%AF%E8%AF%81%E8%BF%87%E6%9C%9F%E4%BA%86_es%E7%A0%B4%E8%A7%A3.pdf">elasticsearch6.4.2许可证过期了_es破解.pdf</a></li>
<li><a href="http://www.ask3.cn/files/elasticsearch%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD%E4%B8%8E%E8%BF%81%E7%A7%BB.pdf">elasticsearch数据备份与迁移.pdf</a></li>
<li><a href="http://www.ask3.cn/files/es-sql_(elasticsearch-sql)%E6%8F%92%E4%BB%B6%E5%AE%89%E8%A3%85.pdf">es-sql_(elasticsearch-sql)插件安装.pdf</a></li>
<li><a href="http://www.ask3.cn/files/es%E6%98%A0%E5%B0%84%E5%88%B0hive_%E7%B1%BB%E5%9E%8B%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E5%9C%B0%E6%96%B9.pdf">es映射到hive_类型需要注意的地方.pdf</a></li>
<li><a href="http://www.ask3.cn/files/filebeat.yml_%E9%85%8D%E7%BD%AE.pdf">filebeat.yml_配置.pdf</a></li>
<li><a href="http://www.ask3.cn/files/filebeat%E8%AF%A6%E8%A7%A3.pdf">filebeat详解.pdf</a></li>
<li><a href="http://www.ask3.cn/files/fuser%E5%91%BD%E4%BB%A4.pdf">fuser命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/github%E6%90%AD%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2.pdf">github搭建hexo博客.pdf</a></li>
<li><a href="http://www.ask3.cn/files/gpu%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95.pdf">gpu压力测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hadoop3.0%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%BA%A0%E5%88%A0%E7%A0%81.pdf">hadoop3.0中使用纠删码.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hadoop%E7%94%9F%E6%80%81%E5%9C%88.pdf">hadoop生态圈.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hadoop%E8%B7%A8%E9%9B%86%E7%BE%A4%E4%B9%8B%E9%97%B4%E8%BF%81%E7%A7%BBhive%E6%95%B0%E6%8D%AE.pdf">hadoop跨集群之间迁移hive数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hadoop%E9%9B%86%E7%BE%A4tmp%E7%9B%AE%E5%BD%95%E8%AE%B8%E5%A4%9ADATANODE_.hprof%E6%96%87%E4%BB%B6.pdf">hadoop集群tmp目录许多DATANODE_.hprof文件.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hadoop%E9%9B%86%E7%BE%A4%E6%8F%90%E9%AB%98%E7%A3%81%E7%9B%98_IO_%E7%9A%84%E6%95%88%E7%8E%87%E3%80%81%E6%8F%90%E5%8D%87%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%82.pdf">hadoop集群提高磁盘_IO_的效率、提升文件系统的性能。.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase2.0__master.pdf">hbase2.0__master.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase_jdbc%E8%BF%9E%E6%8E%A5.pdf">hbase_jdbc连接.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase_shel_l%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E8%BF%87%E6%BB%A4%E5%99%A8.pdf">hbase_shel_l中常用的过滤器.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase_%E7%A0%B4%E5%9D%8F%E6%80%A7%E6%B5%8B%E8%AF%95_%E5%8F%8Cmaster%E6%8C%82%E6%8E%89.pdf">hbase_破坏性测试_双master挂掉.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase%E5%BF%AB%E7%85%A7.pdf">hbase快照.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4%E8%A1%A8%E6%95%B0%E6%8D%AE.pdf">hbase批量删除表数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hbase%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD%E5%AE%9E%E6%88%98.pdf">hbase数据备份实战.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hdfs_balancer.pdf">hdfs_balancer.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hdfs_%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%A4%B9%E5%90%8D%E7%A7%B0(1).pdf">hdfs_批量修改文件夹名称(1).pdf</a></li>
<li><a href="http://www.ask3.cn/files/hdfs_%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%A4%B9%E5%90%8D%E7%A7%B0.pdf">hdfs_批量修改文件夹名称.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hdfs%E5%9D%97%E4%B8%A2%E5%A4%B1%E5%9D%97%E5%AF%BC%E8%87%B4%E7%9A%84%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E8%A7%A3%E5%86%B3.pdf">hdfs块丢失块导致的异常问题排查解决.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive,impala%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8.pdf">hive,impala客户端使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive_jdbc_%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95.pdf">hive_jdbc_压力测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive_%E7%94%9F%E4%BA%A7%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE.pdf">hive_生产测试数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive_%E8%A7%A3%E9%94%81.pdf">hive_解锁.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive_%E8%BF%87%E6%BB%A4%E7%89%B9%E6%AE%8A%E5%AD%97%E7%AC%A6.pdf">hive_过滤特殊字符.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%85%B3%E8%81%94es_,json%E5%B5%8C%E5%A5%97%EF%BC%8Cstruct%EF%BC%8Carry_%E7%AD%89%E7%89%B9%E6%AE%8A%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86.pdf">hive关联es_,json嵌套，struct，arry_等特殊类型数据处理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%88%9B%E5%BB%BA%E6%B0%B8%E4%B9%85%E5%87%BD%E6%95%B0%E5%92%8C%E4%B8%B4%E6%97%B6%E5%87%BD%E6%95%B0.pdf">hive创建永久函数和临时函数.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%8F%AF%E8%A7%86%E5%8C%96%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%E6%80%BB%E7%BB%93.pdf">hive可视化权限控制总结.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%A4%84%E7%90%86json%E5%92%8C%E6%95%B0%E7%BB%84%E6%95%B0%E6%8D%AE.pdf">hive处理json和数组数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%AF%BC%E6%95%B0%E6%8D%AE%E5%88%B0neo4j.pdf">hive导数据到neo4j.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E5%BF%AB%E9%80%9F%E5%A4%8D%E5%88%B6%E4%B8%80%E5%BC%A0%E5%88%86%E5%8C%BA%E8%A1%A8.pdf">hive快速复制一张分区表.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.pdf">hive性能优化.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E6%89%A7%E8%A1%8C%E6%97%A5%E5%BF%97%E8%A7%A3%E6%9E%90%EF%BC%8Cjob%E6%97%A5%E5%BF%97%E8%A7%A3%E6%9E%90.pdf">hive执行日志解析，job日志解析.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E6%98%A0%E5%B0%84hbase.pdf">hive映射hbase.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E6%98%A0%E5%B0%84phoenix_&amp;&amp;_cdh_hive%E7%BB%84%E4%BB%B6%E5%8D%87%E7%BA%A7.pdf">hive映射phoenix_&amp;&amp;_cdh_hive组件升级.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hive%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6.pdf">hive权限控制.pdf</a></li>
<li><a href="http://www.ask3.cn/files/http%E4%B8%8B%E8%BD%BDhdfsd.pdf">http下载hdfsd.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hue4.2_%E8%BF%9E%E6%8E%A5hbase_Api_Error_timed_out.pdf">hue4.2_连接hbase_Api_Error_timed_out.pdf</a></li>
<li><a href="http://www.ask3.cn/files/hue%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEHbase.pdf">hue远程访问Hbase.pdf</a></li>
<li><a href="http://www.ask3.cn/files/impala.pdf">impala.pdf</a></li>
<li><a href="http://www.ask3.cn/files/impala%E4%BC%98%E5%8C%96.pdf">impala优化.pdf</a></li>
<li><a href="http://www.ask3.cn/files/impala%E4%BD%BF%E7%94%A8udf%E5%87%BD%E6%95%B0.pdf">impala使用udf函数.pdf</a></li>
<li><a href="http://www.ask3.cn/files/impala%E5%90%8C%E6%AD%A5%E5%85%83%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8%E5%8E%9F%E5%88%99.pdf">impala同步元数据使用原则.pdf</a></li>
<li><a href="http://www.ask3.cn/files/impala%E9%A9%B1%E5%8A%A8%E7%9A%84%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F.pdf">impala驱动的连接方式.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ipython%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD,_iPython_%E5%AE%89%E8%A3%85_%E6%AF%94shell%E5%A5%BD%E7%94%A8.pdf">ipython常用功能,_iPython_安装_比shell好用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/janusgraph%E9%83%A8%E7%BD%B2.pdf">janusgraph部署.pdf</a></li>
<li><a href="http://www.ask3.cn/files/js_%E8%8E%B7%E5%8F%96%E5%BD%93%E5%A4%A9%E5%87%8C%E6%99%A8%E7%9A%84%E6%97%B6%E9%97%B4%E6%88%B3%EF%BC%8C%E5%87%A0%E5%A4%A9%E5%89%8D%E5%87%8C%E6%99%A8%E7%9A%84%E6%97%B6%E9%97%B4%E6%88%B3.pdf">js_获取当天凌晨的时间戳，几天前凌晨的时间戳.pdf</a></li>
<li><a href="http://www.ask3.cn/files/json_%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5hive_%E5%88%A9%E7%94%A8_get_json_object_%E5%92%8Cjson_tuple_%E5%87%BD%E6%95%B0.pdf">json_数据导入hive_利用_get_json_object_和json_tuple_函数.pdf</a></li>
<li><a href="http://www.ask3.cn/files/jstat_-gcutil_%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8.pdf">jstat_-gcutil_命令使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/jvm%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%E8%B0%83%E4%BC%98.pdf">jvm垃圾回收器调优.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kafka_%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3.pdf">kafka_参数详解.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kerberos%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93.pdf">kerberos使用总结.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kubeflow_%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2.pdf">kubeflow_安装部署.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kubernetes_pv_pvc%E4%B8%8Enfs_%E6%B5%8B%E8%AF%95.pdf">kubernetes_pv_pvc与nfs_测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kudu_%EF%BC%8CNot_enough_live_tablet_servers_to_create_a_table.pdf">kudu_，Not_enough_live_tablet_servers_to_create_a_table.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kudu%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.pdf">kudu常见问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kudu%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.pdf">kudu性能调优.pdf</a></li>
<li><a href="http://www.ask3.cn/files/kylin%E5%AE%89%E8%A3%85_%E5%92%8Ckylin%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86.pdf">kylin安装_和kylin用户权限管理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ldap_%E6%B7%BB%E5%8A%A0%E7%B4%A2%E5%BC%95.pdf">ldap_添加索引.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ldap%E5%AE%9E%E7%8E%B0Linux%E7%99%BB%E5%BD%95%E8%B4%A6%E5%8F%B7%E7%BB%9F%E4%B8%80%E7%AE%A1%E7%90%86.pdf">ldap实现Linux登录账号统一管理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/ldap%E8%BF%87%E6%BB%A4%E5%99%A8.pdf">ldap过滤器.pdf</a></li>
<li><a href="http://www.ask3.cn/files/linux_%E7%94%A8%E6%88%B7%E8%BF%81%E7%A7%BB.pdf">linux_用户迁移.pdf</a></li>
<li><a href="http://www.ask3.cn/files/linux%E4%B8%8B%E8%A7%A3%E5%8E%8B%E7%BC%A9rar%E6%A0%BC%E5%BC%8F%E7%9A%84%E5%8E%8B%E7%BC%A9%E5%8C%85.pdf">linux下解压缩rar格式的压缩包.pdf</a></li>
<li><a href="http://www.ask3.cn/files/linux%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5_NTP%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%BC%80%E6%9C%BA%E8%87%AA%E5%90%AF%E5%8A%A8.pdf">linux时间同步_NTP配置与开机自启动.pdf</a></li>
<li><a href="http://www.ask3.cn/files/linux%E6%9F%A5%E7%9C%8B%E6%9F%90%E4%B8%AA%E6%97%B6%E9%97%B4%E6%AE%B5%E7%9A%84%E6%97%A5%E5%BF%97.pdf">linux查看某个时间段的日志.pdf</a></li>
<li><a href="http://www.ask3.cn/files/linux%E6%9F%A5%E7%9C%8B%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%97%A5%E5%BF%97%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E7%94%A8%E6%93%8D%E4%BD%9C.pdf">linux查看系统的日志的一些实用操作.pdf</a></li>
<li><a href="http://www.ask3.cn/files/messagehive.metastore.sasl.enabled_can%E2%80%99t_be_false_in_non-testing_mode.pdf">messagehive.metastore.sasl.enabled_can’t_be_false_in_non-testing_mode.pdf</a></li>
<li><a href="http://www.ask3.cn/files/messages%E6%97%A5%E5%BF%97%E8%BF%87%E6%BB%A4%E6%8E%89ldapd%E7%9A%84%E9%94%99%E8%AF%AF.pdf">messages日志过滤掉ldapd的错误.pdf</a></li>
<li><a href="http://www.ask3.cn/files/mt5__K%E7%BA%BF%E5%9B%BE_%E6%9C%80%E5%8F%B3%E8%BE%B9%E7%95%99%E7%82%B9%E7%A9%BA%E7%99%BD.pdf">mt5__K线图_最右边留点空白.pdf</a></li>
<li><a href="http://www.ask3.cn/files/mysql%E4%B8%8Ehive_sql_%E5%AF%B9%E6%AF%94.pdf">mysql与hive_sql_对比.pdf</a></li>
<li><a href="http://www.ask3.cn/files/nfs%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4.pdf">nfs常用操作命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/nfs%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86.pdf">nfs问题处理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/nohup%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8Cscp,ftp%E7%AD%89%E9%9C%80%E8%A6%81%E8%BE%93%E5%85%A5%E5%AF%86%E7%A0%81%E7%9A%84%E5%91%BD%E4%BB%A4.pdf">nohup后台运行scp,ftp等需要输入密码的命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/nxlog%E9%85%8D%E7%BD%AE%E5%B8%AE%E5%8A%A9.pdf">nxlog配置帮助.pdf</a></li>
<li><a href="http://www.ask3.cn/files/oozie_%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C.pdf">oozie_命令行操作.pdf</a></li>
<li><a href="http://www.ask3.cn/files/oozie%E8%B0%83%E5%BA%A6ssh%E6%89%A7%E8%A1%8Cshell.pdf">oozie调度ssh执行shell.pdf</a></li>
<li><a href="http://www.ask3.cn/files/openldap_ssl%E9%85%8D%E7%BD%AE.pdf">openldap_ssl配置.pdf</a></li>
<li><a href="http://www.ask3.cn/files/openldap%E5%AE%89%E8%A3%85%EF%BC%8Chue%E3%80%81hive%E3%80%81impala%E9%9B%86%E6%88%90ldap.pdf">openldap安装，hue、hive、impala集成ldap.pdf</a></li>
<li><a href="http://www.ask3.cn/files/openldap%E8%AE%BE%E7%BD%AE%E7%94%A8%E6%88%B7%E4%BF%AE%E6%94%B9%E5%AF%86%E7%A0%81%E6%9D%83%E9%99%90_%E5%AF%86%E7%A0%81%E8%BF%87%E6%9C%9F.pdf">openldap设置用户修改密码权限_密码过期.pdf</a></li>
<li><a href="http://www.ask3.cn/files/oracle_%E5%92%8CSAS%E4%B9%8B%E9%97%B4%E4%BC%A0%E9%80%92%E6%95%B0%E6%8D%AE_sas%E4%B8%AD%E6%96%87%E6%97%A5%E6%9C%9F%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%87%BA.pdf">oracle_和SAS之间传递数据_sas中文日期格式输出.pdf</a></li>
<li><a href="http://www.ask3.cn/files/pgsql%E4%BD%BF%E7%94%A8%E5%B8%AE%E5%8A%A9.pdf">pgsql使用帮助.pdf</a></li>
<li><a href="http://www.ask3.cn/files/pip.pdf">pip.pdf</a></li>
<li><a href="http://www.ask3.cn/files/pip_%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8_%E4%BB%A3%E7%90%86%E5%AE%89%E8%A3%85.pdf">pip_代理服务器_代理安装.pdf</a></li>
<li><a href="http://www.ask3.cn/files/pm2.pdf">pm2.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto-elasticsearch.pdf">presto-elasticsearch.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto_admin_%E5%AE%89%E8%A3%85.pdf">presto_admin_安装.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto_%E6%9B%B4%E6%96%B0hive%E6%95%B0%E6%8D%AE_insert_owerwrite__table.pdf">presto_更新hive数据_insert_owerwrite__table.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto%E6%9F%A5%E8%AF%A2%E5%8C%BA%E5%88%86%E5%A4%A7%E5%B0%8F%E5%86%99%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93(mysql%EF%BC%8Cmongo)%E8%A1%A8%EF%BC%8C%E6%8A%A5%E9%94%99%E8%A1%A8%E5%90%8D%E4%B8%8D%E5%AD%98%E5%9C%A8.pdf">presto查询区分大小写的数据库(mysql，mongo)表，报错表名不存在.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto%E9%85%8D%E7%BD%AEldap%E7%94%A8%E4%BA%8E%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81.pdf">presto配置ldap用于用户认证.pdf</a></li>
<li><a href="http://www.ask3.cn/files/presto%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86.pdf">presto集群管理.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python-module_'pymysql'_has_no_attribute_'connect'.pdf">python-module_‘pymysql’<em>has_no_attribute</em>’connect’.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python3%E6%93%8D%E4%BD%9Chive.pdf">python3操作hive.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python_%E6%93%8D%E4%BD%9Cneo4j.pdf">python_操作neo4j.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python%E4%B8%AD%E7%9A%84urlencode%E4%B8%8Eurldecode.pdf">python中的urlencode与urldecode.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python%E6%93%8D%E4%BD%9Chive.pdf">python操作hive.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python%E6%93%8D%E4%BD%9Cimpala.pdf">python操作impala.pdf</a></li>
<li><a href="http://www.ask3.cn/files/python%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%B0%86%E7%BD%91%E9%A1%B5%E4%B8%AD%E6%89%80%E6%9C%89img_src=XXX_%E5%BD%A2%E5%BC%8F%E4%B8%AD%E7%9A%84XXX%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8F%90%E5%8F%96%E5%87%BA.pdf">python用正则表达式将网页中所有img_src=XXX_形式中的XXX的字符串提取出.pdf</a></li>
<li><a href="http://www.ask3.cn/files/solr_%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96.pdf">solr_查询优化.pdf</a></li>
<li><a href="http://www.ask3.cn/files/spark%E5%81%9Aetl%E6%B8%85%E6%B4%97json%E6%95%B0%E6%8D%AE.pdf">spark做etl清洗json数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/spark%E8%B0%83%E4%BC%98.pdf">spark调优.pdf</a></li>
<li><a href="http://www.ask3.cn/files/sqoop%E8%BF%9E%E6%8E%A5oralce.pdf">sqoop连接oralce.pdf</a></li>
<li><a href="http://www.ask3.cn/files/top%E5%91%BD%E4%BB%A4%E6%8C%87%E5%8D%97.pdf">top命令指南.pdf</a></li>
<li><a href="http://www.ask3.cn/files/visionapp_Remote_Desktop_2010.pdf">visionapp_Remote_Desktop_2010.pdf</a></li>
<li><a href="http://www.ask3.cn/files/win10_%E4%BD%BF%E7%94%A8Tor_%E5%88%9B%E5%BB%BA%E5%8C%BF%E5%90%8D%E6%9C%8D%E5%8A%A1%E5%92%8C%E5%8C%BF%E5%90%8D%E7%BD%91%E7%AB%99%EF%BC%88.onion.pdf">win10_使用Tor_创建匿名服务和匿名网站（.onion.pdf</a></li>
<li><a href="http://www.ask3.cn/files/win10%E4%B8%8B%E5%8F%8C%E7%B3%BB%E7%BB%9F_%E4%BD%BF%E7%94%A8%E5%B8%AE%E5%8A%A9.pdf">win10下双系统_使用帮助.pdf</a></li>
<li><a href="http://www.ask3.cn/files/windows%E4%B8%AD%E7%B1%BB%E4%BC%BClinux%E7%9A%84ln%E5%91%BD%E4%BB%A4.pdf">windows中类似linux的ln命令.pdf</a></li>
<li><a href="http://www.ask3.cn/files/window%E4%B8%8Bpython%E8%BF%9E%E6%8E%A5hive.pdf">window下python连接hive.pdf</a></li>
<li><a href="http://www.ask3.cn/files/wordpress_%E4%B8%BB%E9%A2%98.pdf">wordpress_主题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/zeppelin.pdf">zeppelin.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%B8%8A%E6%B5%B7%E5%8D%81%E6%9D%A1%E9%AA%91%E8%BD%A6%E8%B7%AF%E7%BA%BF%E6%8E%A8%E8%8D%90.pdf">上海十条骑车路线推荐.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8HiveServer2%E7%AE%A1%E7%90%86udf%E5%87%BD%E6%95%B0.pdf">使用HiveServer2管理udf函数.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8Hive%E8%AF%BB%E5%86%99ElasticSearch%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE.pdf">使用Hive读写ElasticSearch中的数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8PHP%E7%9A%84mail%E5%87%BD%E6%95%B0%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6.pdf">使用PHP的mail函数发送邮件.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8dd%E6%B5%8B%E8%AF%95%E7%A1%AC%E7%9B%98%E8%AF%BB%E5%86%99%E9%80%9F%E5%BA%A6%EF%BC%8C%E5%AE%9E%E6%B5%8B%E8%85%BE%E8%AE%AF%E4%BA%91%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8.pdf">使用dd测试硬盘读写速度，实测腾讯云阿里云服务器.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8python%E6%9F%A5%E8%AF%A2Elasticsearch%E5%B9%B6%E5%AF%BC%E5%87%BA%E6%89%80%E6%9C%89%E6%95%B0%E6%8D%AE.pdf">使用python查询Elasticsearch并导出所有数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BD%BF%E7%94%A8scp%E4%BF%9D%E7%95%99%E6%9D%83%E9%99%90.pdf">使用scp保留权限.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E4%BF%AE%E6%94%B9cdh5%E9%9B%86%E7%BE%A4%E4%B8%AD%E4%B8%BB%E6%9C%BAhostName.pdf">修改cdh5集群中主机hostName.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%85%A5%E8%81%8C%E6%8C%87%E5%8D%97.pdf">入职指南.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%88%86%E5%8C%BA%E8%A1%A8%E5%A2%9E%E5%8A%A0%E5%AD%97%E6%AE%B5%E6%8A%A5%E9%94%99_Unable_to_alter_partition._alter_is_not_possible.pdf">分区表增加字段报错_Unable_to_alter_partition._alter_is_not_possible.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%9B%9B%E7%A7%8D%E6%96%B9%E6%B3%95%E6%8A%8AmongDB%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E5%88%B0hive%E6%88%96Hbase.pdf">四种方法把mongDB迁移数据到hive或Hbase.pdf</a></li>
<li>[图数据库JanusGraph实战<a href="http://www.ask3.cn/files/%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93JanusGraph%E5%AE%9E%E6%88%98%5B5%5D_JanusGraph%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BGephi.pdf">5]_JanusGraph可视化之Gephi.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%9C%A8HBase%E9%9B%86%E7%BE%A4%E8%BF%90%E8%A1%8C%E7%9A%84%E6%97%B6%E5%80%99%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB.pdf">在HBase集群运行的时候进行数据迁移.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%9C%A8Hadoop_%E4%B8%8A%E8%BF%90%E8%A1%8CTensorflow.pdf">在Hadoop_上运行Tensorflow.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%E7%9F%A5%E8%AF%86.pdf">堆外内存知识.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%B5%B7%E9%87%8F%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%A6%82%E4%BD%95%E8%BF%90%E7%BB%B4.pdf">大数据环境下海量服务器如何运维.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%A6%82%E4%BD%95%E4%B8%BAPresto%E9%9B%86%E6%88%90Kerberos%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Hive.pdf">如何为Presto集成Kerberos环境下的Hive.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8StreamSets%E5%AE%9E%E7%8E%B0MySQL%E4%B8%AD%E5%8F%98%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%86%99%E5%85%A5Kudu.pdf">如何使用StreamSets实现MySQL中变化数据实时写入Kudu.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEimpala%E8%87%AA%E5%8A%A8%E5%90%8C%E6%AD%A5HMS%E5%85%83%E6%95%B0%E6%8D%AE.pdf">如何配置impala自动同步HMS元数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%AE%89%E8%A3%85kafka%E6%8E%A7%E5%88%B6%E5%8F%B0kafka_web_console.pdf">安装kafka控制台kafka_web_console.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%AE%9D%E4%BB%98%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8_.pdf">宝付业务数据库表_.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%B0%86%E7%B3%BB%E7%BB%9F%E7%94%A8%E6%88%B7%E5%AF%BC%E5%85%A5FreeIPA%E4%B8%AD.pdf">将系统用户导入FreeIPA中.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%B8%A6kerberos%E8%AE%A4%E8%AF%81%E7%9A%84hdfs%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C.pdf">带kerberos认证的hdfs文件操作.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%BC%80%E9%80%9A%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%E5%90%8Ecsv%E5%AF%BC%E5%85%A5hive%E5%BB%BA%E8%A1%A8%E9%97%AE%E9%A2%98.pdf">开通权限管理后csv导入hive建表问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%BC%82%E5%B8%B8%E6%97%A5%E5%BF%97%EF%BC%9A_No_data_or_no_sasl_data_in_the_stream.pdf">异常日志：_No_data_or_no_sasl_data_in_the_stream.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E5%BC%BA%E5%88%B6%E5%88%A0%E9%99%A4hive%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93.pdf">强制删除hive的数据库.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%89%93%E5%8D%B0Spark_RDD%E4%B8%AD%E7%9A%84top_n_%E5%86%85%E5%AE%B9.pdf">打印Spark_RDD中的top_n_内容.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%8A%8Ahdfs%E4%B8%8A%E7%9A%84%E5%A4%9A%E4%B8%AA%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6%E4%B8%BA%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6.pdf">把hdfs上的多个目录下的文件合并为一个文件.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%8B%9B%E8%81%98%E9%9D%A2%E8%AF%95.pdf">招聘面试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%90%AD%E5%BB%BAOpenLDAP%E8%87%AA%E5%8A%A9%E4%BF%AE%E6%94%B9%E5%AF%86%E7%A0%81%E7%B3%BB%E7%BB%9FSelf_Service_Password.pdf">搭建OpenLDAP自助修改密码系统Self_Service_Password.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%90%AD%E5%BB%BAtensorflow-gpu%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83,GPU%E6%B5%8B%E8%AF%95.pdf">搭建tensorflow-gpu深度学习环境,GPU测试.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%95%88%E7%8E%87%E7%AC%94%E8%AE%B0_2019.04.18.pdf">效率笔记_2019.04.18.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%96%B0%E7%A3%81%E7%9B%98%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%B8%8E%E6%8C%82%E8%BD%BD.pdf">新磁盘格式化与挂载.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%97%B6%E9%92%9F%E5%81%8F%E5%B7%AE.pdf">时钟偏差.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%9D%83%E9%99%90_sentry%E8%BF%81%E7%A7%BB.pdf">权限_sentry迁移.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%9F%A5%E7%9C%8Byarn%E6%97%A5%E5%BF%97%E6%8A%A5%E9%94%99Error_getting_logs_at_hostname8041.pdf">查看yarn日志报错Error_getting_logs_at_hostname8041.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%9F%A5%E8%AF%A2cloudera__manager_%E9%85%8D%E7%BD%AE%E4%BF%AE%E6%94%B9.pdf">查询cloudera__manager_配置修改.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%A0%B9%E6%8D%AEmapreduce_%E6%A0%B9%E6%8D%AE_job_id_%E5%BF%AB%E9%80%9F%E6%9F%A5%E5%8E%9F%E5%9B%A0.pdf">根据mapreduce_根据_job_id_快速查原因.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8%E6%9C%89%E9%81%93%E4%BA%91%E7%AC%94%E8%AE%B0.pdf">欢迎使用有道云笔记.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E5%85%A8%E6%96%B0%E7%9A%84%E6%9C%89%E9%81%93%E4%BA%91%E7%AC%94%E8%AE%B0.pdf">欢迎来到全新的有道云笔记.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B5%8B%E8%AF%95cpu%E5%92%8Cgpu%E7%9A%84%E9%80%9F%E5%BA%A6%E5%B7%AE%E8%B7%9D.pdf">测试cpu和gpu的速度差距.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83Kerberos%E3%80%81LDAP%E5%AE%89%E8%A3%85.pdf">测试环境Kerberos、LDAP安装.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B7%BB%E5%8A%A0pgsql%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE_&amp;&amp;_%E9%87%8D%E5%90%AFcloudera_manager%E7%9A%84pgsql.pdf">添加pgsql远程访问_&amp;&amp;_重启cloudera_manager的pgsql.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B9%98%E4%B9%A1%E8%AF%9D%E6%96%B9%E8%A8%80%E7%BF%BB%E8%AF%91.pdf">湘乡话方言翻译.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B9%98%E6%BD%AD%E7%AB%9E%E4%BB%B7%E5%89%8D%E5%8F%B0%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.pdf">湘潭竞价前台管理系统.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E6%B9%98%E6%BD%AD%E7%AB%9E%E4%BB%B7%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.pdf">湘潭竞价后台管理系统.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%94%A8StreamSets%E5%AE%9E%E7%8E%B0MySQL%E4%B8%AD%E5%8F%98%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%86%99%E5%85%A5Kudu.pdf">用StreamSets实现MySQL中变化数据实时写入Kudu.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%94%A8cloudera_manager_%E5%AE%89%E8%A3%85kudu%E6%97%B6%E6%8A%A5%E9%94%99%E8%AF%AF.pdf">用cloudera_manager_安装kudu时报错误.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%9B%91%E5%90%AC%E7%AB%AF%E5%8F%A3_%E5%8F%91%E7%8E%B0%E7%A8%8B%E5%BA%8F%E5%AE%95%E4%BA%86%E3%80%82%E9%87%8D%E5%90%AF%E7%A8%8B%E5%BA%8F.pdf">监听端口_发现程序宕了。重启程序.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%9B%B4%E6%8E%A5%E4%BB%8Ehdfs%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE.pdf">直接从hdfs下载数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%A3%81%E7%9B%98%E4%B8%8D%E8%83%BD%E8%AF%BB%E5%86%99.pdf">磁盘不能读写.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%A6%81%E7%94%A8Hive.pdf">禁用Hive.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E7%A6%81%E7%94%A8_ssh.pdf">禁用_ssh.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%80%81%E9%9B%86%E7%BE%A4%E8%A1%A5%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE.pdf">老集群补历史数据.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7_Ansible_%E5%9C%A8%E9%83%A8%E7%BD%B2%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E4%B8%8B%E7%9A%84%E5%BA%94%E7%94%A8.pdf">自动化运维工具_Ansible_在部署大数据平台下的应用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%A7%A3%E5%86%B3RegionServer_%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E6%95%B0%E6%8A%A5%E8%AD%A6.pdf">解决RegionServer_打开文件描述符数报警.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%A7%A3%E5%86%B3_wordpress_ftp%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5.pdf">解决_wordpress_ftp无法连接.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%A7%A3%E5%86%B3hive_comment_%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98.pdf">解决hive_comment_中文乱码问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%A7%A3%E5%86%B3parquet%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8FImpala%E4%B8%8EHive%E6%97%A5%E6%9C%9F%E6%97%B6%E9%97%B4%E4%B8%8D%E5%90%8C.pdf">解决parquet文件格式Impala与Hive日期时间不同.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%A7%A3%E5%86%B3spark_streaming%E9%95%BF%E6%97%B6%E9%97%B4%E8%BF%90%E8%A1%8C%E6%97%A5%E5%BF%97%E4%B8%8D%E6%96%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98.pdf">解决spark_streaming长时间运行日志不断增长问题.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%BA%BF%E4%B8%8A%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%A2%91%E7%B9%81%E5%AE%95%E6%9C%BA.pdf">记一次线上服务器频繁宕机.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%AE%BE%E7%BD%AE%E6%B5%8F%E8%A7%88%E5%99%A8%E5%85%81%E8%AE%B8Kerberos%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81.pdf">设置浏览器允许Kerberos身份验证.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%B0%88%E8%B0%88spark%E4%B8%AD%E5%AF%B9RDD%E7%9A%84%E8%AE%A4%E8%AF%86%E3%80%82.pdf">谈谈spark中对RDD的认识。.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%B7%A8%E7%89%88%E6%9C%ACdistcp%E6%8A%A5Check-sum%E9%94%99%E8%AF%AF.pdf">跨版本distcp报Check-sum错误.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%BF%9C%E7%A8%8B%E5%8A%9E%E5%85%AC%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88_%E6%90%AD%E5%BB%BA%E5%B1%80%E5%9F%9F%E7%BD%91vpn%E6%9C%8D%E5%8A%A1%E5%99%A8.pdf">远程办公解决方案_搭建局域网vpn服务器.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E8%BF%9E%E6%8E%A5HiveServer2%E4%BC%A0%E9%80%92%E5%8F%82%E6%95%B0%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F.pdf">连接HiveServer2传递参数的几种方式.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%83%A8%E7%BD%B2Harbor%E7%A7%81%E6%9C%89%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93.pdf">部署Harbor私有镜像仓库.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%85%8D%E7%BD%AEOpenLDAP%E7%9A%84%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6.pdf">配置OpenLDAP的日志文件.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%98%BF%E9%87%8Capi%E4%BD%BF%E7%94%A8.pdf">阿里api使用.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%9B%86%E5%9B%A2%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E6%96%B9%E6%A1%88%E8%A7%84%E5%88%92.pdf">集团大数据平台项目整体方案规划.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%9B%86%E5%9B%A2%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E9%A1%B9%E7%9B%AE%E6%95%B4%E6%94%B9%E6%96%B9%E6%A1%88%E8%A7%84%E5%88%92.pdf">集团大数据平台项目整改方案规划.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%9B%86%E7%BE%A4%E6%8B%86%E5%88%86%E9%A1%B9%E7%9B%AE%E5%B7%A5%E4%BD%9C%E8%BF%9B%E5%BA%A6.pdf">集群拆分项目工作进度.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%9B%86%E7%BE%A4%E6%96%B0%E5%8A%A0%E8%8A%82%E7%82%B9%EF%BC%8C%E5%AE%89%E8%A3%85tensorflow.pdf">集群新加节点，安装tensorflow.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%9D%9E%E5%B8%B8%E7%AE%80%E5%8D%95%E7%9A%84PYTHON_HTTP%E6%9C%8D%E5%8A%A1.pdf">非常简单的PYTHON_HTTP服务.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%AA%91%E8%A1%8C%E9%81%82%E6%98%8C.pdf">骑行遂昌.pdf</a></li>
<li><a href="http://www.ask3.cn/files/%E9%B1%BC%E5%84%BF%E8%80%81%E5%B8%88%E7%9A%84%E7%AC%94%E8%AE%B0.pdf">鱼儿老师的笔记.pdf</a></li>
<li></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>IPFS音乐播放器</title>
    <url>/2020/09/30/IPFS%E9%9F%B3%E4%B9%90%E6%92%AD%E6%94%BE%E5%99%A8/</url>
    <content><![CDATA[<h1>IPFS音乐播放器</h1>
<h2 id="IPFS相关">IPFS相关</h2>
<h3 id="IPFS第一次亲密接触">IPFS第一次亲密接触</h3>
<ul>
<li>什么是IPFS</li>
<li>IPFS对比HTTP/FTP等协议的优势</li>
<li>IPFS应用场景</li>
</ul>
<p>-移动数据 交易 路由 网络</p>
<ul>
<li>定义数据 命名</li>
<li>使用数据</li>
</ul>
<p>具体场景;<br>
挂载全球文件<br>
版本管理功能<br>
数据库<br>
加密平台<br>
各种类型cdn<br>
永久访问的链接</p>
<h3 id="ipfs入门">ipfs入门</h3>
<ul>
<li>官网地址：<a href="https://ipfs.io/" target="_blank" rel="noopener">https://ipfs.io</a></li>
<li>下载安装：<a href="https://dist.ipfs.io/#go-ipfs" target="_blank" rel="noopener">https://dist.ipfs.io/#go-ipfs</a></li>
<li>节点初始化
<ul>
<li><code>ipfs init</code></li>
<li>如果出现<code>Error: cannot acquire lock: can't lock file</code>删除其后边给出的repo.lock文件即可</li>
</ul>
</li>
<li>节点配置
<ul>
<li><code>ipfs id</code> 查看当前节点id等信息</li>
<li><code>ipfs config show</code> ipfs配置信息</li>
</ul>
</li>
<li>节点服务器daemon
<ul>
<li><code>ipfs daemon</code></li>
</ul>
</li>
<li>修改IPFS默认路径</li>
<li>开放API请求
<ul>
<li>ipfs config --json Addresses.API ‘&quot;/ip4/0.0.0.0/tcp/5001&quot;’</li>
</ul>
</li>
<li>开放公共网关
<ul>
<li>ipfs config --json Addresses.Gateway ‘&quot;/ip4/0.0.0.0/tcp/8080&quot;’</li>
</ul>
</li>
</ul>
<h3 id="发布数据">发布数据</h3>
<ul>
<li>
<p>上传文件</p>
<p><code>ipfs add haha.txt</code>文本</p>
<p><code>ipfs add cat.jpg</code>图片</p>
<p><code>ipfs add -q cat.jpg</code> 只输出hash结果</p>
<p>hash记录返回的文件路径信息</p>
</li>
<li>
<p>上传/查看目录</p>
<p>添加目录: <code>ipfs add -r dir</code></p>
<p>查看目录: <code>ipfs ls Qmej3u92BHJVkkiC8F5uL1J4a98no76TnXmQxHGXCHGE7t</code></p>
</li>
<li>
<p>发布网站</p>
<p>把包含index.html的文件夹add到ipfs网络即可</p>
</li>
</ul>
<h3 id="获取数据">获取数据</h3>
<p>命令行获取 cat/get</p>
<p><code>ipfs cat QmaoahyA1UejtgPifPvjpsviePsDnJn8vHrYAN9nfF7w9N</code> 查看文本</p>
<p><code>ipfs cat QmaoahyA1UejtgPifPvjpsviePsDnJn8vHrYAN9nfF7w9N &gt; new-haha.txt</code> 保存文本</p>
<p><code>ipfs cat Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u &gt; new-cat.jpg</code>保存图片</p>
<p><code>ipfs get QmZSwQghczB41omjfJipFPQ3FovYyLvXR9oa4Y9LQS7Urp -o tomcat.jpg</code>获取并保存文件</p>
<p><code>ipfs get QmZSwQghczB41omjfJipFPQ3FovYyLvXR9oa4Y9LQS7Urp -Cao tomcat-go</code> 压缩并下载文件</p>
<p><code>ipfs get QmYtrQVXatZGm1WRKZ29vUt5tFa24xnTvhv6D71DSNVttZ -o ipfs-day01</code>获取文件夹</p>
<p>浏览器访问获取</p>
<ol>
<li>
<p>在http://localhost:5001/webui下搜索hash值，可以raw查看原数据，download下载文件</p>
</li>
<li>
<p>在8080端口访问：<a href="http://localhost:8080/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u" target="_blank" rel="noopener">http://localhost:8080/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u</a></p>
</li>
<li>
<p>在ipfs.io访问：<a href="https://ipfs.io/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u" target="_blank" rel="noopener">https://ipfs.io/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u</a></p>
<p>不需要翻墙的网关：<a href="https://ipfs.infura.io/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u" target="_blank" rel="noopener">https://ipfs.infura.io/ipfs/Qmd286K6pohQcTKYqnS1YhWrCiS4gz7Xi34sdwMe9USZ7u</a></p>
</li>
</ol>
<blockquote>
<p>文本数据<br>
图片数据<br>
音频数据</p>
</blockquote>
<h3 id="files文件-文件夹操作">files文件/文件夹操作</h3>
<p><code>ipfs files cp &lt;文件/文件夹的hash&gt; &lt;目标文件/文件夹&gt;</code> 拷贝文件</p>
<p><code>ipfs files ls -l</code> 查看目录</p>
<p><code>ipfs files mkdir</code> 创建目录</p>
<p><code>ipfs files cp</code> 拷贝</p>
<p><code>ipfs files mv</code> 移动</p>
<p><code>ipfs files stat</code> 状态</p>
<p><code>ipfs files read</code> 读取</p>
<h2 id="React音乐播放器">React音乐播放器</h2>
<h3 id="音乐列表数据">音乐列表数据</h3>
<ul>
<li>音乐名称/歌手等文本信息</li>
<li>专辑图片展示</li>
</ul>
<h3 id="播放音频数据">播放音频数据</h3>
<ul>
<li>
<p>网易音乐api地址：</p>
<p><a href="https://github.com/Binaryify/NeteaseCloudMusicApi" target="_blank" rel="noopener">https://github.com/Binaryify/NeteaseCloudMusicApi</a></p>
</li>
<li>
<p>ipfs-api：<a href="https://github.com/ipfs/js-ipfs-api" target="_blank" rel="noopener">https://github.com/ipfs/js-ipfs-api</a></p>
<ul>
<li>初始化环境端口号开启服务：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Show the ipfs config API port to check it is correct</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> ipfs config Addresses.API</span></span><br><span class="line">/ip4/127.0.0.1/tcp/5001</span><br><span class="line"><span class="meta">#</span><span class="bash"> Set it <span class="keyword">if</span> it does not match the above output</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> ipfs config Addresses.API /ip4/127.0.0.1/tcp/5001</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Restart the daemon after changing the config</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run the daemon</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> ipfs daemon</span></span><br></pre></td></tr></table></figure>
<ul>
<li>cat获取数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipfs.files.cat(&quot;QmY4NqRyr9SebC3P6W3pzg22UK3QsJNDKGzDHqQZsEyPi3&quot;, function (err, file) &#123;</span><br><span class="line">    if (err) &#123;</span><br><span class="line">        throw err</span><br><span class="line">    &#125;</span><br><span class="line">    const json &#x3D; file.toString(&#39;utf8&#39;);</span><br><span class="line">    console.log(json)</span><br><span class="line">    that.setState(&#123;</span><br><span class="line">        songInfo: JSON.parse(json)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>add 添加数据</li>
</ul>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> ipfsAPI = <span class="built_in">require</span>(<span class="string">'ipfs-api'</span>)</span><br><span class="line"><span class="keyword">const</span> ipfs = ipfsAPI(<span class="string">'localhost'</span>, <span class="string">'5001'</span>, &#123;<span class="attr">protocol</span>: <span class="string">'http'</span>&#125;)</span><br><span class="line"><span class="keyword">const</span> buffer = Buffer.from(<span class="string">'hello ipfs-api!'</span>)</span><br><span class="line">ipfs.add(buffer)</span><br><span class="line">    .then( <span class="function"><span class="params">rsp</span> =&gt;</span> <span class="built_in">console</span>.log(rsp[<span class="number">0</span>].hash))</span><br><span class="line">	.catch(<span class="function"><span class="params">e</span> =&gt;</span> <span class="built_in">console</span>.error(e))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>设置cors</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ipfs config --json API.HTTPHeaders.Access-Control-Allow-Methods '["PUT", "GET", "POST", "OPTIONS"]'</span><br><span class="line">ipfs config --json API.HTTPHeaders.Access-Control-Allow-Origin '["*"]'</span><br><span class="line">ipfs config --json API.HTTPHeaders.Access-Control-Allow-Credentials '["true"]'</span><br><span class="line">ipfs config --json API.HTTPHeaders.Access-Control-Allow-Headers '["Authorization"]'</span><br><span class="line">ipfs config --json API.HTTPHeaders.Access-Control-Expose-Headers '["Location"]'</span><br></pre></td></tr></table></figure>
<h3 id="ipns">ipns</h3>
<ul>
<li>绑定ipfs节点, 把一个文件/文件夹的hash发布到自己的ID下</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipfs name publish QmSx37PT8iV2XxzfHLMRYSxZEt87uE3jdQwCyz7otd5ktP</span><br></pre></td></tr></table></figure>
<ul>
<li>查看节点绑定的ipfs路径</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ipfs name resolve [peerId]</span><br></pre></td></tr></table></figure>
<ul>
<li>离线客户端框架： <a href="https://github.com/electron/electron" target="_blank" rel="noopener">https://github.com/electron/electron</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>hbase常用命令操作实战</title>
    <url>/2020/09/11/hbase%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C%E5%AE%9E%E6%88%98/</url>
    <content><![CDATA[<h2 id="hbase-hbck2">hbase-hbck2</h2>
<p>从官网下载hbck2 执行文件</p>
<p><a href="http://hbase.apache.org/downloads.html" target="_blank" rel="noopener">http://hbase.apache.org/downloads.html</a></p>
<p>或 wget  <a href="https://www.apache.org/dyn/closer.lua/hbase/hbase-operator-tools-1.0.0/hbase-operator-tools-1.0.0-bin.tar.gz" target="_blank" rel="noopener">https://www.apache.org/dyn/closer.lua/hbase/hbase-operator-tools-1.0.0/hbase-operator-tools-1.0.0-bin.tar.gz</a></p>
<p>cdh官网的使用帮助</p>
<p><a href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hbase_hbck.html#concept_hkk_q25_llb" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hbase_hbck.html#concept_hkk_q25_llb</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /opt/hbase-operator-tools-1.0.0/hbase-hbck2</span><br><span class="line"></span><br><span class="line">hbase hbck -j hbase-hbck2-1.0.0.jar -s  assigns 1588230740</span><br><span class="line"></span><br><span class="line">hbase hbck -j hbase-hbck2-1.0.0.jar -s  assigns hbase:namespace,,1594264903686.db55eec81d86ac0ae26eba718518ce26</span><br><span class="line"></span><br><span class="line">hbase hbck -j hbase-hbck2-1.0.0.jar -s addFsRegionsMissingInMeta default:test n1:tbl_2 n2</span><br></pre></td></tr></table></figure>
<h2 id="hbase-shell-实战">hbase shell  实战</h2>
<p>移动表的数据到另外一个服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 语法：move &#39;encodeRegionName&#39;, &#39;ServerName&#39;&#96;&#96;</span><br><span class="line"># encodeRegionName指的regioName后面的编码，ServerName指的是master-status的Region Servers列表</span><br><span class="line">move &#39;61a647884b761a785daf3b38049aaa27&#39;,&#39;bigdata-5.baofoo.cn,16020,1595233360076&#39;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="如何查看HBase的HFile">如何查看HBase的HFile</h3>
<p><a href="https://www.jianshu.com/p/49043e99795a" target="_blank" rel="noopener">https://www.jianshu.com/p/49043e99795a</a></p>
<h3 id="跨集群备份数据">跨集群备份数据</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">正则查询快照</span></span><br><span class="line">list_snapshots 'snapshot.*_20200902'</span><br><span class="line">echo "list_snapshots 'snapshot.*_`date -d "1 days ago" +%Y%m%d`' " |sudo -u hbase hbase shell </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">同步快照到备份集群</span></span><br><span class="line">curl "http://cdh85-49:20550/" &gt; hbase_tbls.txt</span><br><span class="line">cat hbase_tbls.txt | tr ':' '-' | while read tb; do </span><br><span class="line">sudo -u hbase hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \</span><br><span class="line">-snapshot "snapshot_$&#123;tb&#125;_20200902" \</span><br><span class="line">-copy-from hdfs://cdh85-49:8020/hbase \</span><br><span class="line">-copy-to hdfs://cdh85-106:8020/hbase ;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">还原快照</span></span><br><span class="line">cat hbase_tbls.txt | tr ':' '-' | while read tb; do </span><br><span class="line">echo  "restore_snapshot 'snapshot_$&#123;tb&#125;_20200902'" | sudo -u hbase hbase shell ;  </span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<h3 id="定时备份-批量脚本程序">定时备份  批量脚本程序</h3>
<h3 id="hbase-snapshots-sh-root-cdh85-55">hbase <a href="http://snapshots.sh/" target="_blank" rel="noopener">snapshots.sh</a> root@cdh85-55</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">PRG="$&#123;0&#125;"</span><br><span class="line">BASEDIR=`dirname $&#123;PRG&#125;`</span><br><span class="line">BASEDIR=`cd $&#123;BASEDIR&#125;/;pwd`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 0. 列出所有hbase表 准备做快照</span></span></span><br><span class="line">echo "######################### job start: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line">echo "######################### 0.list hbase tables;"</span><br><span class="line">curl "http://bigdata-5.baofoo.cn:20550/" &gt; $&#123;BASEDIR&#125;/tbls.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1. 生成新的当日的快照  快照命名方式  snapshot_NAMESPARCE-TABLE_YYYYMMDD</span></span></span><br><span class="line">echo "######################### 1.create hbase table snapshots;"</span><br><span class="line">cat  $&#123;BASEDIR&#125;/tbls.txt | while read tbls; do</span><br><span class="line">echo  "######################### create '$&#123;tbls&#125;' snapshot:`date '+%Y-%m%-d %H:%M:%S'`" </span><br><span class="line">echo  "snapshot '$&#123;tbls&#125;','snapshot_$&#123;tbls&#125;_`date +%Y%m%d`' " | awk -F, '&#123; gsub(":","-",$2) ;print $1","$2&#125;' | sudo -u hbase hbase shell ;  done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2.删除指定hbase集群快照， 入参 &#123; $1:n , 表示n天前的快照，比如 0：当天 1：一天前 2：两天前&#125;</span></span></span><br><span class="line">echo "######################### 2.delete hbase history snapshots;"</span><br><span class="line">find_snapshot_list()&#123;</span><br><span class="line">echo "list_snapshots 'snapshot.*_`date -d "$&#123;1&#125; days ago" +%Y%m%d`' " |sudo -u hbase hbase shell  ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">delete_snapshot_list()&#123;</span><br><span class="line">find_snapshot_list $1 | grep "\\[.*\\]"  | sed 's/[]["]//g' | tr ',' '\n' | while read word; do</span><br><span class="line">echo  "delete_snapshot '$&#123;word&#125;'" |sudo -u hbase hbase shell</span><br><span class="line">done</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">删除1天前快照</span></span><br><span class="line">delete_snapshot_list 0</span><br><span class="line"></span><br><span class="line">echo "######################### job end: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">cd /root/hbase_snapshots/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 0. 列出所有hbase表 准备做快照</span></span></span><br><span class="line">echo "######################### job start: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line">echo "######################### 0.list hbase tables;"</span><br><span class="line">curl "http://cdh85-49:20550/" &gt; tbls.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1. 生成新的当日的快照  快照命名方式  snapshot_NAMESPARCE-TABLE_YYYYMMDD</span></span></span><br><span class="line">echo "######################### 1.create hbase table snapshots;"</span><br><span class="line">cat tbls.txt | while read tbls; do</span><br><span class="line">echo  "######################### create '$&#123;tbls&#125;' snapshot:`date '+%Y-%m%-d %H:%M:%S'`" </span><br><span class="line">echo  "snapshot '$&#123;tbls&#125;','snapshot_$&#123;tbls&#125;_`date +%Y%m%d`'; sleep 1; " | awk -F, '&#123; gsub(":","-",$2) ;print $1","$2&#125;' | sudo -u hbase hbase shell ;  done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2.删除指定hbase集群快照， 入参 &#123; $1:n , 表示n天前的快照，比如 0：当天 1：一天前 2：两天前&#125;</span></span></span><br><span class="line">echo "######################### 2.delete hbase history snapshots;"</span><br><span class="line">find_snapshot_list()&#123;</span><br><span class="line">echo "list_snapshots 'snapshot.*_`date -d "$&#123;1&#125; days ago" +%Y%m%d`' " |sudo -u hbase hbase shell  ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">delete_snapshot_list()&#123;</span><br><span class="line">find_snapshot_list $1 | grep "\\[.*\\]"  | sed 's/[]["]//g' | tr ',' '\n' | while read word; do</span><br><span class="line">echo  "delete_snapshot '$&#123;word&#125;'" |sudo -u hbase hbase shell</span><br><span class="line">done</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">删除1天前快照</span></span><br><span class="line">delete_snapshot_list 1</span><br><span class="line"></span><br><span class="line">echo "######################### job end: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 0. 列出所有hbase表 准备做快照</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"list"</span> | sudo -u hbase hbase shell |grep <span class="string">"\\[.*\\]"</span> &gt; a.log</span></span><br><span class="line">curl "http://bigdata-5.baofoo.cn:20550/" &gt; hbase_tbls.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1. 删除当前集群历史快照</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"delete_all_snapshot 'snapshot.*' "</span> |sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase shell  ;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">cat a.log   | sed <span class="string">'s/[]["]//g'</span> | tr <span class="string">','</span> <span class="string">'\n'</span> | <span class="keyword">while</span> <span class="built_in">read</span> word; <span class="keyword">do</span> <span class="built_in">echo</span>  <span class="string">"delete_snapshot   'snapshot_<span class="variable">$&#123;word&#125;</span>_`date -d '1 days ago' +%Y%m%d`' "</span> | awk -F<span class="string">"   "</span> <span class="string">'&#123; gsub(":","-",$2) ;print $1" "$2&#125;'</span> |sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase shell  ; <span class="keyword">done</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2. 生成新的当日的快照  快照命名方式  snapshot_NAMESPARCE-TABLE_YYYYMMDD</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">cat a.log   | sed <span class="string">'s/[]["]//g'</span> | tr <span class="string">','</span> <span class="string">'\n'</span> | <span class="keyword">while</span> <span class="built_in">read</span> word; <span class="keyword">do</span> \</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span>  <span class="string">"snapshot '<span class="variable">$&#123;word&#125;</span>' ,  'snapshot_<span class="variable">$&#123;word&#125;</span>_`date +%Y%m%d`' "</span> | awk -F, <span class="string">'&#123; gsub(":","-",$2) ;print $1","$2&#125;'</span> | sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase shell ;  <span class="keyword">done</span></span></span><br><span class="line"></span><br><span class="line">cat hbase_tbls.txt | while read word; do \</span><br><span class="line">echo  "snapshot '$&#123;word&#125;' ,  'snapshot_$&#123;word&#125;_`date +%Y%m%d`' " | awk -F, '&#123; gsub(":","-",$2) ;print $1","$2&#125;' | sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase shell ;  done</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  3. 删除备份集群历史快照</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">cat a.log |sed <span class="string">'s/"//g'</span>   | sed <span class="string">'s/[][]//g'</span> | tr <span class="string">','</span> <span class="string">'\n'</span> | <span class="keyword">while</span> <span class="built_in">read</span> word; <span class="keyword">do</span> <span class="built_in">echo</span>  <span class="string">"delete_snapshot   'snapshot_<span class="variable">$&#123;word&#125;</span>_`date -d '1 days ago' +%Y%m%d`' "</span> | awk -F<span class="string">"   "</span> <span class="string">'&#123; gsub(":","-",$2) ;print $1" "$2&#125;'</span> |sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase2 shell  ; <span class="keyword">done</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">" delete_all_snapshot 'snapshot.*' "</span> |sudo -u hbase hbase --config  /etc/hbase/conf.cloudera.hbase2 shell  </span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 4. 同步快照到备份集群</span></span></span><br><span class="line">cat a.log |sed 's/"//g'   | sed 's/[][]//g' | tr ',' '\n' | tr ':' '-'  | while read word;do </span><br><span class="line">sudo -u hbase hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \</span><br><span class="line">-Dmapreduce.job.queuename=bf_yarn_pool.development \</span><br><span class="line">--snapshot 'snapshot_$&#123;word&#125;_`date +%Y%m%d`'  \</span><br><span class="line">--copy-from hdfs://ns1/hbase3  \</span><br><span class="line">--copy-to hdfs://ns1/hbase9 \</span><br><span class="line">--chuser hbase -chgroup hbase --overwrite</span><br><span class="line">;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 查找历史快照，   入参 2个 &#123; $1:n , 表示n天前的快照，比如 0：当天 1：一天前 2：两天前；$2: hbase config路径&#125;</span></span></span><br><span class="line"></span><br><span class="line">find_snapshot_list()&#123;</span><br><span class="line">echo "list_snapshots 'snapshot.*_`date -d "$&#123;1&#125; days ago" +%Y%m%d`' " |sudo -u hbase hbase --config  $2 shell  ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 查找一天前快照 hbase1集群</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">find_snapshot_list 1 /etc/hbase/conf.cloudera.hbase</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 删除指定hbase集群快照， 入参 2个，&#123; $1:n , 表示n天前的快照，比如 0：当天 1：一天前 2：两天前；$2: hbase config路径 &#125;</span></span></span><br><span class="line">delete_snapshot_list()&#123;</span><br><span class="line">find_snapshot_list $1 $2 | grep "\\[.*\\]"  | sed 's/[]["]//g' | tr ',' '\n' | while read word; do</span><br><span class="line">echo  "delete_snapshot '$&#123;word&#125;'" |sudo -u hbase hbase --config $2  shell</span><br><span class="line">done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">delete_snapshot_list 1 /etc/hbase/conf.cloudera.hbase</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cat a.log   | sed 's/[]["]//g' | tr ',' '\n' | while read word; do echo   '$&#123;word&#125;' ;done</span><br></pre></td></tr></table></figure>
<h3 id="hbase-restore-snapshot-sh"><a href="http://hbase-restore-snapshot.sh/" target="_blank" rel="noopener">hbase-restore-snapshot.sh</a></h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">PRG="$&#123;0&#125;"</span><br><span class="line">BASEDIR=`dirname $&#123;PRG&#125;`</span><br><span class="line">BASEDIR=`cd $&#123;BASEDIR&#125;/;pwd`</span><br><span class="line"></span><br><span class="line">usage()&#123;</span><br><span class="line">read -p "DO YOU WANT RESTORE ALL HBASE TABLES? PLEASE INPUT[yes OR no]:" yn</span><br><span class="line">case $yn in</span><br><span class="line">        [Yy]* ) continue;;</span><br><span class="line">        [Nn]* ) exit 1;;</span><br><span class="line">        * ) echo "Please answer yes or no.";exit 2;;</span><br><span class="line">esac</span><br><span class="line">&#125;</span><br><span class="line">usage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 0. 列出所有hbase表 准备还原快照</span></span></span><br><span class="line">echo "######################### job start: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line">echo "######################### 0.list hbase tables;"</span><br><span class="line">curl "http://bigdata-3.baofoo.cn:20550/" &gt; $&#123;BASEDIR&#125;/tbls.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1.disable  hbase tables.</span></span></span><br><span class="line">echo "######################### 1.disable hbase  table; "</span><br><span class="line">cat  $&#123;BASEDIR&#125;/tbls.txt | while read tbls; do </span><br><span class="line">echo  "######################### disable '$&#123;tbls&#125;' " </span><br><span class="line">echo  "disable '$&#123;tbls&#125;'  " | sudo -u hbase hbase shell ;  done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2.恢复指定hbase集群快照， 入参 &#123; $1:n , 表示n天前的快照，比如 0：当天 1：一天前 2：两天前&#125;</span></span></span><br><span class="line">echo "######################### 2.restore_snapshot "</span><br><span class="line">find_snapshot_list()&#123;</span><br><span class="line">echo "list_snapshots 'snapshot.*_`date -d "$&#123;1&#125; days ago" +%Y%m%d`' " |sudo -u hbase hbase shell  ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">restore_snapshot_()&#123;</span><br><span class="line">find_snapshot_list $1 | grep "\\[.*\\]"  | sed 's/[]["]//g' | tr ',' '\n' | while read word; do</span><br><span class="line">echo "######################### 2.restore_snapshot '$&#123;word&#125;'; "</span><br><span class="line">echo  "restore_snapshot  '$&#123;word&#125;'" |sudo -u hbase hbase shell</span><br><span class="line">done</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">恢复1天前快照</span></span><br><span class="line">restore_snapshot_ 1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 3.enable  hbase tables.</span></span></span><br><span class="line">echo "######################### 3.enable hbase  table; "</span><br><span class="line">cat  $&#123;BASEDIR&#125;/tbls.txt | while read tbls; do </span><br><span class="line">echo  "######################### enable '$&#123;tbls&#125;' " </span><br><span class="line">echo  "enable '$&#123;tbls&#125;'  "| sudo -u hbase hbase shell ;  done</span><br><span class="line"></span><br><span class="line">echo "######################### job end: `date '+%Y-%m%-d %H:%M:%S'`"</span><br></pre></td></tr></table></figure>
<h3 id="hbase-restore-snashot-1-sh">hbase_restore_snashot_1.sh</h3>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">PRG="$&#123;0&#125;"</span><br><span class="line">BASEDIR=`dirname $&#123;PRG&#125;`</span><br><span class="line">BASEDIR=`cd $&#123;BASEDIR&#125;/;pwd`</span><br><span class="line"></span><br><span class="line">usage()&#123;</span><br><span class="line">echo "RESTORE ALL HBASE TABLES,steps:1.disable hbase tables;2.restore_snapshot;3.enable hbase tables."</span><br><span class="line">read -p "DO YOU WANT RESTORE ALL HBASE TABLES? PLEASE INPUT[yes OR no]:" yn</span><br><span class="line">case $yn in</span><br><span class="line">        [Yy]* ) continue;;</span><br><span class="line">        [Nn]* ) exit 1;;</span><br><span class="line">        * ) echo "Please answer yes or no.";exit 2;;</span><br><span class="line">esac</span><br><span class="line">&#125;</span><br><span class="line">usage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 0. 列出所有hbase表 准备还原快照</span></span></span><br><span class="line">echo "######################### job start: `date '+%Y-%m%-d %H:%M:%S'`"</span><br><span class="line">echo "######################### 0.list hbase tables;"</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#curl "http://bigdata-3.baofoo.cn:20550/" &gt; $&#123;BASEDIR&#125;/tbls.txt</span></span></span><br><span class="line">curl "http://cdh85-49:20550/" &gt; $&#123;BASEDIR&#125;/tbls.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1.disable  hbase tables;  2.restore_snapshot;   3.enable hbase tables;</span></span></span><br><span class="line"></span><br><span class="line">cat  $&#123;BASEDIR&#125;/tbls.txt | while read tbls; do </span><br><span class="line">echo  "######################### 1. disable '$&#123;tbls&#125;' ：`date '+%Y-%m%-d %H:%M:%S'` " </span><br><span class="line">echo  "disable '$&#123;tbls&#125;'" | sudo -u hbase hbase shell ;  </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">deal_date=`date -d <span class="string">"0 days ago"</span> +%Y%m%d`</span></span><br><span class="line">deal_date=`date -d "1 days ago" +%Y%m%d`</span><br><span class="line">tbls_r=`echo $&#123;tbls&#125;|tr ':' '-'`</span><br><span class="line">echo  "######################### 2. restore_snapshot 'snapshot_$&#123;tbls_r&#125;_$&#123;deal_date&#125;'：`date '+%Y-%m%-d %H:%M:%S'`  "</span><br><span class="line">echo  "restore_snapshot 'snapshot_$&#123;tbls_r&#125;_$&#123;deal_date&#125;'" | sudo -u hbase hbase shell ;  </span><br><span class="line"></span><br><span class="line">echo  "######################### 3. enable '$&#123;tbls&#125;' ：`date '+%Y-%m%-d %H:%M:%S'` " </span><br><span class="line">echo  "enable '$&#123;tbls&#125;' " | sudo -u hbase hbase shell ;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">done</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>CDH-HBase 使用 HBCK2 运维</title>
    <url>/2020/08/27/CDH-HBase%20%E4%BD%BF%E7%94%A8%20HBCK2%20%E8%BF%90%E7%BB%B4/</url>
    <content><![CDATA[<h4 id="前言">前言</h4>
<p>周末 CDH6.3 的集群断电，导致 HBase 出现 RIT 状态。</p>
<p>赶紧把之前学的 hbck2 的知识实践顺便回顾下</p>
<h4 id="过程">过程</h4>
<p>将项目拉取到本地 <code>git clone https://github.com/apache/hbase-operator-tools.git --depth 1</code></p>
<p>编译出jar包上传到集群上 <code>mvn clean package -Dmaven.skip.test=true</code></p>
<p>CDH 集群的话将其上传至<code>/opt/cloudera/parcels/CDH/lib/hbase/lib</code>路径下</p>
<h4 id="使用">使用</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hbase.HBCK2 &lt;命令&gt;</span><br><span class="line"></span><br><span class="line"># 验证是否可以使用</span><br><span class="line">hbase org.apache.hbase.HBCK2 -v</span><br></pre></td></tr></table></figure>
<p>结果当头一棒，不支持 2.1.0-cdh6.3.1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">12:48:02.084 [main] INFO  org.apache.hadoop.hbase.client.ConnectionImplementation - Closing master protocol: MasterService</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.UnsupportedOperationException: bypass not supported on server version&#x3D;2.1.0-cdh6.3.1; needs at least a server that matches or exceeds [2.0.3, 2.1.1, 2.2.0, 3.0.0]</span><br><span class="line">        at org.apache.hbase.HBCK2.checkHBCKSupport(HBCK2.java:134)</span><br><span class="line">        at org.apache.hbase.HBCK2.bypass(HBCK2.java:335)</span><br><span class="line">        at org.apache.hbase.HBCK2.doCommandLine(HBCK2.java:686)</span><br><span class="line">        at org.apache.hbase.HBCK2.run(HBCK2.java:631)</span><br><span class="line">        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)</span><br><span class="line">        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)</span><br><span class="line">        at org.apache.hbase.HBCK2.main(HBCK2.java:865)</span><br></pre></td></tr></table></figure>
<p>版本不支持，翻阅文档使用<code>-s</code>跳过版本检查，即 <code>hbase org.apache.hbase.HBCK2 -s</code></p>
<p>更多的命令参考可以参阅下方链接</p>
<ul>
<li><a href="https://github.com/apache/hbase-operator-tools/tree/master/hbase-hbck2" target="_blank" rel="noopener">Apache HBase HBCK2 Tool</a></li>
<li><a href="https://mp.weixin.qq.com/s/GVMWwB1WsKcdvZGfvX1lcA" target="_blank" rel="noopener">HBase 2.0之修复工具HBCK2运维指南</a></li>
</ul>
<h4 id="其他">其他</h4>
<p>Region in transition 的信息是在 active hmater WEB UI 页面上查看，<br>
如果没有RIT状态的 region，其不会显示，所以正常 HBase 集群是看不到 Region in transition 的内容</p>
]]></content>
  </entry>
  <entry>
    <title>Linux(CentOS7)修改mysql默认数据文件目录</title>
    <url>/2020/02/28/Linux(CentOS7)%E4%BF%AE%E6%94%B9mysql%E9%BB%98%E8%AE%A4%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95/</url>
    <content><![CDATA[<p>默认情况下<code>mysql</code>的数据路径应该在</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd /var/lib/mysql</span></span><br></pre></td></tr></table></figure>
<p>现在我们要将它转移到<code>/data</code></p>
<ol>
<li>停掉mysql服务</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># service mysql stop</span></span><br></pre></td></tr></table></figure>
<ol>
<li>将原数据目录转移到<code>data</code>目录下</li>
</ol>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line"># mv /<span class="keyword">var</span>/lib/mysql /<span class="keyword">data</span>/</span><br></pre></td></tr></table></figure>
<ol>
<li>修改<code># vi /etc/my.cnf</code>文件,增加以下行</li>
</ol>
<figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line">datadir = <span class="regexp">/data/my</span>sql</span><br><span class="line">socket = <span class="regexp">/data/my</span>sql/mysql.sock</span><br></pre></td></tr></table></figure>
<ol>
<li>修改<code># vi /etc/init.d/mysql</code>文件,增加以下行</li>
</ol>
<figure class="highlight jsx"><table><tr><td class="code"><pre><span class="line">datadir = <span class="regexp">/data/my</span>sql</span><br></pre></td></tr></table></figure>
<ol>
<li>如果你的 <code># vi /usr/bin/mysqld_safe</code>里面也有指定mysql的数据目录，那么也请按照上面修改</li>
<li>重启<code>mysql</code></li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># service mysql restart</span></span><br></pre></td></tr></table></figure>
<ol>
<li>如果没有成功，重启报错：</li>
</ol>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">尴尬。。忘了提示，差不多就是说找不到/<span class="keyword">var</span>/lib/mysql/mysql.sock</span><br></pre></td></tr></table></figure>
<p>给mysql.sock做个链接</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line"># ln -s /<span class="keyword">data</span>/mysql/mysql.sock /<span class="keyword">var</span>/lib/mysql/mysql.sock</span><br></pre></td></tr></table></figure>
<p>如果提示该链接已经存在，辣就到<code># /var/lib/mysql/</code> <code># rm mysql.sock</code>再进行以上操作。</p>
<ol>
<li>再重启<code>mysql</code>，祝你成功。</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Linux查找含有某字符串的所有文件</title>
    <url>/2020/02/25/Linux%E6%9F%A5%E6%89%BE%E5%90%AB%E6%9C%89%E6%9F%90%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%89%80%E6%9C%89%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<p>如果你想在当前目录下 查找&quot;hello,world!&quot;字符串,可以这样:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep -rn &quot;hello,world!&quot; *</span><br></pre></td></tr></table></figure>
<p><code>*</code> : 表示当前目录所有文件，也可以是某个文件名</p>
<ul>
<li>-r 是递归查找</li>
<li>-n 是显示行号</li>
<li>-R 查找所有文件包含子目录</li>
<li>-i 忽略大小写</li>
</ul>
<h4 id="下面是一些有意思的命令行参数：">下面是一些有意思的命令行参数：</h4>
<p>grep -i pattern files ：不区分大小写地搜索。默认情况区分大小写，<br>
grep -l pattern files ：只列出匹配的文件名，<br>
grep -L pattern files ：列出不匹配的文件名，<br>
grep -w pattern files ：只匹配整个单词，而不是字符串的一部分（如匹配‘magic’，而不是‘magical’），</p>
<p>grep -C number pattern files ：匹配的上下文分别显示[number]行，<br>
grep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的行，<br>
grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。</p>
<h4 id="这里还有些用于搜索的特殊符号：">这里还有些用于搜索的特殊符号：</h4>
<p>&lt; 和 &gt; 分别标注单词的开始与结尾。</p>
<h4 id="例如：">例如：</h4>
<p>grep man  *会匹配 ‘Batman’、‘manic’、‘man’等，<br>
grep '\ 匹配‘manic’和‘man’，但不是‘Batman’，<br>
grep ‘’ 只匹配‘man’，而不是‘Batman’或‘manic’等其他的字符串。</p>
<p><code>'^'</code>：指匹配的字符串在行首，<br>
<code>'$'</code>：指匹配的字符串在行尾，</p>
<h2 id="xargs配合grep查找">xargs配合grep查找</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find -type f -name &#39;*.php&#39;|xargs grep &#39;GroupRecord&#39;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>CDH6 新加节点，不能关掉 Auto-TLS 的解决办法</title>
    <url>/2020/01/16/CDH6%20%E6%96%B0%E5%8A%A0%E8%8A%82%E7%82%B9%EF%BC%8C%E4%B8%8D%E8%83%BD%E5%85%B3%E6%8E%89%20Auto-TLS%20%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</url>
    <content><![CDATA[<h1>CDH6 新加节点，不能关掉 Auto-TLS 的解决办法</h1>
<p>坑一：新加节点 必须关掉TLS ，但是关不到</p>
<p>参考官网</p>
<p><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html</a></p>
<p>使用“添加主机向导”添加主机</p>
<p>您可以使用“添加主机”向导在主机上安装CDH，Impala和Cloudera Manager Agent。</p>
<ol>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__title_214" target="_blank" rel="noopener">禁用TLS加密或身份验证</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__section_t4z_zyk_kcb" target="_blank" rel="noopener">在不禁用TLS的情况下安装Cloudera Manager Agent的替代方法</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__title_215" target="_blank" rel="noopener">使用“添加主机向导”</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__title_217" target="_blank" rel="noopener">启用TLS加密或身份验证</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__section_ejz_fdw_yr" target="_blank" rel="noopener">为CDH组件启用TLS / SSL</a></li>
<li><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/cm_mc_adding_hosts.html#cmug_topic_7_5_1__section_ny1_bxv_ls" target="_blank" rel="noopener">启用Kerberos</a></li>
</ol>
<p>分析：</p>
<p><a href="https://www.cloudera.com/documentation/enterprise/6/6.1/topics/install_cm_server.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/6.1/topics/install_cm_server.html</a></p>
<p>由于安装的时候设置了</p>
<p>sudo JAVA_HOME=/usr/java/jdk1.8.0_141-cloudera /opt/cloudera/cm-agent/bin/certmanager setup --configure-services</p>
<p>导致不能关掉 Auto-TLS</p>
<p>解决办法：</p>
<p>1.cm后台关掉</p>
<p><img src="/images/image-20200116143338485.png" alt="image-20200116143338485"></p>
<ol start="2">
<li>备份cm_init.txt， 然后清空这个文件的内容</li>
</ol>
<p>cp /var/lib/cloudera-scm-server/certmanager/cm_init.txt</p>
<p>3.修改每一个节点的 agent的config.ini</p>
<p>vi /etc/cloudera-scm-agent/config.ini</p>
<p>use_tls = 1 改成 use_tls = 0</p>
<p>4.重启服务</p>
<p>systemctl restart cloudera-scm-server</p>
<p>systemctl restart cloudera-scm-agent</p>
]]></content>
  </entry>
  <entry>
    <title>Typora轻便简洁的Markdown编辑器</title>
    <url>/2020/01/07/Typora%E8%BD%BB%E4%BE%BF%E7%AE%80%E6%B4%81%E7%9A%84Markdown%E7%BC%96%E8%BE%91%E5%99%A8/</url>
    <content><![CDATA[<p>Typora是一款轻便简洁的Markdown编辑器，支持即时渲染技术，这也是与其他Markdown编辑器最显著的区别。即时渲染使得你写Markdown就想是写Word文档一样流畅自如，不像其他编辑器的有编辑栏和显示栏。</p>
<h1>对文字的特殊标注</h1>
<p>标题</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一阶标题  或者快捷键Ctrl+1</span></span><br><span class="line"><span class="comment">## 二阶标题 或者快捷键Ctrl+2</span></span><br><span class="line"><span class="comment">### 三阶标题    或者快捷键Ctrl+3</span></span><br><span class="line"><span class="comment">#### 四阶标题   或者快捷键Ctrl+4</span></span><br><span class="line"><span class="comment">##### 五阶标题  或者快捷键Ctrl+5</span></span><br><span class="line"><span class="comment">###### 六阶标题 或者快捷键Ctrl+6</span></span><br></pre></td></tr></table></figure>
<p>下划线</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">u</span>&gt;</span>下划线的内容<span class="tag">&lt;/<span class="name">u</span>&gt;</span> 或按快捷键Ctrl+U</span><br></pre></td></tr></table></figure>
<p>字体加粗</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">**加粗内容**    或按快捷键Ctrl+B</span><br></pre></td></tr></table></figure>
<p>斜体</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">*倾斜内容*  或按快捷键Ctrl+I</span><br></pre></td></tr></table></figure>
<p>删除线</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~~删除线的内容~~  或按快捷键Alt+Shift+5</span><br></pre></td></tr></table></figure>
<p>文字高亮</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x3D;&#x3D;我是最重要的&#x3D;&#x3D;</span><br></pre></td></tr></table></figure>
<p>角标</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x^2^    H~2~O</span><br></pre></td></tr></table></figure>
<p>文本居中</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span>这是要居中的文本内容<span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>list<br>
有序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">数字+英文小数点(.)+空格</span><br></pre></td></tr></table></figure>
<p>1.策划目标<br>
2.战前准备<br>
3.开始行动<br>
无序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+ 、- 、* 创建无序列，任意数字开始+空格创建有序列表</span><br></pre></td></tr></table></figure>
<p>猪<br>
兔<br>
马<br>
Todolist</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">-</span> <span class="selector-attr">[ ]</span> 参加会议</span><br><span class="line"><span class="selector-tag">-</span> <span class="selector-attr">[x]</span> 中超足球赛</span><br></pre></td></tr></table></figure>
<p>[ ] 参加会议<br>
[x] 中超足球赛<br>
Table</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">快捷键Ctrl+T弹出对话框</span><br></pre></td></tr></table></figure>
<p>分割线</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">***+回车  </span><br><span class="line">---+回车</span><br></pre></td></tr></table></figure>
<h1>插入</h1>
<p>图片</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">![图片内容](http:<span class="comment">//t10.baidu.com/it/u=1069603383,3074552113&amp;fm=170&amp;s=771B15C75C12D8D61C3C69FB0300501F&amp;w=640&amp;h=426&amp;img.JPEG)</span></span><br><span class="line"> 也可使用快捷键Ctrl+K</span><br><span class="line">PS：也可将图片直接拖拽进来，自动生成链接</span><br></pre></td></tr></table></figure>
<p>链接<br>
内行式</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">[<span class="meta">百度一下，你就知道</span>](https:<span class="comment">//www.baidu.com/)</span></span><br></pre></td></tr></table></figure>
<p>参考式</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">[百度一下，你就知道][]https:<span class="comment">//www.baidu.com/          # 第二个括号内可任意填写(不显)</span></span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">[百度一下，你就知道][]https://www.baidu.com/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">快速链接</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">&lt;http:<span class="comment">//www.baidu.com&gt;</span></span><br><span class="line">PS：按住Ctrl点击链接可直接打开。</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#数学公式（简）</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">Typora支持加入用LaTeX写成的数学公式，并且在软件界面下用MathJax直接渲染。</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">*1.行内公式(inline math)，可以在偏好设置中单独打开，由一个美元符号将公式围起来；name=\prod \frac&#123;1&#125;&#123;i^2&#125;$</span></span><br><span class="line"><span class="string">*2.行外公式，直接按Ctrl+Shift+M；(双$+回车也可做到)</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">注：上标和下标可以使用数学表达式来获取</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">#其余</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">引用</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">&gt;+空格    或按快捷键Ctrl+Shift+Q</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">注释</span></span><br><span class="line"><span class="string">要添加注释的文字</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">这是我们的标号[^<span class="number">1</span>]</span><br><span class="line">[^<span class="number">1</span>]:标号的含义</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">表情</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">:单词:</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">目录</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">[TOC]</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br><span class="line"><span class="string">Typora快捷键整合</span></span><br><span class="line"><span class="string">`</span><span class="string">``</span></span><br><span class="line">Ctrl+<span class="number">1</span>  一阶标题    Ctrl+B  字体加粗</span><br><span class="line">Ctrl+<span class="number">2</span>  二阶标题    Ctrl+I  字体倾斜</span><br><span class="line">Ctrl+<span class="number">3</span>  三阶标题    Ctrl+U  下划线</span><br><span class="line">Ctrl+<span class="number">4</span>  四阶标题    Ctrl+Home   返回Typora顶部</span><br><span class="line">Ctrl+<span class="number">5</span>  五阶标题    Ctrl+End    返回Typora底部</span><br><span class="line">Ctrl+<span class="number">6</span>  六阶标题    Ctrl+T  创建表格</span><br><span class="line">Ctrl+L  选中某句话   Ctrl+K  创建超链接</span><br><span class="line">Ctrl+D  选中某个单词  Ctrl+F  搜索</span><br><span class="line">Ctrl+E  选中相同格式的文字   Ctrl+H  搜索并替换</span><br><span class="line">Alt+Shift+<span class="number">5</span> 删除线 Ctrl+Shift+I    插入图片</span><br><span class="line">Ctrl+Shift+M    公式块 Ctrl+Shift+Q    引用</span><br><span class="line"></span><br><span class="line">注：一些实体符号需要在实体符号之前加”\”才能够显示</span><br><span class="line"><span class="string">``</span><span class="string">`</span></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>为hexo添加hexo-admin组件</title>
    <url>/2020/01/07/%E4%B8%BAhexo%E6%B7%BB%E5%8A%A0hexo-admin%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<h4 id="准备工作">准备工作</h4>
<blockquote>
<p>已安装好<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a>，选择好自己的<a href="https://hexo.io/themes/" target="_blank" rel="noopener">主题</a>(我选择的主题是<a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener">melody</a>)，并部署到<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>等静态托管服务器上。</p>
</blockquote>
<h4 id="插件介绍">插件介绍</h4>
<blockquote>
<p><a href="https://github.com/jaredly/hexo-admin" target="_blank" rel="noopener"><strong>hexo-admin</strong></a> 是一个Hexo博客引擎的管理用户界面插件。这个插件最初是作为本地编辑器设计的，在本地运行<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a>使用<strong>hexo-admin</strong>编写文章，然后通过<code>hexo g</code>或<code>hexo d</code>（<code>hexo g</code>是本地渲染，<code>hexo d</code>是将渲染的静态页面发布到<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>）将生成的静态页面发布到<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>等静态服务器。如果你使用的是非静态托管服务器，比如自己买的主机搭建的<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a>，那么一定要设置<a href="https://github.com/jaredly/hexo-admin" target="_blank" rel="noopener"><strong>hexo-admin</strong></a>  的密码，否则谁都可以编辑你的文章。</p>
</blockquote>
<h4 id="插件安装">插件安装</h4>
<ol>
<li>
<p>首先进入hexo创建的博客项目的根目录下，执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install --save hexo-admin</span><br></pre></td></tr></table></figure>
<p>mac可能需要root权限，前面加个<code>sudo</code> 就可以了。如果报错缺少组件，则缺少什么安装什么，<code>npm install</code> 加缺少的组件。</p>
</li>
<li>
<p>运行下列命令启动<a href="https://github.com/jaredly/hexo-admin" target="_blank" rel="noopener"><strong>hexo-admin</strong></a> ：</p>
<p>hexo server -d<br>
打开 <a href="http://localhost:4000/admin/" target="_blank" rel="noopener">http://localhost:4000/admin/</a>  就可以访问到hexo-admin管理页面了。</p>
</li>
</ol>
<h4 id="密码保护">密码保护</h4>
<p>打开<code>setting</code>，点击<code>Setup authentification here</code>输入用户名，密码，密钥，下面会自动生成配置文件，复制加在<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a>根目录下的<code>_config.yml</code>中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">admin:</span><br><span class="line">  username: myfavoritename</span><br><span class="line">  password_hash: be121740bf988b2225a313fa1f107ca1</span><br><span class="line">  secret: a secret something</span><br></pre></td></tr></table></figure>
<p>重启<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">hexo</a>，就可以看到登录页面了</p>
<h4 id="发布文章">发布文章</h4>
<p>进入后台之后点击<code>Deploy</code>，里面的Deploy按钮是用来执行发布脚本的，所以我们先在博客根目录下新建个目录<code>admin_script</code>，然后在目录中新建一个脚本<code>hexo-g.sh</code>，里面写下下面代码然后保存，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure>
<p>然后给hexo-g.sh加入可执行权限</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">chmod</span> +<span class="selector-tag">x</span> <span class="selector-tag">hexo-d</span><span class="selector-class">.sh</span></span><br></pre></td></tr></table></figure>
<p>然后在<code>_config.yml</code>中的admin下添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">admin:</span><br><span class="line">  username: myfavoritename</span><br><span class="line">  password_hash: be121740bf988b2225a313fa1f107ca1</span><br><span class="line">  secret: a secret something</span><br><span class="line">  deployCommand: .&#x2F;admin_script&#x2F;hexo-d.sh</span><br></pre></td></tr></table></figure>
<p>设置发布执行的脚本，点击<code>Deploy</code>就会执行这个命令并提交到<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>上。</p>
]]></content>
  </entry>
  <entry>
    <title>Shell数组</title>
    <url>/2020/01/07/Shell%E6%95%B0%E7%BB%84/</url>
    <content><![CDATA[<p>Shell在编程方面比Windows批处理强大很多，无论是在循环、运算。</p>
<p>bash支持一维数组（不支持多维数组），并且没有限定数组的大小。类似与C语言，数组元素的下标由0开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于0。<br>
定义数组</p>
<p>在Shell中，用括号来表示数组，数组元素用“空格”符号分割开。定义数组的一般形式为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array_name&#x3D;(value1 ... valuen)</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array_name&#x3D;(value0 value1 value2 value3)</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array_name&#x3D;(</span><br><span class="line">value0</span><br><span class="line">value1</span><br><span class="line">value2</span><br><span class="line">value3</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>还可以单独定义数组的各个分量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array_name[0]&#x3D;value0</span><br><span class="line">array_name[1]&#x3D;value1</span><br><span class="line">array_name[2]&#x3D;value2</span><br></pre></td></tr></table></figure>
<p>可以不使用连续的下标，而且下标的范围没有限制。</p>
<h4 id="读取数组">读取数组</h4>
<p>读取数组元素值的一般格式是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;array_name[index]&#125;</span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">valuen&#x3D;$&#123;array_name[2]&#125;</span><br></pre></td></tr></table></figure>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">NAME[0]&#x3D;&quot;Zara&quot;</span><br><span class="line">NAME[1]&#x3D;&quot;Qadir&quot;</span><br><span class="line">NAME[2]&#x3D;&quot;Mahnaz&quot;</span><br><span class="line">NAME[3]&#x3D;&quot;Ayan&quot;</span><br><span class="line">NAME[4]&#x3D;&quot;Daisy&quot;</span><br><span class="line">echo &quot;First Index: $&#123;NAME[0]&#125;&quot;</span><br><span class="line">echo &quot;Second Index: $&#123;NAME[1]&#125;&quot;</span><br></pre></td></tr></table></figure>
<p>运行脚本，输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$.&#x2F;test.sh</span><br><span class="line">First Index: Zara</span><br><span class="line">Second Index: Qadir</span><br></pre></td></tr></table></figure>
<p>使用@ 或 * 可以获取数组中的所有元素，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$&#123;array_name[*]&#125;</span><br><span class="line">$&#123;array_name[@]&#125;</span><br></pre></td></tr></table></figure>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">NAME[0]&#x3D;&quot;Zara&quot;</span><br><span class="line">NAME[1]&#x3D;&quot;Qadir&quot;</span><br><span class="line">NAME[2]&#x3D;&quot;Mahnaz&quot;</span><br><span class="line">NAME[3]&#x3D;&quot;Ayan&quot;</span><br><span class="line">NAME[4]&#x3D;&quot;Daisy&quot;</span><br><span class="line">echo &quot;First Method: $&#123;NAME[*]&#125;&quot;</span><br><span class="line">echo &quot;Second Method: $&#123;NAME[@]&#125;&quot;</span><br></pre></td></tr></table></figure>
<p>运行脚本，输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$.&#x2F;test.sh</span><br><span class="line">First Method: Zara Qadir Mahnaz Ayan Daisy</span><br><span class="line">Second Method: Zara Qadir Mahnaz Ayan Daisy</span><br></pre></td></tr></table></figure>
<h4 id="获取数组的长度">获取数组的长度</h4>
<p>获取数组长度的方法与获取字符串长度的方法相同，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 取得数组元素的个数</span><br><span class="line">length&#x3D;$&#123;#array_name[@]&#125;</span><br><span class="line"># 或者</span><br><span class="line">length&#x3D;$&#123;#array_name[*]&#125;</span><br><span class="line"># 取得数组单个元素的长度</span><br><span class="line">lengthn&#x3D;$&#123;#array_name[n]&#125;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>liunx查看内存</title>
    <url>/2020/01/07/liunx%E6%9F%A5%E7%9C%8B%E5%86%85%E5%AD%98/</url>
    <content><![CDATA[<h3 id="监控内存">监控内存</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">top -d 1</span><br></pre></td></tr></table></figure>
<p>然后使用<code>shift + m</code>以内存排列。<a href="https://www.orchome.com/100" target="_blank" rel="noopener">top命令详解</a></p>
<h3 id="查看内存的使用">查看内存的使用</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">free -g</span><br><span class="line">free -m</span><br></pre></td></tr></table></figure>
<p>可参考：<a href="https://www.orchome.com/296" target="_blank" rel="noopener">free命令详解</a></p>
<h3 id="查看内存">查看内存</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep MemTotal &#x2F;proc&#x2F;meminfo</span><br><span class="line">grep MemTotal &#x2F;proc&#x2F;meminfo | cut -f2 -d:</span><br><span class="line">free -m |grep &quot;Mem&quot; | awk &#39;&#123;print $2&#125;’</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>kubernetes命令大全</title>
    <url>/2020/01/07/kubernetes%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/</url>
    <content><![CDATA[<h2 id="状态查询">状态查询</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看集群信息</span><br><span class="line">kubectl cluster-info</span><br><span class="line"></span><br><span class="line">systemctl status kube-apiserver</span><br><span class="line">systemctl status kubelet</span><br><span class="line">systemctl status kube-proxy</span><br><span class="line">systemctl status kube-scheduler</span><br><span class="line">systemctl status kube-controller-manager</span><br><span class="line">systemctl status docker</span><br></pre></td></tr></table></figure>
<h2 id="node相关">node相关</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看namespaces</span><br><span class="line">kubectl get namespaces</span><br><span class="line"></span><br><span class="line"># 为节点增加lable</span><br><span class="line">kubectl label nodes 10.126.72.31 points&#x3D;test</span><br><span class="line"></span><br><span class="line"># 查看节点和lable</span><br><span class="line">kubectl get nodes --show-labels</span><br><span class="line"></span><br><span class="line"># 查看状态</span><br><span class="line">kubectl get componentstatuses</span><br><span class="line"></span><br><span class="line"># Node的隔离与恢复</span><br><span class="line">## 隔离</span><br><span class="line">kubectl cordon k8s-node1</span><br><span class="line"></span><br><span class="line">## 恢复</span><br><span class="line">kubectl uncordon k8s-node1</span><br></pre></td></tr></table></figure>
<h2 id="查询">查询</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看nodes节点</span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"># 通过yaml文件查询</span><br><span class="line">kubectl get -f xxx-yaml&#x2F;</span><br><span class="line"></span><br><span class="line"># 查询资源</span><br><span class="line">kubectl get resourcequota</span><br><span class="line"></span><br><span class="line"># endpoints端</span><br><span class="line">kubectl get endpoints</span><br><span class="line"></span><br><span class="line"># 查看pods</span><br><span class="line"></span><br><span class="line"># 查看指定空间&#96;kube-system&#96;的pods</span><br><span class="line">kubectl get po -n kube-system</span><br><span class="line"></span><br><span class="line"># 查看所有空间的</span><br><span class="line">kubectl get pods -o wide --all-namespaces</span><br><span class="line"></span><br><span class="line"># 其他的写法</span><br><span class="line">kubectl get pod -o wide --namespace&#x3D;kube-system</span><br><span class="line"></span><br><span class="line"># 获取svc</span><br><span class="line">kubectl get svc --all-namespaces</span><br><span class="line"></span><br><span class="line"># 其他写法</span><br><span class="line">kubectl get services --all-namespaces</span><br><span class="line"></span><br><span class="line"># 通过lable查询</span><br><span class="line">kubectl get pods -l app&#x3D;nginx -o yaml|grep podIP</span><br><span class="line"></span><br><span class="line"># 当我们发现一个pod迟迟无法创建时，描述一个pods</span><br><span class="line">kubectl describe pod xxx</span><br></pre></td></tr></table></figure>
<h2 id="删除所有pod">删除所有pod</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 删除所有pods</span><br><span class="line">kubectl delete pods --all</span><br><span class="line"></span><br><span class="line"># 删除所有包含某个lable的pod和serivce</span><br><span class="line">kubectl delete pods,services -l name&#x3D;&lt;lable-name&gt;</span><br><span class="line"></span><br><span class="line"># 删除ui server,然后重建</span><br><span class="line">kubectl delete deployments kubernetes-dashboard --namespace&#x3D;kube-system</span><br><span class="line">kubectl delete services kubernetes-dashboard --namespace&#x3D;kube-system</span><br><span class="line"></span><br><span class="line"># 强制删除部署</span><br><span class="line">kubectl delete deployment kafka-1</span><br><span class="line"></span><br><span class="line"># 删除rc</span><br><span class="line">kubectl delete rs --all &amp;&amp; kubectl delete rc --all</span><br><span class="line"></span><br><span class="line">## 强制删除Terminating状态的pod</span><br><span class="line">kubectl delete deployment kafka-1 --grace-period&#x3D;0 --force</span><br></pre></td></tr></table></figure>
<h2 id="滚动">滚动</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 升级</span><br><span class="line">kubectl apply -f xxx.yaml --record</span><br><span class="line"></span><br><span class="line"># 回滚</span><br><span class="line">kubectl rollout undo deployment javademo</span><br><span class="line"></span><br><span class="line"># 查看滚动升级记录</span><br><span class="line">kubectl rollout history deployment &#123;名称&#125;</span><br></pre></td></tr></table></figure>
<h2 id="查看日志">查看日志</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看指定镜像的日志</span><br><span class="line">kubectl logs -f kube-dns-699984412-vz1q6 -n kube-system</span><br><span class="line"></span><br><span class="line">kubectl logs --tail&#x3D;10 nginx  </span><br><span class="line"></span><br><span class="line">#指定其中一个查看日志</span><br><span class="line">kubectl logs kube-dns-699984412-n5zkz -c kubedns --namespace&#x3D;kube-system</span><br><span class="line">kubectl logs kube-dns-699984412-vz1q6 -c dnsmasq --namespace&#x3D;kube-system</span><br><span class="line">kubectl logs kube-dns-699984412-mqb14 -c sidecar --namespace&#x3D;kube-system</span><br><span class="line"></span><br><span class="line"># 看日志</span><br><span class="line">journalctl -f</span><br></pre></td></tr></table></figure>
<h2 id="扩展">扩展</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 扩展副本</span><br><span class="line">kubectl scale rc xxxx --replicas&#x3D;3</span><br><span class="line">kubectl scale rc mysql --replicas&#x3D;1</span><br><span class="line">kubectl scale --replicas&#x3D;3 -f foo.yaml</span><br></pre></td></tr></table></figure>
<h2 id="执行">执行</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动</span><br><span class="line">nohup kubectl proxy --address&#x3D;&#39;10.1.70.247&#39; --port&#x3D;8001 --accept-hosts&#x3D;&#39;^*$&#39; &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"># 进入镜像</span><br><span class="line">kubectl exec kube-dns-699984412-vz1q6 -n kube-system -c kubedns ifconfig</span><br><span class="line">kubectl exec kube-dns-699984412-vz1q6 -n kube-system -c kubedns ifconfig &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line"># 执行镜像内命令</span><br><span class="line">kubectl exec kube-dns-4140740281-pfjhr -c etcd --namespace&#x3D;kube-system etcdctl get &#x2F;skydns&#x2F;local&#x2F;cluster&#x2F;default&#x2F;redis-master</span><br></pre></td></tr></table></figure>
<h2 id="无限循环命令">无限循环命令</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while true; do sleep 1; done</span><br></pre></td></tr></table></figure>
<h2 id="资源管理">资源管理</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 暂停资源更新（资源变更不会生效）</span><br><span class="line">kubectl rollout pause deployment xxx</span><br><span class="line"></span><br><span class="line"># 恢复资源更新</span><br><span class="line">kubectl rollout resume deployment xxx</span><br><span class="line"></span><br><span class="line"># 设置内存、cpu限制</span><br><span class="line">kubectl set resources deployment xxx -c&#x3D;xxx --limits&#x3D;cpu&#x3D;200m,memory&#x3D;512Mi --requests&#x3D;cpu&#x3D;1m,memory&#x3D;1Mi</span><br><span class="line"></span><br><span class="line"># 设置storageclass为默认</span><br><span class="line">kubectl patch storageclass &lt;your-class-name&gt; -p &#39;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io&#x2F;is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#39;</span><br></pre></td></tr></table></figure>
<h2 id="其他">其他</h2>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建和删除</span><br><span class="line">kubectl create -f dashboard-controller.yaml</span><br><span class="line">kubectl delete -f dashboard-dashboard.yaml</span><br><span class="line"></span><br><span class="line"># 查看指定pods的环境变量</span><br><span class="line">kubectl exec xxx env</span><br><span class="line"></span><br><span class="line"># 判断dns是否通</span><br><span class="line">kubectl exec busybox -- nslookup kube-dns.kube-system</span><br><span class="line"></span><br><span class="line"># kube-proxy状态</span><br><span class="line">systemctl status kube-proxy -l</span><br><span class="line"></span><br><span class="line"># token的</span><br><span class="line">kubectl get serviceaccount&#x2F;kube-dns --namespace&#x3D;kube-system -o yaml|grep token</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>数据平台规划方案</title>
    <url>/2020/01/07/%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E8%A7%84%E5%88%92%E6%96%B9%E6%A1%88/</url>
    <content><![CDATA[<h2 id="大数据平台现状">大数据平台现状</h2>
<ul>
<li>总可用节点：36个（各节点内存512G，CPU 56核）</li>
<li>总可用内存：9.38T（平均每个节点开放260G）</li>
<li>总可用CPU：1120个（平均每个节点开放31个）</li>
</ul>
<p>包含了文件存储、计算、数据库等集群服务。</p>
<p><strong>现有集群职能包括：</strong></p>
<ul>
<li>离线OLAP数据调度同步（原始数据）</li>
<li>各产品线离线生产任务（雷达、探针、定制产品、回溯测试等业务）</li>
<li>离线OLTP数据更新（HBase数据更新）</li>
<li>线下模型、数据测试（商户定制与联合建模）</li>
<li>模型训练、迭代与更新（评分卡、推荐、联合模型等）</li>
<li>基础数据实时流（底层实时数据处理）</li>
<li>各产品线实时流计算（雷达、探针等上层业务）</li>
<li>日常数据分析任务（大量数据分析、行业分析等需求）</li>
<li>OLTP数据库服务（HBase）</li>
<li>监控报表任务（BI、数据监控等）</li>
<li>数据仓库（数据整合、清洗、调度等）</li>
<li>宝付大数据平台相关任务（Spark、Hive、Impala等）</li>
</ul>
<p>由于当前集群职能繁多，网络带宽、磁盘IO等为集群共享，<strong>会因大型离线任务占用大量网络或磁盘IO峰值，对线上业务会造成短暂延迟。且集群环境较为复杂，有较多对线上业务造成影响的风险。</strong></p>
<h2 id="规划方案">规划方案</h2>
<p>鉴于后续业务发展，大数据平台的使用人数和执行任务将会快速上升，为了避免对线上业务的直接影响，提议部署一个次规模（20个节点内）的生产集群（以下简称在线集群）承接部分职能，减轻现有集群（以下简称离线离线计算集群）的压力，同时规划与隔离不同等级的任务。</p>
<p>在线集群<strong>主要职能为OLTP数据库服务（HBase）</strong>，将业务主库迁移至在线集群可保证业务不受任何大规模计算任务（或者计算量比较集中的情况）所带来的延迟影响。</p>
<p>除此之外，<strong>在线集群将不会进行其他任何程序与任务以保证线上服务的稳定性。</strong></p>
<p>将业务主库分离出去之后，离线计算集群的HBase将作为中间件为流式计算等系统提供服务（同时兼业务备库使用，在需要重启等情况下可以相互切换）。</p>
<p>此时离线计算集群的定位为执行大规模、密集的数据分析、计算、模型等计算资源消耗巨大的任务，在线集群的定位为稳定的业务数据服务。</p>
<p>由于离线计算集群承载了大部分任务，随着业务发展，后续会有更多计算需求产生（如设备指纹与爬虫数据的分析与计算），<strong>需要离线计算集群能够方便、快速的进行横向扩容。</strong></p>
<h2 id="迁移方案">迁移方案</h2>
<p>将使用新采购机器（50台）在新机房部署两个集群（在线+离线计算），<strong>使用的节点个数待定。</strong></p>
<p><strong>在线集群的HBase服务对部署参数、分区配置策略进行优化调整，同时确认跨集群数据同步方案与程序。</strong></p>
<p>集群准备就绪后：</p>
<p>1.同步老机房线上所有业务数据至新机房的在线集群。<br>
2.将老机房集群上数据同步、生产调度任务等相关程序、脚本和配置迁移至新机房的离线计算集群。<br>
3.老机房业务与任务确认迁移完成后，停机下线（分批）调整至新机房并入离线计算集群中，在线集群视情况调整一些新机器扩容。</p>
<p><strong>以上步骤需要整理详细操作过程和确认完成时间点。</strong></p>
<h2 id="离线计算集群资源池">离线计算集群资源池</h2>
<p>当前离线计算集群的资源池划分为宝付的production、development，新颜的production、development，在集群承载了比较多的职能的时候此划分方案已经不满足需求，建议对新颜的资源池进行更详细的划分：</p>
<ul>
<li>stream：流式计算系统使用，占比10%</li>
<li>model：模型训练与标签系统使用，占比30%</li>
<li>experiment：联合建模与商户线下测试使用，占比20%</li>
<li>production：离线生产任务使用，占比20%</li>
<li>development：测试任务与日常分析使用，占比20%</li>
</ul>
<p>以上分配占比仅为预估，需要根据实际情况（包括新采购需求添加之后的集群情况）重新调整。</p>
]]></content>
  </entry>
  <entry>
    <title>12.清理集群</title>
    <url>/2020/01/07/12.%E6%B8%85%E7%90%86%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>tags: clean</p>
<h1>12.清理集群</h1>
<h2 id="清理-Node-节点">清理 Node 节点</h2>
<p>停相关进程：</p>
<pre><code>$ sudo systemctl stop kubelet kube-proxy flanneld docker
$
</code></pre>
<p>清理文件：</p>
<pre><code>$ # umount kubelet 挂载的目录
$ mount | grep '/var/lib/kubelet'| awk '{print $3}'|xargs sudo umount
$ # 删除 kubelet 工作目录
$ sudo rm -rf /var/lib/kubelet
$ # 删除 docker 工作目录
$ sudo rm -rf /var/lib/docker
$ # 删除 flanneld 写入的网络配置文件
$ sudo rm -rf /var/run/flannel/
$ # 删除 docker 的一些运行文件
$ sudo rm -rf /var/run/docker/
$ # 删除 systemd unit 文件
$ sudo rm -rf /etc/systemd/system/{kubelet,docker,flanneld}.service
$ # 删除程序文件
$ sudo rm -rf /opt/k8s/bin/*
$ # 删除证书文件
$ sudo rm -rf /etc/flanneld/cert /etc/kubernetes/cert
$
</code></pre>
<p>清理 kube-proxy 和 docker 创建的 iptables：</p>
<pre><code>$ sudo iptables -F &amp;&amp; sudo iptables -X &amp;&amp; sudo iptables -F -t nat &amp;&amp; sudo iptables -X -t nat
$
</code></pre>
<p>删除 flanneld 和 docker 创建的网桥：</p>
<pre><code>$ ip link del flannel.1
$ ip link del docker0
$
</code></pre>
<h2 id="清理-Master-节点">清理 Master 节点</h2>
<p>停相关进程：</p>
<pre><code>$ sudo systemctl stop kube-apiserver kube-controller-manager kube-scheduler
$
</code></pre>
<p>清理文件：</p>
<pre><code>$ # 删除 kube-apiserver 工作目录
$ sudo rm -rf /var/run/kubernetes
$ # 删除 systemd unit 文件
$ sudo rm -rf /etc/systemd/system/{kube-apiserver,kube-controller-manager,kube-scheduler}.service
$ # 删除程序文件
$ sudo rm -rf /opt/k8s/bin/{kube-apiserver,kube-controller-manager,kube-scheduler}
$ # 删除证书文件
$ sudo rm -rf /etc/flanneld/cert /etc/kubernetes/cert
$
</code></pre>
<h2 id="清理-etcd-集群">清理 etcd 集群</h2>
<p>停相关进程：</p>
<pre><code>$ sudo systemctl stop etcd
$
</code></pre>
<p>清理文件：</p>
<pre><code>$ # 删除 etcd 的工作目录和数据目录
$ sudo rm -rf /var/lib/etcd
$ # 删除 systemd unit 文件
$ sudo rm -rf /etc/systemd/system/etcd.service
$ # 删除程序文件
$ sudo rm -rf /opt/k8s/bin/etcd
$ # 删除 x509 证书文件
$ sudo rm -rf /etc/etcd/cert/*
$</code></pre>
]]></content>
  </entry>
  <entry>
    <title>11.部署 harbor 私有仓库</title>
    <url>/2020/01/07/11.%E9%83%A8%E7%BD%B2%20harbor%20%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/</url>
    <content><![CDATA[<p>tags: registry, harbor</p>
<h1>11.部署 harbor 私有仓库</h1>
<p>本文档介绍使用 docker-compose 部署 harbor 私有仓库的步骤，你也可以使用 docker 官方的 registry 镜像部署私有仓库(<a href="https://www.orchome.com/663" target="_blank" rel="noopener">部署 Docker Registry</a>)。</p>
<h2 id="使用的变量">使用的变量</h2>
<p>本文档用到的变量定义如下：</p>
<pre><code>$ export NODE_IP=10.64.3.7 # 当前部署 harbor 的节点 IP
$
</code></pre>
<h2 id="下载文件">下载文件</h2>
<p>从 docker compose <a href="https://github.com/docker/compose/releases" target="_blank" rel="noopener">发布页面</a>下载最新的 <code>docker-compose</code> 二进制文件</p>
<pre><code>$ wget https://github.com/docker/compose/releases/download/1.21.2/docker-compose-Linux-x86_64
$ mv ~/docker-compose-Linux-x86_64 /opt/k8s/bin/docker-compose
$ chmod a+x  /opt/k8s/bin/docker-compose
$ export PATH=/opt/k8s/bin:$PATH
$
</code></pre>
<p>从 harbor <a href="https://github.com/vmware/harbor/releases" target="_blank" rel="noopener">发布页面</a>下载最新的 harbor 离线安装包</p>
<pre><code>$ wget  --continue https://storage.googleapis.com/harbor-releases/release-1.5.0/harbor-offline-installer-v1.5.1.tgz
$ tar -xzvf harbor-offline-installer-v1.5.1.tgz
$
</code></pre>
<h2 id="导入-docker-images">导入 docker images</h2>
<p>导入离线安装包中 harbor 相关的 docker images：</p>
<pre><code>$ cd harbor
$ docker load -i harbor.v1.5.1.tar.gz
$
</code></pre>
<h2 id="创建-harbor-nginx-服务器使用的-x509-证书">创建 harbor nginx 服务器使用的 x509 证书</h2>
<p>创建 harbor 证书签名请求：</p>
<pre><code>$ cat &gt; harbor-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;harbor&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;${NODE_IP}&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>hosts 字段指定授权使用该证书的当前部署节点 IP，如果后续使用域名访问 harbor 则还需要添加域名；</li>
</ul>
<p>生成 harbor 证书和私钥：</p>
<pre><code>$ cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes harbor-csr.json | cfssljson -bare harbor

$ ls harbor*
harbor.csr  harbor-csr.json  harbor-key.pem harbor.pem

$ sudo mkdir -p /etc/harbor/ssl
$ sudo mv harbor*.pem /etc/harbor/ssl
$ rm harbor.csr  harbor-csr.json
</code></pre>
<h2 id="修改-harbor-cfg-文件">修改 harbor.cfg 文件</h2>
<pre><code>$ cp harbor.cfg{,.bak}
$ vim harbor.cfg
$ diff harbor.cfg{,.bak}
7c7
&lt; hostname = 172.27.129.81
---
&gt; hostname = reg.mydomain.com
11c11
&lt; ui_url_protocol = https
---
&gt; ui_url_protocol = http
23,24c23,24
&lt; ssl_cert =  /etc/harbor/ssl/harbor.pem
&lt; ssl_cert_key = /etc/harbor/ssl/harbor-key.pem
---
&gt; ssl_cert = /data/cert/server.crt
&gt; ssl_cert_key = /data/cert/server.key

$ cp prepare{,.bak}
$ vim prepare
$ diff prepare{,.bak}
453a454
&gt;         print(&quot;%s %w&quot;, args, kw)
490c491
&lt;     empty_subj = &quot;/&quot;
---
&gt;     empty_subj = &quot;/C=/ST=/L=/O=/CN=/&quot;
</code></pre>
<ul>
<li>
<p>需要修改 prepare 脚本的 empyt_subj 参数，否则后续 install 时出错退出：</p>
<p>Fail to generate key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crt</p>
</li>
</ul>
<p>参考：<a href="https://github.com/vmware/harbor/issues/2920" target="_blank" rel="noopener">https://github.com/vmware/harbor/issues/2920</a></p>
<h2 id="加载和启动-harbor-镜像">加载和启动 harbor 镜像</h2>
<pre><code>$ sudo mkdir /data
$ sudo chmod 777 /var/run/docker.sock /data
$ sudo apt-get install python
$ ./install.sh

[Step 0]: checking installation environment ...

Note: docker version: 18.03.0

Note: docker-compose version: 1.21.2

[Step 1]: loading Harbor images ...
Loaded image: vmware/clair-photon:v2.0.1-v1.5.1
Loaded image: vmware/postgresql-photon:v1.5.1
Loaded image: vmware/harbor-adminserver:v1.5.1
Loaded image: vmware/registry-photon:v2.6.2-v1.5.1
Loaded image: vmware/photon:1.0
Loaded image: vmware/harbor-migrator:v1.5.1
Loaded image: vmware/harbor-ui:v1.5.1
Loaded image: vmware/redis-photon:v1.5.1
Loaded image: vmware/nginx-photon:v1.5.1
Loaded image: vmware/mariadb-photon:v1.5.1
Loaded image: vmware/notary-signer-photon:v0.5.1-v1.5.1
Loaded image: vmware/harbor-log:v1.5.1
Loaded image: vmware/harbor-db:v1.5.1
Loaded image: vmware/harbor-jobservice:v1.5.1
Loaded image: vmware/notary-server-photon:v0.5.1-v1.5.1


[Step 2]: preparing environment ...
loaded secret from file: /data/secretkey
Generated configuration file: ./common/config/nginx/nginx.conf
Generated configuration file: ./common/config/adminserver/env
Generated configuration file: ./common/config/ui/env
Generated configuration file: ./common/config/registry/config.yml
Generated configuration file: ./common/config/db/env
Generated configuration file: ./common/config/jobservice/env
Generated configuration file: ./common/config/jobservice/config.yml
Generated configuration file: ./common/config/log/logrotate.conf
Generated configuration file: ./common/config/jobservice/config.yml
Generated configuration file: ./common/config/ui/app.conf
Generated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crt
The configuration files are ready, please use docker-compose to start the service.


[Step 3]: checking existing instance of Harbor ...


[Step 4]: starting Harbor ...
Creating network &quot;harbor_harbor&quot; with the default driver
Creating harbor-log ... done
Creating redis              ... done
Creating harbor-adminserver ... done
Creating harbor-db          ... done
Creating registry           ... done
Creating harbor-ui          ... done
Creating harbor-jobservice  ... done
Creating nginx              ... done

✔ ----Harbor has been installed and started successfully.----

Now you should be able to visit the admin portal at https://172.27.129.81.
For more details, please visit https://github.com/vmware/harbor .
</code></pre>
<h2 id="访问管理界面">访问管理界面</h2>
<p>确认所有组件都工作正常：</p>
<pre><code>$ docker-compose  ps
       Name                     Command                  State                                    Ports
-------------------------------------------------------------------------------------------------------------------------------------
harbor-adminserver   /harbor/start.sh                 Up (healthy)
harbor-db            /usr/local/bin/docker-entr ...   Up (healthy)   3306/tcp
harbor-jobservice    /harbor/start.sh                 Up
harbor-log           /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514-&gt;10514/tcp
harbor-ui            /harbor/start.sh                 Up (healthy)
nginx                nginx -g daemon off;             Up (healthy)   0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcp
redis                docker-entrypoint.sh redis ...   Up             6379/tcp
registry             /entrypoint.sh serve /etc/ ...   Up (healthy)   5000/tcp
</code></pre>
<p>浏览器访问 <code>https://${NODE_IP}</code>，示例的是 <code>https://172.27.129.81</code>；</p>
<p>由于是在 virtualbox 虚机 kube-node2 中运行，所以需要做下端口转发，Vagrant 文件中已经指定 host 端口为 4443，也可以在 virtualbox 的 GUI 中直接添加端口转发：</p>
<p><img src="https://img.orchome.com/group1/M00/00/03/dr5oXFutksSACh_VAAZX5IF_yio990.png" alt="screenshot"></p>
<p>浏览器访问 <code>https://127.0.0.1:443</code>，用账号 <code>admin</code> 和 harbor.cfg 配置文件中的默认密码 <code>Harbor12345</code> 登陆系统。</p>
<p><img src="https://img.orchome.com/group1/M00/00/03/dr5oXFutkuWAGD1XAAOBIWs7iWE167.png" alt="screenshot"></p>
<h2 id="harbor-运行时产生的文件、目录">harbor 运行时产生的文件、目录</h2>
<p>harbor 将日志打印到 /var/log/harbor 的相关目录下，使用 docker logs XXX 或 docker-compose logs XXX 将看不到容器的日志。</p>
<pre><code>$ # 日志目录
$ ls /var/log/harbor
adminserver.log  jobservice.log  mysql.log  proxy.log  registry.log  ui.log
$ # 数据目录，包括数据库、镜像仓库
$ ls /data/
ca_download  config  database  job_logs registry  secretkey
</code></pre>
<h2 id="docker-客户端登陆">docker 客户端登陆</h2>
<p>将签署 harbor 证书的 CA 证书拷贝到 <code>/etc/docker/certs.d/172.27.129.81</code> 目录下</p>
<pre><code>$ sudo mkdir -p /etc/docker/certs.d/172.27.129.81
$ sudo cp /etc/kubernetes/cert/ca.pem /etc/docker/certs.d/172.27.129.81/ca.crt
$
</code></pre>
<p>登陆 harbor</p>
<pre><code>$ docker login 172.27.129.81
Username: admin
Password:
</code></pre>
<p>认证信息自动保存到 <code>~/.docker/config.json</code> 文件。</p>
<h2 id="其它操作">其它操作</h2>
<p>下列操作的工作目录均为 解压离线安装文件后 生成的 harbor 目录。</p>
<pre><code>$ # 停止 harbor
$ docker-compose down -v
$ # 修改配置
$ vim harbor.cfg
$ # 更修改的配置更新到 docker-compose.yml 文件
$ ./prepare
Clearing the configuration file: ./common/config/ui/app.conf
Clearing the configuration file: ./common/config/ui/env
Clearing the configuration file: ./common/config/ui/private_key.pem
Clearing the configuration file: ./common/config/db/env
Clearing the configuration file: ./common/config/registry/root.crt
Clearing the configuration file: ./common/config/registry/config.yml
Clearing the configuration file: ./common/config/jobservice/app.conf
Clearing the configuration file: ./common/config/jobservice/env
Clearing the configuration file: ./common/config/nginx/cert/admin.pem
Clearing the configuration file: ./common/config/nginx/cert/admin-key.pem
Clearing the configuration file: ./common/config/nginx/nginx.conf
Clearing the configuration file: ./common/config/adminserver/env
loaded secret from file: /data/secretkey
Generated configuration file: ./common/config/nginx/nginx.conf
Generated configuration file: ./common/config/adminserver/env
Generated configuration file: ./common/config/ui/env
Generated configuration file: ./common/config/registry/config.yml
Generated configuration file: ./common/config/db/env
Generated configuration file: ./common/config/jobservice/env
Generated configuration file: ./common/config/jobservice/app.conf
Generated configuration file: ./common/config/ui/app.conf
Generated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crt
The configuration files are ready, please use docker-compose to start the service.
$ sudo chmod -R 666 common ## 防止容器进程没有权限读取生成的配置
$ # 启动 harbor
$ docker-compose up -d</code></pre>
]]></content>
  </entry>
  <entry>
    <title>10.部署私有 docker registry</title>
    <url>/2020/01/07/10.%E9%83%A8%E7%BD%B2%E7%A7%81%E6%9C%89%20docker%20registry/</url>
    <content><![CDATA[<p>tags: registry, ceph</p>
<h1>10.部署私有 docker registry</h1>
<p>注意：本文档介绍使用 docker 官方的 registry v2 镜像部署私有仓库的步骤，你也可以部署 Harbor 私有仓库（<a href="https://www.orchome.com/664" target="_blank" rel="noopener">部署 Harbor 私有仓库</a>）。</p>
<p>本文档讲解部署一个 TLS 加密、HTTP Basic 认证、用 ceph rgw 做后端存储的私有 docker registry 步骤，如果使用其它类型的后端存储，则可以从 “创建 docker registry” 节开始；</p>
<p>示例两台机器 IP 如下：</p>
<ul>
<li>ceph rgw: 172.27.132.66</li>
<li>docker registry: 172.27.132.67</li>
</ul>
<h2 id="部署-ceph-RGW-节点">部署 ceph RGW 节点</h2>
<pre><code>$ ceph-deploy rgw create 172.27.132.66 # rgw 默认监听7480端口
$
</code></pre>
<h2 id="创建测试账号-demo">创建测试账号 demo</h2>
<pre><code>$ radosgw-admin user create --uid=demo --display-name=&quot;ceph rgw demo user&quot;
$
</code></pre>
<h2 id="创建-demo-账号的子账号-swift">创建 demo 账号的子账号 swift</h2>
<p>当前 registry 只支持使用 swift 协议访问 ceph rgw 存储，暂时不支持 s3 协议；</p>
<pre><code>$ radosgw-admin subuser create --uid demo --subuser=demo:swift --access=full --secret=secretkey --key-type=swift
$
</code></pre>
<h2 id="创建-demo-swift-子账号的-sercret-key">创建 demo:swift 子账号的 sercret key</h2>
<pre><code>$ radosgw-admin key create --subuser=demo:swift --key-type=swift --gen-secret
{
    &quot;user_id&quot;: &quot;demo&quot;,
    &quot;display_name&quot;: &quot;ceph rgw demo user&quot;,
    &quot;email&quot;: &quot;&quot;,
    &quot;suspended&quot;: 0,
    &quot;max_buckets&quot;: 1000,
    &quot;auid&quot;: 0,
    &quot;subusers&quot;: [
        {
            &quot;id&quot;: &quot;demo:swift&quot;,
            &quot;permissions&quot;: &quot;full-control&quot;
        }
    ],
    &quot;keys&quot;: [
        {
            &quot;user&quot;: &quot;demo&quot;,
            &quot;access_key&quot;: &quot;5Y1B1SIJ2YHKEHO5U36B&quot;,
            &quot;secret_key&quot;: &quot;nrIvtPqUj7pUlccLYPuR3ntVzIa50DToIpe7xFjT&quot;
        }
    ],
    &quot;swift_keys&quot;: [
        {
            &quot;user&quot;: &quot;demo:swift&quot;,
            &quot;secret_key&quot;: &quot;ttQcU1O17DFQ4I9xzKqwgUe7WIYYX99zhcIfU9vb&quot;
        }
    ],
    &quot;caps&quot;: [],
    &quot;op_mask&quot;: &quot;read, write, delete&quot;,
    &quot;default_placement&quot;: &quot;&quot;,
    &quot;placement_tags&quot;: [],
    &quot;bucket_quota&quot;: {
        &quot;enabled&quot;: false,
        &quot;max_size_kb&quot;: -1,
        &quot;max_objects&quot;: -1
    },
    &quot;user_quota&quot;: {
        &quot;enabled&quot;: false,
        &quot;max_size_kb&quot;: -1,
        &quot;max_objects&quot;: -1
    },
        &quot;temp_url_keys&quot;: []
}
</code></pre>
<ul>
<li><code>ttQcU1O17DFQ4I9xzKqwgUe7WIYYX99zhcIfU9vb</code> 为子账号 demo:swift 的 secret key；</li>
</ul>
<h2 id="创建-docker-registry">创建 docker registry</h2>
<p>创建 registry 使用的 x509 证书</p>
<pre><code>$ mkdir -p registry/{auth,certs}
$ cat &gt; registry-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;registry&quot;,
  &quot;hosts&quot;: [
      &quot;127.0.0.1&quot;,
      &quot;172.27.132.67&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
$ cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
    -ca-key=/etc/kubernetes/cert/ca-key.pem \
    -config=/etc/kubernetes/cert/ca-config.json \
    -profile=kubernetes registry-csr.json | cfssljson -bare registry
$ cp registry.pem registry-key.pem registry/certs
$
</code></pre>
<ul>
<li>这里复用以前创建的 CA 证书和秘钥文件；</li>
<li>hosts 字段指定 registry 的 NodeIP；</li>
</ul>
<p>创建 HTTP Baisc 认证文件</p>
<pre><code>$ docker run --entrypoint htpasswd registry:2 -Bbn foo foo123  &gt; registry/auth/htpasswd
$ cat  registry/auth/htpasswd
foo:$2y$05$iZaM45Jxlcg0DJKXZMggLOibAsHLGybyU.CgU9AHqWcVDyBjiScN.
</code></pre>
<p>配置 registry 参数</p>
<pre><code>export RGW_AUTH_URL=&quot;https://172.27.132.66:7480/auth/v1&quot;
export RGW_USER=&quot;demo:swift&quot;
export RGW_SECRET_KEY=&quot;ttQcU1O17DFQ4I9xzKqwgUe7WIYYX99zhcIfU9vb&quot;
cat &gt; config.yml &lt;&lt; EOF
# https://docs.docker.com/registry/configuration/#list-of-configuration-options
version: 0.1
log:
  level: info
  fromatter: text
  fields:
    service: registry

storage:
  cache:
    blobdescriptor: inmemory
  delete:
    enabled: true
  swift:
    authurl: ${RGW_AUTH_URL}
    username: ${RGW_USER}
    password: ${RGW_SECRET_KEY}
    container: registry

auth:
  htpasswd:
    realm: basic-realm
    path: /auth/htpasswd

https:
  addr: 0.0.0.0:8000
  headers:
    X-Content-Type-Options: [nosniff]
  tls:
    certificate: /certs/registry.pem
    key: /certs/registry-key.pem

health:
  storagedriver:
    enabled: true
    interval: 10s
    threshold: 3
EOF
[k8s@kube-node1 cert]$ cp config.yml registry
[k8s@kube-node1 cert]$ scp -r registry 172.27.132.67:/opt/k8s
</code></pre>
<ul>
<li>storage.swift 指定后端使用 swfit 接口协议的存储，这里配置的是 ceph rgw 存储参数；</li>
<li>auth.htpasswd 指定了 HTTP Basic 认证的 token 文件路径；</li>
<li>http.tls 指定了 registry http 服务器的证书和秘钥文件路径；</li>
</ul>
<p>创建 docker registry</p>
<pre><code>ssh k8s@172.27.132.67
$ docker run -d -p 8000:8000 --privileged \
    -v /opt/k8s/registry/auth/:/auth \
    -v /opt/k8s/registry/certs:/certs \
    -v /opt/k8s/registry/config.yml:/etc/docker/registry/config.yml \
    --name registry registry:2
</code></pre>
<ul>
<li>执行该 docker run 命令的机器 IP 为 172.27.132.67；</li>
</ul>
<h2 id="向-registry-push-image">向 registry push image</h2>
<p>将签署 registry 证书的 CA 证书拷贝到 <code>/etc/docker/certs.d/172.27.132.67:8000</code> 目录下</p>
<pre><code>[k8s@kube-node1 cert]$ sudo mkdir -p /etc/docker/certs.d/172.27.132.67:8000
[k8s@kube-node1 cert]$ sudo cp /etc/kubernetes/cert/ca.pem /etc/docker/certs.d/172.27.132.67:8000/ca.crt
</code></pre>
<p>登陆私有 registry</p>
<pre><code>$ docker login 172.27.132.67:8000
Username: foo
Password:
Login Succeeded
</code></pre>
<p>登陆信息被写入 <code>~/.docker/config.json</code> 文件</p>
<pre><code>$ cat ~/.docker/config.json
{
        &quot;auths&quot;: {
                &quot;172.27.132.67:8000&quot;: {
                        &quot;auth&quot;: &quot;Zm9vOmZvbzEyMw==&quot;
                }
        }
}
</code></pre>
<p>将本地的 image 打上私有 registry 的 tag</p>
<pre><code>$ docker tag prom/node-exporter:v0.16.0 172.27.132.67:8000/prom/node-exporter:v0.16.0
$ docker images |grep pause
prom/node-exporter:v0.16.0                            latest              f9d5de079539        2 years ago         239.8 kB
172.27.132.67:8000/prom/node-exporter:v0.16.0                        latest              f9d5de079539        2 years ago         239.8 kB
</code></pre>
<p>将 image push 到私有 registry</p>
<pre><code>$ docker push 172.27.132.67:8000/prom/node-exporter:v0.16.0
The push refers to a repository [172.27.132.67:8000/prom/node-exporter:v0.16.0]
5f70bf18a086: Pushed
e16a89738269: Pushed
latest: digest: sha256:9a6b437e896acad3f5a2a8084625fdd4177b2e7124ee943af642259f2f283359 size: 916
</code></pre>
<p>查看 ceph 上是否已经有 push 的 pause 容器文件</p>
<pre><code>[k8s@kube-node1 ~]$ rados lspools
rbd
cephfs_data
cephfs_metadata
.rgw.root
k8s
default.rgw.control
default.rgw.meta
default.rgw.log
default.rgw.buckets.index
default.rgw.buckets.data

[k8s@kube-node1 ~]$  rados --pool default.rgw.buckets.data ls|grep node-exporter
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_layers/sha256/cdb7590af5f064887f3d6008d46be65e929c74250d747813d85199e04fc70463/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_manifests/revisions/sha256/55302581333c43d540db0e144cf9e7735423117a733cdec27716d87254221086/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_manifests/tags/v0.16.0/current/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_manifests/tags/v0.16.0/index/sha256/55302581333c43d540db0e144cf9e7735423117a733cdec27716d87254221086/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_layers/sha256/224a21997e8ca8514d42eb2ed98b19a7ee2537bce0b3a26b8dff510ab637f15c/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_layers/sha256/528dda9cf23d0fad80347749d6d06229b9a19903e49b7177d5f4f58736538d4e/link
1f3f02c4-fe58-4626-992b-c6c0fe4c8acf.34107.1_files/docker/registry/v2/repositories/prom/node-exporter/_layers/sha256/188af75e2de0203eac7c6e982feff45f9c340eaac4c7a0f59129712524fa2984/link
</code></pre>
<h2 id="私有-registry-的运维操作">私有 registry 的运维操作</h2>
<h3 id="查询私有镜像中的-images">查询私有镜像中的 images</h3>
<pre><code>[k8s@kube-node1 ~]$ curl  --user foo:foo123 --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/_catalog
{&quot;repositories&quot;:[&quot;prom/node-exporter&quot;]}
</code></pre>
<h3 id="查询某个镜像的-tags-列表">查询某个镜像的 tags 列表</h3>
<pre><code>[k8s@kube-node1 ~]$  curl  --user foo:foo123 --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/prom/node-exporter/tags/list
{&quot;name&quot;:&quot;prom/node-exporter&quot;,&quot;tags&quot;:[&quot;v0.16.0&quot;]}
</code></pre>
<h3 id="获取-image-或-layer-的-digest">获取 image 或 layer 的 digest</h3>
<p>向 <code>v2/&lt;repoName&gt;/manifests/&lt;tagName&gt;</code> 发 GET 请求，从响应的头部 <code>Docker-Content-Digest</code> 获取 image digest，从响应的 body 的 <code>fsLayers.blobSum</code> 中获取 layDigests;</p>
<p>注意，必须包含请求头：<code>Accept: application/vnd.docker.distribution.manifest.v2+json</code>：</p>
<pre><code>[k8s@kube-node1 ~]$ curl -v -H &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; --user foo:foo123 --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/prom/node-exporter/manifests/v0.16.0
* About to connect() to 172.27.132.67 port 8000 (#0)
*   Trying 172.27.132.67...
* Connected to 172.27.132.67 (172.27.132.67) port 8000 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
*   CAfile: /etc/docker/certs.d/172.27.132.67:8000/ca.crt
  CApath: none
* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
* Server certificate:
*       subject: CN=registry,OU=4Paradigm,O=k8s,L=BeiJing,ST=BeiJing,C=CN
*       start date: Jul 05 12:52:00 2018 GMT
*       expire date: Jul 02 12:52:00 2028 GMT
*       common name: registry
*       issuer: CN=kubernetes,OU=4Paradigm,O=k8s,L=BeiJing,ST=BeiJing,C=CN
* Server auth using Basic with user 'foo'
&gt; GET /v2/prom/node-exporter/manifests/v0.16.0 HTTP/1.1
&gt; Authorization: Basic Zm9vOmZvbzEyMw==
&gt; User-Agent: curl/7.29.0
&gt; Host: 172.27.132.67:8000
&gt; Accept: application/vnd.docker.distribution.manifest.v2+json
&gt;
&lt; HTTP/1.1 200 OK
&lt; Content-Length: 949
&lt; Content-Type: application/vnd.docker.distribution.manifest.v2+json
&lt; Docker-Content-Digest: sha256:55302581333c43d540db0e144cf9e7735423117a733cdec27716d87254221086
&lt; Docker-Distribution-Api-Version: registry/2.0
&lt; Etag: &quot;sha256:55302581333c43d540db0e144cf9e7735423117a733cdec27716d87254221086&quot;
&lt; X-Content-Type-Options: nosniff
&lt; Date: Fri, 06 Jul 2018 06:18:41 GMT
&lt;
{
   &quot;schemaVersion&quot;: 2,
   &quot;mediaType&quot;: &quot;application/vnd.docker.distribution.manifest.v2+json&quot;,
   &quot;config&quot;: {
      &quot;mediaType&quot;: &quot;application/vnd.docker.container.image.v1+json&quot;,
      &quot;size&quot;: 3511,
      &quot;digest&quot;: &quot;sha256:188af75e2de0203eac7c6e982feff45f9c340eaac4c7a0f59129712524fa2984&quot;
   },
   &quot;layers&quot;: [
      {
         &quot;mediaType&quot;: &quot;application/vnd.docker.image.rootfs.diff.tar.gzip&quot;,
         &quot;size&quot;: 2392417,
         &quot;digest&quot;: &quot;sha256:224a21997e8ca8514d42eb2ed98b19a7ee2537bce0b3a26b8dff510ab637f15c&quot;
      },
      {
         &quot;mediaType&quot;: &quot;application/vnd.docker.image.rootfs.diff.tar.gzip&quot;,
         &quot;size&quot;: 560703,
         &quot;digest&quot;: &quot;sha256:cdb7590af5f064887f3d6008d46be65e929c74250d747813d85199e04fc70463&quot;
      },
      {
         &quot;mediaType&quot;: &quot;application/vnd.docker.image.rootfs.diff.tar.gzip&quot;,
         &quot;size&quot;: 5332460,
         &quot;digest&quot;: &quot;sha256:528dda9cf23d0fad80347749d6d06229b9a19903e49b7177d5f4f58736538d4e&quot;
      }
   ]
</code></pre>
<h3 id="删除-image">删除 image</h3>
<p>向 <code>/v2/&lt;name&gt;/manifests/&lt;reference&gt;</code> 发送 DELETE 请求，reference 为上一步返回的 Docker-Content-Digest 字段内容：</p>
<pre><code>$ curl -X DELETE  --user foo:foo123 --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/prom/node-exporter/manifests/sha256:68effe31a4ae8312e47f54bec52d1fc925908009ce7e6f734e1b54a4169081c5
$
</code></pre>
<h3 id="删除-layer">删除 layer</h3>
<p>向 <code>/v2/&lt;name&gt;/blobs/&lt;digest&gt;</code>发送 DELETE 请求，其中 digest 是上一步返回的 <code>fsLayers.blobSum</code> 字段内容：</p>
<pre><code>$ curl -X DELETE  --user foo:foo123 --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/prom/node-exporter/blobs/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4
$ curl -X DELETE  --cacert /etc/docker/certs.d/172.27.132.67\:8000/ca.crt https://172.27.132.67:8000/v2/prom/node-exporter/blobs/sha256:04176c8b224aa0eb9942af765f66dae866f436e75acef028fe44b8a98e045515
$
</code></pre>
<h2 id="常见问题">常见问题</h2>
<h3 id="login-失败-416">login 失败 416</h3>
<p>执行 <a href="https://docs.ceph.com/docs/master/install/install-ceph-gateway/" target="_blank" rel="noopener">https://docs.ceph.com/docs/master/install/install-ceph-gateway/</a> 里面的 s3 <a href="http://test.py" target="_blank" rel="noopener">test.py</a> 程序失败：</p>
<p>[k8s@kube-node1 cert]$ python <a href="http://s3test.py" target="_blank" rel="noopener">s3test.py</a><br>
Traceback (most recent call last):<br>
File “<a href="http://s3test.py" target="_blank" rel="noopener">s3test.py</a>”, line 12, in<br>
bucket = conn.create_bucket(‘my-new-bucket’)<br>
File “/usr/lib/python2.7/site-packages/boto/s3/connection.py”, line 625, in create_bucket<br>
response.status, response.reason, body)<br>
boto.exception.S3ResponseError: S3ResponseError: 416 Requested Range Not Satisfiable</p>
<p>解决版办法：</p>
<ol>
<li>在管理节点上修改 ceph.conf</li>
<li>ceph-deploy config push kube-node1 kube-node2 kube-node3</li>
<li>systemctl restart ‘ceph-mds@kube-node3.service’<br>
systemctl restart ceph-osd@0<br>
systemctl restart ‘ceph-mon@kube-node1.service’<br>
systemctl restart ‘ceph-mgr@kube-node1.service’</li>
</ol>
<p>For anyone who is hitting this issue<br>
set default pg_num and pgp_num to lower value(8 for example), or set mon_max_pg_per_osd to a high value in ceph.conf<br>
radosgw-admin doesn’ throw proper error when internal pool creation fails, hence the upper level error which is very confusing.</p>
<p><a href="https://tracker.ceph.com/issues/21497" target="_blank" rel="noopener">https://tracker.ceph.com/issues/21497</a></p>
<h3 id="login-失败-503">login 失败 503</h3>
<p>[root@kube-node1 ~]# docker login 172.27.132.67:8000<br>
Username: foo<br>
Password:<br>
Error response from daemon: login attempt to <a href="https://172.27.132.67:8000/v2/" target="_blank" rel="noopener">https://172.27.132.67:8000/v2/</a> failed with status: 503 Service Unavailable</p>
<p>原因： docker run 缺少 --privileged 参数；</p>
]]></content>
  </entry>
  <entry>
    <title>09-5.部署 EFK 插件</title>
    <url>/2020/01/07/09-5.%E9%83%A8%E7%BD%B2%20EFK%20%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<p>tags: addons, EFK, fluentd, elasticsearch, kibana</p>
<h1>09-5.部署 EFK 插件</h1>
<p>EFK 对应的目录：<code>kubernetes/cluster/addons/fluentd-elasticsearch</code></p>
<pre><code>$ cd /opt/k8s/kubernetes/cluster/addons/fluentd-elasticsearch
$ ls *.yaml
es-service.yaml  es-statefulset.yaml  fluentd-es-configmap.yaml  fluentd-es-ds.yaml  kibana-deployment.yaml  kibana-service.yaml
</code></pre>
<h2 id="修改定义文件">修改定义文件</h2>
<pre><code>$ cp es-statefulset.yaml{,.orig}
$ diff es-statefulset.yaml{,.orig}
76c76
&lt;       - image: longtds/elasticsearch:v5.6.4
---
&gt;       - image: k8s.gcr.io/elasticsearch:v5.6.4

$ cp fluentd-es-ds.yaml{,.orig}
$ diff fluentd-es-ds.yaml{,.orig}
79c79
&lt;         image: netonline/fluentd-elasticsearch:v2.0.4
---
&gt;         image: k8s.gcr.io/fluentd-elasticsearch:v2.0.4
</code></pre>
<h2 id="给-Node-设置标签">给 Node 设置标签</h2>
<p>DaemonSet <code>fluentd-es</code> 只会调度到设置了标签 <code>beta.kubernetes.io/fluentd-ds-ready=true</code> 的 Node，需要在期望运行 fluentd 的 Node 上设置该标签；</p>
<pre><code>$ kubectl get nodes
NAME         STATUS    ROLES     AGE       VERSION
kube-node1   Ready     &lt;none&gt;    3d        v1.10.4
kube-node2   Ready     &lt;none&gt;    3d        v1.10.4
kube-node3   Ready     &lt;none&gt;    3d        v1.10.4

$ kubectl label nodes kube-node3 beta.kubernetes.io/fluentd-ds-ready=true
node &quot;kube-node3&quot; labeled
</code></pre>
<h2 id="执行定义文件">执行定义文件</h2>
<pre><code>$ pwd
/opt/k8s/kubernetes/cluster/addons/fluentd-elasticsearch
$ ls *.yaml
es-service.yaml  es-statefulset.yaml  fluentd-es-configmap.yaml  fluentd-es-ds.yaml  kibana-deployment.yaml  kibana-service.yaml
$ kubectl create -f .
</code></pre>
<h2 id="检查执行结果">检查执行结果</h2>
<pre><code>$ kubectl get pods -n kube-system -o wide|grep -E 'elasticsearch|fluentd|kibana'
elasticsearch-logging-0                  1/1       Running   0          5m        172.30.81.7   kube-node1
elasticsearch-logging-1                  1/1       Running   0          2m        172.30.39.8   kube-node3
fluentd-es-v2.0.4-hntfp                  1/1       Running   0          5m        172.30.39.6   kube-node3
kibana-logging-7445dc9757-pvpcv          1/1       Running   0          5m        172.30.39.7   kube-node3

$ kubectl get service  -n kube-system|grep -E 'elasticsearch|kibana'
elasticsearch-logging   ClusterIP   10.254.50.198    &lt;none&gt;        9200/TCP        5m
kibana-logging          ClusterIP   10.254.255.190   &lt;none&gt;        5601/TCP        5m
</code></pre>
<p>kibana Pod 第一次启动时会用**较长时间(0-20分钟)**来优化和 Cache 状态页面，可以 tailf 该 Pod 的日志观察进度：</p>
<pre><code>[k8s@kube-node1 fluentd-elasticsearch]$ kubectl logs kibana-logging-7445dc9757-pvpcv -n kube-system -f
{&quot;type&quot;:&quot;log&quot;,&quot;@timestamp&quot;:&quot;2018-06-16T11:36:18Z&quot;,&quot;tags&quot;:[&quot;info&quot;,&quot;optimize&quot;],&quot;pid&quot;:1,&quot;message&quot;:&quot;Optimizing and caching bundles for graph, ml, kibana, stateSessionStorageRedirect, timelion and status_page. This may take a few minutes&quot;}
{&quot;type&quot;:&quot;log&quot;,&quot;@timestamp&quot;:&quot;2018-06-16T11:40:03Z&quot;,&quot;tags&quot;:[&quot;info&quot;,&quot;optimize&quot;],&quot;pid&quot;:1,&quot;message&quot;:&quot;Optimization of bundles for graph, ml, kibana, stateSessionStorageRedirect, timelion and status_page complete in 224.57 seconds&quot;}
</code></pre>
<p>注意：只有当的 Kibana pod 启动完成后，才能查看 kibana dashboard，否则会提示 refuse。</p>
<h2 id="访问-kibana">访问 kibana</h2>
<ol>
<li>
<p>通过 kube-apiserver 访问：</p>
<pre><code> $ kubectl cluster-info|grep -E 'Elasticsearch|Kibana'
 Elasticsearch is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy
 Kibana is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy
</code></pre>
<p>浏览器访问 URL： <code>https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy</code><br>
对于 virtuabox 做了端口映射： <code>https://127.0.0.1:8080/api/v1/namespaces/kube-system/services/kibana-logging/proxy</code></p>
</li>
<li>
<p>通过 kubectl proxy 访问：</p>
<p>创建代理</p>
<pre><code> $ kubectl proxy --address='172.27.129.105' --port=8086 --accept-hosts='^*$'
 Starting to serve on 172.27.129.80:8086
</code></pre>
<p>浏览器访问 URL：<code>https://172.27.129.105:8086/api/v1/namespaces/kube-system/services/kibana-logging/proxy</code><br>
对于 virtuabox 做了端口映射： <code>https://127.0.0.1:8086/api/v1/namespaces/kube-system/services/kibana-logging/proxy</code></p>
</li>
</ol>
<p>在 Settings -&gt; Indices 页面创建一个 index（相当于 mysql 中的一个 database），选中 <code>Index contains time-based events</code>，使用默认的 <code>logstash-*</code> pattern，点击 <code>Create</code> ;</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufGsGAXPDYAATDIwOmfzI253.png" alt="screenshot"></p>
<p>创建 Index 后，稍等几分钟就可以在 <code>Discover</code> 菜单下看到 ElasticSearch logging 中汇聚的日志；</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufGuGAfailAAn_vbTpJB0015.png" alt="screenshot"></p>
]]></content>
  </entry>
  <entry>
    <title>09-4.部署 metrics-server 插件</title>
    <url>/2020/01/07/09-4.%E9%83%A8%E7%BD%B2%20metrics-server%20%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<p>tags: addons, metrics, metrics-server</p>
<h1>09-4.部署 metrics-server 插件</h1>
<h2 id="创建-metrics-server-使用的证书">创建 metrics-server 使用的证书</h2>
<p>创建 metrics-server 证书签名请求:</p>
<pre><code>cat &gt; metrics-server-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;aggregator&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>注意： CN 名称为 aggregator，需要与 kube-apiserver 的 --requestheader-allowed-names 参数配置一致；</li>
</ul>
<p>生成 metrics-server 证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem  \
  -config=/etc/kubernetes/cert/ca-config.json  \
  -profile=kubernetes metrics-server-csr.json | cfssljson -bare metrics-server
</code></pre>
<p>将生成的证书和私钥文件拷贝到 kube-apiserver 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp metrics-server*.pem k8s@${node_ip}:/etc/kubernetes/cert/
  done
</code></pre>
<h2 id="修改-kubernetes-控制平面组件的配置以支持-metrics-server">修改 kubernetes 控制平面组件的配置以支持 metrics-server</h2>
<h3 id="kube-apiserver">kube-apiserver</h3>
<p>添加如下配置参数：</p>
<pre><code>--requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem
--requestheader-allowed-names=&quot;&quot;
--requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot;
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
--proxy-client-cert-file=/etc/kubernetes/cert/metrics-server.pem
--proxy-client-key-file=/etc/kubernetes/cert/metrics-server-key.pem
--runtime-config=api/all=true
</code></pre>
<ul>
<li><code>--requestheader-XXX</code>、<code>--proxy-client-XXX</code> 是 kube-apiserver 的 aggregator layer 相关的配置参数，metrics-server &amp; HPA 需要使用；</li>
<li><code>--requestheader-client-ca-file</code>：用于签名 <code>--proxy-client-cert-file</code> 和 <code>--proxy-client-key-file</code> 指定的证书；在启用了 metric aggregator 时使用；</li>
<li>如果 --requestheader-allowed-names 不为空，则–proxy-client-cert-file 证书的 CN 必须位于 allowed-names 中，默认为 aggregator;</li>
</ul>
<p>如果 kube-apiserver 机器<strong>没有</strong>运行 kube-proxy，则还需要添加 <code>--enable-aggregator-routing=true</code> 参数；</p>
<p>关于 <code>--requestheader-XXX</code> 相关参数，参考：</p>
<ul>
<li><a href="https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md</a></li>
<li><a href="https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/" target="_blank" rel="noopener">https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/</a></li>
</ul>
<p>注意：requestheader-client-ca-file 指定的 CA 证书，必须具有 client auth and server auth；</p>
<h3 id="kube-controllr-manager">kube-controllr-manager</h3>
<p>添加如下配置参数：</p>
<p>–horizontal-pod-autoscaler-use-rest-clients=true</p>
<p>用于配置 HPA 控制器使用 REST 客户端获取 metrics 数据。</p>
<h2 id="整体架构">整体架构</h2>
<p><img src="https://img.orchome.com/group1/M00/00/06/rBAABF4Nnk-Ab8mMAAErdjdFQWs425.png" alt="screenshot"></p>
<h2 id="修改插件配置文件配置文件">修改插件配置文件配置文件</h2>
<p>metrics-server 插件位于 kubernetes 的 <code>cluster/addons/metrics-server/</code> 目录下。</p>
<p>修改 metrics-server-deployment 文件：</p>
<pre><code>$ cp metrics-server-deployment.yaml{,.orig}
$ diff metrics-server-deployment.yaml.orig metrics-server-deployment.yaml
51c51
&lt;         image: mirrorgooglecontainers/metrics-server-amd64:v0.2.1
---
&gt;         image: k8s.gcr.io/metrics-server-amd64:v0.2.1
54c54
&lt;         - --source=kubernetes.summary_api:''
---
&gt;         - --source=kubernetes.summary_api:https://kubernetes.default?kubeletHttps=true&amp;kubeletPort=10250
60c60
&lt;         image: siriuszg/addon-resizer:1.8.1
---
&gt;         image: k8s.gcr.io/addon-resizer:1.8.1
</code></pre>
<ul>
<li>metrics-server 的参数格式与 heapster 类似。由于 kubelet 只在 10250 监听 https 请求，故添加相关参数；</li>
</ul>
<p>授予 kube-system:metrics-server ServiceAccount 访问 kubelet API 的权限：</p>
<pre><code>$ cat auth-kubelet.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: metrics-server:system:kubelet-api-admin
  labels:
    kubernetes.io/cluster-service: &quot;true&quot;
    addonmanager.kubernetes.io/mode: Reconcile
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kubelet-api-admin
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system
</code></pre>
<ul>
<li>新建一个 ClusterRoleBindings 定义文件，授予相关权限；</li>
</ul>
<h2 id="创建-metrics-server">创建 metrics-server</h2>
<pre><code>$ pwd
/opt/k8s/kubernetes/cluster/addons/metrics-server
$ ls -l *.yaml
-rw-rw-r-- 1 k8s k8s  398 Jun  5 07:17 auth-delegator.yaml
-rw-rw-r-- 1 k8s k8s  404 Jun 16 18:02 auth-kubelet.yaml
-rw-rw-r-- 1 k8s k8s  419 Jun  5 07:17 auth-reader.yaml
-rw-rw-r-- 1 k8s k8s  393 Jun  5 07:17 metrics-apiservice.yaml
-rw-rw-r-- 1 k8s k8s 2640 Jun 16 17:54 metrics-server-deployment.yaml
-rw-rw-r-- 1 k8s k8s  336 Jun  5 07:17 metrics-server-service.yaml
-rw-rw-r-- 1 k8s k8s  801 Jun  5 07:17 resource-reader.yaml
$ kubectl create -f .
</code></pre>
<h2 id="查看运行情况">查看运行情况</h2>
<pre><code>$ kubectl get pods -n kube-system |grep metrics-server
metrics-server-v0.2.1-7486f5bd67-v95q2   2/2       Running   0          45s

$ kubectl get svc -n kube-system|grep metrics-server
metrics-server         ClusterIP   10.254.115.120   &lt;none&gt;        443/TCP         1m
</code></pre>
<h2 id="查看-metrcs-server-输出的-metrics">查看 metrcs-server 输出的 metrics</h2>
<p>metrics-server 输出的 APIs：<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md" target="_blank" rel="noopener">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md</a></p>
<ol>
<li>
<p>通过 kube-apiserver 或 kubectl proxy 访问：</p>
<p><a href="https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/nodes" target="_blank" rel="noopener">https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/nodes</a><br>
<a href="https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/nodes/" target="_blank" rel="noopener">https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/nodes/</a><br>
<a href="https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/pods" target="_blank" rel="noopener">https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/pods</a><br>
<a href="https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/namespace/" target="_blank" rel="noopener">https://172.27.129.105:6443/apis/metrics.k8s.io/v1beta1/namespace/</a>/pods/</p>
</li>
<li>
<p>直接使用 kubectl 命令访问：</p>
<p>kubectl get --raw apis/metrics.k8s.io/v1beta1/nodes<br>
kubectl get --raw apis/metrics.k8s.io/v1beta1/pods<br>
kubectl get --raw apis/metrics.k8s.io/v1beta1/nodes/<br>
kubectl get --raw apis/metrics.k8s.io/v1beta1/namespace//pods/</p>
<p>$ kubectl get --raw “/apis/metrics.k8s.io/v1beta1” | jq .<br>
{<br>
“kind”: “APIResourceList”,<br>
“apiVersion”: “v1”,<br>
“groupVersion”: “<a href="http://metrics.k8s.io/v1beta1" target="_blank" rel="noopener">metrics.k8s.io/v1beta1</a>”,<br>
“resources”: [<br>
{<br>
“name”: “nodes”,<br>
“singularName”: “”,<br>
“namespaced”: false,<br>
“kind”: “NodeMetrics”,<br>
“verbs”: [<br>
“get”,<br>
“list”<br>
]<br>
},<br>
{<br>
“name”: “pods”,<br>
“singularName”: “”,<br>
“namespaced”: true,<br>
“kind”: “PodMetrics”,<br>
“verbs”: [<br>
“get”,<br>
“list”<br>
]<br>
}<br>
]<br>
}</p>
<p>$ kubectl get --raw “/apis/metrics.k8s.io/v1beta1/nodes” | jq .<br>
{<br>
“kind”: “NodeMetricsList”,<br>
“apiVersion”: “<a href="http://metrics.k8s.io/v1beta1" target="_blank" rel="noopener">metrics.k8s.io/v1beta1</a>”,<br>
“metadata”: {<br>
“selfLink”: “/apis/metrics.k8s.io/v1beta1/nodes”<br>
},<br>
“items”: [<br>
{<br>
“metadata”: {<br>
“name”: “kube-node3”,<br>
“selfLink”: “/apis/metrics.k8s.io/v1beta1/nodes/kube-node3”,<br>
“creationTimestamp”: “2018-06-16T10:24:03Z”<br>
},<br>
“timestamp”: “2018-06-16T10:23:00Z”,<br>
“window”: “1m0s”,<br>
“usage”: {<br>
“cpu”: “133m”,<br>
“memory”: “1115728Ki”<br>
}<br>
},<br>
{<br>
“metadata”: {<br>
“name”: “kube-node1”,<br>
“selfLink”: “/apis/metrics.k8s.io/v1beta1/nodes/kube-node1”,<br>
“creationTimestamp”: “2018-06-16T10:24:03Z”<br>
},<br>
“timestamp”: “2018-06-16T10:23:00Z”,<br>
“window”: “1m0s”,<br>
“usage”: {<br>
“cpu”: “221m”,<br>
“memory”: “6799908Ki”<br>
}<br>
},<br>
{<br>
“metadata”: {<br>
“name”: “kube-node2”,<br>
“selfLink”: “/apis/metrics.k8s.io/v1beta1/nodes/kube-node2”,<br>
“creationTimestamp”: “2018-06-16T10:24:03Z”<br>
},<br>
“timestamp”: “2018-06-16T10:23:00Z”,<br>
“window”: “1m0s”,<br>
“usage”: {<br>
“cpu”: “76m”,<br>
“memory”: “1130180Ki”<br>
}<br>
}<br>
]<br>
}</p>
</li>
</ol>
<ul>
<li>/apis/metrics.k8s.io/v1beta1/nodes 和 /apis/metrics.k8s.io/v1beta1/pods 返回的 usage 包含 CPU 和 Memory；</li>
</ul>
<h2 id="参考：">参考：</h2>
<ol>
<li><a href="https://kubernetes.feisky.xyz/zh/addons/metrics.html" target="_blank" rel="noopener">https://kubernetes.feisky.xyz/zh/addons/metrics.html</a></li>
<li>metrics-server RBAC：<a href="https://github.com/kubernetes-incubator/metrics-server/issues/40" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/metrics-server/issues/40</a></li>
<li>metrics-server 参数：<a href="https://github.com/kubernetes-incubator/metrics-server/issues/25" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/metrics-server/issues/25</a></li>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>09-3.部署 heapster 插件</title>
    <url>/2020/01/07/09-3.%E9%83%A8%E7%BD%B2%20heapster%20%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<p>tags: addons, heapster</p>
<h1>09-3.部署 heapster 插件</h1>
<p>Heapster是一个收集者，将每个Node上的cAdvisor的数据进行汇总，然后导到第三方工具(如InfluxDB)。</p>
<p>Heapster 是通过调用 kubelet 的 http API 来获取 cAdvisor 的 metrics 数据的。</p>
<p>由于 kublet 只在 10250 端口接收 https 请求，故需要修改 heapster 的 deployment 配置。同时，需要赋予 kube-system:heapster ServiceAccount 调用 kubelet API 的权限。</p>
<h2 id="下载-heapster-文件">下载 heapster 文件</h2>
<p>到 <a href="https://github.com/kubernetes/heapster/releases" target="_blank" rel="noopener">heapster release 页面</a> 下载最新版本的 heapster</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://github.com/kubernetes/heapster/archive/v1.5.3.tar.gz</span><br><span class="line">tar -xzvf v1.5.3.tar.gz</span><br><span class="line">mv v1.5.3.tar.gz heapster-1.5.3.tar.gz</span><br></pre></td></tr></table></figure>
<p>官方文件目录： <code>heapster-1.5.3/deploy/kube-config/influxdb</code></p>
<h2 id="修改配置">修改配置</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> heapster-1.5.3/deploy/kube-config/influxdb</span><br><span class="line">$ cp grafana.yaml&#123;,.orig&#125;</span><br><span class="line">$ diff grafana.yaml.orig grafana.yaml</span><br><span class="line">16c16</span><br><span class="line">&lt;         image: gcr.io/google_containers/heapster-grafana-amd64:v4.4.3</span><br><span class="line">---</span><br><span class="line">&gt;         image: wanghkkk/heapster-grafana-amd64-v4.4.3:v4.4.3</span><br><span class="line">67c67</span><br><span class="line">&lt;   <span class="comment"># type: NodePort</span></span><br><span class="line">---</span><br><span class="line">&gt;   <span class="built_in">type</span>: NodePort</span><br></pre></td></tr></table></figure>
<ul>
<li>开启 NodePort；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cp heapster.yaml&#123;,.orig&#125;</span><br><span class="line">$ diff heapster.yaml.orig heapster.yaml</span><br><span class="line">23c23</span><br><span class="line">&lt;         image: gcr.io/google_containers/heapster-amd64:v1.5.3</span><br><span class="line">---</span><br><span class="line">&gt;         image: fishchen/heapster-amd64:v1.5.3</span><br><span class="line">27c27</span><br><span class="line">&lt;         - --<span class="built_in">source</span>=kubernetes:https://kubernetes.default</span><br><span class="line">---</span><br><span class="line">&gt;         - --<span class="built_in">source</span>=kubernetes:https://kubernetes.default?kubeletHttps=<span class="literal">true</span>&amp;kubeletPort=10250</span><br></pre></td></tr></table></figure>
<ul>
<li>由于 kubelet 只在 10250 监听 https 请求，故添加相关参数；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cp influxdb.yaml&#123;,.orig&#125;</span><br><span class="line">$ diff influxdb.yaml.orig influxdb.yaml</span><br><span class="line">16c16</span><br><span class="line">&lt;         image: gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3</span><br><span class="line">---</span><br><span class="line">&gt;         image: fishchen/heapster-influxdb-amd64:v1.3.3</span><br></pre></td></tr></table></figure>
<h2 id="执行所有定义文件">执行所有定义文件</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">pwd</span></span><br><span class="line">/opt/k8s/heapster-1.5.2/deploy/kube-config/influxdb</span><br><span class="line">$ ls *.yaml</span><br><span class="line">grafana.yaml  heapster.yaml  influxdb.yaml</span><br><span class="line">$ kubectl create -f  .</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ../rbac/</span><br><span class="line">$ <span class="built_in">pwd</span></span><br><span class="line">/opt/k8s/heapster-1.5.2/deploy/kube-config/rbac</span><br><span class="line">$ ls</span><br><span class="line">heapster-rbac.yaml</span><br><span class="line">$ cp heapster-rbac.yaml&#123;,.orig&#125;</span><br><span class="line">$ diff heapster-rbac.yaml.orig heapster-rbac.yaml</span><br><span class="line">12a13,26</span><br><span class="line">&gt; ---</span><br><span class="line">&gt; kind: ClusterRoleBinding</span><br><span class="line">&gt; apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">&gt; metadata:</span><br><span class="line">&gt;   name: heapster-kubelet-api</span><br><span class="line">&gt; roleRef:</span><br><span class="line">&gt;   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">&gt;   kind: ClusterRole</span><br><span class="line">&gt;   name: system:kubelet-api-admin</span><br><span class="line">&gt; subjects:</span><br><span class="line">&gt; - kind: ServiceAccount</span><br><span class="line">&gt;   name: heapster</span><br><span class="line">&gt;   namespace: kube-system</span><br><span class="line">&gt;</span><br><span class="line"></span><br><span class="line">$ kubectl create -f heapster-rbac.yaml</span><br></pre></td></tr></table></figure>
<ul>
<li>将 serviceAccount kube-system:heapster 与 ClusterRole system:kubelet-api-admin 绑定，授予它调用 kubelet API 的权限；</li>
</ul>
<h2 id="检查执行结果">检查执行结果</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system | grep -E <span class="string">'heapster|monitoring'</span></span><br><span class="line">heapster-ddb6c4994-vnnrn                1/1       Running   0          1m</span><br><span class="line">monitoring-grafana-779bd4dd7b-xqkgk     1/1       Running   0          1m</span><br><span class="line">monitoring-influxdb-f75847d48-2lnz6     1/1       Running   0          1m</span><br></pre></td></tr></table></figure>
<p>检查 kubernets dashboard 界面，可以正确显示各 Nodes、Pods 的 CPU、内存、负载等统计数据和图表：</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufGEuACG4XAAZAW50ZRmU600.png" alt="screenshot"></p>
<h2 id="访问-grafana">访问 grafana</h2>
<ol>
<li>
<p>通过 kube-apiserver 访问：</p>
<p>获取 monitoring-grafana 服务 URL：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl cluster-info</span><br><span class="line">Kubernetes master is running at https://172.27.129.105:6443</span><br><span class="line">CoreDNS is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy</span><br><span class="line">Heapster is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/heapster/proxy</span><br><span class="line">kubernetes-dashboard is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</span><br><span class="line">monitoring-grafana is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</span><br><span class="line">monitoring-influxdb is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/monitoring-influxdb/proxy</span><br><span class="line">   </span><br><span class="line">To further debug and diagnose cluster problems, use <span class="string">'kubectl cluster-info dump'</span>.</span><br></pre></td></tr></table></figure>
<p>浏览器访问 URL： <code>https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</code><br>
对于 virtuabox 做了端口映射： <code>https://127.0.0.1:8080/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</code></p>
</li>
<li>
<p>通过 kubectl proxy 访问：</p>
<p>创建代理</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl proxy --address=<span class="string">'172.27.129.105'</span> --port=8086 --accept-hosts=<span class="string">'^*$'</span></span><br><span class="line">Starting to serve on 172.27.129.80:8086</span><br></pre></td></tr></table></figure>
<p>浏览器访问 URL：<code>https://172.27.129.105:8086/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy/?orgId=1</code><br>
对于 virtuabox 做了端口映射： <code>https://127.0.0.1:8086/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy/?orgId=1</code></p>
</li>
<li>
<p>通过 NodePort 访问：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get svc -n kube-system|grep -E <span class="string">'monitoring|heapster'</span></span><br><span class="line">heapster               ClusterIP   10.254.58.136    &lt;none&gt;        80/TCP          47m</span><br><span class="line">monitoring-grafana     NodePort    10.254.28.196    &lt;none&gt;        80:8452/TCP     47m</span><br><span class="line">monitoring-influxdb    ClusterIP   10.254.138.164   &lt;none&gt;        8086/TCP        47m</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>grafana 监听 NodePort 8452；</p>
<p>浏览器访问 URL：<code>https://172.27.129.105:8452/?orgId=1</code></p>
</li>
</ul>
</li>
</ol>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufGDOANkBEAAVj52nZSPQ139.png" alt="screenshot"><br>
参考：</p>
<ol>
<li>配置 heapster：<a href="https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md" target="_blank" rel="noopener">https://github.com/kubernetes/heapster/blob/master/docs/source-configuration.md</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>09-2.部署 dashboard 插件</title>
    <url>/2020/01/07/09-2.%E9%83%A8%E7%BD%B2%20dashboard%20%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<h1>09-2.部署 dashboard 插件</h1>
<h2 id="修改配置文件">修改配置文件</h2>
<p>将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。</p>
<p>dashboard 对应的目录是：<code>cluster/addons/dashboard</code>。</p>
<pre><code>$ pwd
/opt/k8s/kubernetes/cluster/addons/dashboard

$ cp dashboard-controller.yaml{,.orig}

$ diff dashboard-controller.yaml{,.orig}
33c33
&lt;         image: siriuszg/kubernetes-dashboard-amd64:v1.8.3
---
&gt;         image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3

$ cp dashboard-service.yaml{,.orig}

$ diff dashboard-service.yaml.orig dashboard-service.yaml
10a11
&gt;   type: NodePort
</code></pre>
<ul>
<li>指定端口类型为 NodePort，这样外界可以通过地址 nodeIP:nodePort 访问 dashboard；</li>
</ul>
<h2 id="执行所有定义文件">执行所有定义文件</h2>
<pre><code>$ ls *.yaml
dashboard-configmap.yaml  dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-secret.yaml  dashboard-service.yaml

$ kubectl create -f  .
</code></pre>
<h2 id="查看分配的-NodePort">查看分配的 NodePort</h2>
<pre><code>$ kubectl get deployment kubernetes-dashboard  -n kube-system
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubernetes-dashboard   1         1         1            1           2m

$ kubectl --namespace kube-system get pods -o wide
NAME                                    READY     STATUS    RESTARTS   AGE       IP            NODE
coredns-77c989547b-6l6jr                1/1       Running   0          58m       172.30.39.3   kube-node3
coredns-77c989547b-d9lts                1/1       Running   0          58m       172.30.81.3   kube-node1
kubernetes-dashboard-65f7b4f486-wgc6j   1/1       Running   0          2m        172.30.81.5   kube-node1

$ kubectl get services kubernetes-dashboard -n kube-system
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   NodePort   10.254.96.204   &lt;none&gt;        443:8607/TCP   2m
</code></pre>
<ul>
<li>NodePort 8607 映射到 dashboard pod 443 端口；</li>
</ul>
<p>dashboard 的 --authentication-mode 支持 token、basic，默认为 token。如果使用 basic，则 kube-apiserver 必须配置 ‘–authorization-mode=ABAC’ 和 ‘–basic-auth-file’ 参数。</p>
<h2 id="查看-dashboard-支持的命令行参数">查看 dashboard 支持的命令行参数</h2>
<pre><code>$ kubectl exec --namespace kube-system -it kubernetes-dashboard-65f7b4f486-wgc6j  -- /dashboard --help
2018/06/13 15:17:44 Starting overwatch
Usage of /dashboard:
      --alsologtostderr                   log to standard error as well as files
      --apiserver-host string             The address of the Kubernetes Apiserver to connect to in the format of protocol://address:port, e.g., https://localhost:8080. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and local discovery is attempted.
      --authentication-mode stringSlice   Enables authentication options that will be reflected on login screen. Supported values: token, basic. Default: token.Note that basic option should only be used if apiserver has '--authorization-mode=ABAC' and '--basic-auth-file' flags set. (default [token])
      --auto-generate-certificates        When set to true, Dashboard will automatically generate certificates used to serve HTTPS. Default: false.
      --bind-address ip                   The IP address on which to serve the --secure-port (set to 0.0.0.0 for all interfaces). (default 0.0.0.0)
      --default-cert-dir string           Directory path containing '--tls-cert-file' and '--tls-key-file' files. Used also when auto-generating certificates flag is set. (default &quot;/certs&quot;)
      --disable-settings-authorizer       When enabled, Dashboard settings page will not require user to be logged in and authorized to access settings page.
      --enable-insecure-login             When enabled, Dashboard login view will also be shown when Dashboard is not served over HTTPS. Default: false.
      --heapster-host string              The address of the Heapster Apiserver to connect to in the format of protocol://address:port, e.g., https://localhost:8082. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and service proxy will be used.
      --insecure-bind-address ip          The IP address on which to serve the --port (set to 0.0.0.0 for all interfaces). (default 127.0.0.1)
      --insecure-port int                 The port to listen to for incoming HTTP requests. (default 9090)
      --kubeconfig string                 Path to kubeconfig file with authorization and master location information.
      --log_backtrace_at traceLocation    when logging hits line file:N, emit a stack trace (default :0)
      --log_dir string                    If non-empty, write log files in this directory
      --logtostderr                       log to standard error instead of files
      --metric-client-check-period int    Time in seconds that defines how often configured metric client health check should be run. Default: 30 seconds. (default 30)
      --port int                          The secure port to listen to for incoming HTTPS requests. (default 8443)
      --stderrthreshold severity          logs at or above this threshold go to stderr (default 2)
      --system-banner string              When non-empty displays message to Dashboard users. Accepts simple HTML tags. Default: ''.
      --system-banner-severity string     Severity of system banner. Should be one of 'INFO|WARNING|ERROR'. Default: 'INFO'. (default &quot;INFO&quot;)
      --tls-cert-file string              File containing the default x509 Certificate for HTTPS.
      --tls-key-file string               File containing the default x509 private key matching --tls-cert-file.
      --token-ttl int                     Expiration time (in seconds) of JWE tokens generated by dashboard. Default: 15 min. 0 - never expires (default 900)
  -v, --v Level                           log level for V logs
      --vmodule moduleSpec                comma-separated list of pattern=N settings for file-filtered logging
command terminated with exit code 2
$
</code></pre>
<h2 id="访问-dashboard">访问 dashboard</h2>
<p>为了集群安全，从 1.7 开始，dashboard 只允许通过 https 访问，如果使用 kube proxy 则必须监听 localhost 或 127.0.0.1，对于 NodePort 没有这个限制，但是仅建议在开发环境中使用。</p>
<p>对于不满足这些条件的登录访问，在登录成功后<strong>浏览器不跳转，始终停在登录界面</strong>。</p>
<p>参考：<br>
<a href="https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard---1.7.X-and-above" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard—1.7.X-and-above</a><br>
<a href="https://github.com/kubernetes/dashboard/issues/2540" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard/issues/2540</a></p>
<ol>
<li>kubernetes-dashboard 服务暴露了 NodePort，可以使用 <code>https://NodeIP:NodePort</code> 地址访问 dashboard；</li>
<li>通过 kube-apiserver 访问 dashboard；</li>
<li>通过 kubectl proxy 访问 dashboard：</li>
</ol>
<p>如果使用了 VirtualBox，需要启用 VirtualBox 的 ForworadPort 功能将虚机监听的端口和 Host 的本地端口绑定。</p>
<p>可以在 Vagrant 的配置中指定这些端口转发规则，对于正在运行的虚机，也可以通过 VirtualBox 的界面进行配置：</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufE_6ATXaNAAYSE23OlTM418.png" alt="screenshot"></p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufFBGANtysAAWK0mJBQZQ201.png" alt="screenshot"></p>
<h3 id="通过-kubectl-proxy-访问-dashboard">通过 kubectl proxy 访问 dashboard</h3>
<p>启动代理：</p>
<pre><code>$ kubectl proxy --address='localhost' --port=8086 --accept-hosts='^*$'
Starting to serve on 127.0.0.1:8086
</code></pre>
<ul>
<li>–address 必须为 localhost 或 127.0.0.1；</li>
<li>需要指定 <code>--accept-hosts</code> 选项，否则浏览器访问 dashboard 页面时提示 “Unauthorized”；</li>
</ul>
<p>浏览器访问 URL：<code>https://127.0.0.1:8086/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</code></p>
<h3 id="通过-kube-apiserver-访问-dashboard">通过 kube-apiserver 访问 dashboard</h3>
<p>获取集群服务地址列表：</p>
<pre><code>$ kubectl cluster-info
Kubernetes master is running at https://172.27.129.105:6443
CoreDNS is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy
kubernetes-dashboard is running at https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</code></pre>
<p>必须通过 kube-apiserver 的安全端口(https)访问 dashbaord，访问时浏览器需要使用<strong>自定义证书</strong>，否则会被 kube-apiserver 拒绝访问。</p>
<p>创建和导入自定义证书的步骤，参考：<a href="/1204">A.浏览器访问kube-apiserver安全端口</a></p>
<p>浏览器访问 URL：<code>https://172.27.129.105:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</code><br>
对于 virtuabox 做了端口映射： <code>https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</code></p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufFC2ASDpDAAFzzzBHFKY593.png" alt="screenshot"></p>
<h2 id="创建登录-Dashboard-的-token-和-kubeconfig-配置文件">创建登录 Dashboard 的 token 和 kubeconfig 配置文件</h2>
<p>上面提到，Dashboard 默认只支持 token 认证，所以如果使用 KubeConfig 文件，需要在该文件中指定 token，不支持使用 client 证书认证。</p>
<h3 id="创建登录-token">创建登录 token</h3>
<pre><code>kubectl create sa dashboard-admin -n kube-system
kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin
ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk '{print $1}')
DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system ${ADMIN_SECRET} | grep -E '^token' | awk '{print $2}')
echo ${DASHBOARD_LOGIN_TOKEN}
</code></pre>
<p>使用输出的 token 登录 Dashboard。</p>
<h3 id="创建使用-token-的-KubeConfig-文件">创建使用 token 的 KubeConfig 文件</h3>
<pre><code>source /opt/k8s/bin/environment.sh
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=dashboard.kubeconfig

# 设置客户端认证参数，使用上面创建的 Token
kubectl config set-credentials dashboard_user \
  --token=${DASHBOARD_LOGIN_TOKEN} \
  --kubeconfig=dashboard.kubeconfig

# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=dashboard_user \
  --kubeconfig=dashboard.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=dashboard.kubeconfig
</code></pre>
<p>用生成的 dashboard.kubeconfig 登录 Dashboard。</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufFKCAOkuDAAKWwIAdbSI128.png" alt="screenshot"></p>
<p>由于缺少 Heapster 插件，当前 dashboard 不能展示 Pod、Nodes 的 CPU、内存等统计数据和图表；</p>
<h2 id="参考">参考</h2>
<p><a href="https://github.com/kubernetes/dashboard/wiki/Access-control" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard/wiki/Access-control</a><br>
<a href="https://github.com/kubernetes/dashboard/issues/2558" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard/issues/2558</a><br>
<a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/</a></p>
]]></content>
  </entry>
  <entry>
    <title>09-1.部署 coredns 插件</title>
    <url>/2020/01/07/09-1.%E9%83%A8%E7%BD%B2%20coredns%20%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<h1>09-1.部署 coredns 插件</h1>
<h2 id="修改配置文件">修改配置文件</h2>
<p>将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。</p>
<p>coredns 对应的目录是：<code>cluster/addons/dns</code>。</p>
<pre><code>$ pwd
/opt/k8s/kubernetes/cluster/addons/dns

$ cp coredns.yaml.base coredns.yaml
$ diff coredns.yaml.base coredns.yaml
61c61
&lt;         kubernetes __PILLAR__DNS__DOMAIN__ in-addr.arpa ip6.arpa {
---
&gt;         kubernetes cluster.local. in-addr.arpa ip6.arpa {
153c153
&lt;   clusterIP: __PILLAR__DNS__SERVER__
---
&gt;   clusterIP: 10.254.0.2
</code></pre>
<h2 id="创建-coredns">创建 coredns</h2>
<pre><code>$ kubectl create -f coredns.yaml
</code></pre>
<h2 id="检查-coredns-功能">检查 coredns 功能</h2>
<pre><code>$ kubectl get all -n kube-system
NAME                           READY     STATUS    RESTARTS   AGE
pod/coredns-77c989547b-6l6jr   1/1       Running   0          3m
pod/coredns-77c989547b-d9lts   1/1       Running   0          3m

NAME              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
service/coredns   ClusterIP   10.254.0.2   &lt;none&gt;        53/UDP,53/TCP   3m

NAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2         2         2            2           3m

NAME                                 DESIRED   CURRENT   READY     AGE
replicaset.apps/coredns-77c989547b   2         2         2         3m
</code></pre>
<p>新建一个 Deployment</p>
<pre><code>$ cat &gt; my-nginx.yaml &lt;&lt;EOF
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
EOF
$ kubectl create -f my-nginx.yaml
</code></pre>
<p>Export 该 Deployment, 生成 <code>my-nginx</code> 服务：</p>
<pre><code>$ kubectl expose deploy my-nginx
service &quot;my-nginx&quot; exposed

$ kubectl get services --all-namespaces |grep my-nginx
default       my-nginx     ClusterIP   10.254.242.255   &lt;none&gt;        80/TCP          9s
</code></pre>
<p>创建另一个 Pod，查看 <code>/etc/resolv.conf</code> 是否包含 <code>kubelet</code> 配置的 <code>--cluster-dns</code> 和 <code>--cluster-domain</code>，是否能够将服务 <code>my-nginx</code> 解析到上面显示的 Cluster IP <code>10.254.242.255</code></p>
<pre><code>$ cat &gt; pod-nginx.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.7.9
    ports:
    - containerPort: 80
EOF

$ kubectl create -f pod-nginx.yaml
$ kubectl exec  nginx -i -t -- /bin/bash
root@nginx:/# cat /etc/resolv.conf
nameserver 10.254.0.2
search default.svc.cluster.local. svc.cluster.local. cluster.local. 4pd.io
options ndots:5

root@nginx:/#  ping my-nginx
PING my-nginx.default.svc.cluster.local (10.254.242.255): 48 data bytes
56 bytes from 10.254.242.255: icmp_seq=0 ttl=64 time=0.115 ms
^C--- my-nginx.default.svc.cluster.local ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.115/0.115/0.115/0.000 ms
</code></pre>
<p>​<br>
root@nginx:/# ping my-nginx<br>
PING my-nginx.default.svc.cluster.local (10.254.63.136): 48 data bytes<br>
^C— my-nginx.default.svc.cluster.local ping statistics —<br>
4 packets transmitted, 0 packets received, 100% packet loss</p>
<pre><code>root@nginx:/# ping kubernetes
PING kubernetes.default.svc.cluster.local (10.254.0.1): 48 data bytes
56 bytes from 10.254.0.1: icmp_seq=0 ttl=64 time=0.097 ms
56 bytes from 10.254.0.1: icmp_seq=1 ttl=64 time=0.123 ms
^C--- kubernetes.default.svc.cluster.local ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.097/0.110/0.123/0.000 ms

root@nginx:/# ping coredns.kube-system.svc.cluster.local
PING coredns.kube-system.svc.cluster.local (10.254.0.2): 48 data bytes
56 bytes from 10.254.0.2: icmp_seq=0 ttl=64 time=0.129 ms
^C--- coredns.kube-system.svc.cluster.local ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.129/0.129/0.129/0.000 ms
</code></pre>
<h2 id="参考">参考</h2>
<p><a href="https://community.infoblox.com/t5/Community-Blog/CoreDNS-for-Kubernetes-Service-Discovery/ba-p/8187" target="_blank" rel="noopener">https://community.infoblox.com/t5/Community-Blog/CoreDNS-for-Kubernetes-Service-Discovery/ba-p/8187</a><br>
<a href="https://coredns.io/2017/03/01/coredns-for-kubernetes-service-discovery-take-2/" target="_blank" rel="noopener">https://coredns.io/2017/03/01/coredns-for-kubernetes-service-discovery-take-2/</a><br>
<a href="https://www.cnblogs.com/boshen-hzb/p/7511432.html" target="_blank" rel="noopener">https://www.cnblogs.com/boshen-hzb/p/7511432.html</a><br>
<a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns</a></p>
]]></content>
  </entry>
  <entry>
    <title>08.验证集群功能</title>
    <url>/2020/01/07/08.%E9%AA%8C%E8%AF%81%E9%9B%86%E7%BE%A4%E5%8A%9F%E8%83%BD/</url>
    <content><![CDATA[<p>tags: verify</p>
<h1>08.验证集群功能</h1>
<p>本文档使用 daemonset 验证 master 和 worker 节点是否工作正常。</p>
<h2 id="检查节点状态">检查节点状态</h2>
<pre><code>$ kubectl get nodes
NAME         STATUS    ROLES     AGE       VERSION
kube-node1   Ready     &lt;none&gt;    3h        v1.10.4
kube-node2   Ready     &lt;none&gt;    3h        v1.10.4
kube-node3   Ready     &lt;none&gt;    3h        v1.10.4
</code></pre>
<p>都为 Ready 时正常。</p>
<h2 id="创建测试文件">创建测试文件</h2>
<pre><code>$ cat &gt; nginx-ds.yml &lt;&lt;EOF
apiVersion: v1
kind: Service
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
spec:
  type: NodePort
  selector:
    app: nginx-ds
  ports:
  - name: http
    port: 80
    targetPort: 80
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
EOF
</code></pre>
<h2 id="执行定义文件">执行定义文件</h2>
<pre><code>$ kubectl create -f nginx-ds.yml
service &quot;nginx-ds&quot; created
daemonset.extensions &quot;nginx-ds&quot; created
</code></pre>
<h2 id="检查各-Node-上的-Pod-IP-连通性">检查各 Node 上的 Pod IP 连通性</h2>
<pre><code>$ kubectl get pods  -o wide|grep nginx-ds
nginx-ds-dbn97   1/1       Running   0          2m        172.30.29.2   kube-node2
nginx-ds-rk777   1/1       Running   0          2m        172.30.81.2   kube-node1
nginx-ds-tr9g5   1/1       Running   0          2m        172.30.39.2   kube-node3
</code></pre>
<p>可见，nginx-ds 的 Pod IP 分别是 <code>172.30.39.2</code>、<code>172.30.81.2</code>、<code>172.30.29.2</code>，在所有 Node 上分别 ping 这三个 IP，看是否连通：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.39.2&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.81.2&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.29.2&quot;
  done
</code></pre>
<h2 id="检查服务-IP-和端口可达性">检查服务 IP 和端口可达性</h2>
<pre><code>$ kubectl get svc |grep nginx-ds
nginx-ds     NodePort    10.254.254.228   &lt;none&gt;        80:8900/TCP   4m
</code></pre>
<p>可见：</p>
<ul>
<li>Service Cluster IP：10.254.254.228</li>
<li>服务端口：80</li>
<li>NodePort 端口：8900</li>
</ul>
<p>在所有 Node 上 curl Service IP：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;curl 10.254.254.228&quot;
  done
</code></pre>
<p>预期输出 nginx 欢迎页面内容。</p>
<h2 id="检查服务的-NodePort-可达性">检查服务的 NodePort 可达性</h2>
<p>在所有 Node 上执行：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;curl ${node_ip}:8900&quot;
  done
</code></pre>
<p>预期输出 nginx 欢迎页面内容。</p>
]]></content>
  </entry>
  <entry>
    <title>07-3.部署 kube-proxy 组件</title>
    <url>/2020/01/07/07-3.%E9%83%A8%E7%BD%B2%20kube-proxy%20%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<h1>07-3.部署 kube-proxy 组件</h1>
<p>kube-proxy 运行在所有 worker 节点上，，它监听 apiserver 中 service 和 Endpoint 的变化情况，创建路由规则来进行服务负载均衡。</p>
<p>本文档讲解部署 kube-proxy 的部署，使用 ipvs 模式。</p>
<h2 id="下载和分发-kube-proxy-二进制文件">下载和分发 kube-proxy 二进制文件</h2>
<p>参考 <a href="https://www.orchome.com/657" target="_blank" rel="noopener">06-0.部署master节点.md</a></p>
<h2 id="安装依赖包">安装依赖包</h2>
<p>各节点需要安装 <code>ipvsadm</code> 和 <code>ipset</code> 命令，加载 <code>ip_vs</code> 内核模块。</p>
<p>参考 <a href="https://www.orchome.com/658" target="_blank" rel="noopener">07-0.部署worker节点.md</a></p>
<h2 id="创建-kube-proxy-证书">创建 kube-proxy 证书</h2>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; kube-proxy-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>CN：指定该证书的 User 为 <code>system:kube-proxy</code>；</li>
<li>预定义的 RoleBinding <code>system:node-proxier</code> 将User <code>system:kube-proxy</code> 与 Role <code>system:node-proxier</code> 绑定，该 Role 授予了调用 <code>kube-apiserver</code> Proxy 相关 API 的权限；</li>
<li>该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
</code></pre>
<h2 id="创建和分发-kubeconfig-文件">创建和分发 kubeconfig 文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
</code></pre>
<ul>
<li><code>--embed-certs=true</code>：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl-proxy.kubeconfig 文件中(不加时，写入的是证书文件路径)；</li>
</ul>
<p>分发 kubeconfig 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    scp kube-proxy.kubeconfig k8s@${node_name}:/etc/kubernetes/
  done
</code></pre>
<h2 id="创建-kube-proxy-配置文件">创建 kube-proxy 配置文件</h2>
<p>从 v1.10 开始，kube-proxy <strong>部分参数</strong>可以配置文件中配置。可以使用 <code>--write-config-to</code> 选项生成该配置文件，或者参考 kubeproxyconfig 的类型定义源文件 ：<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go</a></p>
<p>创建 kube-proxy config 文件模板：</p>
<pre><code>cat &gt;kube-proxy.config.yaml.template &lt;&lt;EOF
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: ##NODE_IP##
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: ${CLUSTER_CIDR}
healthzBindAddress: ##NODE_IP##:10256
hostnameOverride: ##NODE_NAME##
kind: KubeProxyConfiguration
metricsBindAddress: ##NODE_IP##:10249
mode: &quot;ipvs&quot;
EOF
</code></pre>
<ul>
<li><code>bindAddress</code>: 监听地址；</li>
<li><code>clientConnection.kubeconfig</code>: 连接 apiserver 的 kubeconfig 文件；</li>
<li><code>clusterCIDR</code>: kube-proxy 根据 <code>--cluster-cidr</code> 判断集群内部和外部流量，指定 <code>--cluster-cidr</code> 或 <code>--masquerade-all</code> 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</li>
<li><code>hostnameOverride</code>: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；</li>
<li><code>mode</code>: 使用 ipvs 模式；</li>
</ul>
<p>为各节点创建和分发 kube-proxy 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; 3; i++ ))
  do 
    echo &quot;&gt;&gt;&gt; ${NODE_NAMES[i]}&quot;
    sed -e &quot;s/##NODE_NAME##/${NODE_NAMES[i]}/&quot; -e &quot;s/##NODE_IP##/${NODE_IPS[i]}/&quot; kube-proxy.config.yaml.template &gt; kube-proxy-${NODE_NAMES[i]}.config.yaml
    scp kube-proxy-${NODE_NAMES[i]}.config.yaml root@${NODE_NAMES[i]}:/etc/kubernetes/kube-proxy.config.yaml
  done
</code></pre>
<h2 id="创建和分发-kube-proxy-systemd-unit-文件">创建和分发 kube-proxy systemd unit 文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kube-proxy.service &lt;&lt;EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/opt/k8s/bin/kube-proxy \\
  --config=/etc/kubernetes/kube-proxy.config.yaml \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<p>替换后的 unit 文件：<a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kube-proxy.service" target="_blank" rel="noopener">kube-proxy.service</a></p>
<p>分发 kube-proxy systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do 
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    scp kube-proxy.service root@${node_name}:/etc/systemd/system/
  done
</code></pre>
<h2 id="启动-kube-proxy-服务">启动 kube-proxy 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/lib/kube-proxy&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy&quot;
  done
</code></pre>
<ul>
<li>必须先创建工作和日志目录；</li>
</ul>
<h2 id="检查启动结果">检查启动结果</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status kube-proxy|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u kube-proxy
</code></pre>
<h2 id="查看监听端口和-metrics">查看监听端口和 metrics</h2>
<pre><code>[k8s@kube-node1 ~]$ sudo netstat -lnpt|grep kube-prox
tcp        0      0 172.27.129.105:10249    0.0.0.0:*               LISTEN      16847/kube-proxy
tcp        0      0 172.27.129.105:10256    0.0.0.0:*               LISTEN      16847/kube-proxy
</code></pre>
<ul>
<li>10249：http prometheus metrics port;</li>
<li>10256：http healthz port;</li>
</ul>
<h2 id="查看-ipvs-路由规则">查看 ipvs 路由规则</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;/usr/sbin/ipvsadm -ln&quot;
  done
</code></pre>
<p>预期输出：</p>
<pre><code>&gt;&gt;&gt; 172.27.129.105
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&gt; 172.27.129.105:6443          Masq    1      0          0
&gt;&gt;&gt; 172.27.129.111
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&gt; 172.27.129.105:6443          Masq    1      0          0
&gt;&gt;&gt; 172.27.129.112
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 rr persistent 10800
  -&gt; 172.27.129.105:6443          Masq    1      0          0
</code></pre>
<p>可见将所有到 kubernetes cluster ip 443 端口的请求都转发到 kube-apiserver 的 6443 端口；</p>
]]></content>
  </entry>
  <entry>
    <title>07-2.部署 kubelet 组件</title>
    <url>/2020/01/07/07-2-%E9%83%A8%E7%BD%B2-kubelet-%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<p>tags: worker, kubelet</p>
<h1>07-2.部署 kubelet 组件</h1>
<p>kublet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如 exec、run、logs 等。</p>
<p>kublet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况。</p>
<p>为确保安全，本文档只开启接收 https 请求的安全端口，对请求进行认证和授权，拒绝未授权的访问(如 apiserver、heapster)。</p>
<h2 id="下载和分发-kubelet-二进制文件">下载和分发 kubelet 二进制文件</h2>
<p>参考 <a href="https://www.orchome.com/657" target="_blank" rel="noopener">06-0.部署master节点.md</a></p>
<h2 id="安装依赖包">安装依赖包</h2>
<p>参考 <a href="https://www.orchome.com/658" target="_blank" rel="noopener">07-0.部署worker节点.md</a></p>
<h2 id="创建-kubelet-bootstrap-kubeconfig-文件">创建 kubelet bootstrap kubeconfig 文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;

    # 创建 token
    export BOOTSTRAP_TOKEN=$(kubeadm token create \
      --description kubelet-bootstrap-token \
      --groups system:bootstrappers:${node_name} \
      --kubeconfig ~/.kube/config)

    # 设置集群参数
    kubectl config set-cluster kubernetes \
      --certificate-authority=/etc/kubernetes/cert/ca.pem \
      --embed-certs=true \
      --server=${KUBE_APISERVER} \
      --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig

    # 设置客户端认证参数
    kubectl config set-credentials kubelet-bootstrap \
      --token=${BOOTSTRAP_TOKEN} \
      --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig

    # 设置上下文参数
    kubectl config set-context default \
      --cluster=kubernetes \
      --user=kubelet-bootstrap \
      --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig

    # 设置默认上下文
    kubectl config use-context default --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig
  done
</code></pre>
<ul>
<li>证书中写入 Token 而非证书，证书后续由 controller-manager 创建。</li>
</ul>
<p>查看 kubeadm 为各节点创建的 token：</p>
<pre><code>$ kubeadm token list --kubeconfig ~/.kube/config
TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS
k0s2bj.7nvw1zi1nalyz4gz   23h       2018-06-14T15:14:31+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:kube-node1
mkus5s.vilnjk3kutei600l   23h       2018-06-14T15:14:32+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:kube-node3
zkiem5.0m4xhw0jc8r466nk   23h       2018-06-14T15:14:32+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:kube-node2
</code></pre>
<ul>
<li>创建的 token 有效期为 1 天，超期后将不能再被使用，且会被 kube-controller-manager 的 tokencleaner 清理(如果启用该 controller 的话)；</li>
<li>kube-apiserver 接收 kubelet 的 bootstrap token 后，将请求的 user 设置为 system:bootstrap:，group 设置为 system:bootstrappers；</li>
</ul>
<p>各 token 关联的 Secret：</p>
<pre><code>$ kubectl get secrets  -n kube-system
NAME                     TYPE                                  DATA      AGE
bootstrap-token-k0s2bj   bootstrap.kubernetes.io/token         7         1m
bootstrap-token-mkus5s   bootstrap.kubernetes.io/token         7         1m
bootstrap-token-zkiem5   bootstrap.kubernetes.io/token         7         1m
default-token-99st7      kubernetes.io/service-account-token   3         2d
</code></pre>
<h2 id="分发-bootstrap-kubeconfig-文件到所有-worker-节点">分发 bootstrap kubeconfig 文件到所有 worker 节点</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    scp kubelet-bootstrap-${node_name}.kubeconfig k8s@${node_name}:/etc/kubernetes/kubelet-bootstrap.kubeconfig
  done
</code></pre>
<h2 id="创建和分发-kubelet-参数配置文件">创建和分发 kubelet 参数配置文件</h2>
<p>从 v1.10 开始，kubelet <strong>部分参数</strong>需在配置文件中配置，<code>kubelet --help</code> 会提示：</p>
<pre><code>DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag
</code></pre>
<p>创建 kubelet 参数配置模板文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kubelet.config.json.template &lt;&lt;EOF
{
  &quot;kind&quot;: &quot;KubeletConfiguration&quot;,
  &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;,
  &quot;authentication&quot;: {
    &quot;x509&quot;: {
      &quot;clientCAFile&quot;: &quot;/etc/kubernetes/cert/ca.pem&quot;
    },
    &quot;webhook&quot;: {
      &quot;enabled&quot;: true,
      &quot;cacheTTL&quot;: &quot;2m0s&quot;
    },
    &quot;anonymous&quot;: {
      &quot;enabled&quot;: false
    }
  },
  &quot;authorization&quot;: {
    &quot;mode&quot;: &quot;Webhook&quot;,
    &quot;webhook&quot;: {
      &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;,
      &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot;
    }
  },
  &quot;address&quot;: &quot;##NODE_IP##&quot;,
  &quot;port&quot;: 10250,
  &quot;readOnlyPort&quot;: 0,
  &quot;cgroupDriver&quot;: &quot;cgroupfs&quot;,
  &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;,
  &quot;serializeImagePulls&quot;: false,
  &quot;featureGates&quot;: {
    &quot;RotateKubeletClientCertificate&quot;: true,
    &quot;RotateKubeletServerCertificate&quot;: true
  },
  &quot;clusterDomain&quot;: &quot;${CLUSTER_DNS_DOMAIN}&quot;,
  &quot;clusterDNS&quot;: [&quot;${CLUSTER_DNS_SVC_IP}&quot;]
}
EOF
</code></pre>
<ul>
<li>address：API 监听地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API；</li>
<li>readOnlyPort=0：关闭只读端口(默认 10255)，等效为未指定；</li>
<li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li>
<li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTP 证书认证；</li>
<li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li>
<li>对于未通过 x509 证书和 webhook 认证的请求(kube-apiserver 或其他客户端)，将被拒绝，提示 Unauthorized；</li>
<li>authroization.mode=Webhook：kubelet 使用 SubjectAccessReview API 查询 kube-apiserver 某 user、group 是否具有操作资源的权限(RBAC)；</li>
<li>featureGates.RotateKubeletClientCertificate、featureGates.RotateKubeletServerCertificate：自动 rotate 证书，证书的有效期取决于 kube-controller-manager 的 --experimental-cluster-signing-duration 参数；</li>
<li>需要 root 账户运行；</li>
</ul>
<p>为各节点创建和分发 kubelet 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do 
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    sed -e &quot;s/##NODE_IP##/${node_ip}/&quot; kubelet.config.json.template &gt; kubelet.config-${node_ip}.json
    scp kubelet.config-${node_ip}.json root@${node_ip}:/etc/kubernetes/kubelet.config.json
  done
</code></pre>
<h2 id="创建和分发-kubelet-systemd-unit-文件">创建和分发 kubelet systemd unit 文件</h2>
<p>创建 kubelet systemd unit 文件模板：</p>
<pre><code>cat &gt; kubelet.service.template &lt;&lt;EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/opt/k8s/bin/kubelet \\
  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \\
  --cert-dir=/etc/kubernetes/cert \\
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\
  --config=/etc/kubernetes/kubelet.config.json \\
  --hostname-override=##NODE_NAME## \\
  --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\
  --allow-privileged=true \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li>如果设置了 <code>--hostname-override</code> 选项，则 <code>kube-proxy</code> 也需要设置该选项，否则会出现找不到 Node 的情况；</li>
<li><code>--bootstrap-kubeconfig</code>：指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；</li>
<li>K8S approve kubelet 的 csr 请求后，在 <code>--cert-dir</code> 目录创建证书和私钥文件，然后写入 <code>--kubeconfig</code> 文件；</li>
</ul>
<p>替换后的 unit 文件：<a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kubelet.service" target="_blank" rel="noopener">kubelet.service</a></p>
<p>为各节点创建和分发 kubelet systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do 
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    sed -e &quot;s/##NODE_NAME##/${node_name}/&quot; kubelet.service.template &gt; kubelet-${node_name}.service
    scp kubelet-${node_name}.service root@${node_name}:/etc/systemd/system/kubelet.service
  done
</code></pre>
<h2 id="Bootstrap-Token-Auth-和授予权限">Bootstrap Token Auth 和授予权限</h2>
<p>kublet 启动时查找配置的 --kubeletconfig 文件是否存在，如果不存在则使用 --bootstrap-kubeconfig 向 kube-apiserver 发送证书签名请求 (CSR)。</p>
<p>kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证（事先使用 kubeadm 创建的 token），认证通过后将请求的 user 设置为 system:bootstrap:，group 设置为 system:bootstrappers，这一过程称为 Bootstrap Token Auth。</p>
<p>默认情况下，这个 user 和 group 没有创建 CSR 的权限，kubelet 启动失败，错误日志如下：</p>
<pre><code>$ sudo journalctl -u kubelet -a |grep -A 2 'certificatesigningrequests'
May 06 06:42:36 kube-node1 kubelet[26986]: F0506 06:42:36.314378   26986 server.go:233] failed to run Kubelet: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &quot;system:bootstrap:lemy40&quot; cannot create certificatesigningrequests.certificates.k8s.io at the cluster scope
May 06 06:42:36 kube-node1 systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
May 06 06:42:36 kube-node1 systemd[1]: kubelet.service: Failed with result 'exit-code'.
</code></pre>
<p>解决办法是：创建一个 clusterrolebinding，将 group system:bootstrappers 和 clusterrole system:node-bootstrapper 绑定：</p>
<pre><code>$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
</code></pre>
<h2 id="启动-kubelet-服务">启动 kubelet 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/lib/kubelet&quot;
    ssh root@${node_ip} &quot;/usr/sbin/swapoff -a&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet&quot;
  done
</code></pre>
<ul>
<li>
<p>关闭 swap 分区，否则 kubelet 会启动失败；</p>
</li>
<li>
<p>必须先创建工作和日志目录；</p>
<p>$ journalctl -u kubelet |tail</p>
</li>
</ul>
<p>kubelet 启动后使用 --bootstrap-kubeconfig 向 kube-apiserver 发送 CSR 请求，当这个 CSR 被 approve 后，kube-controller-manager 为 kubelet 创建 TLS 客户端证书、私钥和 --kubeletconfig 文件。</p>
<p>注意：kube-controller-manager 需要配置 <code>--cluster-signing-cert-file</code> 和 <code>--cluster-signing-key-file</code> 参数，才会为 TLS Bootstrap 创建证书和私钥。</p>
<pre><code>$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk   43s       system:bootstrap:zkiem5   Pending
node-csr-oVbPmU-ikVknpynwu0Ckz_MvkAO_F1j0hmbcDa__sGA   27s       system:bootstrap:mkus5s   Pending
node-csr-u0E1-ugxgotO_9FiGXo8DkD6a7-ew8sX2qPE6KPS2IY   13m       system:bootstrap:k0s2bj   Pending

$ kubectl get nodes
No resources found.
</code></pre>
<ul>
<li>三个 work 节点的 csr 均处于 pending 状态；</li>
</ul>
<h2 id="approve-kubelet-CSR-请求">approve kubelet CSR 请求</h2>
<p>可以手动或自动 approve CSR 请求。推荐使用自动的方式，因为从 v1.8 版本开始，可以自动轮转approve csr 后生成的证书。</p>
<h3 id="手动-approve-CSR-请求">手动 approve CSR 请求</h3>
<p>查看 CSR 列表：</p>
<pre><code>$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk   43s       system:bootstrap:zkiem5   Pending
node-csr-oVbPmU-ikVknpynwu0Ckz_MvkAO_F1j0hmbcDa__sGA   27s       system:bootstrap:mkus5s   Pending
node-csr-u0E1-ugxgotO_9FiGXo8DkD6a7-ew8sX2qPE6KPS2IY   13m       system:bootstrap:k0s2bj   Pending
</code></pre>
<p>approve CSR：</p>
<pre><code>$ kubectl certificate approve node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk
certificatesigningrequest.certificates.k8s.io &quot;node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk&quot; approved
</code></pre>
<p>查看 Approve 结果：</p>
<pre><code>$ kubectl describe  csr node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk
Name:               node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk
Labels:             &lt;none&gt;
Annotations:        &lt;none&gt;
CreationTimestamp:  Wed, 13 Jun 2018 16:05:04 +0800
Requesting User:    system:bootstrap:zkiem5
Status:             Approved
Subject:
         Common Name:    system:node:kube-node2
         Serial Number:
         Organization:   system:nodes
Events:  &lt;none&gt;
</code></pre>
<ul>
<li><code>Requesting User</code>：请求 CSR 的用户，kube-apiserver 对它进行认证和授权；</li>
<li><code>Subject</code>：请求签名的证书信息；</li>
<li>证书的 CN 是 system:node:kube-node2， Organization 是 system:nodes，kube-apiserver 的 Node 授权模式会授予该证书的相关权限；</li>
</ul>
<h3 id="自动-approve-CSR-请求">自动 approve CSR 请求</h3>
<p>创建三个 ClusterRoleBinding，分别用于自动 approve client、renew client、renew server 证书：</p>
<pre><code>cat &gt; csr-crb.yaml &lt;&lt;EOF
 # Approve all CSRs for the group &quot;system:bootstrappers&quot;
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: auto-approve-csrs-for-group
 subjects:
 - kind: Group
   name: system:bootstrappers
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
   apiGroup: rbac.authorization.k8s.io
---
 # To let a node of the group &quot;system:nodes&quot; renew its own credentials
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: node-client-cert-renewal
 subjects:
 - kind: Group
   name: system:nodes
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
   apiGroup: rbac.authorization.k8s.io
---
# A ClusterRole which instructs the CSR approver to approve a node requesting a
# serving cert matching its client cert.
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: approve-node-server-renewal-csr
rules:
- apiGroups: [&quot;certificates.k8s.io&quot;]
  resources: [&quot;certificatesigningrequests/selfnodeserver&quot;]
  verbs: [&quot;create&quot;]
---
 # To let a node of the group &quot;system:nodes&quot; renew its own server credentials
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: node-server-cert-renewal
 subjects:
 - kind: Group
   name: system:nodes
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: approve-node-server-renewal-csr
   apiGroup: rbac.authorization.k8s.io
EOF
</code></pre>
<ul>
<li>auto-approve-csrs-for-group：自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrappers；</li>
<li>node-client-cert-renewal：自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes;</li>
<li>node-server-cert-renewal：自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes;</li>
</ul>
<p>生效配置：</p>
<pre><code>$ kubectl apply -f csr-crb.yaml
</code></pre>
<h2 id="查看-kublet-的情况">查看 kublet 的情况</h2>
<p>等待一段时间(1-10 分钟)，三个节点的 CSR 都被自动 approve：</p>
<pre><code>$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
csr-98h25                                              6m        system:node:kube-node2    Approved,Issued
csr-lb5c9                                              7m        system:node:kube-node3    Approved,Issued
csr-m2hn4                                              14m       system:node:kube-node1    Approved,Issued
node-csr-7q7i0q4MF_K2TSEJj16At4CJFLlJkHIqei6nMIAaJCU   28m       system:bootstrap:k0s2bj   Approved,Issued
node-csr-ND77wk2P8k2lHBtgBaObiyYw0uz1Um7g2pRvveMF-c4   35m       system:bootstrap:mkus5s   Approved,Issued
node-csr-Nysmrw55nnM48NKwEJuiuCGmZoxouK4N8jiEHBtLQso   6m        system:bootstrap:zkiem5   Approved,Issued
node-csr-QzuuQiuUfcSdp3j5W4B2UOuvQ_n9aTNHAlrLzVFiqrk   1h        system:bootstrap:zkiem5   Approved,Issued
node-csr-oVbPmU-ikVknpynwu0Ckz_MvkAO_F1j0hmbcDa__sGA   1h        system:bootstrap:mkus5s   Approved,Issued
node-csr-u0E1-ugxgotO_9FiGXo8DkD6a7-ew8sX2qPE6KPS2IY   1h        system:bootstrap:k0s2bj   Approved,Issued
</code></pre>
<p>所有节点均 ready：</p>
<pre><code>$ kubectl get nodes
NAME         STATUS    ROLES     AGE       VERSION
kube-node1   Ready     &lt;none&gt;    18m       v1.10.4
kube-node2   Ready     &lt;none&gt;    10m       v1.10.4
kube-node3   Ready     &lt;none&gt;    11m       v1.10.4
</code></pre>
<p>kube-controller-manager 为各 node 生成了 kubeconfig 文件和公私钥：</p>
<pre><code>$ ls -l /etc/kubernetes/kubelet.kubeconfig
-rw------- 1 root root 2293 Jun 13 17:07 /etc/kubernetes/kubelet.kubeconfig

$ ls -l /etc/kubernetes/cert/|grep kubelet
-rw-r--r-- 1 root root 1046 Jun 13 17:07 kubelet-client.crt
-rw------- 1 root root  227 Jun 13 17:07 kubelet-client.key
-rw------- 1 root root 1334 Jun 13 17:07 kubelet-server-2018-06-13-17-07-45.pem
lrwxrwxrwx 1 root root   58 Jun 13 17:07 kubelet-server-current.pem -&gt; /etc/kubernetes/cert/kubelet-server-2018-06-13-17-07-45.pem
</code></pre>
<ul>
<li>kubelet-server 证书会周期轮转；</li>
</ul>
<h2 id="kubelet-提供的-API-接口">kubelet 提供的 API 接口</h2>
<p>kublet 启动后监听多个端口，用于接收 kube-apiserver 或其它组件发送的请求：</p>
<pre><code>$ sudo netstat -lnpt|grep kubelet
tcp        0      0 172.27.129.111:4194     0.0.0.0:*               LISTEN      2490/kubelet
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      2490/kubelet
tcp        0      0 172.27.129.111:10250    0.0.0.0:*               LISTEN      2490/kubelet
</code></pre>
<ul>
<li>4194: cadvisor http 服务；</li>
<li>10248: healthz http 服务；</li>
<li>10250: https API 服务；注意：未开启只读端口 10255；</li>
</ul>
<p>例如执行 <code>kubectl ec -it nginx-ds-5rmws -- sh</code> 命令时，kube-apiserver 会向 kubelet 发送如下请求：</p>
<pre><code>POST /exec/default/nginx-ds-5rmws/my-nginx?command=sh&amp;input=1&amp;output=1&amp;tty=1
</code></pre>
<p>kubelet 接收 10250 端口的 https 请求：</p>
<ul>
<li>/pods、/runningpods</li>
<li>/metrics、/metrics/cadvisor、/metrics/probes</li>
<li>/spec</li>
<li>/stats、/stats/container</li>
<li>/logs</li>
<li>/run/、&quot;/exec/&quot;, “/attach/”, “/portForward/”, “/containerLogs/” 等管理；</li>
</ul>
<p>详情参考：<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3</a></p>
<p>由于关闭了匿名认证，同时开启了 webhook 授权，所有访问 10250 端口 https API 的请求都需要被认证和授权。</p>
<p>预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限：</p>
<pre><code>$ kubectl describe clusterrole system:kubelet-api-admin
Name:         system:kubelet-api-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources      Non-Resource URLs  Resource Names  Verbs
  ---------      -----------------  --------------  -----
  nodes          []                 []              [get list watch proxy]
  nodes/log      []                 []              [*]
  nodes/metrics  []                 []              [*]
  nodes/proxy    []                 []              [*]
  nodes/spec     []                 []              [*]
  nodes/stats    []                 []              [*]
</code></pre>
<h2 id="kublet-api-认证和授权">kublet api 认证和授权</h2>
<p>kublet 配置了如下认证参数：</p>
<ul>
<li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li>
<li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证；</li>
<li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li>
</ul>
<p>同时配置了如下授权参数：</p>
<ul>
<li>authroization.mode=Webhook：开启 RBAC 授权；</li>
</ul>
<p>kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized：</p>
<pre><code>$ curl -s --cacert /etc/kubernetes/cert/ca.pem https://172.27.129.111:10250/metrics
Unauthorized

$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer 123456&quot; https://172.27.129.111:10250/metrics
Unauthorized
</code></pre>
<p>通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)；</p>
<p>证书认证和授权：</p>
<pre><code>$ # 权限不足的证书；
$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /etc/kubernetes/cert/kube-controller-manager.pem --key /etc/kubernetes/cert/kube-controller-manager-key.pem https://172.27.129.111:10250/metrics
Forbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)

$ # 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；
$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert ./admin.pem --key ./admin-key.pem https://172.27.129.111:10250/metrics|head
# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;0&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;21600&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;43200&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;86400&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;172800&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;345600&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;604800&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;2.592e+06&quot;} 0
</code></pre>
<ul>
<li><code>--cacert</code>、<code>--cert</code>、<code>--key</code> 的参数值必须是文件路径，如上面的 <code>./admin.pem</code> 不能省略 <code>./</code>，否则返回 <code>401 Unauthorized</code>；</li>
</ul>
<p>bear token 认证和授权：</p>
<p>创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限：</p>
<pre><code>kubectl create sa kubelet-api-test
kubectl create clusterrolebinding kubelet-api-test --clusterrole=system:kubelet-api-admin --serviceaccount=default:kubelet-api-test
SECRET=$(kubectl get secrets | grep kubelet-api-test | awk '{print $1}')
TOKEN=$(kubectl describe secret ${SECRET} | grep -E '^token' | awk '{print $2}')
echo ${TOKEN}

$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer ${TOKEN}&quot; https://172.27.129.111:10250/metrics|head
# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;0&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;21600&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;43200&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;86400&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;172800&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;345600&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;604800&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;2.592e+06&quot;} 0
</code></pre>
<h3 id="cadvisor-和-metrics">cadvisor 和 metrics</h3>
<p>cadvisor 统计所在节点各容器的资源(CPU、内存、磁盘、网卡)使用情况，分别在自己的 http web 页面(4194 端口)和 10250 以 promehteus metrics 的形式输出。</p>
<p>浏览器访问 <a href="https://172.27.129.105:4194/containers/" target="_blank" rel="noopener">https://172.27.129.105:4194/containers/</a> 可以查看到 cadvisor 的监控页面：</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFuiOhaAUnECAAIXh1n8zAg265.png" alt="screenshot"></p>
<p>浏览器访问 <a href="https://172.27.129.80:10250/metrics" target="_blank" rel="noopener">https://172.27.129.80:10250/metrics</a> 和 <a href="https://172.27.129.80:10250/metrics/cadvisor" target="_blank" rel="noopener">https://172.27.129.80:10250/metrics/cadvisor</a> 分别返回 kublet 和 cadvisor 的 metrics。</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFuiOmuAISyfAAiaZ-dFVNs239.png" alt="screenshot"><br>
注意：</p>
<ul>
<li>kublet.config.json 设置 authentication.anonymous.enabled 为 false，不允许匿名证书访问 10250 的 https 服务；</li>
<li>参考<a href="A.%E6%B5%8F%E8%A7%88%E5%99%A8%E8%AE%BF%E9%97%AEkube-apiserver%E5%AE%89%E5%85%A8%E7%AB%AF%E5%8F%A3.md">A.浏览器访问kube-apiserver安全端口.md</a>，创建和导入相关证书，然后访问上面的 10250 端口；</li>
</ul>
<h2 id="获取-kublet-的配置">获取 kublet 的配置</h2>
<p>从 kube-apiserver 获取各 node 的配置：</p>
<pre><code>$ source /opt/k8s/bin/environment.sh
$ # 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；
$ curl -sSL --cacert /etc/kubernetes/cert/ca.pem --cert ./admin.pem --key ./admin-key.pem ${KUBE_APISERVER}/api/v1/nodes/kube-node1/proxy/configz | jq \
  '.kubeletconfig|.kind=&quot;KubeletConfiguration&quot;|.apiVersion=&quot;kubelet.config.k8s.io/v1beta1&quot;'
{
  &quot;syncFrequency&quot;: &quot;1m0s&quot;,
  &quot;fileCheckFrequency&quot;: &quot;20s&quot;,
  &quot;httpCheckFrequency&quot;: &quot;20s&quot;,
  &quot;address&quot;: &quot;172.27.129.80&quot;,
  &quot;port&quot;: 10250,
  &quot;readOnlyPort&quot;: 10255,
  &quot;authentication&quot;: {
    &quot;x509&quot;: {},
    &quot;webhook&quot;: {
      &quot;enabled&quot;: false,
      &quot;cacheTTL&quot;: &quot;2m0s&quot;
    },
    &quot;anonymous&quot;: {
      &quot;enabled&quot;: true
    }
  },
  &quot;authorization&quot;: {
    &quot;mode&quot;: &quot;AlwaysAllow&quot;,
    &quot;webhook&quot;: {
      &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;,
      &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot;
    }
  },
  &quot;registryPullQPS&quot;: 5,
  &quot;registryBurst&quot;: 10,
  &quot;eventRecordQPS&quot;: 5,
  &quot;eventBurst&quot;: 10,
  &quot;enableDebuggingHandlers&quot;: true,
  &quot;healthzPort&quot;: 10248,
  &quot;healthzBindAddress&quot;: &quot;127.0.0.1&quot;,
  &quot;oomScoreAdj&quot;: -999,
  &quot;clusterDomain&quot;: &quot;cluster.local.&quot;,
  &quot;clusterDNS&quot;: [
    &quot;10.254.0.2&quot;
  ],
  &quot;streamingConnectionIdleTimeout&quot;: &quot;4h0m0s&quot;,
  &quot;nodeStatusUpdateFrequency&quot;: &quot;10s&quot;,
  &quot;imageMinimumGCAge&quot;: &quot;2m0s&quot;,
  &quot;imageGCHighThresholdPercent&quot;: 85,
  &quot;imageGCLowThresholdPercent&quot;: 80,
  &quot;volumeStatsAggPeriod&quot;: &quot;1m0s&quot;,
  &quot;cgroupsPerQOS&quot;: true,
  &quot;cgroupDriver&quot;: &quot;cgroupfs&quot;,
  &quot;cpuManagerPolicy&quot;: &quot;none&quot;,
  &quot;cpuManagerReconcilePeriod&quot;: &quot;10s&quot;,
  &quot;runtimeRequestTimeout&quot;: &quot;2m0s&quot;,
  &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;,
  &quot;maxPods&quot;: 110,
  &quot;podPidsLimit&quot;: -1,
  &quot;resolvConf&quot;: &quot;/etc/resolv.conf&quot;,
  &quot;cpuCFSQuota&quot;: true,
  &quot;maxOpenFiles&quot;: 1000000,
  &quot;contentType&quot;: &quot;application/vnd.kubernetes.protobuf&quot;,
  &quot;kubeAPIQPS&quot;: 5,
  &quot;kubeAPIBurst&quot;: 10,
  &quot;serializeImagePulls&quot;: false,
  &quot;evictionHard&quot;: {
    &quot;imagefs.available&quot;: &quot;15%&quot;,
    &quot;memory.available&quot;: &quot;100Mi&quot;,
    &quot;nodefs.available&quot;: &quot;10%&quot;,
    &quot;nodefs.inodesFree&quot;: &quot;5%&quot;
  },
  &quot;evictionPressureTransitionPeriod&quot;: &quot;5m0s&quot;,
  &quot;enableControllerAttachDetach&quot;: true,
  &quot;makeIPTablesUtilChains&quot;: true,
  &quot;iptablesMasqueradeBit&quot;: 14,
  &quot;iptablesDropBit&quot;: 15,
  &quot;featureGates&quot;: {
    &quot;RotateKubeletClientCertificate&quot;: true,
    &quot;RotateKubeletServerCertificate&quot;: true
  },
  &quot;failSwapOn&quot;: true,
  &quot;containerLogMaxSize&quot;: &quot;10Mi&quot;,
  &quot;containerLogMaxFiles&quot;: 5,
  &quot;enforceNodeAllocatable&quot;: [
    &quot;pods&quot;
  ],
  &quot;kind&quot;: &quot;KubeletConfiguration&quot;,
  &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io/v1beta1&quot;
}
</code></pre>
<p>或者参考代码中的注释：<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go</a></p>
<h2 id="参考">参考</h2>
<ol>
<li>kubelet 认证和授权：<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/" target="_blank" rel="noopener">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>07-1.部署 docker 组件</title>
    <url>/2020/01/07/07-1.%E9%83%A8%E7%BD%B2%20docker%20%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<h1>07-1.部署 docker 组件</h1>
<p>docker 是容器的运行环境，管理它的生命周期。kubelet 通过 Container Runtime Interface (CRI) 与 docker 进行交互。</p>
<h2 id="安装依赖包">安装依赖包</h2>
<p>参考 <a href="https://www.orchome.com/658" target="_blank" rel="noopener">07-0.部署worker节点.md</a></p>
<h2 id="下载和分发-docker-二进制文件">下载和分发 docker 二进制文件</h2>
<p>到 <a href="https://download.docker.com/linux/static/stable/x86_64/" target="_blank" rel="noopener">https://download.docker.com/linux/static/stable/x86_64/</a> 页面下载最新发布包：</p>
<pre><code>wget https://download.docker.com/linux/static/stable/x86_64/docker-18.03.1-ce.tgz
tar -xvf docker-18.03.1-ce.tgz
</code></pre>
<p>分发二进制文件到所有 worker 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp docker/docker*  k8s@${node_ip}:/opt/k8s/bin/
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h2 id="创建和分发-systemd-unit-文件">创建和分发 systemd unit 文件</h2>
<pre><code>cat &gt; docker.service &lt;&lt;&quot;EOF&quot;
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.io

[Service]
Environment=&quot;PATH=/opt/k8s/bin:/bin:/sbin:/usr/bin:/usr/sbin&quot;
EnvironmentFile=-/run/flannel/docker
ExecStart=/opt/k8s/bin/dockerd --log-level=error $DOCKER_NETWORK_OPTIONS
ExecReload=/bin/kill -s HUP $MAINPID
Restart=on-failure
RestartSec=5
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li>
<p>EOF 前后有双引号，这样 bash 不会替换文档中的变量，如 $DOCKER_NETWORK_OPTIONS；</p>
</li>
<li>
<p>dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中；</p>
</li>
<li>
<p>flanneld 启动时将网络配置写入 <code>/run/flannel/docker</code> 文件中，dockerd 启动前读取该文件中的环境变量 <code>DOCKER_NETWORK_OPTIONS</code> ，然后设置 docker0 网桥网段；</p>
</li>
<li>
<p>如果指定了多个 <code>EnvironmentFile</code> 选项，则必须将 <code>/run/flannel/docker</code> 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；</p>
</li>
<li>
<p>docker 需要以 root 用于运行；</p>
</li>
<li>
<p>docker 从 1.13 版本开始，可能将 <strong>iptables FORWARD chain的默认策略设置为DROP</strong>，从而导致 ping 其它 Node 上的 Pod IP 失败，遇到这种情况时，需要手动设置策略为 <code>ACCEPT</code>：</p>
<pre><code>$ sudo iptables -P FORWARD ACCEPT
</code></pre>
<p>并且把以下命令写入 <code>/etc/rc.local</code> 文件中，防止节点重启<strong>iptables FORWARD chain的默认策略又还原为DROP</strong></p>
<pre><code>/sbin/iptables -P FORWARD ACCEPT
</code></pre>
</li>
</ul>
<p>分发 systemd unit 文件到所有 worker 机器:</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp docker.service root@${node_ip}:/etc/systemd/system/
  done
</code></pre>
<h2 id="配置和分发-docker-配置文件">配置和分发 docker 配置文件</h2>
<p>使用国内的仓库镜像服务器以加快 pull image 的速度，同时增加下载的并发数 (需要重启 dockerd 生效)：</p>
<pre><code>cat &gt; docker-daemon.json &lt;&lt;EOF
{
    &quot;registry-mirrors&quot;: [&quot;https://hub-mirror.c.163.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;],
    &quot;max-concurrent-downloads&quot;: 20
}
EOF
</code></pre>
<p>分发 docker 配置文件到所有 work 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p  /etc/docker/&quot;
    scp docker-daemon.json root@${node_ip}:/etc/docker/daemon.json
  done
</code></pre>
<h2 id="启动-docker-服务">启动 docker 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl stop firewalld &amp;&amp; systemctl disable firewalld&quot;
    ssh root@${node_ip} &quot;/usr/sbin/iptables -F &amp;&amp; /usr/sbin/iptables -X &amp;&amp; /usr/sbin/iptables -F -t nat &amp;&amp; /usr/sbin/iptables -X -t nat&quot;
    ssh root@${node_ip} &quot;/usr/sbin/iptables -P FORWARD ACCEPT&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker&quot;
    ssh root@${node_ip} 'for intf in /sys/devices/virtual/net/docker0/brif/*; do echo 1 &gt; $intf/hairpin_mode; done'
    ssh root@${node_ip} &quot;sudo sysctl -p /etc/sysctl.d/kubernetes.conf&quot;
  done
</code></pre>
<ul>
<li>关闭 firewalld(centos7)/ufw(ubuntu16.04)，否则可能会重复创建 iptables 规则；</li>
<li>清理旧的 iptables rules 和 chains 规则；</li>
<li>开启 docker0 网桥下虚拟网卡的 hairpin 模式;</li>
</ul>
<h2 id="检查服务运行状态">检查服务运行状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status docker|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>$ journalctl -u docker
</code></pre>
<h3 id="检查-docker0-网桥">检查 docker0 网桥</h3>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;/usr/sbin/ip addr show flannel.1 &amp;&amp; /usr/sbin/ip addr show docker0&quot;
  done
</code></pre>
<p>确认各 work 节点的 docker0 网桥和 flannel.1 接口的 IP 处于同一个网段中(如下 172.30.39.0 和 172.30.39.1)：</p>
<pre><code>3: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether ce:2f:d6:53:e5:f3 brd ff:ff:ff:ff:ff:ff
    inet 172.30.39.0/32 scope global flannel.1
      valid_lft forever preferred_lft forever
    inet6 fe80::cc2f:d6ff:fe53:e5f3/64 scope link
      valid_lft forever preferred_lft forever
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:bf:65:16:5c brd ff:ff:ff:ff:ff:ff
    inet 172.30.39.1/24 brd 172.30.39.255 scope global docker0
      valid_lft forever preferred_lft forever</code></pre>
]]></content>
  </entry>
  <entry>
    <title>07-0.部署 worker 节点</title>
    <url>/2020/01/07/07-0.%E9%83%A8%E7%BD%B2%20worker%20%E8%8A%82%E7%82%B9/</url>
    <content><![CDATA[<h1>07-0.部署 worker 节点</h1>
<p>kubernetes work 节点运行如下组件：</p>
<ul>
<li>docker</li>
<li>kubelet</li>
<li>kube-proxy</li>
</ul>
<h2 id="安装和配置-flanneld">安装和配置 flanneld</h2>
<p>参考 <a href="/656">05-部署flannel网络.md</a></p>
<h2 id="安装依赖包">安装依赖包</h2>
<p>CentOS:</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;yum install -y epel-release&quot;
    ssh root@${node_ip} &quot;yum install -y conntrack ipvsadm ipset jq iptables curl sysstat libseccomp &amp;&amp; /usr/sbin/modprobe ip_vs &quot;
  done
</code></pre>
<p>Ubuntu:</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;apt-get install -y conntrack ipvsadm ipset jq iptables curl sysstat libseccomp &amp;&amp; /usr/sbin/modprobe ip_vs &quot;
  done</code></pre>
]]></content>
  </entry>
  <entry>
    <title>06-4.部署高可用 kube-scheduler 集群</title>
    <url>/2020/01/07/06-4.%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8%20kube-scheduler%20%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h1>06-4.部署高可用 kube-scheduler 集群</h1>
<p>本文档介绍部署高可用 kube-scheduler 集群的步骤。</p>
<p>该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。</p>
<p>为保证通信安全，本文档先生成 x509 证书和私钥，kube-scheduler 在如下两种情况下使用该证书：</p>
<ol>
<li>与 kube-apiserver 的安全端口通信;</li>
<li>在<strong>安全端口</strong>(https，10251) 输出 prometheus 格式的 metrics；</li>
</ol>
<h2 id="准备工作">准备工作</h2>
<p>下载最新版本的二进制文件、安装和配置 flanneld 参考：<a href="https://www.orchome.com/657" target="_blank" rel="noopener">06-0.部署master节点</a></p>
<h2 id="创建-kube-scheduler-证书和私钥">创建 kube-scheduler 证书和私钥</h2>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; kube-scheduler-csr.json &lt;&lt;EOF
{
    &quot;CN&quot;: &quot;system:kube-scheduler&quot;,
    &quot;hosts&quot;: [
      &quot;127.0.0.1&quot;,
      &quot;172.27.129.105&quot;,
      &quot;172.27.129.111&quot;,
      &quot;172.27.129.112&quot;
    ],
    &quot;key&quot;: {
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    },
    &quot;names&quot;: [
      {
        &quot;C&quot;: &quot;CN&quot;,
        &quot;ST&quot;: &quot;BeiJing&quot;,
        &quot;L&quot;: &quot;BeiJing&quot;,
        &quot;O&quot;: &quot;system:kube-scheduler&quot;,
        &quot;OU&quot;: &quot;4Paradigm&quot;
      }
    ]
}
EOF
</code></pre>
<ul>
<li>hosts 列表包含<strong>所有</strong> kube-scheduler 节点 IP；</li>
<li>CN 为 system:kube-scheduler、O 为 system:kube-scheduler，kubernetes 内置的 ClusterRoleBindings system:kube-scheduler 将赋予 kube-scheduler 工作所需的权限。</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
</code></pre>
<h2 id="创建和分发-kubeconfig-文件">创建和分发 kubeconfig 文件</h2>
<p>kubeconfig 文件包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p>
<pre><code>source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context system:kube-scheduler \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
</code></pre>
<ul>
<li>上一步创建的证书、私钥以及 kube-apiserver 地址被写入到 kubeconfig 文件中；</li>
</ul>
<p>分发 kubeconfig 到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-scheduler.kubeconfig k8s@${node_ip}:/etc/kubernetes/
  done
</code></pre>
<h2 id="创建和分发-kube-scheduler-systemd-unit-文件">创建和分发 kube-scheduler systemd unit 文件</h2>
<pre><code>cat &gt; kube-scheduler.service &lt;&lt;EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/k8s/bin/kube-scheduler \\
  --address=127.0.0.1 \\
  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
  --leader-elect=true \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5
User=k8s

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li><code>--address</code>：在 127.0.0.1:10251 端口接收 http /metrics 请求；kube-scheduler 目前还不支持接收 https 请求；</li>
<li><code>--kubeconfig</code>：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；</li>
<li><code>--leader-elect=true</code>：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li>
<li><code>User=k8s</code>：使用 k8s 账户运行；</li>
</ul>
<p>完整 unit 见 <a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kube-scheduler.service" target="_blank" rel="noopener">kube-scheduler.service</a>。</p>
<p>分发 systemd unit 文件到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-scheduler.service root@${node_ip}:/etc/systemd/system/
  done
</code></pre>
<h2 id="启动-kube-scheduler-服务">启动 kube-scheduler 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler&quot;
  done
</code></pre>
<ul>
<li>必须先创建日志目录；</li>
</ul>
<h2 id="检查服务运行状态">检查服务运行状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status kube-scheduler|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u kube-scheduler
</code></pre>
<h2 id="查看输出的-metric">查看输出的 metric</h2>
<p>注意：以下命令在 kube-scheduler 节点上执行。</p>
<p>kube-scheduler 监听 10251 端口，接收 http 请求：</p>
<pre><code>$ sudo netstat -lnpt|grep kube-sche
tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      23783/kube-schedule


$ curl -s https://127.0.0.1:10251/metrics |head
# HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.
# TYPE apiserver_audit_event_total counter
apiserver_audit_event_total 0
# HELP go_gc_duration_seconds A summary of the GC invocation durations.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile=&quot;0&quot;} 9.7715e-05
go_gc_duration_seconds{quantile=&quot;0.25&quot;} 0.000107676
go_gc_duration_seconds{quantile=&quot;0.5&quot;} 0.00017868
go_gc_duration_seconds{quantile=&quot;0.75&quot;} 0.000262444
go_gc_duration_seconds{quantile=&quot;1&quot;} 0.001205223
</code></pre>
<h2 id="测试-kube-scheduler-集群的高可用">测试 kube-scheduler 集群的高可用</h2>
<p>随便找一个或两个 master 节点，停掉 kube-scheduler 服务，看其它节点是否获取了 leader 权限（systemd 日志）。</p>
<h2 id="查看当前的-leader">查看当前的 leader</h2>
<pre><code>$ kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{&quot;holderIdentity&quot;:&quot;kube-node3_61f34593-6cc8-11e8-8af7-5254002f288e&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2018-06-10T16:09:56Z&quot;,&quot;renewTime&quot;:&quot;2018-06-10T16:20:54Z&quot;,&quot;leaderTransitions&quot;:1}'
  creationTimestamp: 2018-06-10T16:07:33Z
  name: kube-scheduler
  namespace: kube-system
  resourceVersion: &quot;4645&quot;
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
  uid: 62382d98-6cc8-11e8-96fa-525400ba84c6
</code></pre>
<p>可见，当前的 leader 为 kube-node3 节点。</p>
]]></content>
  </entry>
  <entry>
    <title>06-3.部署高可用 kube-controller-manager 集群</title>
    <url>/2020/01/07/06-3.%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8%20kube-controller-manager%20%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h1>06-3.部署高可用 kube-controller-manager 集群</h1>
<p>本文档介绍部署高可用 kube-controller-manager 集群的步骤。</p>
<p>该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。</p>
<p>为保证通信安全，本文档先生成 x509 证书和私钥，kube-controller-manager 在如下两种情况下使用该证书：</p>
<ol>
<li>与 kube-apiserver 的安全端口通信时;</li>
<li>在<strong>安全端口</strong>(https，10252) 输出 prometheus 格式的 metrics；</li>
</ol>
<h2 id="准备工作">准备工作</h2>
<p>下载最新版本的二进制文件、安装和配置 flanneld 参考：<a href="https://www.orchome.com/657" target="_blank" rel="noopener">06-0.部署master节点.md</a></p>
<h2 id="创建-kube-controller-manager-证书和私钥">创建 kube-controller-manager 证书和私钥</h2>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF
{
    &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,
    &quot;key&quot;: {
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    },
    &quot;hosts&quot;: [
      &quot;127.0.0.1&quot;,
      &quot;172.27.129.105&quot;,
      &quot;172.27.129.111&quot;,
      &quot;172.27.129.112&quot;
    ],
    &quot;names&quot;: [
      {
        &quot;C&quot;: &quot;CN&quot;,
        &quot;ST&quot;: &quot;BeiJing&quot;,
        &quot;L&quot;: &quot;BeiJing&quot;,
        &quot;O&quot;: &quot;system:kube-controller-manager&quot;,
        &quot;OU&quot;: &quot;4Paradigm&quot;
      }
    ]
}
EOF
</code></pre>
<ul>
<li>hosts 列表包含<strong>所有</strong> kube-controller-manager 节点 IP；</li>
<li>CN 为 system:kube-controller-manager、O 为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:kube-controller-manager 赋予 kube-controller-manager 工作所需的权限。</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
</code></pre>
<p>将生成的证书和私钥分发到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-controller-manager*.pem k8s@${node_ip}:/etc/kubernetes/cert/
  done
</code></pre>
<h2 id="创建和分发-kubeconfig-文件">创建和分发 kubeconfig 文件</h2>
<p>kubeconfig 文件包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p>
<pre><code>source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context system:kube-controller-manager \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
</code></pre>
<p>分发 kubeconfig 到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-controller-manager.kubeconfig k8s@${node_ip}:/etc/kubernetes/
  done
</code></pre>
<h2 id="创建和分发-kube-controller-manager-systemd-unit-文件">创建和分发 kube-controller-manager systemd unit 文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kube-controller-manager.service &lt;&lt;EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/k8s/bin/kube-controller-manager \\
  --port=0 \\
  --secure-port=10252 \\
  --bind-address=127.0.0.1 \\
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --service-cluster-ip-range=${SERVICE_CIDR} \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/etc/kubernetes/cert/ca.pem \\
  --cluster-signing-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --experimental-cluster-signing-duration=8760h \\
  --root-ca-file=/etc/kubernetes/cert/ca.pem \\
  --service-account-private-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --leader-elect=true \\
  --feature-gates=RotateKubeletServerCertificate=true \\
  --controllers=*,bootstrapsigner,tokencleaner \\
  --horizontal-pod-autoscaler-use-rest-clients=true \\
  --horizontal-pod-autoscaler-sync-period=10s \\
  --tls-cert-file=/etc/kubernetes/cert/kube-controller-manager.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kube-controller-manager-key.pem \\
  --use-service-account-credentials=true \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on
Restart=on-failure
RestartSec=5
User=k8s

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li><code>--port=0</code>：关闭监听 http /metrics 的请求，同时 <code>--address</code> 参数无效，<code>--bind-address</code> 参数有效；</li>
<li><code>--secure-port=10252</code>、<code>--bind-address=0.0.0.0</code>: 在所有网络接口监听 10252 端口的 https /metrics 请求；</li>
<li><code>--kubeconfig</code>：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver；</li>
<li><code>--cluster-signing-*-file</code>：签名 TLS Bootstrap 创建的证书；</li>
<li><code>--experimental-cluster-signing-duration</code>：指定 TLS Bootstrap 证书的有效期；</li>
<li><code>--root-ca-file</code>：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验；</li>
<li><code>--service-account-private-key-file</code>：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 <code>--service-account-key-file</code> 指定的公钥文件配对使用；</li>
<li><code>--service-cluster-ip-range</code> ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致；</li>
<li><code>--leader-elect=true</code>：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li>
<li><code>--feature-gates=RotateKubeletServerCertificate=true</code>：开启 kublet server 证书的自动更新特性；</li>
<li><code>--controllers=*,bootstrapsigner,tokencleaner</code>：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token；</li>
<li><code>--horizontal-pod-autoscaler-*</code>：custom metrics 相关参数，支持 autoscaling/v2alpha1；</li>
<li><code>--tls-cert-file</code>、<code>--tls-private-key-file</code>：使用 https 输出 metrics 时使用的 Server 证书和秘钥；</li>
<li><code>--use-service-account-credentials=true</code>:</li>
<li><code>User=k8s</code>：使用 k8s 账户运行；</li>
</ul>
<p>kube-controller-manager 不对请求 https metrics 的 Client 证书进行校验，故不需要指定 <code>--tls-ca-file</code> 参数，而且该参数已被淘汰。</p>
<p>完整 unit 见 <a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/kube-controller-manager.service" target="_blank" rel="noopener">kube-controller-manager.service</a></p>
<p>分发 systemd unit 文件到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-controller-manager.service root@${node_ip}:/etc/systemd/system/
  done
</code></pre>
<h2 id="kube-controller-manager-的权限">kube-controller-manager 的权限</h2>
<p>ClusteRole: system:kube-controller-manager 的<strong>权限很小</strong>，只能创建 secret、serviceaccount 等资源对象，各 controller 的权限分散到 ClusterRole system:controller:XXX 中。</p>
<p>需要在 kube-controller-manager 的启动参数中添加 <code>--use-service-account-credentials=true</code> 参数，这样 main controller 会为各 controller 创建对应的 ServiceAccount XXX-controller。</p>
<p>内置的 ClusterRoleBinding system:controller:XXX 将赋予各 XXX-controller ServiceAccount 对应的 ClusterRole system:controller:XXX 权限。</p>
<h2 id="启动-kube-controller-manager-服务">启动 kube-controller-manager 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager&quot;
  done
</code></pre>
<ul>
<li>必须先创建日志目录；</li>
</ul>
<h2 id="检查服务运行状态">检查服务运行状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status kube-controller-manager|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>$ journalctl -u kube-controller-manager
</code></pre>
<h2 id="查看输出的-metric">查看输出的 metric</h2>
<p>注意：以下命令在 kube-controller-manager 节点上执行。</p>
<p>kube-controller-manager 监听 10252 端口，接收 https 请求：</p>
<pre><code>$ sudo netstat -lnpt|grep kube-controll
tcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      18377/kube-controll


$ curl -s --cacert /etc/kubernetes/cert/ca.pem https://127.0.0.1:10252/metrics |head
# HELP ClusterRoleAggregator_adds Total number of adds handled by workqueue: ClusterRoleAggregator
# TYPE ClusterRoleAggregator_adds counter
ClusterRoleAggregator_adds 3
# HELP ClusterRoleAggregator_depth Current depth of workqueue: ClusterRoleAggregator
# TYPE ClusterRoleAggregator_depth gauge
ClusterRoleAggregator_depth 0
# HELP ClusterRoleAggregator_queue_latency How long an item stays in workqueueClusterRoleAggregator before being requested.
# TYPE ClusterRoleAggregator_queue_latency summary
ClusterRoleAggregator_queue_latency{quantile=&quot;0.5&quot;} 57018
ClusterRoleAggregator_queue_latency{quantile=&quot;0.9&quot;} 57268
</code></pre>
<ul>
<li>curl --cacert CA 证书用来验证 kube-controller-manager https server 证书；</li>
</ul>
<h2 id="测试-kube-controller-manager-集群的高可用">测试 kube-controller-manager 集群的高可用</h2>
<p>停掉一个或两个节点的 kube-controller-manager 服务，观察其它节点的日志，看是否获取了 leader 权限。</p>
<h2 id="查看当前的-leader">查看当前的 leader</h2>
<pre><code>$ kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{&quot;holderIdentity&quot;:&quot;kube-node2_084534e2-6cc4-11e8-a418-5254001f5b65&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2018-06-10T15:40:33Z&quot;,&quot;renewTime&quot;:&quot;2018-06-10T16:19:08Z&quot;,&quot;leaderTransitions&quot;:12}'
  creationTimestamp: 2018-06-10T13:59:42Z
  name: kube-controller-manager
  namespace: kube-system
  resourceVersion: &quot;4540&quot;
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager
  uid: 862cc048-6cb6-11e8-96fa-525400ba84c6
</code></pre>
<p>可见，当前的 leader 为 kube-node2 节点。</p>
<h2 id="参考">参考</h2>
<ol>
<li>关于 controller 权限和 use-service-account-credentials 参数：<a href="https://github.com/kubernetes/kubernetes/issues/48208" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/48208</a></li>
<li>kublet 认证和授权：<a href="https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet-authorization" target="_blank" rel="noopener">https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet-authorization</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>06-2.部署 kube-apiserver 组件</title>
    <url>/2020/01/07/06-2.%E9%83%A8%E7%BD%B2%20kube-apiserver%20%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<h1>06-2.部署 kube-apiserver 组件</h1>
<p>本文档讲解使用 keepalived 和 haproxy 部署一个 3 节点高可用 master 集群的步骤，对应的 LB VIP 为环境变量 ${MASTER_VIP}。</p>
<h2 id="准备工作">准备工作</h2>
<p>下载最新版本的二进制文件、安装和配置 flanneld 参考：<a href="https://www.orchome.com/657" target="_blank" rel="noopener">06-0.部署master节点.md</a></p>
<h2 id="创建-kubernetes-证书和私钥">创建 kubernetes 证书和私钥</h2>
<p>创建证书签名请求：</p>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kubernetes-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;172.27.129.105&quot;,
    &quot;172.27.129.111&quot;,
    &quot;172.27.129.112&quot;,
    &quot;${MASTER_VIP}&quot;,
    &quot;${CLUSTER_KUBERNETES_SVC_IP}&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>
<p>hosts 字段指定授权使用该证书的 <strong>IP 或域名列表</strong>，这里列出了 VIP 、apiserver 节点 IP、kubernetes 服务 IP 和域名；</p>
</li>
<li>
<p>域名最后字符不能是 <code>.</code>(如不能为 <code>kubernetes.default.svc.cluster.local.</code>)，否则解析时失败，提示： <code>x509: cannot parse dnsName &quot;kubernetes.default.svc.cluster.local.&quot;</code>；</p>
</li>
<li>
<p>如果使用非 <code>cluster.local</code> 域名，如 <code>opsnull.com</code>，则需要修改域名列表中的最后两个域名为：<code>kubernetes.default.svc.opsnull</code>、<code>kubernetes.default.svc.opsnull.com</code></p>
</li>
<li>
<p>kubernetes 服务 IP 是 apiserver 自动创建的，一般是 <code>--service-cluster-ip-range</code> 参数指定的网段的<strong>第一个IP</strong>，后续可以通过如下命令获取：</p>
<pre><code>$ kubectl get svc kubernetes
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.254.0.1   &lt;none&gt;        443/TCP   1d
</code></pre>
</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
ls kubernetes*pem
</code></pre>
<p>将生成的证书和私钥文件拷贝到 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/kubernetes/cert/ &amp;&amp; sudo chown -R k8s /etc/kubernetes/cert/&quot;
    scp kubernetes*.pem k8s@${node_ip}:/etc/kubernetes/cert/
  done
</code></pre>
<ul>
<li>k8s 账户可以读写 /etc/kubernetes/cert/ 目录；</li>
</ul>
<h2 id="创建加密配置文件">创建加密配置文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; encryption-config.yaml &lt;&lt;EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
</code></pre>
<p>将加密配置文件拷贝到 master 节点的 <code>/etc/kubernetes</code> 目录下：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp encryption-config.yaml root@${node_ip}:/etc/kubernetes/
  done
</code></pre>
<p>替换后的 encryption-config.yaml 文件：<a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/encryption-config.yaml" target="_blank" rel="noopener">encryption-config.yaml</a></p>
<h2 id="创建-kube-apiserver-systemd-unit-模板文件">创建 kube-apiserver systemd unit 模板文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; kube-apiserver.service.template &lt;&lt;EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/opt/k8s/bin/kube-apiserver \\
  --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --anonymous-auth=false \\
  --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\
  --advertise-address=##NODE_IP## \\
  --bind-address=##NODE_IP## \\
  --insecure-port=0 \\
  --authorization-mode=Node,RBAC \\
  --runtime-config=api/all \\
  --enable-bootstrap-token-auth \\
  --service-cluster-ip-range=${SERVICE_CIDR} \\
  --service-node-port-range=${NODE_PORT_RANGE} \\
  --tls-cert-file=/etc/kubernetes/cert/kubernetes.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kubernetes-key.pem \\
  --client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --kubelet-client-certificate=/etc/kubernetes/cert/kubernetes.pem \\
  --kubelet-client-key=/etc/kubernetes/cert/kubernetes-key.pem \\
  --service-account-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --etcd-cafile=/etc/kubernetes/cert/ca.pem \\
  --etcd-certfile=/etc/kubernetes/cert/kubernetes.pem \\
  --etcd-keyfile=/etc/kubernetes/cert/kubernetes-key.pem \\
  --etcd-servers=${ETCD_ENDPOINTS} \\
  --enable-swagger-ui=true \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/kube-apiserver-audit.log \\
  --event-ttl=1h \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
User=k8s
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li><code>--experimental-encryption-provider-config</code>：启用加密特性；</li>
<li><code>--authorization-mode=Node,RBAC</code>： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求；</li>
<li><code>--enable-admission-plugins</code>：启用 <code>ServiceAccount</code> 和 <code>NodeRestriction</code>；</li>
<li><code>--service-account-key-file</code>：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 <code>--service-account-private-key-file</code> 指定私钥文件，两者配对使用；</li>
<li><code>--tls-*-file</code>：指定 apiserver 使用的证书、私钥和 CA 文件。<code>--client-ca-file</code> 用于验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书；</li>
<li><code>--kubelet-client-certificate</code>、<code>--kubelet-client-key</code>：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes*.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API 时提示未授权；</li>
<li><code>--bind-address</code>： 不能为 <code>127.0.0.1</code>，否则外界不能访问它的安全端口 6443；</li>
<li><code>--insecure-port=0</code>：关闭监听非安全端口(8080)；</li>
<li><code>--service-cluster-ip-range</code>： 指定 Service Cluster IP 地址段；</li>
<li><code>--service-node-port-range</code>： 指定 NodePort 的端口范围；</li>
<li><code>--runtime-config=api/all=true</code>： 启用所有版本的 APIs，如 autoscaling/v2alpha1；</li>
<li><code>--enable-bootstrap-token-auth</code>：启用 kubelet bootstrap 的 token 认证；</li>
<li><code>--apiserver-count=3</code>：指定集群运行模式，多台 kube-apiserver 会通过 leader 选举产生一个工作节点，其它节点处于阻塞状态；</li>
<li><code>User=k8s</code>：使用 k8s 账户运行；</li>
</ul>
<h2 id="为各节点创建和分发-kube-apiserver-systemd-unit-文件">为各节点创建和分发 kube-apiserver systemd unit 文件</h2>
<p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; 3; i++ ))
  do
    sed -e &quot;s/##NODE_NAME##/${NODE_NAMES[i]}/&quot; -e &quot;s/##NODE_IP##/${NODE_IPS[i]}/&quot; kube-apiserver.service.template &gt; kube-apiserver-${NODE_IPS[i]}.service 
  done
ls kube-apiserver*.service
</code></pre>
<ul>
<li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li>
</ul>
<p>分发生成的 systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/log/kubernetes &amp;&amp; chown -R k8s /var/log/kubernetes&quot;
    scp kube-apiserver-${node_ip}.service root@${node_ip}:/etc/systemd/system/kube-apiserver.service
  done
</code></pre>
<ul>
<li>必须先创建日志目录；</li>
<li>文件重命名为 kube-apiserver.service;</li>
</ul>
<h2 id="启动-kube-apiserver-服务">启动 kube-apiserver 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver&quot;
  done
</code></pre>
<h2 id="检查-kube-apiserver-运行状态">检查 kube-apiserver 运行状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl status kube-apiserver |grep 'Active:'&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则到 master 节点查看日志，确认原因：</p>
<pre><code>journalctl -u kube-apiserver
</code></pre>
<h2 id="打印-kube-apiserver-写入-etcd-的数据">打印 kube-apiserver 写入 etcd 的数据</h2>
<pre><code>source /opt/k8s/bin/environment.sh
ETCDCTL_API=3 etcdctl \
    --endpoints=${ETCD_ENDPOINTS} \
    --cacert=/etc/kubernetes/cert/ca.pem \
    --cert=/etc/etcd/cert/etcd.pem \
    --key=/etc/etcd/cert/etcd-key.pem \
    get /registry/ --prefix --keys-only
</code></pre>
<h2 id="检查集群信息">检查集群信息</h2>
<pre><code>$ kubectl cluster-info
Kubernetes master is running at https://172.27.129.253:8443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

$ kubectl get all --all-namespaces
NAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
default     service/kubernetes   ClusterIP   10.254.0.1   &lt;none&gt;        443/TCP   35m

$ kubectl get componentstatuses
NAME                 STATUS      MESSAGE                                                                                        ERROR
controller-manager   Unhealthy   Get https://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: getsockopt: connection refused
scheduler            Unhealthy   Get https://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused
etcd-1               Healthy     {&quot;health&quot;:&quot;true&quot;}
etcd-0               Healthy     {&quot;health&quot;:&quot;true&quot;}
etcd-2               Healthy     {&quot;health&quot;:&quot;true&quot;}
</code></pre>
<p>注意：</p>
<ol>
<li>
<p>如果执行 kubectl 命令式时输出如下错误信息，则说明使用的 <code>~/.kube/config</code> 文件不对，请切换到正确的账户后再执行该命令：</p>
<p><code>The connection to the server localhost:8080 was refused - did you specify the right host or port?</code></p>
</li>
<li>
<p>执行 <code>kubectl get componentstatuses</code> 命令时，apiserver 默认向 127.0.0.1 发送请求。当 controller-manager、scheduler 以集群模式运行时，有可能和 kube-apiserver <strong>不在一台机器</strong>上，这时 controller-manager 或 scheduler 的状态为 Unhealthy，但实际上它们工作<strong>正常</strong>。</p>
</li>
</ol>
<h2 id="检查-kube-apiserver-监听的端口">检查 kube-apiserver 监听的端口</h2>
<pre><code>$ sudo netstat -lnpt|grep kube
tcp        0      0 172.27.129.105:6443     0.0.0.0:*               LISTEN      13075/kube-apiserve
</code></pre>
<ul>
<li>6443: 接收 https 请求的安全端口，对所有请求做认证和授权；</li>
<li>由于关闭了非安全端口，故没有监听 8080；</li>
</ul>
<h2 id="授予-kubernetes-证书访问-kubelet-API-的权限">授予 kubernetes 证书访问 kubelet API 的权限</h2>
<p>在执行 kubectl exec、run、logs 等命令时，apiserver 会转发到 kubelet。这里定义 RBAC 规则，授权 apiserver 调用 kubelet API。</p>
<pre><code>$ kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
</code></pre>
<h2 id="参考">参考</h2>
<ol>
<li>关于证书域名最后字符不能是 <code>.</code> 的问题，实际和 Go 的版本有关，1.9 不支持这种类型的证书：<a href="https://github.com/kubernetes/ingress-nginx/issues/2188" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/issues/2188</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>06-1.部署高可用组件</title>
    <url>/2020/01/07/06-1.%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<h1>06-1.部署高可用组件</h1>
<p>本文档讲解使用 keepalived 和 haproxy 实现 kube-apiserver 高可用的步骤：</p>
<ul>
<li>keepalived 提供 kube-apiserver 对外服务的 VIP；</li>
<li>haproxy 监听 VIP，后端连接所有 kube-apiserver 实例，提供健康检查和负载均衡功能；</li>
</ul>
<p>运行 keepalived 和 haproxy 的节点称为 LB 节点。由于 keepalived 是一主多备运行模式，故至少两个 LB 节点。</p>
<p>本文档复用 master 节点的三台机器，haproxy 监听的端口(8443) 需要与 kube-apiserver 的端口 6443 不同，避免冲突。</p>
<p>keepalived 在运行过程中周期检查本机的 haproxy 进程状态，如果检测到 haproxy 进程异常，则触发重新选主的过程，VIP 将飘移到新选出来的主节点，从而实现 VIP 的高可用。</p>
<p>所有组件（如 kubeclt、apiserver、controller-manager、scheduler 等）都通过 VIP 和 haproxy 监听的 8443 端口访问 kube-apiserver 服务。</p>
<h2 id="安装软件包">安装软件包</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;yum install -y keepalived haproxy&quot;
  done
</code></pre>
<h2 id="配置和下发-haproxy-配置文件">配置和下发 haproxy 配置文件</h2>
<p>haproxy 配置文件：</p>
<pre><code>cat &gt; haproxy.cfg &lt;&lt;EOF
global
    log /dev/log    local0
    log /dev/log    local1 notice
    chroot /var/lib/haproxy
    stats socket /var/run/haproxy-admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon
    nbproc 1

defaults
    log     global
    timeout connect 5000
    timeout client  10m
    timeout server  10m

listen  admin_stats
    bind 0.0.0.0:10080
    mode http
    log 127.0.0.1 local0 err
    stats refresh 30s
    stats uri /status
    stats realm welcome login\ Haproxy
    stats auth admin:123456
    stats hide-version
    stats admin if TRUE

listen kube-master
    bind 0.0.0.0:8443
    mode tcp
    option tcplog
    balance source
    server 172.27.129.105 172.27.129.105:6443 check inter 2000 fall 2 rise 2 weight 1
    server 172.27.129.111 172.27.129.111:6443 check inter 2000 fall 2 rise 2 weight 1
    server 172.27.129.112 172.27.129.112:6443 check inter 2000 fall 2 rise 2 weight 1
EOF
</code></pre>
<ul>
<li>haproxy 在 10080 端口输出 status 信息；</li>
<li>haproxy 监听<strong>所有接口</strong>的 8443 端口，该端口与环境变量 ${KUBE_APISERVER} 指定的端口必须一致；</li>
<li>server 字段列出所有 kube-apiserver 监听的 IP 和端口；</li>
</ul>
<p>下发 haproxy.cfg 到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp haproxy.cfg root@${node_ip}:/etc/haproxy
  done
</code></pre>
<h2 id="起-haproxy-服务">起 haproxy 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl restart haproxy&quot;
  done
</code></pre>
<h2 id="检查-haproxy-服务状态">检查 haproxy 服务状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl status haproxy|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u haproxy
</code></pre>
<p>检查 haproxy 是否监听 8443 端口：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;netstat -lnpt|grep haproxy&quot;
  done
</code></pre>
<p>确保输出类似于:</p>
<pre><code>tcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN      120583/haproxy
</code></pre>
<h2 id="配置和下发-keepalived-配置文件">配置和下发 keepalived 配置文件</h2>
<p>keepalived 是一主（master）多备（backup）运行模式，故有两种类型的配置文件。master 配置文件只有一份，backup 配置文件视节点数目而定，对于本文档而言，规划如下：</p>
<ul>
<li>master: 172.27.129.105</li>
<li>backup：172.27.129.111、172.27.129.112</li>
</ul>
<p>master 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
cat  &gt; keepalived-master.conf &lt;&lt;EOF
global_defs {
    router_id lb-master-105
}

vrrp_script check-haproxy {
    script &quot;killall -0 haproxy&quot;
    interval 5
    weight -30
}

vrrp_instance VI-kube-master {
    state MASTER
    priority 120
    dont_track_primary
    interface ${VIP_IF}
    virtual_router_id 68
    advert_int 3
    track_script {
        check-haproxy
    }
    virtual_ipaddress {
        ${MASTER_VIP}
    }
}
EOF
</code></pre>
<ul>
<li>VIP 所在的接口（interface ${VIP_IF}）为 <code>eth0</code>；</li>
<li>使用 <code>killall -0 haproxy</code> 命令检查所在节点的 haproxy 进程是否正常。如果异常则将权重减少（-30）,从而触发重新选主过程；</li>
<li>router_id、virtual_router_id 用于标识属于该 HA 的 keepalived 实例，如果有多套 keepalived HA，则必须各不相同；</li>
</ul>
<p>backup 配置文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
cat  &gt; keepalived-backup.conf &lt;&lt;EOF
global_defs {
    router_id lb-backup-105
}

vrrp_script check-haproxy {
    script &quot;killall -0 haproxy&quot;
    interval 5
    weight -30
}

vrrp_instance VI-kube-master {
    state BACKUP
    priority 110
    dont_track_primary
    interface ${VIP_IF}
    virtual_router_id 68
    advert_int 3
    track_script {
        check-haproxy
    }
    virtual_ipaddress {
        ${MASTER_VIP}
    }
}
EOF
</code></pre>
<ul>
<li>VIP 所在的接口（interface ${VIP_IF}）为 <code>eth0</code>；</li>
<li>使用 <code>killall -0 haproxy</code> 命令检查所在节点的 haproxy 进程是否正常。如果异常则将权重减少（-30）,从而触发重新选主过程；</li>
<li>router_id、virtual_router_id 用于标识属于该 HA 的 keepalived 实例，如果有多套 keepalived HA，则必须各不相同；</li>
<li>priority 的值必须小于 master 的值；</li>
</ul>
<h2 id="下发-keepalived-配置文件">下发 keepalived 配置文件</h2>
<p>下发 master 配置文件：</p>
<pre><code>scp keepalived-master.conf root@172.27.129.105:/etc/keepalived/keepalived.conf
</code></pre>
<p>下发 backup 配置文件：</p>
<pre><code>scp keepalived-backup.conf root@172.27.129.111:/etc/keepalived/keepalived.conf
scp keepalived-backup.conf root@172.27.129.112:/etc/keepalived/keepalived.conf
</code></pre>
<h2 id="起-keepalived-服务">起 keepalived 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl restart keepalived&quot;
  done
</code></pre>
<h2 id="检查-keepalived-服务">检查 keepalived 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl status keepalived|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u keepalived
</code></pre>
<p>查看 VIP 所在的节点，确保可以 ping 通 VIP：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;/usr/sbin/ip addr show ${VIP_IF}&quot;
    ssh ${node_ip} &quot;ping -c 1 ${MASTER_VIP}&quot;
  done
</code></pre>
<h2 id="查看-haproxy-状态页面">查看 haproxy 状态页面</h2>
<p>浏览器访问 ${MASTER_VIP}:10080/status 地址，查看 haproxy 状态页面：</p>
<p><img src="https://img.orchome.com/group1/M00/00/02/dr5oXFufDj2AOO3kAALXua6sc-M561.png" alt="screenshot"></p>
]]></content>
  </entry>
  <entry>
    <title>06-0.部署 master 节点</title>
    <url>/2020/01/07/06-0-%E9%83%A8%E7%BD%B2-master-%E8%8A%82%E7%82%B9/</url>
    <content><![CDATA[<h1>06-0.部署 master 节点</h1>
<p>kubernetes master 节点运行如下组件：</p>
<ul>
<li>kube-apiserver</li>
<li>kube-scheduler</li>
<li>kube-controller-manager</li>
</ul>
<p>kube-scheduler 和 kube-controller-manager 可以以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。</p>
<p>对于 kube-apiserver，可以运行多个实例（本文档是 3 实例），但对其它组件需要提供统一的访问地址，该地址需要高可用。本文档使用 keepalived 和 haproxy 实现 kube-apiserver VIP 高可用和负载均衡。</p>
<h2 id="下载最新版本的二进制文件">下载最新版本的二进制文件</h2>
<p>从 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md" target="_blank" rel="noopener"><code>CHANGELOG</code>页面</a> 下载 server tarball 文件。</p>
<pre><code>wget https://dl.k8s.io/v1.10.4/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
cd kubernetes
tar -xzvf  kubernetes-src.tar.gz
</code></pre>
<p>将二进制文件拷贝到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp server/bin/* k8s@${node_ip}:/opt/k8s/bin/
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done</code></pre>
]]></content>
      <tags>
        <tag>master, kube-apiserver, kube-scheduler, kube-controller-manager</tag>
      </tags>
  </entry>
  <entry>
    <title>05.部署 flannel 网络</title>
    <url>/2020/01/07/05-%E9%83%A8%E7%BD%B2-flannel-%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>tags: flanneld</p>
<h1>05.部署 flannel 网络</h1>
<p>kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络。</p>
<p>flaneel 第一次启动时，从 etcd 获取 Pod 网段信息，为本节点分配一个未使用的 <code>/24</code> 段地址，然后创建 <code>flannedl.1</code>（也可能是其它名称，如 flannel1 等） 接口。</p>
<p>flannel 将分配的 Pod 网段信息写入 <code>/run/flannel/docker</code> 文件，docker 后续使用这个文件中的环境变量设置 <code>docker0</code> 网桥。</p>
<h2 id="下载和分发-flanneld-二进制文件">下载和分发 flanneld 二进制文件</h2>
<p>到 <a href="https://github.com/coreos/flannel/releases" target="_blank" rel="noopener">https://github.com/coreos/flannel/releases</a> 页面下载最新版本的发布包：</p>
<pre><code>mkdir flannel
wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz
tar -xzvf flannel-v0.10.0-linux-amd64.tar.gz -C flannel
</code></pre>
<p>分发 flanneld 二进制文件到集群所有节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp  flannel/{flanneld,mk-docker-opts.sh} k8s@${node_ip}:/opt/k8s/bin/
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h2 id="创建-flannel-证书和私钥">创建 flannel 证书和私钥</h2>
<p>flannel 从 etcd 集群存取网段分配信息，而 etcd 集群启用了双向 x509 证书认证，所以需要为 flanneld 生成证书和私钥。</p>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; flanneld-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;flanneld&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld
ls flanneld*pem
</code></pre>
<p>将生成的证书和私钥分发到<strong>所有节点</strong>（master 和 worker）：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/flanneld/cert &amp;&amp; chown -R k8s /etc/flanneld&quot;
    scp flanneld*.pem k8s@${node_ip}:/etc/flanneld/cert
  done
</code></pre>
<h2 id="向-etcd-写入集群-Pod-网段信息">向 etcd 写入集群 Pod 网段信息</h2>
<p>注意：本步骤<strong>只需执行一次</strong>。</p>
<pre><code>source /opt/k8s/bin/environment.sh
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/cert/ca.pem \
  --cert-file=/etc/flanneld/cert/flanneld.pem \
  --key-file=/etc/flanneld/cert/flanneld-key.pem \
  set ${FLANNEL_ETCD_PREFIX}/config '{&quot;Network&quot;:&quot;'${CLUSTER_CIDR}'&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}}'
</code></pre>
<ul>
<li>flanneld <strong>当前版本 (v0.10.0) 不支持 etcd v3</strong>，故使用 etcd v2 API 写入配置 key 和网段数据；</li>
<li>写入的 Pod 网段 ${CLUSTER_CIDR} 必须是 /16 段地址，必须与 kube-controller-manager 的 <code>--cluster-cidr</code> 参数值一致；</li>
</ul>
<h2 id="创建-flanneld-的-systemd-unit-文件">创建 flanneld 的 systemd unit 文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
export IFACE=eth0
cat &gt; flanneld.service &lt;&lt; EOF
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
ExecStart=/opt/k8s/bin/flanneld \\
  -etcd-cafile=/etc/kubernetes/cert/ca.pem \\
  -etcd-certfile=/etc/flanneld/cert/flanneld.pem \\
  -etcd-keyfile=/etc/flanneld/cert/flanneld-key.pem \\
  -etcd-endpoints=${ETCD_ENDPOINTS} \\
  -etcd-prefix=${FLANNEL_ETCD_PREFIX} \\
  -iface=${IFACE}
ExecStartPost=/opt/k8s/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
EOF
</code></pre>
<ul>
<li><code>mk-docker-opts.sh</code> 脚本将分配给 flanneld 的 Pod 子网网段信息写入 <code>/run/flannel/docker</code> 文件，后续 docker 启动时使用这个文件中的环境变量配置 docker0 网桥；</li>
<li>flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 <code>-iface</code> 参数指定通信接口，如上面的 eth0 接口;</li>
<li>flanneld 运行时需要 root 权限；</li>
</ul>
<h2 id="分发-flanneld-systemd-unit-文件到所有节点">分发 flanneld systemd unit 文件到<strong>所有节点</strong></h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp flanneld.service root@${node_ip}:/etc/systemd/system/
  done
</code></pre>
<h2 id="启动-flanneld-服务">启动 flanneld 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld&quot;
  done
</code></pre>
<h2 id="检查启动结果">检查启动结果</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status flanneld|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>$ journalctl -u flanneld
</code></pre>
<h2 id="检查分配给各-flanneld-的-Pod-网段信息">检查分配给各 flanneld 的 Pod 网段信息</h2>
<p>查看集群 Pod 网段(/16)：</p>
<pre><code>source /opt/k8s/bin/environment.sh
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/cert/ca.pem \
  --cert-file=/etc/flanneld/cert/flanneld.pem \
  --key-file=/etc/flanneld/cert/flanneld-key.pem \
  get ${FLANNEL_ETCD_PREFIX}/config
</code></pre>
<p>输出：</p>
<p><code>{&quot;Network&quot;:&quot;172.30.0.0/16&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}}</code></p>
<p>查看已分配的 Pod 子网段列表(/24):</p>
<pre><code>source /opt/k8s/bin/environment.sh
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/cert/ca.pem \
  --cert-file=/etc/flanneld/cert/flanneld.pem \
  --key-file=/etc/flanneld/cert/flanneld-key.pem \
  ls ${FLANNEL_ETCD_PREFIX}/subnets
</code></pre>
<p>输出：</p>
<pre><code>/kubernetes/network/subnets/172.30.81.0-24
/kubernetes/network/subnets/172.30.29.0-24
/kubernetes/network/subnets/172.30.39.0-24
</code></pre>
<p>查看某一 Pod 网段对应的节点 IP 和 flannel 接口地址:</p>
<pre><code>source /opt/k8s/bin/environment.sh
etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/etc/kubernetes/cert/ca.pem \
  --cert-file=/etc/flanneld/cert/flanneld.pem \
  --key-file=/etc/flanneld/cert/flanneld-key.pem \
  get ${FLANNEL_ETCD_PREFIX}/subnets/172.30.81.0-24
</code></pre>
<p>输出：</p>
<p><code>{&quot;PublicIP&quot;:&quot;172.27.129.105&quot;,&quot;BackendType&quot;:&quot;vxlan&quot;,&quot;BackendData&quot;:{&quot;VtepMAC&quot;:&quot;12:21:93:9e:b1:eb&quot;}}</code></p>
<h2 id="验证各节点能通过-Pod-网段互通">验证各节点能通过 Pod 网段互通</h2>
<p>在<strong>各节点上部署</strong> flannel 后，检查是否创建了 flannel 接口(名称可能为 flannel0、flannel.0、flannel.1 等)：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;/usr/sbin/ip addr show flannel.1|grep -w inet&quot;
  done
</code></pre>
<p>输出：</p>
<pre><code>inet 172.30.81.0/32 scope global flannel.1
inet 172.30.29.0/32 scope global flannel.1
inet 172.30.39.0/32 scope global flannel.1
</code></pre>
<p>在各节点上 ping 所有 flannel 接口 IP，确保能通：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.81.0&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.29.0&quot;
    ssh ${node_ip} &quot;ping -c 1 172.30.39.0&quot;
  done</code></pre>
]]></content>
  </entry>
  <entry>
    <title>04.部署 etcd 集群</title>
    <url>/2020/01/07/04-%E9%83%A8%E7%BD%B2-etcd-%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>tags: etcd</p>
<h1>04.部署 etcd 集群</h1>
<p>etcd 是基于 Raft 的分布式 key-value 存储系统，由 CoreOS 开发，常用于服务发现、共享配置以及并发控制（如 leader 选举、分布式锁等）。kubernetes 使用 etcd 存储所有运行数据。</p>
<p>本文档介绍部署一个三节点高可用 etcd 集群的步骤：</p>
<ul>
<li>下载和分发 etcd 二进制文件；</li>
<li>创建 etcd 集群各节点的 x509 证书，用于加密客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的数据流；</li>
<li>创建 etcd 的 systemd unit 文件，配置服务参数；</li>
<li>检查集群工作状态；</li>
</ul>
<p>etcd 集群各节点的名称和 IP 如下：</p>
<ul>
<li>kube-node1：172.27.129.105</li>
<li>kube-node2：172.27.129.111</li>
<li>kube-node3：172.27.129.112</li>
</ul>
<h2 id="下载和分发-etcd-二进制文件">下载和分发 etcd 二进制文件</h2>
<p>到 <a href="https://github.com/coreos/etcd/releases" target="_blank" rel="noopener">https://github.com/coreos/etcd/releases</a> 页面下载最新版本的发布包：</p>
<pre><code>wget https://github.com/coreos/etcd/releases/download/v3.3.7/etcd-v3.3.7-linux-amd64.tar.gz
tar -xvf etcd-v3.3.7-linux-amd64.tar.gz
</code></pre>
<p>分发二进制文件到集群所有节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp etcd-v3.3.7-linux-amd64/etcd* k8s@${node_ip}:/opt/k8s/bin
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h2 id="创建-etcd-证书和私钥">创建 etcd 证书和私钥</h2>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; etcd-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;etcd&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;172.27.129.105&quot;,
    &quot;172.27.129.111&quot;,
    &quot;172.27.129.112&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>hosts 字段指定授权使用该证书的 etcd 节点 IP 或域名列表，这里将 etcd 集群的三个节点 IP 都列在其中；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
    -ca-key=/etc/kubernetes/cert/ca-key.pem \
    -config=/etc/kubernetes/cert/ca-config.json \
    -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
ls etcd*
</code></pre>
<p>分发生成的证书和私钥到各 etcd 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/etcd/cert &amp;&amp; chown -R k8s /etc/etcd/cert&quot;
    scp etcd*.pem k8s@${node_ip}:/etc/etcd/cert/
  done
</code></pre>
<h2 id="创建-etcd-的-systemd-unit-模板文件">创建 etcd 的 systemd unit 模板文件</h2>
<pre><code>source /opt/k8s/bin/environment.sh
cat &gt; etcd.service.template &lt;&lt;EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
User=k8s
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/opt/k8s/bin/etcd \\
  --data-dir=/var/lib/etcd \\
  --name=##NODE_NAME## \\
  --cert-file=/etc/etcd/cert/etcd.pem \\
  --key-file=/etc/etcd/cert/etcd-key.pem \\
  --trusted-ca-file=/etc/kubernetes/cert/ca.pem \\
  --peer-cert-file=/etc/etcd/cert/etcd.pem \\
  --peer-key-file=/etc/etcd/cert/etcd-key.pem \\
  --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --listen-peer-urls=https://##NODE_IP##:2380 \\
  --initial-advertise-peer-urls=https://##NODE_IP##:2380 \\
  --listen-client-urls=https://##NODE_IP##:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls=https://##NODE_IP##:2379 \\
  --initial-cluster-token=etcd-cluster-0 \\
  --initial-cluster=${ETCD_NODES} \\
  --initial-cluster-state=new
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<ul>
<li><code>User</code>：指定以 k8s 账户运行；</li>
<li><code>WorkingDirectory</code>、<code>--data-dir</code>：指定工作目录和数据目录为 <code>/var/lib/etcd</code>，需在启动服务前创建这个目录；</li>
<li><code>--name</code>：指定节点名称，当 <code>--initial-cluster-state</code> 值为 <code>new</code> 时，<code>--name</code> 的参数值必须位于 <code>--initial-cluster</code> 列表中；</li>
<li><code>--cert-file</code>、<code>--key-file</code>：etcd server 与 client 通信时使用的证书和私钥；</li>
<li><code>--trusted-ca-file</code>：签名 client 证书的 CA 证书，用于验证 client 证书；</li>
<li><code>--peer-cert-file</code>、<code>--peer-key-file</code>：etcd 与 peer 通信使用的证书和私钥；</li>
<li><code>--peer-trusted-ca-file</code>：签名 peer 证书的 CA 证书，用于验证 peer 证书；</li>
</ul>
<h2 id="为各节点创建和分发-etcd-systemd-unit-文件">为各节点创建和分发 etcd systemd unit 文件</h2>
<p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; 3; i++ ))
  do
    sed -e &quot;s/##NODE_NAME##/${NODE_NAMES[i]}/&quot; -e &quot;s/##NODE_IP##/${NODE_IPS[i]}/&quot; etcd.service.template &gt; etcd-${NODE_IPS[i]}.service 
  done
ls *.service
</code></pre>
<ul>
<li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li>
</ul>
<p>分发生成的 systemd unit 文件：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /var/lib/etcd &amp;&amp; chown -R k8s /var/lib/etcd&quot; 
    scp etcd-${node_ip}.service root@${node_ip}:/etc/systemd/system/etcd.service
  done
</code></pre>
<ul>
<li>必须先创建 etcd 数据目录和工作目录;</li>
<li>文件重命名为 etcd.service;</li>
</ul>
<p>完整 unit 文件见：<a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/systemd/etcd.service" target="_blank" rel="noopener">etcd.service</a></p>
<h2 id="启动-etcd-服务">启动 etcd 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl restart etcd &amp;&quot;
  done
</code></pre>
<ul>
<li>etcd 进程首次启动时会等待其它节点的 etcd 加入集群，命令 <code>systemctl start etcd</code> 会卡住一段时间，为正常现象。</li>
</ul>
<h2 id="检查启动结果">检查启动结果</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;systemctl status etcd|grep Active&quot;
  done
</code></pre>
<p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>$ journalctl -u etcd
</code></pre>
<h2 id="验证服务状态">验证服务状态</h2>
<p>部署完 etcd 集群后，在任一 etc 节点上执行如下命令：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ETCDCTL_API=3 /opt/k8s/bin/etcdctl \
    --endpoints=https://${node_ip}:2379 \
    --cacert=/etc/kubernetes/cert/ca.pem \
    --cert=/etc/etcd/cert/etcd.pem \
    --key=/etc/etcd/cert/etcd-key.pem endpoint health;
  done
</code></pre>
<p>预期输出：</p>
<pre><code>https://172.27.129.105:2379 is healthy: successfully committed proposal: took = 2.192932ms
https://172.27.129.111:2379 is healthy: successfully committed proposal: took = 3.546896ms
https://172.27.129.112:2379 is healthy: successfully committed proposal: took = 3.013667ms
</code></pre>
<p>输出均为 <code>healthy</code> 时表示集群服务正常。</p>
]]></content>
  </entry>
  <entry>
    <title>03.部署 kubectl 命令行工具</title>
    <url>/2020/01/07/03-%E9%83%A8%E7%BD%B2-kubectl-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<p>tags: kubectl</p>
<h1>03.部署 kubectl 命令行工具</h1>
<p>kubectl 是 kubernetes 集群的命令行管理工具，本文档介绍安装和配置它的步骤。</p>
<p>kubectl 默认从 <code>~/.kube/config</code> 文件读取 kube-apiserver 地址、证书、用户名等信息，如果没有配置，执行 kubectl 命令时可能会出错：</p>
<pre><code>$ kubectl get pods
The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre>
<p>本文档只需要<strong>部署一次</strong>，生成的 kubeconfig 文件与机器无关。</p>
<h2 id="下载和分发-kubectl-二进制文件">下载和分发 kubectl 二进制文件</h2>
<p>下载和解压：</p>
<pre><code>wget https://dl.k8s.io/v1.10.4/kubernetes-client-linux-amd64.tar.gz
tar -xzvf kubernetes-client-linux-amd64.tar.gz
</code></pre>
<p>分发到所有使用 kubectl 的节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kubernetes/client/bin/kubectl k8s@${node_ip}:/opt/k8s/bin/
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h2 id="创建-admin-证书和私钥">创建 admin 证书和私钥</h2>
<p>kubectl 与 apiserver https 安全端口通信，apiserver 对提供的证书进行认证和授权。</p>
<p>kubectl 作为集群的管理工具，需要被授予最高权限。这里创建具有<strong>最高权限</strong>的 admin 证书。</p>
<p>创建证书签名请求：</p>
<pre><code>cat &gt; admin-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>O 为 <code>system:masters</code>，kube-apiserver 收到该证书后将请求的 Group 设置为 system:masters；</li>
<li>预定义的 ClusterRoleBinding <code>cluster-admin</code> 将 Group <code>system:masters</code> 与 Role <code>cluster-admin</code> 绑定，该 Role 授予<strong>所有 API</strong>的权限；</li>
<li>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem \
  -config=/etc/kubernetes/cert/ca-config.json \
  -profile=kubernetes admin-csr.json | cfssljson -bare admin
ls admin*
</code></pre>
<h2 id="创建-kubeconfig-文件">创建 kubeconfig 文件</h2>
<p>kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p>
<pre><code>source /opt/k8s/bin/environment.sh
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/cert/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kubectl.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=kubectl.kubeconfig

# 设置上下文参数
kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin \
  --kubeconfig=kubectl.kubeconfig

# 设置默认上下文
kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig
</code></pre>
<ul>
<li><code>--certificate-authority</code>：验证 kube-apiserver 证书的根证书；</li>
<li><code>--client-certificate</code>、<code>--client-key</code>：刚生成的 <code>admin</code> 证书和私钥，连接 kube-apiserver 时使用；</li>
<li><code>--embed-certs=true</code>：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径)；</li>
</ul>
<h2 id="分发-kubeconfig-文件">分发 kubeconfig 文件</h2>
<p>分发到所有使用 <code>kubectl</code> 命令的节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh k8s@${node_ip} &quot;mkdir -p ~/.kube&quot;
    scp kubectl.kubeconfig k8s@${node_ip}:~/.kube/config
    ssh root@${node_ip} &quot;mkdir -p ~/.kube&quot;
    scp kubectl.kubeconfig root@${node_ip}:~/.kube/config
  done
</code></pre>
<ul>
<li>保存到用户的 <code>~/.kube/config</code> 文件；</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>02.创建 CA 证书和秘钥</title>
    <url>/2020/01/07/02-%E5%88%9B%E5%BB%BA-CA-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%98%E9%92%A5/</url>
    <content><![CDATA[<p>tags: TLS, CA, x509</p>
<h1>02.创建 CA 证书和秘钥</h1>
<p>为确保安全，<code>kubernetes</code> 系统各组件需要使用 <code>x509</code> 证书对通信进行加密和认证。</p>
<p>CA (Certificate Authority) 是自签名的根证书，用来签名后续创建的其它证书。</p>
<p>本文档使用 <code>CloudFlare</code> 的 PKI 工具集 <a href="https://github.com/cloudflare/cfssl" target="_blank" rel="noopener">cfssl</a> 创建所有证书。</p>
<h2 id="安装-cfssl-工具集">安装 cfssl 工具集</h2>
<pre><code>sudo mkdir -p /opt/k8s/cert &amp;&amp; sudo chown -R k8s /opt/k8s &amp;&amp; cd /opt/k8s
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
mv cfssl_linux-amd64 /opt/k8s/bin/cfssl

wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
mv cfssljson_linux-amd64 /opt/k8s/bin/cfssljson

wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
mv cfssl-certinfo_linux-amd64 /opt/k8s/bin/cfssl-certinfo

chmod +x /opt/k8s/bin/*
export PATH=/opt/k8s/bin:$PATH
</code></pre>
<h2 id="创建根证书-CA">创建根证书 (CA)</h2>
<p>CA 证书是集群所有节点共享的，<strong>只需要创建一个 CA 证书</strong>，后续创建的所有证书都由它签名。</p>
<h3 id="创建配置文件">创建配置文件</h3>
<p>CA 配置文件用于配置根证书的使用场景 (profile) 和具体参数 (usage，过期时间、服务端认证、客户端认证、加密等)，后续在签名其它证书时需要指定特定场景。</p>
<pre><code>cat &gt; ca-config.json &lt;&lt;EOF
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;87600h&quot;
      }
    }
  }
}
EOF
</code></pre>
<ul>
<li><code>signing</code>：表示该证书可用于签名其它证书，生成的 <code>ca.pem</code> 证书中 <code>CA=TRUE</code>；</li>
<li><code>server auth</code>：表示 client 可以用该该证书对 server 提供的证书进行验证；</li>
<li><code>client auth</code>：表示 server 可以用该该证书对 client 提供的证书进行验证；</li>
</ul>
<h3 id="创建证书签名请求文件">创建证书签名请求文件</h3>
<pre><code>cat &gt; ca-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre>
<ul>
<li>CN：<code>Common Name</code>，kube-apiserver 从证书中提取该字段作为请求的<strong>用户名 (User Name)</strong>，浏览器使用该字段验证网站是否合法；</li>
<li>O：<code>Organization</code>，kube-apiserver 从证书中提取该字段作为请求用户所属的<strong>组 (Group)</strong>；</li>
<li>kube-apiserver 将提取的 User、Group 作为 <code>RBAC</code> 授权的用户标识；</li>
</ul>
<h3 id="生成-CA-证书和私钥">生成 CA 证书和私钥</h3>
<pre><code>cfssl gencert -initca ca-csr.json | cfssljson -bare ca
ls ca*
</code></pre>
<h2 id="分发证书文件">分发证书文件</h2>
<p>将生成的 CA 证书、秘钥文件、配置文件拷贝到<strong>所有节点</strong>的 <code>/etc/kubernetes/cert</code> 目录下：</p>
<pre><code>source /opt/k8s/bin/environment.sh # 导入 NODE_IPS 环境变量
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/kubernetes/cert &amp;&amp; chown -R k8s /etc/kubernetes&quot;
    scp ca*.pem ca-config.json k8s@${node_ip}:/etc/kubernetes/cert
  done
</code></pre>
<ul>
<li>k8s 账户需要有读写 /etc/kubernetes 目录及其子目录文件的权限；</li>
</ul>
<h2 id="参考">参考</h2>
<ol>
<li>各种 CA 证书类型：<a href="https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>01.系统初始化和全局变量</title>
    <url>/2020/01/07/01.%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F/</url>
    <content><![CDATA[<p>tags: environment</p>
<h1>01.系统初始化和全局变量</h1>
<p>集群机器</p>
<hr>
<ul>
<li>kube-node1：172.27.129.105</li>
<li>kube-node2：172.27.129.111</li>
<li>kube-node3：172.27.129.112</li>
</ul>
<p>本着测试的目的，etcd 集群、kubernetes master 集群、kubernetes node 均使用这三台机器。</p>
<blockquote>
<p>若有安装 Vagrant 与 Virtualbox，这三台机器可以用本着提供的 Vagrantfile 来建置：</p>
</blockquote>
<pre><code>$ cd vagrant
$ vagrant up
</code></pre>
<p>主机名</p>
<hr>
<p>设置永久主机名称，然后重新登录:</p>
<pre><code>$ sudo hostnamectl set-hostname kube-node1 # 将 kube-node1 替换为当前主机名
</code></pre>
<ul>
<li>设置的主机名保存在 <code>/etc/hostname</code> 文件中；</li>
</ul>
<p>修改每台机器的 <code>/etc/hosts</code> 文件，添加主机名和 IP 的对应关系：</p>
<pre><code>$ grep kube-node /etc/hosts
172.27.129.105 kube-node1    kube-node1
172.27.129.111 kube-node2    kube-node2
172.27.129.112 kube-node3    kube-node3
</code></pre>
<h2 id="添加-k8s-和-docker-账户">添加 k8s 和 docker 账户</h2>
<p>在每台机器上添加 k8s 账户，可以无密码 sudo：</p>
<pre><code>$ sudo useradd -m k8s
$ sudo sh -c 'echo 123456 | passwd k8s --stdin' # 为 k8s 账户设置密码
$ sudo visudo
$ sudo grep '%wheel.*NOPASSWD: ALL' /etc/sudoers
%wheel    ALL=(ALL)    NOPASSWD: ALL
$ sudo gpasswd -a k8s wheel
</code></pre>
<p>在每台机器上添加 docker 账户，将 k8s 账户添加到 docker 组中，同时配置 dockerd 参数：</p>
<pre><code>$ sudo useradd -m docker
$ sudo gpasswd -a k8s docker
$ sudo mkdir -p  /etc/docker/
$ cat /etc/docker/daemon.json
{
    &quot;registry-mirrors&quot;: [&quot;https://hub-mirror.c.163.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot;],
    &quot;max-concurrent-downloads&quot;: 20
}
</code></pre>
<h2 id="无密码-ssh-登录其它节点">无密码 ssh 登录其它节点</h2>
<p>如果没有特殊指明，本文档的所有操作<strong>均在 kube-node1 节点上执行</strong>，然后远程分发文件和执行命令。</p>
<p>设置 kube-node1 可以无密码登录<strong>所有节点</strong>的 k8s 和 root 账户：</p>
<pre><code>[k8s@kube-node1 k8s]$ ssh-keygen -t rsa
[k8s@kube-node1 k8s]$ ssh-copy-id root@kube-node1
[k8s@kube-node1 k8s]$ ssh-copy-id root@kube-node2
[k8s@kube-node1 k8s]$ ssh-copy-id root@kube-node3

[k8s@kube-node1 k8s]$ ssh-copy-id k8s@kube-node1
[k8s@kube-node1 k8s]$ ssh-copy-id k8s@kube-node2
[k8s@kube-node1 k8s]$ ssh-copy-id k8s@kube-node3
</code></pre>
<h2 id="将可执行文件路径-opt-k8s-bin-添加到-PATH-变量中">将可执行文件路径 /opt/k8s/bin 添加到 PATH 变量中</h2>
<p>在每台机器上添加环境变量：</p>
<pre><code>$ sudo sh -c &quot;echo 'PATH=/opt/k8s/bin:$PATH:$HOME/bin:$JAVA_HOME/bin' &gt;&gt;/root/.bashrc&quot;
$ echo 'PATH=/opt/k8s/bin:$PATH:$HOME/bin:$JAVA_HOME/bin' &gt;&gt;~/.bashrc
</code></pre>
<h2 id="安装依赖包">安装依赖包</h2>
<p>在每台机器上安装依赖包：</p>
<p>CentOS:</p>
<pre><code>$ sudo yum install -y epel-release
$ sudo yum install -y conntrack ipvsadm ipset jq sysstat curl iptables libseccomp
</code></pre>
<p>Ubuntu:</p>
<pre><code>$ sudo apt-get install -y conntrack ipvsadm ipset jq sysstat curl iptables libseccomp
</code></pre>
<ul>
<li>ipvs 依赖 ipset；</li>
</ul>
<h2 id="关闭防火墙">关闭防火墙</h2>
<p>在每台机器上关闭防火墙：</p>
<pre><code>$ sudo systemctl stop firewalld
$ sudo systemctl disable firewalld
$ sudo iptables -F &amp;&amp; sudo iptables -X &amp;&amp; sudo iptables -F -t nat &amp;&amp; sudo iptables -X -t nat
$ sudo sudo iptables -P FORWARD ACCEPT
</code></pre>
<h2 id="关闭-swap-分区">关闭 swap 分区</h2>
<p>如果开启了 swap 分区，kubelet 会启动失败(可以通过将参数 --fail-swap-on 设置为 false 来忽略 swap on)，故需要在每台机器上关闭 swap 分区：</p>
<pre><code>$ sudo swapoff -a
</code></pre>
<p>为了防止开机自动挂载 swap 分区，可以注释 <code>/etc/fstab</code> 中相应的条目：</p>
<pre><code>$ sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
</code></pre>
<h2 id="关闭-SELinux">关闭 SELinux</h2>
<p>关闭 SELinux，否则后续 K8S 挂载目录时可能报错 <code>Permission denied</code>：</p>
<pre><code>$ sudo setenforce 0
$ grep SELINUX /etc/selinux/config 
SELINUX=disabled
</code></pre>
<ul>
<li>修改配置文件，永久生效；</li>
</ul>
<h2 id="关闭-dnsmasq">关闭 dnsmasq</h2>
<p>linux 系统开启了 dnsmasq 后(如 GUI 环境)，将系统 DNS Server 设置为 127.0.0.1，这会导致 docker 容器无法解析域名，需要关闭它：</p>
<pre><code>$ sudo service dnsmasq stop
$ sudo systemctl disable dnsmasq
</code></pre>
<h2 id="设置系统参数">设置系统参数</h2>
<pre><code>$ cat &gt; kubernetes.conf &lt;&lt;EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
EOF
$ sudo cp kubernetes.conf  /etc/sysctl.d/kubernetes.conf
$ sudo sysctl -p /etc/sysctl.d/kubernetes.conf
$ sudo mount -t cgroup -o cpu,cpuacct none /sys/fs/cgroup/cpu,cpuacct
</code></pre>
<h2 id="加载内核模块">加载内核模块</h2>
<pre><code>$ sudo modprobe br_netfilter
$ sudo modprobe ip_vs
</code></pre>
<h2 id="设置系统时区">设置系统时区</h2>
<pre><code>$ # 调整系统 TimeZone
$ sudo timedatectl set-timezone Asia/Shanghai

$ # 将当前的 UTC 时间写入硬件时钟
$ sudo timedatectl set-local-rtc 0

$ # 重启依赖于系统时间的服务
$ sudo systemctl restart rsyslog 
$ sudo systemctl restart crond
</code></pre>
<h2 id="创建目录">创建目录</h2>
<p>在每台机器上创建目录：</p>
<pre><code>$ sudo mkdir -p /opt/k8s/bin
$ sudo chown -R k8s /opt/k8s

$ sudo sudo mkdir -p /etc/kubernetes/cert
$ sudo chown -R k8s /etc/kubernetes

$ sudo mkdir -p /etc/etcd/cert
$ sudo chown -R k8s /etc/etcd/cert

$ sudo mkdir -p /var/lib/etcd &amp;&amp; chown -R k8s /etc/etcd/cert
</code></pre>
<h2 id="检查系统内核和模块是否适合运行-docker-仅适用于-linux-系统">检查系统内核和模块是否适合运行 docker (仅适用于 linux 系统)</h2>
<pre><code>$ curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh &gt; check-config.sh
$ bash ./check-config.sh
</code></pre>
<h2 id="集群环境变量">集群环境变量</h2>
<p>后续的部署步骤将使用下面定义的全局环境变量，请根据<strong>自己的机器、网络情况</strong>修改：</p>
<pre><code>#!/usr/bin/bash

# 生成 EncryptionConfig 所需的加密 key
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)

# 最好使用 当前未用的网段 来定义服务网段和 Pod 网段

# 服务网段，部署前路由不可达，部署后集群内路由可达(kube-proxy 和 ipvs 保证)
SERVICE_CIDR=&quot;10.254.0.0/16&quot;

# Pod 网段，建议 /16 段地址，部署前路由不可达，部署后集群内路由可达(flanneld 保证)
CLUSTER_CIDR=&quot;172.30.0.0/16&quot;

# 服务端口范围 (NodePort Range)
export NODE_PORT_RANGE=&quot;8400-9000&quot;

# 集群各机器 IP 数组
export NODE_IPS=(172.27.129.105 172.27.129.111 172.27.129.112)

# 集群各 IP 对应的 主机名数组
export NODE_NAMES=(kube-node1 kube-node2 kube-node3)

# kube-apiserver 的 VIP（HA 组件 keepalived 发布的 IP）
export MASTER_VIP=172.27.129.253

# kube-apiserver VIP 地址（HA 组件 haproxy 监听 8443 端口）
export KUBE_APISERVER=&quot;https://${MASTER_VIP}:8443&quot;

# HA 节点，VIP 所在的网络接口名称
export VIP_IF=&quot;eth0&quot;

# etcd 集群服务地址列表
export ETCD_ENDPOINTS=&quot;https://172.27.129.105:2379,https://172.27.129.111:2379,https://172.27.129.112:2379&quot;

# etcd 集群间通信的 IP 和端口
export ETCD_NODES=&quot;kube-node1=https://172.27.129.105:2380,kube-node2=https://172.27.129.111:2380,kube-node3=https://172.27.129.112:2380&quot;

# flanneld 网络配置前缀
export FLANNEL_ETCD_PREFIX=&quot;/kubernetes/network&quot;

# kubernetes 服务 IP (一般是 SERVICE_CIDR 中第一个IP)
export CLUSTER_KUBERNETES_SVC_IP=&quot;10.254.0.1&quot;

# 集群 DNS 服务 IP (从 SERVICE_CIDR 中预分配)
export CLUSTER_DNS_SVC_IP=&quot;10.254.0.2&quot;

# 集群 DNS 域名
export CLUSTER_DNS_DOMAIN=&quot;cluster.local.&quot;

# 将二进制目录 /opt/k8s/bin 加到 PATH 中
export PATH=/opt/k8s/bin:$PATH
</code></pre>
<ul>
<li>打包后的变量定义见 <a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/manifests/environment.sh" target="_blank" rel="noopener">environment.sh</a>，后续部署时会<strong>提示导入</strong>该脚本；</li>
</ul>
<h2 id="分发集群环境变量定义脚本">分发集群环境变量定义脚本</h2>
<p>把全局变量定义脚本拷贝到<strong>所有</strong>节点的 <code>/opt/k8s/bin</code> 目录：</p>
<pre><code>source environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp environment.sh k8s@${node_ip}:/opt/k8s/bin/
    ssh k8s@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre>
<h2 id="参考">参考</h2>
<ol>
<li>系统内核相关参数参考：<a href="https://docs.openshift.com/enterprise/3.2/admin_guide/overcommit.html" target="_blank" rel="noopener">https://docs.openshift.com/enterprise/3.2/admin_guide/overcommit.html</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>00.组件版本和配置策略</title>
    <url>/2020/01/07/00.%E7%BB%84%E4%BB%B6%E7%89%88%E6%9C%AC%E5%92%8C%E9%85%8D%E7%BD%AE%E7%AD%96%E7%95%A5/</url>
    <content><![CDATA[<p>tags: version</p>
<h1>00.组件版本和配置策略</h1>
<h2 id="组件版本">组件版本</h2>
<ul>
<li>Kubernetes 1.10.4</li>
<li>Docker 18.03.1-ce</li>
<li>Etcd 3.3.7</li>
<li>Flanneld 0.10.0</li>
<li>插件：
<ul>
<li>Coredns
<ul>
<li>Dashboard</li>
<li>Heapster (influxdb、grafana)</li>
<li>Metrics-Server</li>
<li>EFK (elasticsearch、fluentd、kibana)</li>
</ul>
</li>
</ul>
</li>
<li>镜像仓库：
<ul>
<li>docker registry
<ul>
<li>harbor</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="主要配置策略">主要配置策略</h2>
<p>kube-apiserver：</p>
<ul>
<li>使用 keepalived 和 haproxy 实现 3 节点高可用；</li>
<li>关闭非安全端口 8080 和匿名访问；</li>
<li>在安全端口 6443 接收 https 请求；</li>
<li>严格的认证和授权策略 (x509、token、RBAC)；</li>
<li>开启 bootstrap token 认证，支持 kubelet TLS bootstrapping；</li>
<li>使用 https 访问 kubelet、etcd，加密通信；</li>
</ul>
<p>kube-controller-manager：</p>
<ul>
<li>3 节点高可用；</li>
<li>关闭非安全端口，在安全端口 10252 接收 https 请求；</li>
<li>使用 kubeconfig 访问 apiserver 的安全端口；</li>
<li>自动 approve kubelet 证书签名请求 (CSR)，证书过期后自动轮转；</li>
<li>各 controller 使用自己的 ServiceAccount 访问 apiserver；</li>
</ul>
<p>kube-scheduler：</p>
<ul>
<li>3 节点高可用；</li>
<li>使用 kubeconfig 访问 apiserver 的安全端口；</li>
</ul>
<p>kubelet：</p>
<ul>
<li>使用 kubeadm 动态创建 bootstrap token，而不是在 apiserver 中静态配置；</li>
<li>使用 TLS bootstrap 机制自动生成 client 和 server 证书，过期后自动轮转；</li>
<li>在 KubeletConfiguration 类型的 JSON 文件配置主要参数；</li>
<li>关闭只读端口，在安全端口 10250 接收 https 请求，对请求进行认证和授权，拒绝匿名访问和非授权访问；</li>
<li>使用 kubeconfig 访问 apiserver 的安全端口；</li>
</ul>
<p>kube-proxy：</p>
<ul>
<li>使用 kubeconfig 访问 apiserver 的安全端口；</li>
<li>在 KubeProxyConfiguration 类型的 JSON 文件配置主要参数；</li>
<li>使用 ipvs 代理模式；</li>
</ul>
<p>集群插件：</p>
<ul>
<li>DNS：使用功能、性能更好的 coredns；</li>
<li>Dashboard：支持登录认证；</li>
<li>Metric：heapster、metrics-server，使用 https 访问 kubelet 安全端口；</li>
<li>Log：Elasticsearch、Fluend、Kibana；</li>
<li>Registry 镜像库：docker-registry、harbor；</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>有道markdown离线测试</title>
    <url>/2020/01/07/%E6%9C%89%E9%81%93markdown%E7%A6%BB%E7%BA%BF%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th>header 1</th>
<th>header 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>row 1 col 1</td>
<td>row 1 col 2</td>
</tr>
<tr>
<td>row 2 col 1</td>
<td>row 2 col 2</td>
</tr>
</tbody>
</table>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">E &#x3D; mc^2</span><br></pre></td></tr></table></figure>
<html>
<!--在这里插入内容-->
</html>
<ul>
<li>
<ol>
<li></li>
</ol>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>solr添加ik分词器</title>
    <url>/2020/01/07/solr%E6%B7%BB%E5%8A%A0ik%E5%88%86%E8%AF%8D%E5%99%A8/</url>
    <content><![CDATA[<h3 id="查找solr服务器web地址">查找solr服务器web地址</h3>
<p>find / -name WEB-INF</p>
<p>[root@bigdata-3 lib]# pwd<br>
/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/solr/server/solr-webapp/webapp/WEB-INF/lib</p>
<p>/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/solr/server/solr-webapp/webapp/WEB-INF/lib</p>
<p>/opt/cloudera/parcels/CDH/lib/solr/server/solr-webapp/webapp/WEB-INF/lib</p>
<h3 id="添加ik-jar包到指定位置-并修改权限">添加ik jar包到指定位置 并修改权限</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-rwxr-xr-x 1 root root 1184820 May  7 10:29 ik-analyzer-7.5.0.jar</span><br><span class="line">[root@bigdata-3 lib]#</span><br></pre></td></tr></table></figure>
<h3 id="WEB-INF-创建classes-我们把IKAnalyzer-cfg-xml、stopword-dic拷贝到需要使用分词器的core的conf下面，">WEB-INF 创建classes  我们把IKAnalyzer.cfg.xml、stopword.dic拷贝到需要使用分词器的core的conf下面，</h3>
<p>将resources目录下的5个配置文件放入solr服务的Jetty或Tomcat的webapp/WEB-INF/classes/目录下；</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">① IKAnalyzer.cfg.xml</span><br><span class="line">② ext.dic</span><br><span class="line">③ stopword.dic</span><br><span class="line">④ ik.conf</span><br><span class="line">⑤ dynamicdic.txt</span><br></pre></td></tr></table></figure>
<h3 id="配置文档：">配置文档：</h3>
<p><a href="https://github.com/magese/ik-analyzer-solr7" target="_blank" rel="noopener">https://github.com/magese/ik-analyzer-solr7</a></p>
<h3 id="后面开发人员执行。。。。">后面开发人员执行。。。。</h3>
<p>和core的schema.xml文件一个目录。<br>
修改core的schema.xml:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;fieldType name&#x3D;&quot;text_ik&quot; class&#x3D;&quot;solr.TextField&quot;&gt;   </span><br><span class="line"></span><br><span class="line">        &lt;analyzer type&#x3D;&quot;index&quot; isMaxWordLength&#x3D;&quot;false&quot; class&#x3D;&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;&#x2F;&gt;  </span><br><span class="line"></span><br><span class="line">        &lt;analyzer type&#x3D;&quot;query&quot; isMaxWordLength&#x3D;&quot;true&quot; class&#x3D;&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;&#x2F;&gt;  </span><br><span class="line"></span><br><span class="line">&lt;&#x2F;fieldType&gt;</span><br></pre></td></tr></table></figure>
<h3 id="配置测试字段：">配置测试字段：</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;field name&#x3D;&quot;quesContent&quot; type&#x3D;&quot;text_ik&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure>
<h3 id="参考">参考</h3>
<p><a href="https://my.oschina.net/u/2293326/blog/515883" target="_blank" rel="noopener">https://my.oschina.net/u/2293326/blog/515883</a></p>
<p><a href="https://github.com/magese/ik-analyzer-solr7" target="_blank" rel="noopener">https://github.com/magese/ik-analyzer-solr7</a></p>
<p><a href="https://blog.csdn.net/u011967615/article/details/69400263" target="_blank" rel="noopener">https://blog.csdn.net/u011967615/article/details/69400263</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp cdh85-42:&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;solr&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;lib&#x2F;ik-analyzer-*.jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;solr&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;lib&#x2F;</span><br><span class="line">mkdir &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;solr&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;classes&#x2F;</span><br><span class="line">scp cdh85-42:&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;solr&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;classes&#x2F;* &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;lib&#x2F;solr&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;classes&#x2F;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>solr  command</title>
    <url>/2020/01/07/solr%20%20command/</url>
    <content><![CDATA[<h3 id="生成本地配置">生成本地配置</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl instancedir --generate $HOME&#x2F;test_collection_config</span><br></pre></td></tr></table></figure>
<h3 id="上传到zk">上传到zk</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl instancedir --create test_collection_config $HOME&#x2F;test_collection_config</span><br></pre></td></tr></table></figure>
<h3 id="创建collection">创建collection</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl collection --create test_collection -s 1 -c test_collection_config</span><br></pre></td></tr></table></figure>
<h3 id="4-post数据">4. post数据</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH&#x2F;share&#x2F;doc&#x2F;solr-doc*&#x2F;example&#x2F;exampledocs</span><br><span class="line">java -Durl&#x3D;http:&#x2F;&#x2F;bigdata-3.baofoo.cn:8983&#x2F;solr&#x2F;test_collection&#x2F;update -jar post.jar *.xml</span><br></pre></td></tr></table></figure>
<h3 id="查看-hdfs-dir">查看 hdfs dir</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -ls -R &#x2F;solr&#x2F;test_co*&#x2F;</span><br><span class="line"> </span><br><span class="line">drwxr-xr-x   - solr solr          0 2019-05-08 22:07 &#x2F;solr&#x2F;test_collection&#x2F;core_node2</span><br><span class="line">drwxr-xr-x   - solr solr          0 2019-05-08 22:07 &#x2F;solr&#x2F;test_collection&#x2F;core_node2&#x2F;data</span><br><span class="line">drwxr-xr-x   - solr solr          0 2019-05-08 22:12 &#x2F;solr&#x2F;test_collection&#x2F;core_node2&#x2F;data&#x2F;index</span><br><span class="line">-rwxr-xr-x   3 solr solr         82 2019-05-08 22:12 &#x2F;solr&#x2F;test_collection&#x2F;core_node2&#x2F;data&#x2F;index&#x2F;_0.dii</span><br></pre></td></tr></table></figure>
<h3 id="查看-zk-dir">查看 zk dir</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-6.2.0-1.cdh6.2.0.p0.967373&#x2F;lib&#x2F;zookeeper&#x2F;bin&#x2F;zkCli.sh -server localhost:2181</span><br></pre></td></tr></table></figure>
<h3 id="配置-config">配置 config</h3>
<h4 id="Manager-Configs">Manager Configs</h4>
<p>The solrctl config command syntax is as follows:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl config [--create &lt;name&gt; &lt;baseConfig&gt; [-p &lt;name&gt;&#x3D;&lt;value&gt;]...]</span><br><span class="line">               [--delete &lt;name&gt;]</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>–create <name> <baseConfig>: Creates a new config based on an existing config. The config is created with the specified <name>, using <baseConfig> as the template. For more information about config templates, see Config Templates.</p>
</li>
<li>
<p>-p <name>=<value>: Overrides a <baseConfig> setting. The only config property that you can override is immutable, so the possible options are -p immutable=true and -p immutable=false. If you are copying an immutable config, such as a template, use -p immutable=false to make sure that you can edit the new config.</p>
</li>
<li>
<p>–delete <name>: Deletes the specified config. You cannot delete an immutable config without accessing ZooKeeper directly as the solr super user.</p>
</li>
<li>
<p>example:</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl config --create newConfig test_collection_config -p immutable&#x3D;false</span><br></pre></td></tr></table></figure>
<h4 id="Managing-Instance-Directories">Managing Instance Directories</h4>
<ul>
<li>An instance directory is a named set of configuration files. You can generate an instance directory template locally, edit the configuration, and then upload the directory to ZooKeeper as a named configuration set. You can then reference this named configuration set when creating a collection.</li>
</ul>
<p>The solrctl instancedir command syntax is as follows:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl instancedir [--generate &lt;path&gt; [-schemaless]]</span><br><span class="line">                    [--create &lt;name&gt; &lt;path&gt;]</span><br><span class="line">                    [--update &lt;name&gt; &lt;path&gt;]</span><br><span class="line">                    [--get &lt;name&gt; &lt;path&gt;]</span><br><span class="line">                    [--delete &lt;name&gt;]</span><br><span class="line">                    [--list]</span><br></pre></td></tr></table></figure>
<ul>
<li>–generate <path>: Generates an instance directory template on the local filesystem at <path>. The configuration files are located in the conf subdirectory under <path>.</li>
<li>-schemaless: Generates a schemaless instance directory template. For more information on schemaless support, see Schemaless Mode Overview and Best Practices.</li>
<li>–create <name> <path>: Uploads a copy of the instance directory from <path> on the local filesystem to ZooKeeper. If an instance directory with the specified <name> already exists, this command fails. Use --update to modify existing instance directories.</li>
<li>–update <name> <path>: Overwrites an existing instance directory in ZooKeeper using the specified files on the local filesystem. This command is analogous to first running --delete <name> followed by --create <name> <path>.</li>
<li>–get <name> <path>: Downloads the specified instance directory from ZooKeeper to the specified path on the local filesystem. You can then edit the configuration and then re-upload it using --update.</li>
<li>–delete <name>: Deletes the specified instance directory from ZooKeeper.</li>
<li>–list: Lists existing instance directories as well as configs created by the solrctl config command.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">solrctl instancedir --get test_collection_config  &#x2F;tmp&#x2F;test_collection_config</span><br><span class="line"></span><br><span class="line"> cd &#x2F;tmp&#x2F;test_collection_config&#x2F;conf&#x2F;</span><br><span class="line">vim managed-schema</span><br><span class="line"></span><br><span class="line">solrctl instancedir --update test_collection_config  &#x2F;tmp&#x2F;test_collection_config</span><br></pre></td></tr></table></figure>
<h3 id="schema-api">schema api</h3>
<ul>
<li>Add a New Field</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST -H &#39;Content-type:application&#x2F;json&#39; --data-binary &#39;&#123;</span><br><span class="line">  &quot;add-field&quot;:&#123;</span><br><span class="line">     &quot;name&quot;:&quot;sell-by&quot;,</span><br><span class="line">     &quot;type&quot;:&quot;pdate&quot;,</span><br><span class="line">     &quot;stored&quot;:true &#125;</span><br><span class="line">&#125;&#39; http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br></pre></td></tr></table></figure>
<ul>
<li>Delete a Field</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST -H &#39;Content-type:application&#x2F;json&#39; --data-binary &#39;&#123;</span><br><span class="line">  &quot;delete-field&quot; : &#123; &quot;name&quot;:&quot;sell-by&quot; &#125;</span><br><span class="line">&#125;&#39; http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br></pre></td></tr></table></figure>
<ul>
<li>Replace a Field</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST -H &#39;Content-type:application&#x2F;json&#39; --data-binary &#39;&#123;</span><br><span class="line">  &quot;replace-field&quot;:&#123;</span><br><span class="line">     &quot;name&quot;:&quot;sell-by&quot;,</span><br><span class="line">     &quot;type&quot;:&quot;date&quot;,</span><br><span class="line">     &quot;stored&quot;:false &#125;</span><br><span class="line">&#125;&#39; http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br></pre></td></tr></table></figure>
<ul>
<li>Add a Dynamic Field Rule</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST -H &#39;Content-type:application&#x2F;json&#39; --data-binary &#39;&#123;</span><br><span class="line">  &quot;add-dynamic-field&quot;:&#123;</span><br><span class="line">     &quot;name&quot;:&quot;*_s&quot;,</span><br><span class="line">     &quot;type&quot;:&quot;string&quot;,</span><br><span class="line">     &quot;stored&quot;:true &#125;</span><br><span class="line">&#125;&#39; http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br></pre></td></tr></table></figure>
<ul>
<li>Multiple Commands in a Single POST</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST -H &#39;Content-type:application&#x2F;json&#39; --data-binary &#39;&#123;</span><br><span class="line">  &quot;add-field-type&quot;:&#123;</span><br><span class="line">     &quot;name&quot;:&quot;myNewTxtField&quot;,</span><br><span class="line">     &quot;class&quot;:&quot;solr.TextField&quot;,</span><br><span class="line">     &quot;positionIncrementGap&quot;:&quot;100&quot;,</span><br><span class="line">     &quot;analyzer&quot;:&#123;</span><br><span class="line">        &quot;charFilters&quot;:[&#123;</span><br><span class="line">           &quot;class&quot;:&quot;solr.PatternReplaceCharFilterFactory&quot;,</span><br><span class="line">           &quot;replacement&quot;:&quot;$1$1&quot;,</span><br><span class="line">           &quot;pattern&quot;:&quot;([a-zA-Z])\\\\1+&quot; &#125;],</span><br><span class="line">        &quot;tokenizer&quot;:&#123;</span><br><span class="line">           &quot;class&quot;:&quot;solr.WhitespaceTokenizerFactory&quot; &#125;,</span><br><span class="line">        &quot;filters&quot;:[&#123;</span><br><span class="line">           &quot;class&quot;:&quot;solr.WordDelimiterFilterFactory&quot;,</span><br><span class="line">           &quot;preserveOriginal&quot;:&quot;0&quot; &#125;]&#125;&#125;,</span><br><span class="line">   &quot;add-field&quot; : &#123;</span><br><span class="line">      &quot;name&quot;:&quot;sell-by&quot;,</span><br><span class="line">      &quot;type&quot;:&quot;myNewTxtField&quot;,</span><br><span class="line">      &quot;stored&quot;:true &#125;</span><br><span class="line">&#125;&#39; http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br></pre></td></tr></table></figure>
<ul>
<li>Get the entire schema in JSON.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;responseHeader&quot;:&#123;</span><br><span class="line">    &quot;status&quot;:0,</span><br><span class="line">    &quot;QTime&quot;:5&#125;,</span><br><span class="line">  &quot;schema&quot;:&#123;</span><br><span class="line">    &quot;name&quot;:&quot;example&quot;,</span><br><span class="line">    &quot;version&quot;:1.5,</span><br><span class="line">    &quot;uniqueKey&quot;:&quot;id&quot;,</span><br><span class="line">    &quot;fieldTypes&quot;:[&#123;</span><br><span class="line">        &quot;name&quot;:&quot;alphaOnlySort&quot;,</span><br><span class="line">        &quot;class&quot;:&quot;solr.TextField&quot;,</span><br><span class="line">        &quot;sortMissingLast&quot;:true,</span><br><span class="line">        &quot;omitNorms&quot;:true,</span><br><span class="line">        &quot;analyzer&quot;:&#123;</span><br><span class="line">          &quot;tokenizer&quot;:&#123;</span><br><span class="line">            &quot;class&quot;:&quot;solr.KeywordTokenizerFactory&quot;&#125;,</span><br><span class="line">          &quot;filters&quot;:[&#123;</span><br><span class="line">              &quot;class&quot;:&quot;solr.LowerCaseFilterFactory&quot;&#125;,</span><br><span class="line">            &#123;</span><br><span class="line">              &quot;class&quot;:&quot;solr.TrimFilterFactory&quot;&#125;,</span><br><span class="line">            &#123;</span><br><span class="line">              &quot;class&quot;:&quot;solr.PatternReplaceFilterFactory&quot;,</span><br><span class="line">              &quot;replace&quot;:&quot;all&quot;,</span><br><span class="line">              &quot;replacement&quot;:&quot;&quot;,</span><br><span class="line">              &quot;pattern&quot;:&quot;([^a-z])&quot;&#125;]&#125;&#125;],</span><br><span class="line">    &quot;fields&quot;:[&#123;</span><br><span class="line">        &quot;name&quot;:&quot;_version_&quot;,</span><br><span class="line">        &quot;type&quot;:&quot;long&quot;,</span><br><span class="line">        &quot;indexed&quot;:true,</span><br><span class="line">        &quot;stored&quot;:true&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;name&quot;:&quot;author&quot;,</span><br><span class="line">        &quot;type&quot;:&quot;text_general&quot;,</span><br><span class="line">        &quot;indexed&quot;:true,</span><br><span class="line">        &quot;stored&quot;:true&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;name&quot;:&quot;cat&quot;,</span><br><span class="line">        &quot;type&quot;:&quot;string&quot;,</span><br><span class="line">        &quot;multiValued&quot;:true,</span><br><span class="line">        &quot;indexed&quot;:true,</span><br><span class="line">        &quot;stored&quot;:true&#125;],</span><br><span class="line">    &quot;copyFields&quot;:[&#123;</span><br><span class="line">        &quot;source&quot;:&quot;author&quot;,</span><br><span class="line">        &quot;dest&quot;:&quot;text&quot;&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;source&quot;:&quot;cat&quot;,</span><br><span class="line">        &quot;dest&quot;:&quot;text&quot;&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;source&quot;:&quot;content&quot;,</span><br><span class="line">        &quot;dest&quot;:&quot;text&quot;&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;source&quot;:&quot;author&quot;,</span><br><span class="line">        &quot;dest&quot;:&quot;author_s&quot;&#125;]&#125;&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema?wt&#x3D;xml</span><br><span class="line"></span><br><span class="line">&lt;response&gt;</span><br><span class="line">&lt;lst name&#x3D;&quot;responseHeader&quot;&gt;</span><br><span class="line">  &lt;int name&#x3D;&quot;status&quot;&gt;0&lt;&#x2F;int&gt;</span><br><span class="line">  &lt;int name&#x3D;&quot;QTime&quot;&gt;5&lt;&#x2F;int&gt;</span><br><span class="line">&lt;&#x2F;lst&gt;</span><br><span class="line">&lt;lst name&#x3D;&quot;schema&quot;&gt;</span><br><span class="line">  &lt;str name&#x3D;&quot;name&quot;&gt;example&lt;&#x2F;str&gt;</span><br><span class="line">  &lt;float name&#x3D;&quot;version&quot;&gt;1.5&lt;&#x2F;float&gt;</span><br><span class="line">  &lt;str name&#x3D;&quot;uniqueKey&quot;&gt;id&lt;&#x2F;str&gt;</span><br><span class="line">  &lt;arr name&#x3D;&quot;fieldTypes&quot;&gt;</span><br><span class="line">    &lt;lst&gt;</span><br><span class="line">      &lt;str name&#x3D;&quot;name&quot;&gt;alphaOnlySort&lt;&#x2F;str&gt;</span><br><span class="line">      &lt;str name&#x3D;&quot;class&quot;&gt;solr.TextField&lt;&#x2F;str&gt;</span><br><span class="line">      &lt;bool name&#x3D;&quot;sortMissingLast&quot;&gt;true&lt;&#x2F;bool&gt;</span><br><span class="line">      &lt;bool name&#x3D;&quot;omitNorms&quot;&gt;true&lt;&#x2F;bool&gt;</span><br><span class="line">      &lt;lst name&#x3D;&quot;analyzer&quot;&gt;</span><br><span class="line">        &lt;lst name&#x3D;&quot;tokenizer&quot;&gt;</span><br><span class="line">          &lt;str name&#x3D;&quot;class&quot;&gt;solr.KeywordTokenizerFactory&lt;&#x2F;str&gt;</span><br><span class="line">        &lt;&#x2F;lst&gt;</span><br><span class="line">        &lt;arr name&#x3D;&quot;filters&quot;&gt;</span><br><span class="line">          &lt;lst&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;class&quot;&gt;solr.LowerCaseFilterFactory&lt;&#x2F;str&gt;</span><br><span class="line">          &lt;&#x2F;lst&gt;</span><br><span class="line">          &lt;lst&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;class&quot;&gt;solr.TrimFilterFactory&lt;&#x2F;str&gt;</span><br><span class="line">          &lt;&#x2F;lst&gt;</span><br><span class="line">          &lt;lst&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;class&quot;&gt;solr.PatternReplaceFilterFactory&lt;&#x2F;str&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;replace&quot;&gt;all&lt;&#x2F;str&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;replacement&quot;&#x2F;&gt;</span><br><span class="line">            &lt;str name&#x3D;&quot;pattern&quot;&gt;([^a-z])&lt;&#x2F;str&gt;</span><br><span class="line">          &lt;&#x2F;lst&gt;</span><br><span class="line">        &lt;&#x2F;arr&gt;</span><br><span class="line">      &lt;&#x2F;lst&gt;</span><br><span class="line">    &lt;&#x2F;lst&gt;</span><br><span class="line">...</span><br><span class="line">    &lt;lst&gt;</span><br><span class="line">      &lt;str name&#x3D;&quot;source&quot;&gt;author&lt;&#x2F;str&gt;</span><br><span class="line">      &lt;str name&#x3D;&quot;dest&quot;&gt;author_s&lt;&#x2F;str&gt;</span><br><span class="line">    &lt;&#x2F;lst&gt;</span><br><span class="line">  &lt;&#x2F;arr&gt;</span><br><span class="line">&lt;&#x2F;lst&gt;</span><br><span class="line">&lt;&#x2F;response&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema?wt&#x3D;schema.xml</span><br><span class="line"></span><br><span class="line">&lt;schema name&#x3D;&quot;example&quot; version&#x3D;&quot;1.5&quot;&gt;</span><br><span class="line">  &lt;uniqueKey&gt;id&lt;&#x2F;uniqueKey&gt;</span><br><span class="line">  &lt;types&gt;</span><br><span class="line">    &lt;fieldType name&#x3D;&quot;alphaOnlySort&quot; class&#x3D;&quot;solr.TextField&quot; sortMissingLast&#x3D;&quot;true&quot; omitNorms&#x3D;&quot;true&quot;&gt;</span><br><span class="line">      &lt;analyzer&gt;</span><br><span class="line">        &lt;tokenizer class&#x3D;&quot;solr.KeywordTokenizerFactory&quot;&#x2F;&gt;</span><br><span class="line">        &lt;filter class&#x3D;&quot;solr.LowerCaseFilterFactory&quot;&#x2F;&gt;</span><br><span class="line">        &lt;filter class&#x3D;&quot;solr.TrimFilterFactory&quot;&#x2F;&gt;</span><br><span class="line">        &lt;filter class&#x3D;&quot;solr.PatternReplaceFilterFactory&quot; replace&#x3D;&quot;all&quot; replacement&#x3D;&quot;&quot; pattern&#x3D;&quot;([^a-z])&quot;&#x2F;&gt;</span><br><span class="line">      &lt;&#x2F;analyzer&gt;</span><br><span class="line">    &lt;&#x2F;fieldType&gt;</span><br><span class="line">...</span><br><span class="line">  &lt;copyField source&#x3D;&quot;url&quot; dest&#x3D;&quot;text&quot;&#x2F;&gt;</span><br><span class="line">  &lt;copyField source&#x3D;&quot;price&quot; dest&#x3D;&quot;price_c&quot;&#x2F;&gt;</span><br><span class="line">  &lt;copyField source&#x3D;&quot;author&quot; dest&#x3D;&quot;author_s&quot;&#x2F;&gt;</span><br><span class="line">&lt;&#x2F;schema&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>Get a list of all fields.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema&#x2F;fields</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;fields&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;indexed&quot;: true,</span><br><span class="line">            &quot;name&quot;: &quot;_version_&quot;,</span><br><span class="line">            &quot;stored&quot;: true,</span><br><span class="line">            &quot;type&quot;: &quot;long&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;indexed&quot;: true,</span><br><span class="line">            &quot;name&quot;: &quot;author&quot;,</span><br><span class="line">            &quot;stored&quot;: true,</span><br><span class="line">            &quot;type&quot;: &quot;text_general&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;indexed&quot;: true,</span><br><span class="line">            &quot;multiValued&quot;: true,</span><br><span class="line">            &quot;name&quot;: &quot;cat&quot;,</span><br><span class="line">            &quot;stored&quot;: true,</span><br><span class="line">            &quot;type&quot;: &quot;string&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">&quot;...&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;responseHeader&quot;: &#123;</span><br><span class="line">        &quot;QTime&quot;: 1,</span><br><span class="line">        &quot;status&quot;: 0</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Get a list of all dynamic field declarations:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema&#x2F;dynamicfields</span><br></pre></td></tr></table></figure>
<ul>
<li>Get a list of all field types.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:8983&#x2F;solr&#x2F;gettingstarted&#x2F;schema&#x2F;fieldtypes</span><br></pre></td></tr></table></figure>
<h3 id="引用参考">引用参考</h3>
<p><a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/search_validate_deploy_solr_rest_api.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/6.0/topics/search_validate_deploy_solr_rest_api.html</a></p>
<p><a href="https://www.cloudera.com/documentation/enterprise/6/6.0/topics/search_configuration.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/6/6.0/topics/search_configuration.html</a></p>
<p><a href="https://lucene.apache.org/solr/guide/7_0/schema-api.html" target="_blank" rel="noopener">https://lucene.apache.org/solr/guide/7_0/schema-api.html</a></p>
<h3 id="管理页面">管理页面</h3>
<p><a href="http://bigdata-3.baofoo.cn:8983/solr/#/~collections" target="_blank" rel="noopener">http://bigdata-3.baofoo.cn:8983/solr/#/~collections</a></p>
<p><a href="http://bigdata-3.baofoo.cn:8889/hue/dashboard/browse/test_collection" target="_blank" rel="noopener">http://bigdata-3.baofoo.cn:8889/hue/dashboard/browse/test_collection</a></p>
]]></content>
  </entry>
  <entry>
    <title>Linux中文编码转换</title>
    <url>/2020/01/07/Linux%E4%B8%AD%E6%96%87%E7%BC%96%E7%A0%81%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[<p>linux shell 配置文件中默认的字符集编码为UTF－8</p>
<p>accii 文件显示中文乱码</p>
<p>用iconv进行转换就可以了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">iconv -f GBK -t UTF-8 1.csv &gt; 3.csv</span><br></pre></td></tr></table></figure>
<p>查了下iconv命令用法如下：</p>
<p>iconv [选项…] [文件…]</p>
<p>有如下选项可用:</p>
<p>输入/输出格式规范：<br>
-f, --from-code=名称 原始文本编码<br>
-t, --to-code=名称 输出编码</p>
<p>信息：<br>
-l, --list 列举所有已知的字符集</p>
<p>输出控制：<br>
-c 从输出中忽略无效的字符<br>
-o, --output=FILE 输出文件<br>
-s, --silent 关闭警告<br>
–verbose 打印进度信息</p>
<p>iconv -f utf-8 -t gb2312 /server_test/reports/software_.txt &gt; /server_test/reports/software_asserts.txt</p>
<p>中文字符集编码有 gb2312 , cp936 ,GBK，GB18030</p>
]]></content>
  </entry>
  <entry>
    <title>JanusGraph Server搭建 hbase+ solr</title>
    <url>/2020/01/07/JanusGraph%20Server%E6%90%AD%E5%BB%BA%20hbase+%20solr/</url>
    <content><![CDATA[<p>JanusGraph Server搭建 hbase+ solr</p>
<p><a href="https://blog.csdn.net/goandozhf/article/details/80105895#2068-1524796329245" target="_blank" rel="noopener">https://blog.csdn.net/goandozhf/article/details/80105895#2068-1524796329245</a></p>
<p>创建janusgraph-hbase-solr-server.properties文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gremlin.graph&#x3D;org.janusgraph.core.JanusGraphFactory</span><br><span class="line">storage.backend&#x3D;hbase</span><br><span class="line">storage.batch-loading&#x3D;true</span><br><span class="line">storage.hbase.table &#x3D; janusgraph-test</span><br><span class="line">storage.hostname&#x3D;172.20.85.111</span><br><span class="line">cache.db-cache &#x3D; true</span><br><span class="line">cache.db-cache-clean-wait &#x3D; 20</span><br><span class="line">cache.db-cache-time &#x3D; 180000</span><br><span class="line">cache.db-cache-size &#x3D; 0.5</span><br><span class="line">ids.block-size&#x3D;100000000</span><br><span class="line">storage.buffer-size&#x3D;102400</span><br><span class="line">storage.hbase.region-count &#x3D; 15</span><br><span class="line">index.search.backend&#x3D;solr</span><br><span class="line">index.search.solr.mode&#x3D;http</span><br><span class="line">index.search.solr.http-urls&#x3D;http:&#x2F;&#x2F;172.20.85.111:8983&#x2F;solr</span><br><span class="line">index.search.hostname&#x3D;172.20.85.111</span><br><span class="line">index.search.index-name&#x3D;janusgraph-test</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>HBase 工具和实用程序</title>
    <url>/2020/01/07/HBase%20%E5%B7%A5%E5%85%B7%E5%92%8C%E5%AE%9E%E7%94%A8%E7%A8%8B%E5%BA%8F/</url>
    <content><![CDATA[<h3 id="hbase-pe">hbase pe</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ bin&#x2F;hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows&#x3D;10 --nomapred increment 10</span><br></pre></td></tr></table></figure>
<h3 id="hbase-ltt">hbase ltt</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.util.LoadTestTool -compression NONE -write 8:8 -num_keys 1048576</span><br><span class="line"></span><br><span class="line">hbase ltt -compression NONE -write 8:8 -num_keys 1048576</span><br></pre></td></tr></table></figure>
<h3 id="hbase-canary">hbase canary</h3>
<p>Canary 工具可以帮助用户“测试”HBase 集群状态</p>
<p><strong>测试每个表的每个区域的每个列族</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase canary</span><br></pre></td></tr></table></figure>
<p><strong>对特定表格的每个区域的每个列族进行 Canary 测试</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase canary test-01 test-02</span><br></pre></td></tr></table></figure>
<h3 id="CompleteBulkLoad">CompleteBulkLoad</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles &lt;hdfs:&#x2F;&#x2F;storefileoutput&gt; &lt;tablename&gt;</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>distcp 文件迁移</title>
    <url>/2020/01/07/distcp%20%E6%96%87%E4%BB%B6%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line">for DB in `cat db_name.txt`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"> hadoop distcp -D mapreduce.job.queuename=bf_yarn_pool.production -D ipc.client.fallback-<span class="keyword">to</span>-simple-auth-allowed=<span class="literal">true</span> -i -overwrite hdfs://<span class="number">192.168</span><span class="number">.81</span><span class="number">.30</span>:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/$DB.db hdfs://<span class="number">172.20</span><span class="number">.85</span><span class="number">.39</span>:<span class="number">8020</span>/<span class="keyword">user</span>/hive/warehouse/$DB.db</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop distcp \</span><br><span class="line">-Dmapred.jobtracker.maxtasks.per.job&#x3D;1800000 \   #任务最大map数（数据分成多map任务）</span><br><span class="line">-Dmapred.job.max.map.running&#x3D;4000 \              #最大map并发</span><br><span class="line">-Ddistcp.bandwidth&#x3D;150000000 \                   #带宽</span><br><span class="line">-Ddfs.replication&#x3D;2 \                            #复制因子，两副本</span><br><span class="line">-Ddistcp.skip.dir&#x3D;$skipPath \                    #过滤的目录（不拷贝的目录）</span><br><span class="line">-Dmapred.map.max.attempts&#x3D;9 \                    #每个task最大尝试次数</span><br><span class="line">-Dmapred.fairscheduler.pool&#x3D;distcp \             #指定任务运行的pool</span><br><span class="line">-pugp \                                          #保留属性（用户，组，权限）</span><br><span class="line">-i \                                             #忽略失败的task</span><br><span class="line">-skipcrccheck \                                  #忽略CRC校验（防止源，目标集群hdfs版本不一致导致任务失败。）</span><br><span class="line">hdfs:&#x2F;&#x2F;clusterA:9000&#x2F;AAA&#x2F;data  \                 #源地址</span><br><span class="line">hdfs:&#x2F;&#x2F;clusterB:9000&#x2F;BBB&#x2F;data                    #目标地址</span><br></pre></td></tr></table></figure>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/xy_app_spark/tables/fi_gw_express_order_idcard1_encrypt/pk_year=2018/pk_month=2018-10 hdfs://172.20.85.39:8020/user/hive/warehouse/credit_mining.db/fi_gw_express_order_idcard1_encrypt/pk_year=2018/pk_month=2018-10</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/xy_app_spark/tables/fi_gw_express_order_idcard1_encrypt/pk_year=2018/pk_month=2018-11 hdfs://172.20.85.39:8020/user/hive/warehouse/credit_mining.db/fi_gw_express_order_idcard1_encrypt/pk_year=2018/pk_month=2018-11</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/xy_app_spark/tables/fo_payment_encrypt/pk_year=2018/pk_month=2018-10 hdfs://172.20.85.39:8020/user/hive/warehouse/credit_mining.db/fo_payment_encrypt/pk_year=2018/pk_month=2018-10</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/xy_app_spark/tables/fo_payment_encrypt/pk_year=2018/pk_month=2018-11 hdfs://172.20.85.39:8020/user/hive/warehouse/credit_mining.db/fo_payment_encrypt/pk_year=2018/pk_month=2018-11</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020//user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time_v2_encrypt hdfs://172.20.85.39:8020/user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time_v2_encrypt</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i hdfs://192.168.81.30:8020/user/hive/warehouse/xy_ods_db.db/credit_logprocessor_rocord hdfs://172.20.85.39:8020/user/hive/warehouse/xy_ods_db.db/credit_logprocessor_rocord</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/hive/warehouse/xy_ods_db.db/credit_logprocessor_rocord/pk_day=2018-11-11 hdfs://172.20.85.39:8020/user/hive/warehouse/xy_ods_db.db/credit_logprocessor_rocord/pk_day=2018-11-11</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/hive/warehouse/xy_ods.db/ods_verification_cardno_d_incr/pk_year=2017 hdfs://172.20.85.39:8020/user/hive/warehouse/xy_ods.db/ods_verification_cardno_d_incr/pk_year=2017</p>
<p>hadoop distcp -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -update -skipcrccheck hdfs://192.168.81.30:8020/user/hive/warehouse/xy_ods.db/ods_verification_cardno_d_incr/pk_year=2017/pk_month=2017-07/pk_day=2017-07-23/000011_0 hdfs://172.20.85.39:8020/user/hive/warehouse/xy_ods.db/ods_verification_cardno_d_incr/pk_year=2017/pk_month=2017-07/pk_day=2017-07-23/000011_0</p>
<p>sudo -u xy_app_spark hadoop distcp -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -update -skipcrccheck hdfs://172.20.85.29:8020/user/xy_app_spark/bulkload/NORMAL/xy_app_spark-image_current_report hdfs://172.20.85.59:8020/user/xy_app_spark/bulkload/NORMAL/xy_app_spark-image_current_report</p>
<p>sudo -u hdfs hdfs ec -getPolicy -path /user</p>
<p>sudo -u xy_app_spark hadoop distcp -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -update  hdfs://172.20.85.29:8020/user/xy_app_spark/bulkload/NORMAL/xy_app_spark-image_current_report hdfs://172.20.85.59:8020/user/xy_app_spark/bulkload/NORMAL/xy_app_spark-image_current_report</p>
<p>hadoop fs -get /user/hive/warehouse/baofoo_cutpayment.db/protocol_payment_order /home/yarn/protocol_payment_order<br>
hadoop fs -get /user/hive/warehouse/baofoo_cutpayment.db/protocol_payment_business_order /home/yarn/protocol_payment_business_order</p>
<p>baofoo_cutpayment.protocol_payment_order<br>
baofoo_cutpayment.protocol_payment_business_order</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true  hdfs://192.168.81.30:8020/user/hue/oozie/workspaces/ hdfs://172.20.85.39:8020/user/hue/oozie/workspaces/</p>
<p>Bad status for request TOpenSessionReq(username=‘hue’, password=None, client_protocol=6, configuration={‘idle_session_timeout’: ‘900’, ‘impala.doas.user’: u’hue’}): TOpenSessionResp(status=TStatus(errorCode=None, errorMessage=“User ‘yarn’ is not authorized to delegate to ‘hue’.\n”, sqlState=‘HY000’, infoMessages=None, statusCode=3), sessionHandle=TSessionHandle(sessionId=THandleIdentifier(secret=’\xdcs{\xf9A\x12N\xb1\x97\x18\xf4\xbb\xc8\x90#\xa7’, guid=’\x12\x13\x8b\xcd\xa1\xdfA\x07\x9c\xf9\x16i\x97\rU9’)), configuration=None, serverProtocolVersion=5)</p>
<p>/user/xy_app_spark/tables/fo_payment_encrypt<br>
/user/xy_app_spark/tables/t_serve_business_order_real_time_encrypt<br>
/user/xy_app_spark/tables/fi_gw_agrt_express_order_encrypt<br>
/user/xy_app_spark/tables/fi_gw_express_order_idcard1_encrypt</p>
<p>ifactive=<code>sudo -u hdfs hdfs haadmin -getServiceState namenode402</code><br>
echo $ifactive<br>
if [[ $ifactive =~ “active” ]]; then<br>
  nameservice=172.20.85.29<br>
else<br>
nameservice=172.20.85.39<br>
fi<br>
echo $nameservice</p>
<p>hdfs://ns1/user/hive/warehouse/credit_dfp.db/t_device_access_d_incr</p>
<p>hdfs://ns1/user/hive/warehouse/credit_dfp.db/t_device_query_d_incr</p>
<p>hdfs://ns1/user/hive/warehouse/credit_dfp.db/t_device_access_d_incr</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time/pk_year=2019 hdfs://172.20.85.29:8020/user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time/pk_year=2019</p>
<p>旧集群/user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time/pk_year=2018/pk_month=2018-12<br>
到新集群 旧集群/user/hive/warehouse/xy_ods.db/t_serve_business_order_real_time/pk_year=2019/pk_month=2018-12</p>
<p>hadoop distcp /user/hive/warehouse/sample_08 /user/hive/warehouse/t3</p>
<p>/etc/init.d/mysql</p>
<p>hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/hive/warehouse/xy_app_spark.db/snapshot/current/nono/washer_all/step4 hdfs://172.20.85.29:8020/user/hive/warehouse/xy_app_spark.db/snapshot/current/nono/washer_all/step4</p>
<p>#!/bin/bash<br>
for tb in <code>cat tb_name.txt</code><br>
do<br>
hadoop distcp  -D mapreduce.job.queuename=xy_yarn_pool.development -D ipc.client.fallback-to-simple-auth-allowed=true -i -overwrite hdfs://192.168.81.30:8020/user/hive/warehouse/xy_wulichuang.db/$tb hdfs://172.20.85.29:8020/user/hive/warehouse/xy_wulichuang.db/$tb<br>
done</p>
]]></content>
  </entry>
  <entry>
    <title>cdh85-42 内存分析 cache 占用过高 434G 改后 变为 384G</title>
    <url>/2020/01/07/cdh85-42%20%E5%86%85%E5%AD%98%E5%88%86%E6%9E%90%20cache%20%E5%8D%A0%E7%94%A8%E8%BF%87%E9%AB%98%20434G%20%E6%94%B9%E5%90%8E%20%E5%8F%98%E4%B8%BA%20384G/</url>
    <content><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">[root@cdh85-42 ~]# free -h</span><br><span class="line">              total        used        free      shared  buff&#x2F;cache   available</span><br><span class="line">Mem:           502G         34G        1.5G        4.2G        467G        461G</span><br><span class="line">Swap:          4.0G          0B        4.0G</span><br><span class="line">[root@cdh85-42 ~]# </span><br><span class="line">[root@cdh85-42 ~]# </span><br><span class="line">[root@cdh85-42 ~]# cat &#x2F;proc&#x2F;meminfo </span><br><span class="line">MemTotal:       527318720 kB</span><br><span class="line">MemFree:         1513916 kB</span><br><span class="line">MemAvailable:   483579148 kB</span><br><span class="line">Buffers:              28 kB</span><br><span class="line">Cached:         472560904 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">Active:         249755248 kB</span><br><span class="line">Inactive:       254111684 kB</span><br><span class="line">Active(anon):   25622464 kB</span><br><span class="line">Inactive(anon): 10038512 kB</span><br><span class="line">Active(file):   224132784 kB</span><br><span class="line">Inactive(file): 244073172 kB</span><br><span class="line">Unevictable:           0 kB</span><br><span class="line">Mlocked:               0 kB</span><br><span class="line">SwapTotal:       4194300 kB</span><br><span class="line">SwapFree:        4194300 kB</span><br><span class="line">Dirty:              2216 kB</span><br><span class="line">Writeback:            64 kB</span><br><span class="line">AnonPages:      31309320 kB</span><br><span class="line">Mapped:           113004 kB</span><br><span class="line">Shmem:           4352956 kB</span><br><span class="line">Slab:           17466532 kB</span><br><span class="line">SReclaimable:   15190848 kB</span><br><span class="line">SUnreclaim:      2275684 kB</span><br><span class="line">KernelStack:       38048 kB</span><br><span class="line">PageTables:       574580 kB</span><br><span class="line">NFS_Unstable:          0 kB</span><br><span class="line">Bounce:                0 kB</span><br><span class="line">WritebackTmp:          0 kB</span><br><span class="line">CommitLimit:    267853660 kB</span><br><span class="line">Committed_AS:   313602836 kB</span><br><span class="line">VmallocTotal:   34359738367 kB</span><br><span class="line">VmallocUsed:     1170752 kB</span><br><span class="line">VmallocChunk:   34089979900 kB</span><br><span class="line">HardwareCorrupted:     0 kB</span><br><span class="line">AnonHugePages:      6144 kB</span><br><span class="line">CmaTotal:              0 kB</span><br><span class="line">CmaFree:               0 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">DirectMap4k:      372288 kB</span><br><span class="line">DirectMap2M:    32319488 kB</span><br><span class="line">DirectMap1G:    505413632 kB</span><br><span class="line">[root@cdh85-42 ~]#</span><br></pre></td></tr></table></figure>
<p><a href="https://fivezh.github.io/2017/06/18/centos-7-memory-available/" target="_blank" rel="noopener">https://fivezh.github.io/2017/06/18/centos-7-memory-available/</a></p>
<p><a href="https://blog.csdn.net/starshine/article/details/7434942" target="_blank" rel="noopener">https://blog.csdn.net/starshine/article/details/7434942</a></p>
<h3 id="centos-6-7-linux-初始化脚本">centos 6 7 linux 初始化脚本</h3>
<p><a href="https://blog.51cto.com/12445535/2362407" target="_blank" rel="noopener">https://blog.51cto.com/12445535/2362407</a></p>
<h4 id="min-free-kbytes-调整-为-50G">min_free_kbytes 调整 为 50G</h4>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes</span><br><span class="line">cat &#x2F;etc&#x2F;sysctl.conf # add vm.min_free_kbytes &#x3D; 52428800</span><br><span class="line">sysctl -p</span><br><span class="line">cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@cdh85-42 ~]# vim &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">## add</span><br><span class="line">vm.min_free_kbytes &#x3D; 52428800</span><br><span class="line">## add</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@cdh85-42 ~]# cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes</span><br><span class="line">90112</span><br><span class="line"></span><br><span class="line">[root@cdh85-42 ~]# sysctl -p</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_tw_reuse &#x3D; 1</span><br><span class="line">net.ipv4.tcp_tw_recycle &#x3D; 1</span><br><span class="line">net.ipv4.tcp_keepalive_time &#x3D; 1200</span><br><span class="line">net.ipv4.ip_local_port_range &#x3D; 10000 65000</span><br><span class="line">net.ipv4.tcp_max_syn_backlog &#x3D; 8192</span><br><span class="line">net.ipv4.tcp_max_tw_buckets &#x3D; 5000</span><br><span class="line">fs.file-max &#x3D; 655350</span><br><span class="line">net.ipv4.route.gc_timeout &#x3D; 100</span><br><span class="line">net.ipv4.tcp_syn_retries &#x3D; 1</span><br><span class="line">net.ipv4.tcp_synack_retries &#x3D; 1</span><br><span class="line">net.core.netdev_max_backlog &#x3D; 16384</span><br><span class="line">net.ipv4.tcp_max_orphans &#x3D; 16384</span><br><span class="line">net.ipv4.tcp_fin_timeout &#x3D; 2</span><br><span class="line">net.core.somaxconn &#x3D; 32768</span><br><span class="line">kernel.threads-max &#x3D; 196605</span><br><span class="line">kernel.pid_max &#x3D; 196605</span><br><span class="line">vm.max_map_count &#x3D; 393210</span><br><span class="line">vm.swappiness &#x3D; 0</span><br><span class="line">vm.min_free_kbytes &#x3D; 52428800</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@cdh85-42 ~]# </span><br><span class="line">[root@cdh85-42 ~]# </span><br><span class="line">[root@cdh85-42 ~]# free -m</span><br><span class="line">              total        used        free      shared  buff&#x2F;cache   available</span><br><span class="line">Mem:         514959       34901       82648        4258      397410      324618</span><br><span class="line">Swap:          4095           0        4095</span><br><span class="line">[root@cdh85-42 ~]# cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;min_free_kbytes</span><br><span class="line">52428800</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>anaconda3-5.2.0+python3.6.5+tensorflow1.11.0</title>
    <url>/2020/01/07/anaconda3-5.2.0+python3.6.5+tensorflow1.11.0/</url>
    <content><![CDATA[<h3 id="anaconda3-5-2-0">anaconda3-5.2.0</h3>
<p>官网<br>
<a href="https://www.anaconda.com/" target="_blank" rel="noopener">https://www.anaconda.com/</a></p>
<p>历史版本下载地址</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;repo.continuum.io&#x2F;archive&#x2F;</span><br><span class="line">https:&#x2F;&#x2F;repo.anaconda.com&#x2F;archive&#x2F;</span><br><span class="line">https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;archive&#x2F;?C&#x3D;N&amp;O&#x3D;D</span><br></pre></td></tr></table></figure>
<h3 id="win10安装">win10安装</h3>
<p><a href="https://blog.51cto.com/acevi/2103437" target="_blank" rel="noopener">https://blog.51cto.com/acevi/2103437</a></p>
<h3 id="linux-7-安装-anaconda-3-5-2-0-tensorflow1-11-0">linux 7 安装 anaconda 3-5.2.0  tensorflow1.11.0</h3>
<p><a href="https://blog.51cto.com/moerjinrong/2155178" target="_blank" rel="noopener">https://blog.51cto.com/moerjinrong/2155178</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">安装</span><br><span class="line">chmod +x  Anaconda3-5.2.0-Linux-x86_64.sh</span><br><span class="line">.&#x2F;Anaconda3-5.2.0-Linux-x86_64.sh</span><br><span class="line">安装过程中会需要不断回车来阅读并同意license。安装路径默认为用户目录(可以自己指定)，最后需要确认将路径加入用户的.bashrc中。</span><br><span class="line">In order to continue the installation process, please review the license</span><br><span class="line">agreement.</span><br><span class="line">Please, press ENTER to continue       </span><br><span class="line">&gt;&gt;&gt;  # 要继续安装过程，请查看许可证协议。请按ENTER继续</span><br><span class="line"></span><br><span class="line">然后按空格阅读许可协议，</span><br><span class="line"></span><br><span class="line">Do you accept the license terms? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes      # 是否接受协议，选yes</span><br><span class="line"></span><br><span class="line">Anaconda3 will now be installed into this location:</span><br><span class="line">&#x2F;root&#x2F;anaconda3</span><br><span class="line"></span><br><span class="line">  - Press ENTER to confirm the location</span><br><span class="line">  - Press CTRL-C to abort the installation</span><br><span class="line">  - Or specify a different location below</span><br><span class="line"></span><br><span class="line">[&#x2F;root&#x2F;anaconda3] &gt;&gt;&gt;  # 是否安装到当前家目录的anaconda3目录中，默认回车即可</span><br><span class="line"></span><br><span class="line">Do you wish the installer to prepend the Anaconda3 install location</span><br><span class="line">to PATH in your &#x2F;root&#x2F;.bashrc ? [yes|no]</span><br><span class="line">[no] &gt;&gt;&gt; yes            # 是否添加环境变量到&#x2F;root&#x2F;.bashrc文件</span><br><span class="line">重新加载环境变量，执行：</span><br><span class="line"></span><br><span class="line">source ~&#x2F;.bashrc</span><br><span class="line">python -V</span><br><span class="line">pip list</span><br><span class="line">conda list</span><br></pre></td></tr></table></figure>
<p>silent install</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[xy_zhangpeng@bigdata-2 ~]$ bash Anaconda3-5.2.0-Linux-x86_64.sh -b -p $HOME&#x2F;anaconda3 -f</span><br></pre></td></tr></table></figure>
<p>To run the silent installation of Miniconda for macOS or Linux, specify the -b and -p arguments of the bash installer. The following arguments are supported:</p>
<ul>
<li>-b—Batch mode with no PATH modifications to ~/.bashrc. Assumes that you agree to the license agreement. Does not edit the .bashrc or .bash_profile files.</li>
<li>-p—Installation prefix/path.</li>
<li>-f—Force installation even if prefix -p already exists.</li>
</ul>
<h3 id="tensorflow-在-anaconda。。">tensorflow 在 anaconda。。</h3>
<p><a href="https://www.anaconda.com/tensorflow-in-anaconda/" target="_blank" rel="noopener">https://www.anaconda.com/tensorflow-in-anaconda/</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">安装Tensorflow</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如果GPU是NVIDIA的，就可以安装GPU版本的TensorFlow；如果不是，安装CPU版本的就好了。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1.因为要下载Tensorflow，所以我先在Anaconda的配置文件中添加清华镜像库，这样下载和更新的速度会快很多，命令：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line">#查看</span><br><span class="line">conda config --show</span><br><span class="line"></span><br><span class="line">#CPU版本</span><br><span class="line">pip install --upgrade tensorflow</span><br><span class="line"></span><br><span class="line">#GPU版本</span><br><span class="line">pip install --upgrade tensorflow-gpu</span><br><span class="line"></span><br><span class="line">#指定版本</span><br><span class="line">pip install tensorflow&#x3D;&#x3D;1.11.0</span><br></pre></td></tr></table></figure>
<h3 id="分布式TF">分布式TF</h3>
<p><a href="https://www.jianshu.com/p/fdb93e44a8cc" target="_blank" rel="noopener">https://www.jianshu.com/p/fdb93e44a8cc</a></p>
<h3 id="例子">例子</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 引入 tensorflow 模块</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># 创建一个整型常量，即 0 阶 Tensor</span><br><span class="line">t0 &#x3D; tf.constant(3, dtype&#x3D;tf.int32)</span><br><span class="line"></span><br><span class="line"># 创建一个浮点数的一维数组，即 1 阶 Tensor</span><br><span class="line">t1 &#x3D; tf.constant([3., 4.1, 5.2], dtype&#x3D;tf.float32)</span><br><span class="line"></span><br><span class="line"># 创建一个字符串的2x2数组，即 2 阶 Tensor</span><br><span class="line">t2 &#x3D; tf.constant([[&#39;Apple&#39;, &#39;Orange&#39;], [&#39;Potato&#39;, &#39;Tomato&#39;]], dtype&#x3D;tf.string)</span><br><span class="line"></span><br><span class="line"># 创建一个 2x3x1 数组，即 3 阶张量，数据类型默认为整型</span><br><span class="line">t3 &#x3D; tf.constant([[[5], [6], [7]], [[4], [3], [2]]])</span><br><span class="line"></span><br><span class="line"># 打印上面创建的几个 Tensor</span><br><span class="line">print(t0)</span><br><span class="line">print(t1)</span><br><span class="line">print(t2)</span><br><span class="line">print(t3)</span><br><span class="line">--------------------- </span><br><span class="line">作者：戈云飞 </span><br><span class="line">来源：CSDN </span><br><span class="line">原文：https:&#x2F;&#x2F;blog.csdn.net&#x2F;geyunfei_&#x2F;article&#x2F;details&#x2F;78782804 </span><br><span class="line">版权声明：本文为博主原创文章，转载请附上博文链接！</span><br></pre></td></tr></table></figure>
<h3 id="参考">参考</h3>
<p><a href="https://cloud.tencent.com/developer/article/1078485" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1078485</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1078028" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1078028</a></p>
<h3 id="parcel包安装">parcel包安装</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;repo.anaconda.com&#x2F;pkgs&#x2F;misc&#x2F;parcels&#x2F;</span><br><span class="line">替换为</span><br><span class="line">https:&#x2F;&#x2F;repo.continuum.io&#x2F;pkgs&#x2F;misc&#x2F;parcels&#x2F;</span><br></pre></td></tr></table></figure>
<h4 id="离线下载tensorlfow。-Keras">离线下载tensorlfow。 Keras</h4>
<p>方法：在有网络的机器下载好。zip到离线服务器。</p>
<ul>
<li>download</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">pip download msgpack-python&#x3D;&#x3D;0.5.6  -d pip_package</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mkdir pip_package</span><br><span class="line"> pip download  tensorflow&#x3D;&#x3D;1.11.0 -d pip_package</span><br><span class="line"> zip -r pip_package.zip pip_package</span><br></pre></td></tr></table></figure>
<ul>
<li>install</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>…</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F;tensorflow&#x2F;</span><br><span class="line"></span><br><span class="line">http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;packages&#x2F;2c&#x2F;0c&#x2F;74410a32bf753b280b28b685dc6620c65ccc3a09494398d47198af9f2bbb&#x2F;tensorflow-1.11.0rc2-cp36-cp36m-manylinux1_x86_64.whl#sha256&#x3D;b137211744ccbfec6fd5a5f62a47ce1a467fd760be8169a38c7a88121e8f6341</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;packages&#x2F;5e&#x2F;10&#x2F;aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab&#x2F;Keras-2.2.4-py2.py3-none-any.whl#sha256&#x3D;794d0c92c6c4122f1f0fcf3a7bc2f49054c6a54ddbef8d8ffafca62795d760b6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pip install xxx.whl</span><br></pre></td></tr></table></figure>
<h3 id="anaconda-cdh-parcels">anaconda  cdh  parcels</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.cloudera.com&#x2F;downloads&#x2F;partner&#x2F;anaconda.html</span><br><span class="line">https:&#x2F;&#x2F;docs.anaconda.com&#x2F;anaconda-scale&#x2F;cloudera-cdh&#x2F;</span><br><span class="line">http:&#x2F;&#x2F;docs.anaconda.com&#x2F;anaconda-repository&#x2F;user-guide&#x2F;tasks&#x2F;work-with-cloudera-parcels&#x2F;</span><br><span class="line">https:&#x2F;&#x2F;www.anaconda.com&#x2F;how-to-generate-custom-anaconda-parcels-for-cloudera-cdh-with-anaconda-enterprise-5&#x2F;</span><br></pre></td></tr></table></figure>
<h3 id="histroy">histroy</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1063  bash Anaconda3-5.2.0-Linux-x86_64.sh  -b -p &#x2F;opt&#x2F;anaconda3 -f</span><br><span class="line"> 1064  ll &#x2F;opt&#x2F;</span><br><span class="line"> 1065  python -V</span><br><span class="line"> 1066  ll</span><br><span class="line"> 1067  cd &#x2F;opt&#x2F;</span><br><span class="line"> 1068  ll</span><br><span class="line"> 1069  cd anaconda3&#x2F;</span><br><span class="line"> 1070  ll</span><br><span class="line"> 1071  .&#x2F;bin&#x2F;python -V</span><br><span class="line"> 1072  .&#x2F;bin&#x2F;conda -V</span><br><span class="line"> 1074  .&#x2F;bin&#x2F;conda config -h</span><br><span class="line"> 1075  .&#x2F;bin&#x2F;conda config --show</span><br><span class="line"> 1077  .&#x2F;bin&#x2F;conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line"> 1078  .&#x2F;bin&#x2F;conda config --show</span><br><span class="line"> 1080  .&#x2F;bin&#x2F;conda config --set show_channel_urls yes</span><br><span class="line"> 1081  .&#x2F;bin&#x2F;conda config --show</span><br><span class="line"> 1082  .&#x2F;pip -V</span><br><span class="line"> 1083  .&#x2F;bin&#x2F;pip -V</span><br><span class="line"> 1084  .&#x2F;bin&#x2F;pip install tensorflow&#x3D;&#x3D;1.11.0</span><br><span class="line"> 1085  .&#x2F;bin&#x2F;pip list all</span><br></pre></td></tr></table></figure>
<h3 id="Tensorflow报错-tf-estimator-package-not-installed">Tensorflow报错 tf.estimator package not installed</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、tf.estimator package 未安装</span><br><span class="line">tf.estimator package not installed</span><br><span class="line"></span><br><span class="line">我使用的环境是：</span><br><span class="line">anaconda 5.2.0， Python 3.6和TensorFlow 1.12</span><br><span class="line">然后在网上找了一下解决方案之后，发现需要更新一下numpy、pandas和matplotlib等package:</span><br><span class="line">pip install -U pandas</span><br><span class="line">pip install -U matplotlib</span><br><span class="line"></span><br><span class="line">将pandas的版本更新到了0.23.4，matplotlib更新到3.0.2就好了，然后import TensorFlow的时候就不会报错了</span><br><span class="line"></span><br><span class="line">2、import tensorflow的提醒</span><br><span class="line">FutureWarning: Conversion of the second argument of issubdtype from &#96;float&#96; to &#96;np.floating&#96; is deprecated. In future, it will be treated as &#96;np.float64 &#x3D;&#x3D; np.dtype(float).type&#96;.</span><br><span class="line">  from ._conv import register_converters as _register_converters</span><br><span class="line"></span><br><span class="line">这个问题还是环境问题，h5py出了问题，对h5py进行一个升级了就，后面再import tensorflow的时候，就不会出错了。</span><br><span class="line">pip install h5py&#x3D;&#x3D;2.8.0rc1</span><br><span class="line"></span><br><span class="line">3、</span><br><span class="line">TypeError: __init__() got an unexpected keyword argument &#39;serialized_options&#39;</span><br><span class="line"></span><br><span class="line">很有可能是，终端上的 protoc 版本 与python库内的protobuf版本不一样。</span><br><span class="line">pip install -U protobuf</span><br><span class="line"></span><br><span class="line">4、将Python程序不挂断的跑到服务器上面，</span><br><span class="line">注意CUDA_VISIBLE_DEVICES&#x3D;0需要放在nohup的前面。</span><br><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0 nohup python -u main.py</span><br><span class="line"></span><br><span class="line">作者：奔向算法的喵</span><br><span class="line">链接：https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;2ea51363b080</span><br><span class="line">来源：简书</span><br><span class="line">简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>中文测试</title>
    <url>/2019/12/23/%E4%B8%AD%E6%96%87%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<p>中文测试</p>
<p>Hexo Admin 测试</p>
<p>免密登陆 测试</p>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/12/20/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start">Quick Start</h2>
<h3 id="Create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
